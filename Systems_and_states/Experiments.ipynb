{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62552a38-98a1-489d-96b6-adc96bec0748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processes completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import multiprocessing\n",
    "import os\n",
    "import zipfile\n",
    "from minepy import MINE\n",
    "\n",
    "# Define directories\n",
    "base_dir = '/home/vincent/MySSD/JupyterProjects/AAA_projects/UnlimitedResearchCooperative/Synthetic_Intelligence_Labs/EEG_Chaos_Kuramoto_Neural_Net'\n",
    "embedding_2d_dir = os.path.join(base_dir, '2dembedding_data')\n",
    "embedding_3d_dir = os.path.join(base_dir, '3dembedding_data')\n",
    "plots_directory = os.path.join(base_dir, 'plots')\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir_path in [embedding_2d_dir, embedding_3d_dir, plots_directory]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Load EEG data\n",
    "EEG_data = np.load(os.path.join(base_dir, 'eeg_data_with_channels.npy'), allow_pickle=True)\n",
    "\n",
    "# EEG channel names\n",
    "eeg_channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                     'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                     'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "sampling_rate = 1000  # Hz\n",
    "start_time, end_time = 805.571, 921.515\n",
    "start_index, end_index = int(start_time * sampling_rate), int(end_time * sampling_rate)\n",
    "filtered_EEG_data = EEG_data[start_index:end_index, :]\n",
    "\n",
    "max_dim = 20\n",
    "\n",
    "def mutual_info_worker(args):\n",
    "    data1, data2 = args\n",
    "    mine = MINE(alpha=0.6, c=15)\n",
    "    mine.compute_score(data1, data2)\n",
    "    return mine.mic()\n",
    "\n",
    "def determine_delay(data, max_delay=100, subsample_factor=10):\n",
    "    subsampled_data = data[::subsample_factor]\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        mi_values = pool.map(mutual_info_worker, [(subsampled_data[:-i], subsampled_data[i:]) for i in range(1, max_delay + 1)])\n",
    "    return np.argmin(mi_values) + 1\n",
    "\n",
    "def delay_embedding(data, emb_dim, delay):\n",
    "    N = len(data)\n",
    "    return np.array([data[i:i + emb_dim * delay:delay] for i in range(N - (emb_dim - 1) * delay)])\n",
    "\n",
    "def false_nearest_neighbors(data, emb_dim, delay, R=10):\n",
    "    N = len(data)\n",
    "    false_neighbors = np.zeros(emb_dim)\n",
    "    for d in range(1, emb_dim + 1):\n",
    "        emb_data = delay_embedding(data, d, delay)\n",
    "        nbrs = NearestNeighbors(n_neighbors=2).fit(emb_data)\n",
    "        distances, indices = nbrs.kneighbors(emb_data)\n",
    "        false_neighbors[d - 1] = np.mean((np.abs(distances[:, 0] - distances[:, 1]) / distances[:, 1]) > R)\n",
    "    return false_neighbors\n",
    "\n",
    "# Analyze each EEG channel\n",
    "for channel_name in eeg_channel_names:\n",
    "    channel_index = eeg_channel_names.index(channel_name)\n",
    "    channel_data = filtered_EEG_data[:, channel_index]\n",
    "\n",
    "    optimal_delay = determine_delay(channel_data, max_delay=20, subsample_factor=10)\n",
    "    emb_dim_2d, emb_dim_3d = 2, 3\n",
    "\n",
    "    embedded_data_2d = delay_embedding(channel_data, emb_dim_2d, optimal_delay)\n",
    "    embedded_data_3d = delay_embedding(channel_data, emb_dim_3d, optimal_delay)\n",
    "\n",
    "    np.save(os.path.join(embedding_2d_dir, f'2dembedded_{channel_name}.npy'), embedded_data_2d)\n",
    "    np.save(os.path.join(embedding_3d_dir, f'3dembedded_{channel_name}.npy'), embedded_data_3d)\n",
    "\n",
    "    # Generate and save plots\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(embedded_data_2d[:, 0], embedded_data_2d[:, 1], s=1)\n",
    "    plt.title(f'2D Phase Space Reconstruction for {channel_name}')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.savefig(os.path.join(plots_directory, f'2D_{channel_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(embedded_data_3d[:, 0], embedded_data_3d[:, 1], embedded_data_3d[:, 2], s=1)\n",
    "    ax.set_title(f'3D Phase Space Reconstruction for {channel_name}')\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "    plt.savefig(os.path.join(plots_directory, f'3D_{channel_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Zip the embedded data directories\n",
    "for dir_path, zip_name in [(embedding_2d_dir, '2d_embedded_data.zip'), (embedding_3d_dir, '3d_embedded_data.zip')]:\n",
    "    with zipfile.ZipFile(os.path.join(base_dir, zip_name), 'w') as zipf:\n",
    "        for file in os.listdir(dir_path):\n",
    "            zipf.write(os.path.join(dir_path, file), file)\n",
    "\n",
    "print(\"All processes completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0242bdc-e0c0-4c1a-afb3-bbaff7d5c329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def sample_entropy(u, m, r):\n",
    "    def _count_matches(m, r):\n",
    "        count = 0\n",
    "        for i in range(len(u) - m):\n",
    "            for j in range(i + 1, len(u) - m):\n",
    "                if max(abs(u[i:i + m] - u[j:j + m])) <= r:\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    n = len(u)\n",
    "    r *= np.std(u)\n",
    "    matches_m = _count_matches(m, r)\n",
    "    matches_m_plus_one = _count_matches(m + 1, r)\n",
    "    try:\n",
    "        return -np.log(matches_m_plus_one / matches_m)\n",
    "    except ZeroDivisionError:\n",
    "        return np.inf\n",
    "\n",
    "# Define a function to calculate the box-counting dimension\n",
    "def box_counting_dimension(data, k=4):\n",
    "    \"\"\"\n",
    "    Estimate the box-counting dimension of a dataset using the k-nearest neighbor distances.\n",
    "    \"\"\"\n",
    "    distances = pdist(data, 'euclidean')\n",
    "    distance_matrix = squareform(distances)\n",
    "    k_distances = np.sort(distance_matrix, axis=0)[k]\n",
    "    r = np.mean(k_distances)\n",
    "    N = len(data)\n",
    "    return np.log(N) / np.log(1/r)\n",
    "\n",
    "# Update the analyze_channel_data function to use the new sample_entropy function\n",
    "def analyze_channel_data(channel_data_3d, channel_name):\n",
    "    # Apply DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "    clusters = dbscan.fit_predict(channel_data_3d)\n",
    "    \n",
    "    # Flatten the data for sample entropy calculation\n",
    "    flattened_data = channel_data_3d.reshape(-1)\n",
    "    \n",
    "    # Calculate sample entropy\n",
    "    sample_entropy_value = sample_entropy(flattened_data, m=2, r=0.2)  # Adjust m and r based on your data\n",
    "    \n",
    "    # Estimate the box-counting dimension\n",
    "    box_dim = box_counting_dimension(channel_data_3d)\n",
    "    \n",
    "    print(f\"Channel: {channel_name}\")\n",
    "    print(f\"  Number of Clusters: {len(set(clusters)) - (1 if -1 in clusters else 0)}\")\n",
    "    print(f\"  Sample Entropy: {sample_entropy_value}\")\n",
    "    print(f\"  Box-Counting Dimension: {box_dim}\")\n",
    "\n",
    "\n",
    "# Loop through each channel's 3D embedded data\n",
    "for channel_name in eeg_channel_names:\n",
    "    data_path = os.path.join(embedding_3d_dir, f'3dembedded_{channel_name}.npy')\n",
    "    channel_data_3d = np.load(data_path)\n",
    "    \n",
    "    # Analyze the channel data\n",
    "    analyze_channel_data(channel_data_3d, channel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15a381-d8b8-4ecd-a27e-7039db79b2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258136b6-20f5-4a2c-8863-59e920410c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import multiprocessing\n",
    "import os\n",
    "import zipfile\n",
    "from minepy import MINE\n",
    "\n",
    "# Define directories\n",
    "base_dir = '/home/vincent/MySSD/JupyterProjects/AAA_projects/UnlimitedResearchCooperative/Synthetic_Intelligence_Labs/EEG_Chaos_Kuramoto_Neural_Net'\n",
    "embedding_2d_dir = os.path.join(base_dir, '2dembedding_data')\n",
    "embedding_3d_dir = os.path.join(base_dir, '3dembedding_data')\n",
    "plots_directory = os.path.join(base_dir, 'plots')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(embedding_2d_dir, exist_ok=True)\n",
    "os.makedirs(embedding_3d_dir, exist_ok=True)\n",
    "os.makedirs(plots_directory, exist_ok=True)\n",
    "\n",
    "# Load EEG data\n",
    "EEG_data = np.load(os.path.join(base_dir, 'eeg_data_with_channels.npy'), allow_pickle=True)\n",
    "\n",
    "# EEG channel names\n",
    "eeg_channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "sampling_rate = 1000  # Hz\n",
    "start_time, end_time = 805.571, 921.515\n",
    "start_index, end_index = int(start_time * sampling_rate), int(end_time * sampling_rate)\n",
    "filtered_EEG_data = EEG_data[start_index:end_index, :]\n",
    "\n",
    "max_dim = 20\n",
    "\n",
    "def mutual_info_worker(args):\n",
    "    data1, data2 = args\n",
    "    mine = MINE(alpha=0.6, c=15)\n",
    "    mine.compute_score(data1, data2)\n",
    "    return mine.mic()\n",
    "\n",
    "def determine_delay(data, max_delay=100, subsample_factor=10):\n",
    "    subsampled_data = data[::subsample_factor]\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        mi_values = pool.map(mutual_info_worker, [(subsampled_data[:-i], subsampled_data[i:]) for i in range(1, max_delay + 1)])\n",
    "    return np.argmin(mi_values) + 1\n",
    "\n",
    "def delay_embedding(data, emb_dim, delay):\n",
    "    N = len(data)\n",
    "    return np.array([data[i:i + emb_dim * delay:delay] for i in range(N - (emb_dim - 1) * delay)])\n",
    "\n",
    "def false_nearest_neighbors(data, emb_dim, delay, R=10):\n",
    "    N = len(data)\n",
    "    false_neighbors = np.zeros(emb_dim)\n",
    "    for d in range(1, emb_dim + 1):\n",
    "        emb_data = delay_embedding(data, d, delay)\n",
    "        nbrs = NearestNeighbors(n_neighbors=2).fit(emb_data)\n",
    "        distances, indices = nbrs.kneighbors(emb_data)\n",
    "        false_neighbors[d - 1] = np.mean((np.abs(distances[:, 0] - distances[:, 1]) / distances[:, 1]) > R)\n",
    "    return false_neighbors\n",
    "    \n",
    "def caos_method(data, max_dim=20, tau=1):\n",
    "    \"\"\"\n",
    "    Simple implementation of Cao's method for determining the minimal embedding dimension.\n",
    "    :param data: Input time series data.\n",
    "    :param max_dim: Maximum embedding dimension to consider.\n",
    "    :param tau: Time delay.\n",
    "    :return: E1 and E2 values for dimensions 1 to max_dim. A plateau or change in E1 indicates\n",
    "             sufficient embedding, whereas E2 approaching 1 suggests false nearest neighbors are minimal.\n",
    "    \"\"\"\n",
    "    def embedding(data, dim, tau):\n",
    "        \"\"\"Embeds data according to given dimension and time delay.\"\"\"\n",
    "        N = len(data)\n",
    "        if N - (dim - 1) * tau < 1:\n",
    "            return None\n",
    "        embedded_data = np.array([data[i:N - (dim - 1) * tau + i:tau] for i in range(dim)]).T\n",
    "        return embedded_data\n",
    "    \n",
    "    E1 = np.zeros(max_dim)\n",
    "    E2 = np.zeros(max_dim - 1)\n",
    "    \n",
    "    for dim in range(1, max_dim + 1):\n",
    "        embedded_data = embedding(data, dim, tau)\n",
    "        embedded_data_next = embedding(data, dim + 1, tau)\n",
    "        \n",
    "        if embedded_data is None or embedded_data_next is None:\n",
    "            break\n",
    "        \n",
    "        distances = np.sqrt(np.sum(np.power(embedded_data[:, None, :] - embedded_data[None, :, :], 2), axis=2))\n",
    "        min_distances = np.min(distances + np.eye(len(data) - (dim - 1) * tau) * np.max(distances), axis=1)\n",
    "        \n",
    "        distances_next = np.sqrt(np.sum(np.power(embedded_data_next[:, None, :] - embedded_data_next[None, :, :], 2), axis=2))\n",
    "        min_distances_next = np.min(distances_next + np.eye(len(data) - (dim - 1) * tau) * np.max(distances_next), axis=1)\n",
    "        \n",
    "        E1[dim - 1] = np.mean(min_distances_next / min_distances)\n",
    "        if dim > 1:\n",
    "            E2[dim - 2] = E1[dim - 1] / E1[dim - 2]\n",
    "    \n",
    "    return E1, E2\n",
    "    return np.ones(max_dim), np.ones(max_dim - 1)\n",
    "\n",
    "# Analyze each EEG channel using Cao's method and generate plots\n",
    "for channel_name in eeg_channel_names:\n",
    "    channel_index = eeg_channel_names.index(channel_name)\n",
    "    channel_data = filtered_EEG_data[:, channel_index]\n",
    "\n",
    "    # Assuming optimal_delay is predefined or determined by another method\n",
    "    optimal_delay = determine_delay(channel_data, max_delay=20, subsample_factor=10)\n",
    "\n",
    "    # Determine the optimal embedding dimension using Cao's method\n",
    "    E1, E2 = caos_method(channel_data, max_dim=20, tau=optimal_delay)\n",
    "    optimal_dim = np.argmax(E2 < 1.05) + 2  # Adjust this criterion based on your analysis\n",
    "\n",
    "    # Perform embedding with the determined optimal dimension\n",
    "    embedded_data = delay_embedding(channel_data, emb_dim=optimal_dim, delay=optimal_delay)\n",
    "\n",
    "    # Save the embedded data\n",
    "    if optimal_dim == 2:\n",
    "        np.save(os.path.join(embedding_2d_dir, f'2dembedded_{channel_name}.npy'), embedded_data)\n",
    "    else:\n",
    "        np.save(os.path.join(embedding_3d_dir, f'3dembedded_{channel_name}.npy'), embedded_data)\n",
    "\n",
    "    # Generate and save plot for the optimal embedding\n",
    "    if emb_dim == 2:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(embedded_data[:, 0], embedded_data[:, 1], s=1)\n",
    "        plt.title(f'2D Phase Space Reconstruction for {channel_name}')\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        plt.savefig(os.path.join(plots_directory, f'2D_{channel_name}.png'))\n",
    "        plt.close()\n",
    "    elif emb_dim == 3:\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(embedded_data[:, 0], embedded_data[:, 1], embedded_data[:, 2], s=1)\n",
    "        ax.set_title(f'3D Phase Space Reconstruction for {channel_name}')\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.set_zlabel('Component 3')\n",
    "        plt.savefig(os.path.join(plots_directory, f'3D_{channel_name}.png'))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Zip the embedded data directories\n",
    "for dir_path, zip_name in [(embedding_2d_dir, '2d_embedded_data_caos.zip'), (embedding_3d_dir, '3d_embedded_data_caos.zip')]:\n",
    "    with zipfile.ZipFile(os.path.join(base_dir, zip_name), 'w') as zipf:\n",
    "        for file in os.listdir(dir_path):\n",
    "            zipf.write(os.path.join(dir_path, file), file)\n",
    "\n",
    "print(\"All processes completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22a03d-d9f7-4ed2-a9be-df3711c13b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gudhi as gd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Assuming `reduced_data_umap` is your dimensionality-reduced data from the previous steps\n",
    "\n",
    "def compute_persistence_diagrams(data):\n",
    "    \"\"\"\n",
    "    Compute the persistence diagrams for a given dataset using Vietoris-Rips complex.\n",
    "    :param data: Input dataset, assumed to be the output of a dimensionality reduction method.\n",
    "    :return: Persistence diagrams for the dataset.\n",
    "    \"\"\"\n",
    "    rips_complex = gd.RipsComplex(points=data, max_edge_length=2)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistence = simplex_tree.persistence()\n",
    "    return persistence\n",
    "\n",
    "def plot_persistence_diagrams(persistence):\n",
    "    \"\"\"\n",
    "    Plot the persistence diagrams.\n",
    "    :param persistence: Persistence diagrams.\n",
    "    \"\"\"\n",
    "    gd.plot_persistence_diagram(persistence)\n",
    "    plt.show()\n",
    "\n",
    "def calculate_betti_numbers(persistence):\n",
    "    \"\"\"\n",
    "    Calculate Betti numbers from the persistence diagrams.\n",
    "    :param persistence: Persistence diagrams.\n",
    "    :return: Betti numbers (b0, b1, b2) counting the number of connected components, loops, and voids respectively.\n",
    "    \"\"\"\n",
    "    betti_numbers = {i: 0 for i in range(3)}  # Assuming we're only interested in dimensions 0, 1, and 2\n",
    "    for interval in persistence:\n",
    "        if interval[0] < 3:  # Filter out infinite persistence intervals\n",
    "            betti_numbers[interval[0]] += 1\n",
    "    return betti_numbers['b0'], betti_numbers['b1'], betti_numbers['b2']\n",
    "\n",
    "# Compute Persistence Diagrams\n",
    "persistence = compute_persistence_diagrams(reduced_data_umap)\n",
    "\n",
    "# Plot Persistence Diagrams\n",
    "plot_persistence_diagrams(persistence)\n",
    "\n",
    "# Calculate Betti Numbers\n",
    "betti_numbers = calculate_betti_numbers(persistence)\n",
    "print(\"Betti Numbers:\", betti_numbers)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
