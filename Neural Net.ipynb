{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516441f8-ccaa-478a-a7e3-6db52f2a74ef",
   "metadata": {},
   "source": [
    "# Load Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43be465b-9c39-4493-9313-db7eef868865",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arnold_tongues_rotation_numbers_tensor: torch.Size([32, 300, 300])\n",
      "dspm_tensor: torch.Size([19, 18840, 10])\n",
      "higuchi_fractal_dimensions_tensor: torch.Size([1, 1, 4, 8])\n",
      "Hurst_tensor: torch.Size([1, 1, 32, 1])\n",
      "mfdfa_concatd_tensor: torch.Size([32, 1, 30, 2])\n",
      "mfdfa_tensor: torch.Size([9, 32, 10, 1])\n",
      "short_time_fourier_transform_tensor: torch.Size([32, 1001, 4229])\n",
      "transfer_entropy_granular_tensor: torch.Size([4, 4])\n",
      "transfer_entropy_hemispheric_avg_input_tensor: torch.Size([92, 92])\n",
      "transfer_entropy_regional_tensor: torch.Size([4, 4])\n",
      "spectral_entropy_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_centroids_tensor: torch.Size([1, 1, 32, 1])\n",
      "freq_max_power_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_edge_freqs_tensor: torch.Size([1, 1, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/arnold_tongues_rotation_numbers_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/dspm_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/higuchi_fractal_dimensions_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/Hurst_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_concatd_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/short_time_fourier_transform_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_granular_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_hemispheric_avg_input_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_regional_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_entropy_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_centroids_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/freq_max_power_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_edge_freqs_tensor.pt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store the tensors and another for their shapes\n",
    "tensors = {}\n",
    "tensor_shapes = {}\n",
    "\n",
    "# Load the tensors into a dictionary and collect their shapes\n",
    "for path in tensor_paths:\n",
    "    tensor_name = path.split('/')[-1].replace('.pt', '').replace('.pth', '')\n",
    "\n",
    "    # Remove the 'h' from the end, if it exists\n",
    "    if tensor_name.endswith(\"h\"):\n",
    "        tensor_name = tensor_name[:-1]\n",
    "\n",
    "    # Load the tensor\n",
    "    data = torch.load(path)\n",
    "    tensors[tensor_name] = data\n",
    "\n",
    "    # Check the type of the loaded data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor_shapes[tensor_name] = data.shape\n",
    "    elif isinstance(data, dict):  # Likely a state_dict\n",
    "        tensor_shapes[tensor_name] = \"state_dict (model parameters)\"\n",
    "    else:\n",
    "        tensor_shapes[tensor_name] = \"unknown type\"\n",
    "\n",
    "# Print the shapes of all loaded tensors\n",
    "for name, shape in tensor_shapes.items():\n",
    "    print(f\"{name}: {shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7667071-ad74-4a9e-ab1b-f29354a5816c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Match dimensions, reshape, and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f60a69-caf6-47d1-a2c1-b71536f18f65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tensor 1 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 2 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 3 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 4 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 5 shape: torch.Size([32, 1, 32, 32])\n",
      "Processed tensor 6 shape: torch.Size([9, 1, 32, 32])\n",
      "Processed tensor 7 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 8 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 9 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 10 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 11 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 12 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 13 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 14 shape: torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_and_resize_tensor(tensor, target_shape):\n",
    "    # Add missing batch and channel dimensions\n",
    "    while len(tensor.shape) < 4:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Reduce the channel dimension to 1 by taking the mean along that axis\n",
    "    tensor = torch.mean(tensor, dim=1, keepdim=True)\n",
    "\n",
    "    # Normalize\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    if std != 0:\n",
    "        tensor = (tensor - mean) / std\n",
    "\n",
    "    # Reshape/resize to target_shape\n",
    "    tensor = F.interpolate(tensor, size=target_shape[2:], mode='bilinear', align_corners=True)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "arnold_tongues_rotation_numbers_tensor = torch.rand([32, 300, 300])\n",
    "dspm_tensor = torch.rand([19, 18840, 10])\n",
    "higuchi_fractal_dimensions_tensor = torch.rand([1, 1, 4, 8])\n",
    "Hurst_tensor = torch.rand([1, 1, 32, 1])\n",
    "mfdfa_concatd_tensor = torch.rand([32, 1, 30, 2])\n",
    "mfdfa_tensor = torch.rand([9, 32, 10, 1])\n",
    "short_time_fourier_transform_tensor = torch.rand([32, 1001, 4229])\n",
    "transfer_entropy_granular_tensor = torch.rand([4, 4])\n",
    "transfer_entropy_hemispheric_avg_input_tensor = torch.rand([92, 92])\n",
    "transfer_entropy_regional_tensor = torch.rand([4, 4])\n",
    "spectral_entropy_tensor = torch.rand([1, 1, 32, 1])\n",
    "spectral_centroids_tensor = torch.rand([1, 1, 32, 1])\n",
    "freq_max_power_tensor = torch.rand([1, 1, 32, 1])\n",
    "spectral_edge_freqs_tensor = torch.rand([1, 1, 32, 1])\n",
    "\n",
    "target_shape = [1, 1, 32, 32]\n",
    "\n",
    "# List of all your tensors \n",
    "all_tensors = [\n",
    "    arnold_tongues_rotation_numbers_tensor,\n",
    "    dspm_tensor,\n",
    "    higuchi_fractal_dimensions_tensor,\n",
    "    Hurst_tensor,\n",
    "    mfdfa_concatd_tensor,\n",
    "    mfdfa_tensor,\n",
    "    short_time_fourier_transform_tensor,\n",
    "    transfer_entropy_granular_tensor,\n",
    "    transfer_entropy_hemispheric_avg_input_tensor,\n",
    "    transfer_entropy_regional_tensor,\n",
    "    spectral_entropy_tensor,\n",
    "    spectral_centroids_tensor,\n",
    "    freq_max_power_tensor,\n",
    "    spectral_edge_freqs_tensor,\n",
    "]\n",
    "\n",
    "# Preprocess all tensors\n",
    "processed_tensors = [preprocess_and_resize_tensor(tensor, target_shape) for tensor in all_tensors]\n",
    "\n",
    "# Print out the new shapes\n",
    "for i, tensor in enumerate(processed_tensors):\n",
    "    print(f\"Processed tensor {i+1} shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc365929-f568-48e7-af9e-aa60e517f841",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ff40f7-f03a-43bc-9afb-1ce7afd9f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tensor_names = [\n",
    "    'arnold_tongues_rotation_numbers_tensor',\n",
    "    'dspm_tensor',\n",
    "    'higuchi_fractal_dimensions_tensor',\n",
    "    'Hurst_tensor',\n",
    "    'mfdfa_concatd_tensor',\n",
    "    'mfdfa_tensor',\n",
    "    'short_time_fourier_transform_tensor',\n",
    "    'transfer_entropy_granular_tensor',\n",
    "    'transfer_entropy_hemispheric_avg_input_tensor',\n",
    "    'transfer_entropy_regional_tensor',\n",
    "    'spectral_entropy_tensor',\n",
    "    'spectral_centroids_tensor',\n",
    "    'freq_max_power_tensor',\n",
    "    'spectral_edge_freqs_tensor',\n",
    "]\n",
    "\n",
    "class BaseEmbeddingNet(nn.Module):\n",
    "    def __init__(self, input_channels, conv_output_channels, reduce_to_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, conv_output_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(conv_output_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_reduce = nn.Linear(conv_output_channels, reduce_to_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_reduce(x)\n",
    "        return x\n",
    "\n",
    "processed_tensors_dict = {name: tensor for name, tensor in zip(tensor_names, processed_tensors)}\n",
    "\n",
    "net_params = {name: {'input_channels': 1, 'conv_output_channels': 16, 'reduce_to_dim': 8} \n",
    "              for name in processed_tensors_dict.keys()}\n",
    "\n",
    "# Create BaseEmbeddingNets for each tensor\n",
    "embedding_nets = {name: BaseEmbeddingNet(**params) for name, params in net_params.items()}\n",
    "\n",
    "# Move networks to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for net in embedding_nets.values():\n",
    "    net.to(device)\n",
    "\n",
    "# Create custom dataset and dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = tensors\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __len__(self):\n",
    "        first_tensor = next(iter(self.tensors.values()))\n",
    "        return first_tensor.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        result = {}\n",
    "        for key, val in self.tensors.items():\n",
    "            if val.shape[0] > idx:\n",
    "                result[key] = val[idx]\n",
    "        return result\n",
    "\n",
    "def custom_collate(batch):\n",
    "    collated_batch = {}\n",
    "    all_keys = set([key for item in batch for key in item.keys()])\n",
    "    \n",
    "    for key in all_keys:\n",
    "        collated_batch[key] = torch.stack([item[key] for item in batch if key in item.keys()], dim=0)\n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "# Use processed_tensors for your CustomDataset\n",
    "dataset = CustomDataset(processed_tensors_dict)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=custom_collate)\n",
    "\n",
    "# Collect feature embeddings\n",
    "all_features = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "    features_list = [net(batch[key].to(device, dtype=torch.float32)) for key, net in embedding_nets.items()]\n",
    "    concatenated_features = torch.cat(features_list, dim=1)\n",
    "    all_features.append(concatenated_features.cpu().detach())\n",
    "\n",
    "# Convert list to tensor\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "\n",
    "# Save the feature embeddings\n",
    "save_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto'\n",
    "torch.save(all_features, f'{save_path}/all_features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0ec40-2261-41fc-82e1-13d73334d058",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344ba273-0591-476a-aa5c-07fb403af45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def create_windows(data, window_size, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        windows.append(data[i:i+window_size])\n",
    "    return torch.stack(windows)\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "EEG_data = np.load('/home/vincent/AAA_projects/MVCS/Neuroscience/eeg_data_with_channels.npy', allow_pickle=True)\n",
    "EEG_tensor = torch.FloatTensor(EEG_data)  # Assumes EEG_data is a NumPy ndarray\n",
    "\n",
    "window_size = 1600  \n",
    "stride = 10  \n",
    "\n",
    "train_data = create_windows(EEG_tensor[:int(0.7 * len(EEG_tensor))], window_size, stride)\n",
    "train_dataset = EEGDataset(data=train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88f8ba-0bd5-45f7-9a84-deae168e531d",
   "metadata": {},
   "source": [
    "# Kuramoto layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefb207-c729-4e70-bfc3-9a3c3dfd476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Function to create windows for time-series data\n",
    "def create_windows(data, window_size, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        windows.append(data[i:i+window_size])\n",
    "    return torch.stack(windows)\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "window_size = 1600\n",
    "stride = 10\n",
    "\n",
    "# Add necessary transformations here to EEG_tensor if required\n",
    "EEG_tensor = EEG_tensor.clone().detach().to(device)\n",
    "analytic_signal = hilbert(EEG_tensor.cpu().numpy())\n",
    "phases = torch.tensor(np.angle(analytic_signal), dtype=torch.float32).to(device)\n",
    "\n",
    "# Load PLV matrix\n",
    "plv_matrix_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Phase Syncronization/plv_matrix.npy\"\n",
    "plv_matrix = torch.tensor(np.load(plv_matrix_path), dtype=torch.float32).to(device)\n",
    "\n",
    "# Compute_phase_diff_matrix function\n",
    "def compute_phase_diff_matrix(phases):\n",
    "    time, channels = phases.shape[:2]\n",
    "    phase_diff_matrix = torch.zeros(channels, channels, device=phases.device)\n",
    "    for i in range(channels):\n",
    "        for j in range(channels):\n",
    "            phase_diff_matrix[i, j] = torch.mean(phases[:, i] - phases[:, j])\n",
    "    return phase_diff_matrix\n",
    "\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)\n",
    "\n",
    "# EEG channel names\n",
    "eeg_channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                     'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                     'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "# Broad regions and corresponding channels\n",
    "regions = {\n",
    "    \"frontal\": ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8'],\n",
    "    \"temporal\": ['T7', 'T8'],\n",
    "    \"parietal\": ['CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8'],\n",
    "    \"occipital\": ['O1', 'Oz', 'O2']\n",
    "}\n",
    "\n",
    "# Precompute omega and phase_diff_matrix\n",
    "N = len(eeg_channel_names)\n",
    "omega = torch.mean(plv_matrix, dim=1).to(device)\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)\n",
    "\n",
    "# Modify the Kuramoto function to use PyTorch functions instead of NumPy\n",
    "def kuramoto_weighted_bias(t, y, omega, K):\n",
    "    weighted_sin = plv_matrix * torch.sin(y - y[:, None] - phase_diff_matrix)\n",
    "    dydt = omega + K / N * torch.sum(weighted_sin, axis=1)\n",
    "    return dydt\n",
    "\n",
    "class KuramotoODEFunc(nn.Module):\n",
    "    def __init__(self, omega, K, plv_matrix, phase_diff_matrix):\n",
    "        super(KuramotoODEFunc, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.K = K\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "    def forward(self, t, theta):\n",
    "        # Reshape to accommodate the additional time dimension.\n",
    "        theta = theta.view(-1, theta.shape[-1])\n",
    "        N = theta.shape[1]\n",
    "        print(\"Theta shape: \", theta.shape)\n",
    "        print(\"PLV Matrix shape: \", self.plv_matrix.shape)\n",
    "        print(\"Phase Diff Matrix shape: \", self.phase_diff_matrix.shape)\n",
    "        weighted_sin = self.plv_matrix * torch.sin(theta.unsqueeze(1) - theta.unsqueeze(2) - self.phase_diff_matrix)\n",
    "        dtheta = self.omega + (self.K / N) * torch.sum(weighted_sin, dim=1)\n",
    "        return dtheta.view(theta.shape)\n",
    "\n",
    "class KuramotoLayer(nn.Module):\n",
    "    def __init__(self, oscillator_count, time_steps, dt=0.01, plv_matrix=None, phase_diff_matrix=None):\n",
    "        super(KuramotoLayer, self).__init__()\n",
    "        self.oscillator_count = oscillator_count\n",
    "        self.time_steps = time_steps\n",
    "        self.dt = dt\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "        if plv_matrix is not None:\n",
    "            omega_init = torch.mean(plv_matrix, dim=1)  \n",
    "            self.omega = nn.Parameter(omega_init, requires_grad=True)\n",
    "        else:\n",
    "            self.omega = nn.Parameter(torch.randn(oscillator_count), requires_grad=True)\n",
    "\n",
    "        self.K = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "    \n",
    "    def custom_forward(self, *inputs):\n",
    "        ode_func = KuramotoODEFunc(self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        time_points = torch.arange(0, self.time_steps * self.dt, self.dt).to(device)\n",
    "        return odeint(ode_func, inputs[0], time_points)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        theta = checkpoint(self.custom_forward, x)\n",
    "        theta = theta.to(torch.float32)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "    \n",
    "    def forward_with_checkpoint(self, x):\n",
    "        x = x.to(device)\n",
    "        theta = checkpoint(self.custom_forward, x)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mean_coherence(theta):\n",
    "        N, _, _ = theta.shape\n",
    "        mean_coherence = torch.mean(torch.cos(theta[:, -1, :] - theta[:, -1, :].mean(dim=1).unsqueeze(1)))\n",
    "        return mean_coherence\n",
    "\n",
    "# Make sure all tensors are on the correct device\n",
    "phases = phases.to(device)\n",
    "\n",
    "# Compute natural frequencies and phase differences just once\n",
    "print(\"PLV Matrix shape: \", plv_matrix.shape)\n",
    "num_channels = len(eeg_channel_names)  # Get the number of channels\n",
    "#print(\"Theta shape: \", theta.shape)\n",
    "#print(\"Theta Unsqueeze(1) shape: \", theta.unsqueeze(1).shape)\n",
    "#print(\"Theta Unsqueeze(2) shape: \", theta.unsqueeze(2).shape)\n",
    "print(\"Phase Diff Matrix shape: \", phase_diff_matrix.shape)\n",
    "\n",
    "# Number of channels\n",
    "N = len(eeg_channel_names)\n",
    "\n",
    "# Initialize model and move to device\n",
    "kuramoto_model = KuramotoLayer(N, 12800, plv_matrix=plv_matrix, phase_diff_matrix=phase_diff_matrix).to(device)\n",
    "\n",
    "# Data Parallelism for multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    kuramoto_model = nn.DataParallel(kuramoto_model)\n",
    "\n",
    "train_data = create_windows(EEG_tensor[:int(0.7 * len(EEG_tensor))], window_size, stride).detach().requires_grad_(True)\n",
    "train_dataset = EEGDataset(data=train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training loop and feature extraction\n",
    "kuramoto_features_list = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    batch = batch.to(device)\n",
    "    theta, mean_coherence = kuramoto_model(batch)\n",
    "    kuramoto_features_list.append(mean_coherence)\n",
    "\n",
    "# Save the combined features\n",
    "kuramoto_features_tensor = torch.stack(kuramoto_features_list)\n",
    "all_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt'\n",
    "all_features = torch.load(all_features_path)\n",
    "combined_features = torch.cat([all_features, kuramoto_features_tensor.unsqueeze(1)], dim=1)\n",
    "combined_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/combined_features.pt'\n",
    "torch.save(combined_features, combined_features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56cafa-30b8-415d-823c-fafd212eecfa",
   "metadata": {},
   "source": [
    "# Kuramoto split up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd4ba5d-a115-4eb9-beb4-98d6f37c95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Function to create windows for time-series data\n",
    "def create_windows(data, window_size, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        windows.append(data[i:i+window_size])\n",
    "    return torch.stack(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "040d257f-994d-409b-9875-849ff643ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "window_size = 1600\n",
    "stride = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "174b3288-f313-4b35-b037-e6eec6892a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4227788, 32])\n",
      "541156864\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add necessary transformations here to EEG_tensor if required\n",
    "print(EEG_tensor.shape)\n",
    "print(EEG_tensor.element_size() * EEG_tensor.nelement())\n",
    "\n",
    "EEG_tensor = EEG_tensor.clone().detach().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0a84dd-6970-4888-8168-7489406b22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_hilbert_in_batches(data, batch_size):\n",
    "    n_batches = int(np.ceil(data.shape[0] / batch_size))\n",
    "    analytic_signal = np.zeros_like(data, dtype=np.complex64)  # change dtype as needed\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        analytic_signal[start_idx:end_idx, :] = hilbert(data[start_idx:end_idx, :])\n",
    "        \n",
    "    return analytic_signal\n",
    "\n",
    "batch_size = 100000  # Set as appropriate\n",
    "EEG_numpy = EEG_tensor.cpu().numpy()\n",
    "analytic_signal_batches = apply_hilbert_in_batches(EEG_numpy, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b34c43-dc25-45be-b0d0-2fd9bfbdfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = torch.tensor(np.angle(analytic_signal_batches), dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed965961-347e-4c66-821a-9737b8b9262b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183a4245-8eb2-4e28-9bd1-8999ad619d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load PLV matrix\n",
    "plv_matrix_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Phase Syncronization/plv_matrix.npy\"\n",
    "plv_matrix = torch.tensor(np.load(plv_matrix_path), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6df412-bdd8-40cb-9b12-c5964f9349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute_phase_diff_matrix function\n",
    "def compute_phase_diff_matrix(phases):\n",
    "    time, channels = phases.shape[:2]\n",
    "    phase_diff_matrix = torch.zeros(channels, channels, device=phases.device)\n",
    "    for i in range(channels):\n",
    "        for j in range(channels):\n",
    "            phase_diff_matrix[i, j] = torch.mean(phases[:, i] - phases[:, j])\n",
    "    return phase_diff_matrix\n",
    "\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)\n",
    "\n",
    "# EEG channel names\n",
    "eeg_channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                     'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                     'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "# Broad regions and corresponding channels\n",
    "regions = {\n",
    "    \"frontal\": ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8'],\n",
    "    \"temporal\": ['T7', 'T8'],\n",
    "    \"parietal\": ['CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8'],\n",
    "    \"occipital\": ['O1', 'Oz', 'O2']\n",
    "}\n",
    "\n",
    "# Precompute omega and phase_diff_matrix\n",
    "N = len(eeg_channel_names)\n",
    "omega = torch.mean(plv_matrix, dim=1).to(device)\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)\n",
    "\n",
    "# Modify the Kuramoto function to use PyTorch functions instead of NumPy\n",
    "def kuramoto_weighted_bias(t, y, omega, K):\n",
    "    weighted_sin = plv_matrix * torch.sin(y - y[:, None] - phase_diff_matrix)\n",
    "    dydt = omega + K / N * torch.sum(weighted_sin, axis=1)\n",
    "    return dydt\n",
    "\n",
    "class KuramotoODEFunc(nn.Module):\n",
    "    def __init__(self, omega, K, plv_matrix, phase_diff_matrix):\n",
    "        super(KuramotoODEFunc, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.K = K\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "    def forward(self, t, theta):\n",
    "        # Reshape to accommodate the additional time dimension.\n",
    "        theta = theta.view(-1, theta.shape[-1])\n",
    "        N = theta.shape[1]\n",
    "        print(\"Theta shape: \", theta.shape)\n",
    "        print(\"PLV Matrix shape: \", self.plv_matrix.shape)\n",
    "        print(\"Phase Diff Matrix shape: \", self.phase_diff_matrix.shape)\n",
    "        weighted_sin = self.plv_matrix * torch.sin(theta.unsqueeze(1) - theta.unsqueeze(2) - self.phase_diff_matrix)\n",
    "        dtheta = self.omega + (self.K / N) * torch.sum(weighted_sin, dim=1)\n",
    "        return dtheta.view(theta.shape)\n",
    "\n",
    "class KuramotoLayer(nn.Module):\n",
    "    def __init__(self, oscillator_count, time_steps, dt=0.01, plv_matrix=None, phase_diff_matrix=None):\n",
    "        super(KuramotoLayer, self).__init__()\n",
    "        self.oscillator_count = oscillator_count\n",
    "        self.time_steps = time_steps\n",
    "        self.dt = dt\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "        if plv_matrix is not None:\n",
    "            omega_init = torch.mean(plv_matrix, dim=1)  \n",
    "            self.omega = nn.Parameter(omega_init, requires_grad=True)\n",
    "        else:\n",
    "            self.omega = nn.Parameter(torch.randn(oscillator_count), requires_grad=True)\n",
    "\n",
    "        self.K = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "    \n",
    "    def custom_forward(self, *inputs):\n",
    "        ode_func = KuramotoODEFunc(self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        time_points = torch.arange(0, self.time_steps * self.dt, self.dt).to(device)\n",
    "        return odeint(ode_func, inputs[0], time_points)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        theta = checkpoint(self.custom_forward, x)\n",
    "        theta = theta.to(torch.float32)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "    \n",
    "    def forward_with_checkpoint(self, x):\n",
    "        x = x.to(device)\n",
    "        theta = checkpoint(self.custom_forward, x)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mean_coherence(theta):\n",
    "        N, _, _ = theta.shape\n",
    "        mean_coherence = torch.mean(torch.cos(theta[:, -1, :] - theta[:, -1, :].mean(dim=1).unsqueeze(1)))\n",
    "        return mean_coherence\n",
    "\n",
    "# Make sure all tensors are on the correct device\n",
    "phases = phases.to(device)\n",
    "\n",
    "# Compute natural frequencies and phase differences just once\n",
    "print(\"PLV Matrix shape: \", plv_matrix.shape)\n",
    "num_channels = len(eeg_channel_names)  # Get the number of channels\n",
    "#print(\"Theta shape: \", theta.shape)\n",
    "#print(\"Theta Unsqueeze(1) shape: \", theta.unsqueeze(1).shape)\n",
    "#print(\"Theta Unsqueeze(2) shape: \", theta.unsqueeze(2).shape)\n",
    "print(\"Phase Diff Matrix shape: \", phase_diff_matrix.shape)\n",
    "\n",
    "# Number of channels\n",
    "N = len(eeg_channel_names)\n",
    "\n",
    "# Initialize model and move to device\n",
    "kuramoto_model = KuramotoLayer(N, 12800, plv_matrix=plv_matrix, phase_diff_matrix=phase_diff_matrix).to(device)\n",
    "\n",
    "# Data Parallelism for multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    kuramoto_model = nn.DataParallel(kuramoto_model)\n",
    "\n",
    "train_data = create_windows(EEG_tensor[:int(0.7 * len(EEG_tensor))], window_size, stride).detach().requires_grad_(True)\n",
    "train_dataset = EEGDataset(data=train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training loop and feature extraction\n",
    "kuramoto_features_list = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    batch = batch.to(device)\n",
    "    theta, mean_coherence = kuramoto_model(batch)\n",
    "    kuramoto_features_list.append(mean_coherence)\n",
    "\n",
    "# Save the combined features\n",
    "kuramoto_features_tensor = torch.stack(kuramoto_features_list)\n",
    "all_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt'\n",
    "all_features = torch.load(all_features_path)\n",
    "combined_features = torch.cat([all_features, kuramoto_features_tensor.unsqueeze(1)], dim=1)\n",
    "combined_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/combined_features.pt'\n",
    "torch.save(combined_features, combined_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de9cf1-96b2-4d6a-9e16-cf3211bbf35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59c75e-edd9-45f0-ad11-bd3ebb70922e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8756d6-b965-433b-93e1-4d5660ffcfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c44d05-e238-4f74-87a1-3747cccf0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8dc5e-228b-45c0-b743-8d2d47a713ba",
   "metadata": {},
   "source": [
    "# Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770cc1e-fd3f-48fd-88e6-727cc87e212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.pos_encoder = nn.Embedding(4227788, d_model)  # Adjust this based on your data\n",
    "        self.position = torch.arange(0, 4227788, dtype=torch.long).unsqueeze(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pos_encoder(self.position[:x.size(0), :])\n",
    "        x = x + pos_encoding\n",
    "        return self.transformer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982d702-9951-408b-9e74-665019135379",
   "metadata": {},
   "source": [
    "# RNN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6ee57-1c00-42bf-8e6d-a7cf9650efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(RNNBlock, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_size, 32)  # 32 channels of EEG data\n",
    "        \n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        return self.fc_out(rnn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03410b62-483c-4734-aa81-c0f209e867a6",
   "metadata": {},
   "source": [
    "# Complete EEG Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3a4cb-5393-4695-a3cc-fc5604ecab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteEEGPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward):\n",
    "        super(CompleteEEGPredictor, self).__init__()\n",
    "        \n",
    "        # Initialize all feature embedding networks\n",
    "        self.eeg_embedding = EEGEmbeddingNet()\n",
    "        self.rotation_embedding = RotationEmbeddingNet()\n",
    "        self.band_power_embedding = BandPowerEmbeddingNet()\n",
    "        self.dspm_embedding = DSPMEmbeddingNet()\n",
    "        self.fast_fourier_embedding = FastFourierEmbeddingNet()\n",
    "        self.higuchi_fractal_embedding = HiguchiFractalEmbeddingNet()\n",
    "        self.hurst_embedding = HurstEmbeddingNet()\n",
    "        self.mfdfa_concatd_embedding = MFDFAConcatdEmbeddingNet()\n",
    "        self.mfdfa_embedding = MFDFAEmbeddingNet()\n",
    "        self.short_time_fourier_embedding = ShortTimeFourierEmbeddingNet()\n",
    "        self.spectral_entropy_embedding = SpectralEntropyEmbeddingNet()\n",
    "        self.spectral_centroids_embedding = SpectralCentroidsEmbeddingNet()\n",
    "        self.freq_max_power_embedding = FreqMaxPowerEmbeddingNet()\n",
    "        self.spectral_edge_freqs_embedding = SpectralEdgeFreqsEmbeddingNet()\n",
    "        self.pairwise_measure_net = PairwiseMeasureNet(input_channels=32, output_channels=64)\n",
    "        \n",
    "        # Initialize Kuramoto layer\n",
    "        self.kuramoto = KuramotoLayer(oscillator_count=32, time_steps=100)\n",
    "        \n",
    "        # Initialize Transformer block\n",
    "        self.transformer_block = TransformerBlock(d_model, nhead, num_encoder_layers, dim_feedforward)\n",
    "        \n",
    "        # Initialize RNN block\n",
    "        self.rnn_block = RNNBlock(input_size=d_model, hidden_size=256, num_layers=2, dropout=0.5)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # Feature extraction using all the embedding networks\n",
    "        embeddings = [\n",
    "            self.eeg_embedding(src),\n",
    "            self.rotation_embedding(src),\n",
    "            self.band_power_embedding(src),\n",
    "            self.dspm_embedding(src),\n",
    "            self.fast_fourier_embedding(src),\n",
    "            self.higuchi_fractal_embedding(src),\n",
    "            self.hurst_embedding(src),\n",
    "            self.mfdfa_concatd_embedding(src),\n",
    "            self.mfdfa_embedding(src),\n",
    "            self.short_time_fourier_embedding(src),\n",
    "            self.spectral_entropy_embedding(src),\n",
    "            self.spectral_centroids_embedding(src),\n",
    "            self.freq_max_power_embedding(src),\n",
    "            self.spectral_edge_freqs_embedding(src),\n",
    "            self.pairwise_measure_net(src)\n",
    "        ]\n",
    "        \n",
    "        # Concatenating the embeddings\n",
    "        combined_features = torch.cat(embeddings, dim=-1)\n",
    "        \n",
    "        # Kuramoto layer\n",
    "        src_kuramoto = self.kuramoto(combined_features)\n",
    "        \n",
    "        # Transformer block\n",
    "        src_transformed = self.transformer_block(src_kuramoto)\n",
    "        \n",
    "        # RNN block\n",
    "        output = self.rnn_block(src_transformed)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6d9a9-557d-493b-a292-f0ed1e3531fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308a730-b820-4bdf-bada-51be5fadfd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808a72e-0026-4437-919d-089d256e655d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
