{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516441f8-ccaa-478a-a7e3-6db52f2a74ef",
   "metadata": {},
   "source": [
    "# Load Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43be465b-9c39-4493-9313-db7eef868865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arnold_tongues_rotation_numbers_tensor: torch.Size([32, 300, 300])\n",
      "dspm_tensor: torch.Size([19, 18840, 10])\n",
      "higuchi_fractal_dimensions_tensor: torch.Size([1, 1, 4, 8])\n",
      "Hurst_tensor: torch.Size([1, 1, 32, 1])\n",
      "mfdfa_concatd_tensor: torch.Size([32, 1, 30, 2])\n",
      "mfdfa_tensor: torch.Size([9, 32, 10, 1])\n",
      "short_time_fourier_transform_tensor: torch.Size([32, 1001, 4229])\n",
      "transfer_entropy_granular_tensor: torch.Size([4, 4])\n",
      "transfer_entropy_hemispheric_avg_input_tensor: torch.Size([92, 92])\n",
      "transfer_entropy_regional_tensor: torch.Size([4, 4])\n",
      "spectral_entropy_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_centroids_tensor: torch.Size([1, 1, 32, 1])\n",
      "freq_max_power_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_edge_freqs_tensor: torch.Size([1, 1, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/arnold_tongues_rotation_numbers_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/dspm_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/higuchi_fractal_dimensions_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/Hurst_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_concatd_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/short_time_fourier_transform_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_granular_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_hemispheric_avg_input_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_regional_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_entropy_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_centroids_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/freq_max_power_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_edge_freqs_tensor.pt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store the tensors and another for their shapes\n",
    "tensors = {}\n",
    "tensor_shapes = {}\n",
    "\n",
    "# Load the tensors into a dictionary and collect their shapes\n",
    "for path in tensor_paths:\n",
    "    tensor_name = path.split('/')[-1].replace('.pt', '').replace('.pth', '')\n",
    "\n",
    "    # Remove the 'h' from the end, if it exists\n",
    "    if tensor_name.endswith(\"h\"):\n",
    "        tensor_name = tensor_name[:-1]\n",
    "\n",
    "    # Load the tensor\n",
    "    data = torch.load(path)\n",
    "    tensors[tensor_name] = data\n",
    "\n",
    "    # Check the type of the loaded data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor_shapes[tensor_name] = data.shape\n",
    "    elif isinstance(data, dict):  # Likely a state_dict\n",
    "        tensor_shapes[tensor_name] = \"state_dict (model parameters)\"\n",
    "    else:\n",
    "        tensor_shapes[tensor_name] = \"unknown type\"\n",
    "\n",
    "# Print the shapes of all loaded tensors\n",
    "for name, shape in tensor_shapes.items():\n",
    "    print(f\"{name}: {shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7667071-ad74-4a9e-ab1b-f29354a5816c",
   "metadata": {},
   "source": [
    "# Match dimensions, reshape, and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f60a69-caf6-47d1-a2c1-b71536f18f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tensor 1 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 2 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 3 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 4 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 5 shape: torch.Size([32, 1, 32, 32])\n",
      "Processed tensor 6 shape: torch.Size([9, 1, 32, 32])\n",
      "Processed tensor 7 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 8 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 9 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 10 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 11 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 12 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 13 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 14 shape: torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_and_resize_tensor(tensor, target_shape):\n",
    "    # Add missing batch and channel dimensions\n",
    "    while len(tensor.shape) < 4:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Reduce the channel dimension to 1 by taking the mean along that axis\n",
    "    tensor = torch.mean(tensor, dim=1, keepdim=True)\n",
    "\n",
    "    # Normalize\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    if std != 0:\n",
    "        tensor = (tensor - mean) / std\n",
    "\n",
    "    # Reshape/resize to target_shape\n",
    "    tensor = F.interpolate(tensor, size=target_shape[2:], mode='bilinear', align_corners=True)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Your tensor shapes here are just for illustration\n",
    "# Replace these with your actual loaded tensors\n",
    "arnold_tongues_rotation_numbers_tensor = torch.rand([32, 300, 300])\n",
    "dspm_tensor = torch.rand([19, 18840, 10])\n",
    "higuchi_fractal_dimensions_tensor = torch.rand([1, 1, 4, 8])\n",
    "Hurst_tensor = torch.rand([1, 1, 32, 1])\n",
    "mfdfa_concatd_tensor = torch.rand([32, 1, 30, 2])\n",
    "mfdfa_tensor = torch.rand([9, 32, 10, 1])\n",
    "short_time_fourier_transform_tensor = torch.rand([32, 1001, 4229])\n",
    "transfer_entropy_granular_tensor = torch.rand([4, 4])\n",
    "transfer_entropy_hemispheric_avg_input_tensor = torch.rand([92, 92])\n",
    "transfer_entropy_regional_tensor = torch.rand([4, 4])\n",
    "spectral_entropy_tensor = torch.rand([1, 1, 32, 1])\n",
    "spectral_centroids_tensor = torch.rand([1, 1, 32, 1])\n",
    "freq_max_power_tensor = torch.rand([1, 1, 32, 1])\n",
    "spectral_edge_freqs_tensor = torch.rand([1, 1, 32, 1])\n",
    "\n",
    "target_shape = [1, 1, 32, 32]\n",
    "\n",
    "# List of all your tensors (use your actual tensors here)\n",
    "all_tensors = [\n",
    "    arnold_tongues_rotation_numbers_tensor,\n",
    "    dspm_tensor,\n",
    "    higuchi_fractal_dimensions_tensor,\n",
    "    Hurst_tensor,\n",
    "    mfdfa_concatd_tensor,\n",
    "    mfdfa_tensor,\n",
    "    short_time_fourier_transform_tensor,\n",
    "    transfer_entropy_granular_tensor,\n",
    "    transfer_entropy_hemispheric_avg_input_tensor,\n",
    "    transfer_entropy_regional_tensor,\n",
    "    spectral_entropy_tensor,\n",
    "    spectral_centroids_tensor,\n",
    "    freq_max_power_tensor,\n",
    "    spectral_edge_freqs_tensor,\n",
    "]\n",
    "\n",
    "# Preprocess all tensors\n",
    "processed_tensors = [preprocess_and_resize_tensor(tensor, target_shape) for tensor in all_tensors]\n",
    "\n",
    "# Print out the new shapes\n",
    "for i, tensor in enumerate(processed_tensors):\n",
    "    print(f\"Processed tensor {i+1} shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc365929-f568-48e7-af9e-aa60e517f841",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ff40f7-f03a-43bc-9afb-1ce7afd9f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tensor_names = [\n",
    "    'arnold_tongues_rotation_numbers_tensor',\n",
    "    'dspm_tensor',\n",
    "    'higuchi_fractal_dimensions_tensor',\n",
    "    'Hurst_tensor',\n",
    "    'mfdfa_concatd_tensor',\n",
    "    'mfdfa_tensor',\n",
    "    'short_time_fourier_transform_tensor',\n",
    "    'transfer_entropy_granular_tensor',\n",
    "    'transfer_entropy_hemispheric_avg_input_tensor',\n",
    "    'transfer_entropy_regional_tensor',\n",
    "    'spectral_entropy_tensor',\n",
    "    'spectral_centroids_tensor',\n",
    "    'freq_max_power_tensor',\n",
    "    'spectral_edge_freqs_tensor',\n",
    "]\n",
    "\n",
    "class BaseEmbeddingNet(nn.Module):\n",
    "    def __init__(self, input_channels, conv_output_channels, reduce_to_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, conv_output_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(conv_output_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_reduce = nn.Linear(conv_output_channels, reduce_to_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_reduce(x)\n",
    "        return x\n",
    "\n",
    "processed_tensors_dict = {name: tensor for name, tensor in zip(tensor_names, processed_tensors)}\n",
    "\n",
    "net_params = {name: {'input_channels': 1, 'conv_output_channels': 16, 'reduce_to_dim': 8} \n",
    "              for name in processed_tensors_dict.keys()}\n",
    "\n",
    "# Create BaseEmbeddingNets for each tensor\n",
    "embedding_nets = {name: BaseEmbeddingNet(**params) for name, params in net_params.items()}\n",
    "\n",
    "# Move networks to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for net in embedding_nets.values():\n",
    "    net.to(device)\n",
    "\n",
    "# Create custom dataset and dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = tensors\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __len__(self):\n",
    "        first_tensor = next(iter(self.tensors.values()))\n",
    "        return first_tensor.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        result = {}\n",
    "        for key, val in self.tensors.items():\n",
    "            if val.shape[0] > idx:\n",
    "                result[key] = val[idx]\n",
    "        return result\n",
    "\n",
    "def custom_collate(batch):\n",
    "    collated_batch = {}\n",
    "    all_keys = set([key for item in batch for key in item.keys()])\n",
    "    \n",
    "    for key in all_keys:\n",
    "        collated_batch[key] = torch.stack([item[key] for item in batch if key in item.keys()], dim=0)\n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "# Use processed_tensors for your CustomDataset\n",
    "dataset = CustomDataset(processed_tensors_dict)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=custom_collate)\n",
    "\n",
    "# Collect feature embeddings\n",
    "all_features = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "    features_list = [net(batch[key].to(device, dtype=torch.float32)) for key, net in embedding_nets.items()]\n",
    "    concatenated_features = torch.cat(features_list, dim=1)\n",
    "    all_features.append(concatenated_features.cpu().detach())\n",
    "\n",
    "# Convert list to tensor\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "\n",
    "# Save the feature embeddings\n",
    "save_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto'\n",
    "torch.save(all_features, f'{save_path}/all_features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0ec40-2261-41fc-82e1-13d73334d058",
   "metadata": {},
   "source": [
    "# Train Test Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344ba273-0591-476a-aa5c-07fb403af45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load EEG data\n",
    "EEG_data = np.load('/home/vincent/AAA_projects/MVCS/Neuroscience/eeg_data_with_channels.npy', allow_pickle=True)\n",
    "EEG_tensor = torch.FloatTensor(EEG_data)  # Assumes EEG_data is a NumPy ndarray\n",
    "\n",
    "window_size = 100  # You can adjust this\n",
    "stride = 10  # This is also adjustable\n",
    "\n",
    "# Create windows\n",
    "def create_windows(data, window_size, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        windows.append(data[i:i+window_size])\n",
    "    return torch.stack(windows)\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "# Create windowed data\n",
    "train_data = create_windows(EEG_tensor[:int(0.7 * len(EEG_tensor))], window_size, stride)\n",
    "val_data = create_windows(EEG_tensor[int(0.7 * len(EEG_tensor)):int(0.85 * len(EEG_tensor))], window_size, stride)\n",
    "test_data = create_windows(EEG_tensor[int(0.85 * len(EEG_tensor)):], window_size, stride)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = EEGDataset(data=train_data)\n",
    "val_dataset = EEGDataset(data=val_data)\n",
    "test_dataset = EEGDataset(data=test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)  # Set shuffle=False for time series data\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88f8ba-0bd5-45f7-9a84-deae168e531d",
   "metadata": {},
   "source": [
    "# Kuramoto layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baefb207-c729-4e70-bfc3-9a3c3dfd476a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta shape:  torch.Size([6400, 32])\n",
      "PLV Matrix shape:  torch.Size([32, 32])\n",
      "Phase Diff Matrix shape:  torch.Size([1, 1, 1, 4227788])\n",
      "Theta shape:  torch.Size([6400, 32])\n",
      "PLV Matrix shape:  torch.Size([32, 32])\n",
      "Phase Diff Matrix shape:  torch.Size([1, 1, 1, 4227788])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniconda3/lib/python3.10/site-packages/torchdiffeq/_impl/misc.py:296: UserWarning: t is not on the same device as y0. Coercing to y0.device.\n",
      "  warnings.warn(\"t is not on the same device as y0. Coercing to y0.device.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_59630/2060336086.py\", line 93, in forward\n    theta = odeint(ode_func, x, time_points)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torchdiffeq/_impl/odeint.py\", line 77, in odeint\n    solution = solver.integrate(t)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torchdiffeq/_impl/solvers.py\", line 28, in integrate\n    self._before_integrate(t)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torchdiffeq/_impl/rk_common.py\", line 161, in _before_integrate\n    f0 = self.func(t[0], self.y0)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torchdiffeq/_impl/misc.py\", line 189, in forward\n    return self.base_func(t, y)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_59630/2060336086.py\", line 66, in forward\n    weighted_sin = self.plv_matrix * torch.sin(theta.unsqueeze(1) - theta.unsqueeze(2) - self.phase_diff_matrix)\nRuntimeError: The size of tensor a (32) must match the size of tensor b (4227788) at non-singleton dimension 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 130\u001b[0m\n\u001b[1;32m    128\u001b[0m kuramoto_features_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m--> 130\u001b[0m     theta, mean_coherence \u001b[38;5;241m=\u001b[39m \u001b[43mkuramoto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     kuramoto_features_list\u001b[38;5;241m.\u001b[39mappend(mean_coherence)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Save the combined features\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_59630/2060336086.py\", line 93, in forward\n    theta = odeint(ode_func, x, time_points)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torchdiffeq/_impl/odeint.py\", line 77, in odeint\n    solution = solver.integrate(t)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torchdiffeq/_impl/solvers.py\", line 28, in integrate\n    self._before_integrate(t)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torchdiffeq/_impl/rk_common.py\", line 161, in _before_integrate\n    f0 = self.func(t[0], self.y0)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torchdiffeq/_impl/misc.py\", line 189, in forward\n    return self.base_func(t, y)\n  File \"/home/vincent/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_59630/2060336086.py\", line 66, in forward\n    weighted_sin = self.plv_matrix * torch.sin(theta.unsqueeze(1) - theta.unsqueeze(2) - self.phase_diff_matrix)\nRuntimeError: The size of tensor a (32) must match the size of tensor b (4227788) at non-singleton dimension 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load EEG data\n",
    "EEG_data_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/EEG_tensor.pth\"\n",
    "EEG_data = torch.load(EEG_data_path).to(device)\n",
    "\n",
    "# Hilbert Transform for analytical signal\n",
    "analytic_signal = hilbert(EEG_data.cpu().numpy())\n",
    "phases = torch.tensor(np.angle(analytic_signal), dtype=torch.float32).to(device)\n",
    "\n",
    "# EEG channel names\n",
    "eeg_channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                     'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                     'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "# Broad regions and corresponding channels\n",
    "regions = {\n",
    "    \"frontal\": ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8'],\n",
    "    \"temporal\": ['T7', 'T8'],\n",
    "    \"parietal\": ['CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8'],\n",
    "    \"occipital\": ['O1', 'Oz', 'O2']\n",
    "}\n",
    "\n",
    "# Load PLV matrix\n",
    "plv_matrix = torch.tensor(np.load(\"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Phase Syncronization/plv_matrix.npy\"), dtype=torch.float32).to(device)\n",
    "\n",
    "# Compute_phase_diff_matrix function\n",
    "def compute_phase_diff_matrix(phases, num_channels):\n",
    "    # Assuming phases is shape (Time, Channels), and you want (Channels, Channels)\n",
    "    phase_diff_matrix = torch.mean(phases.unsqueeze(1) - phases.unsqueeze(0), dim=2)\n",
    "    return phase_diff_matrix\n",
    "\n",
    "\n",
    "# Precompute omega and phase_diff_matrix\n",
    "N = len(eeg_channel_names)\n",
    "omega = torch.mean(plv_matrix, dim=1).to(device)\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases, N).to(device)  # Added N as the second argument\n",
    "\n",
    "# Modify the Kuramoto function to use PyTorch functions instead of NumPy\n",
    "def kuramoto_weighted_bias(t, y, omega, K):\n",
    "    weighted_sin = plv_matrix * torch.sin(y - y[:, None] - phase_diff_matrix)\n",
    "    dydt = omega + K / N * torch.sum(weighted_sin, axis=1)\n",
    "    return dydt\n",
    "\n",
    "class KuramotoODEFunc(nn.Module):\n",
    "    def __init__(self, omega, K, plv_matrix, phase_diff_matrix):\n",
    "        super(KuramotoODEFunc, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.K = K\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "    def forward(self, t, theta):\n",
    "        # Reshape to accommodate the additional time dimension.\n",
    "        theta = theta.view(-1, theta.shape[-1])\n",
    "        N = theta.shape[1]\n",
    "        print(\"Theta shape: \", theta.shape)\n",
    "        print(\"PLV Matrix shape: \", self.plv_matrix.shape)\n",
    "        print(\"Phase Diff Matrix shape: \", self.phase_diff_matrix.shape)\n",
    "        weighted_sin = self.plv_matrix * torch.sin(theta.unsqueeze(1) - theta.unsqueeze(2) - self.phase_diff_matrix)\n",
    "        dtheta = self.omega + (self.K / N) * torch.sum(weighted_sin, dim=1)\n",
    "        return dtheta.view(theta.shape)\n",
    "\n",
    "class KuramotoLayer(nn.Module):\n",
    "    def __init__(self, oscillator_count, time_steps, dt=0.01, plv_matrix=None, phase_diff_matrix=None):\n",
    "        super(KuramotoLayer, self).__init__()\n",
    "        self.oscillator_count = oscillator_count\n",
    "        self.time_steps = time_steps\n",
    "        self.dt = dt\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "        \n",
    "        # Initialize omega based on PLV if available\n",
    "        if plv_matrix is not None:\n",
    "            omega_init = torch.mean(plv_matrix, dim=1)  \n",
    "            self.omega = nn.Parameter(omega_init)\n",
    "\n",
    "        else:\n",
    "            self.omega = nn.Parameter(torch.randn(oscillator_count))\n",
    "        \n",
    "        self.K = nn.Parameter(torch.tensor(1.0))  # global coupling strength\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device) \n",
    "        ode_func = KuramotoODEFunc(self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        time_points = torch.arange(0, self.time_steps * self.dt, self.dt)\n",
    "        theta = odeint(ode_func, x, time_points)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mean_coherence(theta):\n",
    "        # Calculate phase coherence across oscillators\n",
    "        N, _, _ = theta.shape\n",
    "        mean_coherence = torch.mean(torch.cos(theta[:, -1, :] - theta[:, -1, :].mean(dim=1).unsqueeze(1)))\n",
    "        return mean_coherence\n",
    "\n",
    "# Number of channels\n",
    "N = len(eeg_channel_names)\n",
    "\n",
    "# Make sure all tensors are on the correct device\n",
    "phases = phases.to(device)\n",
    "\n",
    "# Compute natural frequencies and phase differences just once\n",
    "omega = torch.mean(plv_matrix, dim=1).to(device)\n",
    "num_channels = len(eeg_channel_names)  # Get the number of channels\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases, num_channels).to(device)\n",
    "\n",
    "# Initialize model and move to device\n",
    "kuramoto_model = KuramotoLayer(N, 100, plv_matrix=plv_matrix, phase_diff_matrix=phase_diff_matrix).to(device)\n",
    "\n",
    "# Data Parallelism for multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    kuramoto_model = nn.DataParallel(kuramoto_model)\n",
    "\n",
    "# Placeholder for training data\n",
    "train_data = create_windows(EEG_tensor[:int(0.7 * len(EEG_tensor))], window_size, stride)\n",
    "train_dataset = EEGDataset(data=train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Extract Kuramoto features\n",
    "kuramoto_features_list = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    theta, mean_coherence = kuramoto_model(batch.to(device))\n",
    "    kuramoto_features_list.append(mean_coherence)\n",
    "\n",
    "# Save the combined features\n",
    "kuramoto_features_tensor = torch.stack(kuramoto_features_list)\n",
    "all_features = torch.load('/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt')\n",
    "combined_features = torch.cat([all_features, kuramoto_features_tensor.unsqueeze(1)], dim=1)\n",
    "torch.save(combined_features, '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/combined_features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8dc5e-228b-45c0-b743-8d2d47a713ba",
   "metadata": {},
   "source": [
    "# Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770cc1e-fd3f-48fd-88e6-727cc87e212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.pos_encoder = nn.Embedding(4227788, d_model)  # Adjust this based on your data\n",
    "        self.position = torch.arange(0, 4227788, dtype=torch.long).unsqueeze(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pos_encoder(self.position[:x.size(0), :])\n",
    "        x = x + pos_encoding\n",
    "        return self.transformer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982d702-9951-408b-9e74-665019135379",
   "metadata": {},
   "source": [
    "# RNN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6ee57-1c00-42bf-8e6d-a7cf9650efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(RNNBlock, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_size, 32)  # 32 channels of EEG data\n",
    "        \n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        return self.fc_out(rnn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03410b62-483c-4734-aa81-c0f209e867a6",
   "metadata": {},
   "source": [
    "# Complete EEG Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3a4cb-5393-4695-a3cc-fc5604ecab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteEEGPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward):\n",
    "        super(CompleteEEGPredictor, self).__init__()\n",
    "        \n",
    "        # Initialize all feature embedding networks\n",
    "        self.eeg_embedding = EEGEmbeddingNet()\n",
    "        self.rotation_embedding = RotationEmbeddingNet()\n",
    "        self.band_power_embedding = BandPowerEmbeddingNet()\n",
    "        self.dspm_embedding = DSPMEmbeddingNet()\n",
    "        self.fast_fourier_embedding = FastFourierEmbeddingNet()\n",
    "        self.higuchi_fractal_embedding = HiguchiFractalEmbeddingNet()\n",
    "        self.hurst_embedding = HurstEmbeddingNet()\n",
    "        self.mfdfa_concatd_embedding = MFDFAConcatdEmbeddingNet()\n",
    "        self.mfdfa_embedding = MFDFAEmbeddingNet()\n",
    "        self.short_time_fourier_embedding = ShortTimeFourierEmbeddingNet()\n",
    "        self.spectral_entropy_embedding = SpectralEntropyEmbeddingNet()\n",
    "        self.spectral_centroids_embedding = SpectralCentroidsEmbeddingNet()\n",
    "        self.freq_max_power_embedding = FreqMaxPowerEmbeddingNet()\n",
    "        self.spectral_edge_freqs_embedding = SpectralEdgeFreqsEmbeddingNet()\n",
    "        self.pairwise_measure_net = PairwiseMeasureNet(input_channels=32, output_channels=64)\n",
    "        \n",
    "        # Initialize Kuramoto layer\n",
    "        self.kuramoto = KuramotoLayer(oscillator_count=32, time_steps=100)\n",
    "        \n",
    "        # Initialize Transformer block\n",
    "        self.transformer_block = TransformerBlock(d_model, nhead, num_encoder_layers, dim_feedforward)\n",
    "        \n",
    "        # Initialize RNN block\n",
    "        self.rnn_block = RNNBlock(input_size=d_model, hidden_size=256, num_layers=2, dropout=0.5)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # Feature extraction using all the embedding networks\n",
    "        embeddings = [\n",
    "            self.eeg_embedding(src),\n",
    "            self.rotation_embedding(src),\n",
    "            self.band_power_embedding(src),\n",
    "            self.dspm_embedding(src),\n",
    "            self.fast_fourier_embedding(src),\n",
    "            self.higuchi_fractal_embedding(src),\n",
    "            self.hurst_embedding(src),\n",
    "            self.mfdfa_concatd_embedding(src),\n",
    "            self.mfdfa_embedding(src),\n",
    "            self.short_time_fourier_embedding(src),\n",
    "            self.spectral_entropy_embedding(src),\n",
    "            self.spectral_centroids_embedding(src),\n",
    "            self.freq_max_power_embedding(src),\n",
    "            self.spectral_edge_freqs_embedding(src),\n",
    "            self.pairwise_measure_net(src)\n",
    "        ]\n",
    "        \n",
    "        # Concatenating the embeddings\n",
    "        combined_features = torch.cat(embeddings, dim=-1)\n",
    "        \n",
    "        # Kuramoto layer\n",
    "        src_kuramoto = self.kuramoto(combined_features)\n",
    "        \n",
    "        # Transformer block\n",
    "        src_transformed = self.transformer_block(src_kuramoto)\n",
    "        \n",
    "        # RNN block\n",
    "        output = self.rnn_block(src_transformed)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6d9a9-557d-493b-a292-f0ed1e3531fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308a730-b820-4bdf-bada-51be5fadfd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808a72e-0026-4437-919d-089d256e655d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
