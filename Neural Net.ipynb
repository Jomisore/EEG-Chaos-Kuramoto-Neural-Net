{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516441f8-ccaa-478a-a7e3-6db52f2a74ef",
   "metadata": {},
   "source": [
    "# Load Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43be465b-9c39-4493-9313-db7eef868865",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arnold_tongues_rotation_numbers_tensor: torch.Size([32, 300, 300])\n",
      "dspm_tensor: torch.Size([19, 18840, 10])\n",
      "higuchi_fractal_dimensions_tensor: torch.Size([1, 1, 4, 8])\n",
      "Hurst_tensor: torch.Size([1, 1, 32, 1])\n",
      "mfdfa_concatd_tensor: torch.Size([32, 1, 30, 2])\n",
      "mfdfa_tensor: torch.Size([9, 32, 10, 1])\n",
      "short_time_fourier_transform_tensor: torch.Size([32, 1001, 4229])\n",
      "transfer_entropy_granular_tensor: torch.Size([4, 4])\n",
      "transfer_entropy_hemispheric_avg_input_tensor: torch.Size([92, 92])\n",
      "transfer_entropy_regional_tensor: torch.Size([4, 4])\n",
      "spectral_entropy_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_centroids_tensor: torch.Size([1, 1, 32, 1])\n",
      "freq_max_power_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_edge_freqs_tensor: torch.Size([1, 1, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/arnold_tongues_rotation_numbers_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/dspm_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/higuchi_fractal_dimensions_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/Hurst_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_concatd_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/short_time_fourier_transform_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_granular_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_hemispheric_avg_input_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_regional_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_entropy_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_centroids_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/freq_max_power_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_edge_freqs_tensor.pt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store the tensors and another for their shapes\n",
    "tensors = {}\n",
    "tensor_shapes = {}\n",
    "\n",
    "# Load the tensors into a dictionary and collect their shapes\n",
    "for path in tensor_paths:\n",
    "    tensor_name = path.split('/')[-1].replace('.pt', '').replace('.pth', '')\n",
    "\n",
    "    # Remove the 'h' from the end, if it exists\n",
    "    if tensor_name.endswith(\"h\"):\n",
    "        tensor_name = tensor_name[:-1]\n",
    "\n",
    "    # Load the tensor\n",
    "    data = torch.load(path)\n",
    "    tensors[tensor_name] = data\n",
    "\n",
    "    # Check the type of the loaded data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor_shapes[tensor_name] = data.shape\n",
    "    elif isinstance(data, dict):  # Likely a state_dict\n",
    "        tensor_shapes[tensor_name] = \"state_dict (model parameters)\"\n",
    "    else:\n",
    "        tensor_shapes[tensor_name] = \"unknown type\"\n",
    "\n",
    "# Print the shapes of all loaded tensors\n",
    "for name, shape in tensor_shapes.items():\n",
    "    print(f\"{name}: {shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7667071-ad74-4a9e-ab1b-f29354a5816c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Match dimensions, reshape, and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f60a69-caf6-47d1-a2c1-b71536f18f65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tensor 1 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 2 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 3 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 4 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 5 shape: torch.Size([32, 1, 32, 32])\n",
      "Processed tensor 6 shape: torch.Size([9, 1, 32, 32])\n",
      "Processed tensor 7 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 8 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 9 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 10 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 11 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 12 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 13 shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor 14 shape: torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_and_resize_tensor(tensor, target_shape):\n",
    "    # Add missing batch and channel dimensions\n",
    "    while len(tensor.shape) < 4:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Reduce the channel dimension to 1 by taking the mean along that axis\n",
    "    tensor = torch.mean(tensor, dim=1, keepdim=True)\n",
    "\n",
    "    # Normalize\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    if std != 0:\n",
    "        tensor = (tensor - mean) / std\n",
    "\n",
    "    # Reshape/resize to target_shape\n",
    "    tensor = F.interpolate(tensor, size=target_shape[2:], mode='bilinear', align_corners=True)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "arnold_tongues_rotation_numbers_tensor = torch.rand([32, 300, 300])\n",
    "dspm_tensor = torch.rand([19, 18840, 10])\n",
    "higuchi_fractal_dimensions_tensor = torch.rand([1, 1, 4, 8])\n",
    "Hurst_tensor = torch.rand([1, 1, 32, 1])\n",
    "mfdfa_concatd_tensor = torch.rand([32, 1, 30, 2])\n",
    "mfdfa_tensor = torch.rand([9, 32, 10, 1])\n",
    "short_time_fourier_transform_tensor = torch.rand([32, 1001, 4229])\n",
    "transfer_entropy_granular_tensor = torch.rand([4, 4])\n",
    "transfer_entropy_hemispheric_avg_input_tensor = torch.rand([92, 92])\n",
    "transfer_entropy_regional_tensor = torch.rand([4, 4])\n",
    "spectral_entropy_tensor = torch.rand([1, 1, 32, 1])\n",
    "spectral_centroids_tensor = torch.rand([1, 1, 32, 1])\n",
    "freq_max_power_tensor = torch.rand([1, 1, 32, 1])\n",
    "spectral_edge_freqs_tensor = torch.rand([1, 1, 32, 1])\n",
    "\n",
    "target_shape = [1, 1, 32, 32]\n",
    "\n",
    "# List of all your tensors \n",
    "all_tensors = [\n",
    "    arnold_tongues_rotation_numbers_tensor,\n",
    "    dspm_tensor,\n",
    "    higuchi_fractal_dimensions_tensor,\n",
    "    Hurst_tensor,\n",
    "    mfdfa_concatd_tensor,\n",
    "    mfdfa_tensor,\n",
    "    short_time_fourier_transform_tensor,\n",
    "    transfer_entropy_granular_tensor,\n",
    "    transfer_entropy_hemispheric_avg_input_tensor,\n",
    "    transfer_entropy_regional_tensor,\n",
    "    spectral_entropy_tensor,\n",
    "    spectral_centroids_tensor,\n",
    "    freq_max_power_tensor,\n",
    "    spectral_edge_freqs_tensor,\n",
    "]\n",
    "\n",
    "# Preprocess all tensors\n",
    "processed_tensors = [preprocess_and_resize_tensor(tensor, target_shape) for tensor in all_tensors]\n",
    "\n",
    "# Print out the new shapes\n",
    "for i, tensor in enumerate(processed_tensors):\n",
    "    print(f\"Processed tensor {i+1} shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc365929-f568-48e7-af9e-aa60e517f841",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ff40f7-f03a-43bc-9afb-1ce7afd9f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tensor_names = [\n",
    "    'arnold_tongues_rotation_numbers_tensor',\n",
    "    'dspm_tensor',\n",
    "    'higuchi_fractal_dimensions_tensor',\n",
    "    'Hurst_tensor',\n",
    "    'mfdfa_concatd_tensor',\n",
    "    'mfdfa_tensor',\n",
    "    'short_time_fourier_transform_tensor',\n",
    "    'transfer_entropy_granular_tensor',\n",
    "    'transfer_entropy_hemispheric_avg_input_tensor',\n",
    "    'transfer_entropy_regional_tensor',\n",
    "    'spectral_entropy_tensor',\n",
    "    'spectral_centroids_tensor',\n",
    "    'freq_max_power_tensor',\n",
    "    'spectral_edge_freqs_tensor',\n",
    "]\n",
    "\n",
    "class BaseEmbeddingNet(nn.Module):\n",
    "    def __init__(self, input_channels, conv_output_channels, reduce_to_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, conv_output_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(conv_output_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_reduce = nn.Linear(conv_output_channels, reduce_to_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_reduce(x)\n",
    "        return x\n",
    "\n",
    "processed_tensors_dict = {name: tensor for name, tensor in zip(tensor_names, processed_tensors)}\n",
    "\n",
    "net_params = {name: {'input_channels': 1, 'conv_output_channels': 16, 'reduce_to_dim': 8} \n",
    "              for name in processed_tensors_dict.keys()}\n",
    "\n",
    "# Create BaseEmbeddingNets for each tensor\n",
    "embedding_nets = {name: BaseEmbeddingNet(**params) for name, params in net_params.items()}\n",
    "\n",
    "# Move networks to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for net in embedding_nets.values():\n",
    "    net.to(device)\n",
    "\n",
    "# Create custom dataset and dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = tensors\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __len__(self):\n",
    "        first_tensor = next(iter(self.tensors.values()))\n",
    "        return first_tensor.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        result = {}\n",
    "        for key, val in self.tensors.items():\n",
    "            if val.shape[0] > idx:\n",
    "                result[key] = val[idx]\n",
    "        return result\n",
    "\n",
    "def custom_collate(batch):\n",
    "    collated_batch = {}\n",
    "    all_keys = set([key for item in batch for key in item.keys()])\n",
    "    \n",
    "    for key in all_keys:\n",
    "        collated_batch[key] = torch.stack([item[key] for item in batch if key in item.keys()], dim=0)\n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "# Use processed_tensors for your CustomDataset\n",
    "dataset = CustomDataset(processed_tensors_dict)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=custom_collate)\n",
    "\n",
    "# Collect feature embeddings\n",
    "all_features = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "    features_list = [net(batch[key].to(device, dtype=torch.float32)) for key, net in embedding_nets.items()]\n",
    "    concatenated_features = torch.cat(features_list, dim=1)\n",
    "    all_features.append(concatenated_features.cpu().detach())\n",
    "\n",
    "# Convert list to tensor\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "\n",
    "# Save the feature embeddings\n",
    "save_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto'\n",
    "torch.save(all_features, f'{save_path}/all_features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56cafa-30b8-415d-823c-fafd212eecfa",
   "metadata": {},
   "source": [
    "# Kuramoto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b58e085-dc2d-4f47-91b1-f2709000a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344ba273-0591-476a-aa5c-07fb403af45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484061c8-7040-4833-a73c-c9d3974b7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd4ba5d-a115-4eb9-beb4-98d6f37c95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint\n",
    "from torch.cuda.amp import autocast, GradScaler  # Importing the AMP utilities\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "EEG_data = np.load('/home/vincent/AAA_projects/MVCS/Neuroscience/eeg_data_with_channels.npy', allow_pickle=True)\n",
    "EEG_tensor = torch.FloatTensor(EEG_data)  # Assumes EEG_data is a NumPy ndarray\n",
    "\n",
    "# Function to create windows for time-series data\n",
    "def create_windows(data, window_size, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        windows.append(data[i:i+window_size])\n",
    "    return torch.stack(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040d257f-994d-409b-9875-849ff643ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "window_size = 50\n",
    "stride = 10\n",
    "\n",
    "# Add necessary transformations here to EEG_tensor if required\n",
    "EEG_tensor = EEG_tensor.clone().detach().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174b3288-f313-4b35-b037-e6eec6892a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hilbert_in_batches(data, batch_size):\n",
    "    n_batches = int(np.ceil(data.shape[0] / batch_size))\n",
    "    analytic_signal = np.zeros_like(data, dtype=np.complex64)  # change dtype as needed\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        analytic_signal[start_idx:end_idx, :] = hilbert(data[start_idx:end_idx, :])\n",
    "        \n",
    "    return analytic_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f0a84dd-6970-4888-8168-7489406b22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # Set as appropriate\n",
    "\n",
    "# Apply Hilbert transform in batches\n",
    "EEG_numpy = EEG_tensor.cpu().numpy()\n",
    "analytic_signal_batches = apply_hilbert_in_batches(EEG_numpy, batch_size)\n",
    "\n",
    "# Convert the angle to phases and move to GPU\n",
    "phases = torch.tensor(np.angle(analytic_signal_batches), dtype=torch.float16).to(device)\n",
    "\n",
    "# Load PLV matrix\n",
    "plv_matrix_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Phase Syncronization/plv_matrix.npy\"\n",
    "plv_matrix = torch.tensor(np.load(plv_matrix_path), dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b34c43-dc25-45be-b0d0-2fd9bfbdfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute_phase_diff_matrix function\n",
    "def compute_phase_diff_matrix(phases):\n",
    "    time, channels = phases.shape[:2]\n",
    "    phase_diff_matrix = torch.zeros(channels, channels, device=phases.device)\n",
    "    for i in range(channels):\n",
    "        for j in range(channels):\n",
    "            phase_diff_matrix[i, j] = torch.mean(phases[:, i] - phases[:, j])\n",
    "    return phase_diff_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed965961-347e-4c66-821a-9737b8b9262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)\n",
    "\n",
    "# EEG channel names\n",
    "eeg_channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                     'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                     'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "# Broad regions and corresponding channels\n",
    "regions = {\n",
    "    \"frontal\": ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8'],\n",
    "    \"temporal\": ['T7', 'T8'],\n",
    "    \"parietal\": ['CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8'],\n",
    "    \"occipital\": ['O1', 'Oz', 'O2']\n",
    "}\n",
    "\n",
    "# Precompute omega and phase_diff_matrix\n",
    "N = len(eeg_channel_names)\n",
    "omega = torch.mean(plv_matrix, dim=1).to(device)\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73de9cf1-96b2-4d6a-9e16-cf3211bbf35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the Kuramoto function to use PyTorch functions instead of NumPy\n",
    "def kuramoto_weighted_bias(t, y, omega, K):\n",
    "    weighted_sin = plv_matrix * torch.sin(y - y[:, None] - phase_diff_matrix)\n",
    "    dydt = omega + K / N * torch.sum(weighted_sin, axis=1)\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a59c75e-edd9-45f0-ad11-bd3ebb70922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuramotoODEFunc(nn.Module):\n",
    "    def __init__(self, omega, K, plv_matrix, phase_diff_matrix):\n",
    "        super(KuramotoODEFunc, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.K = K\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "    def forward(self, t, theta):\n",
    "        # Reshape to accommodate the additional time dimension.\n",
    "        theta = theta.view(-1, theta.shape[-1])\n",
    "        N = theta.shape[1]\n",
    "    \n",
    "        # Compute the phase differences without unsqueezing\n",
    "        theta_diff = theta[:, :, None] - theta[:, None, :]\n",
    "        phase_diff_with_matrix = theta_diff - self.phase_diff_matrix\n",
    "    \n",
    "        # Compute the weighted sine values\n",
    "        weighted_sin = self.plv_matrix * torch.sin(phase_diff_with_matrix)\n",
    "    \n",
    "        dtheta = self.omega + (self.K / N) * torch.sum(weighted_sin, dim=1)\n",
    "    \n",
    "        return dtheta.view(theta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8756d6-b965-433b-93e1-4d5660ffcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuramotoLayer(nn.Module):\n",
    "    def __init__(self, oscillator_count, time_steps, dt=0.01, plv_matrix=None, phase_diff_matrix=None):\n",
    "        super(KuramotoLayer, self).__init__()\n",
    "        self.oscillator_count = oscillator_count\n",
    "        self.time_steps = time_steps\n",
    "        self.dt = dt\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "        if plv_matrix is not None:\n",
    "            omega_init = torch.mean(plv_matrix, dim=1)\n",
    "            self.omega = nn.Parameter(omega_init, requires_grad=True)\n",
    "        else:\n",
    "            self.omega = nn.Parameter(torch.randn(oscillator_count), requires_grad=True)\n",
    "\n",
    "        self.K = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "\n",
    "    def custom_forward(self, *inputs):\n",
    "        initial_shape = inputs[0].shape  # Store the initial shape\n",
    "    \n",
    "        # Flatten the batch and time dimensions\n",
    "        inputs_flattened = inputs[0].reshape(-1, initial_shape[-1])\n",
    "        \n",
    "        ode_func = KuramotoODEFunc(self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        time_points = torch.arange(0, 10000 * self.dt, self.dt).to(device)  # Assume device is defined elsewhere\n",
    "        theta_flattened = odeint(ode_func, inputs_flattened, time_points, method='bosh3', rtol=1e-6, atol=1e-8)\n",
    "\n",
    "        # Reshape theta to its original shape\n",
    "        theta = theta_flattened.reshape(*initial_shape, -1)  # -1 will automatically compute the required size\n",
    "        return theta\n",
    "        \n",
    "    def forward(self, theta):\n",
    "        device = theta.device\n",
    "        self.plv_matrix = self.plv_matrix.to(device)\n",
    "        self.phase_diff_matrix = self.phase_diff_matrix.to(device)\n",
    "        theta = checkpoint(self.custom_forward, theta, self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        theta = theta.to(torch.float16)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    def forward_with_checkpoint(self, x):\n",
    "        x = x.to(device)\n",
    "        theta = checkpoint(self.custom_forward, x)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mean_coherence(theta):\n",
    "        N, _, _, _ = theta.shape\n",
    "        mean_coherence = torch.mean(torch.cos(theta[:, -1, :] - theta[:, -1, :].mean(dim=1).unsqueeze(1)))\n",
    "        return mean_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceaef628-97ee-44c6-9da2-3ae5655b9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all tensors are on the correct device\n",
    "phases = phases.to(device)\n",
    "\n",
    "# Compute natural frequencies and phase differences just once\n",
    "num_channels = len(eeg_channel_names)  # Get the number of channels\n",
    "#print(\"Theta shape: \", theta.shape)\n",
    "#print(\"Theta Unsqueeze(1) shape: \", theta.unsqueeze(1).shape)\n",
    "#print(\"Theta Unsqueeze(2) shape: \", theta.unsqueeze(2).shape)\n",
    "\n",
    "# Number of channels\n",
    "N = len(eeg_channel_names)\n",
    "\n",
    "# Initialize model and move to device\n",
    "kuramoto_model = KuramotoLayer(N, 12800, plv_matrix=plv_matrix, phase_diff_matrix=phase_diff_matrix).to(dtype=torch.float16).to(device)\n",
    "\n",
    "# Data Parallelism for multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    kuramoto_model = nn.DataParallel(kuramoto_model)\n",
    "\n",
    "scaler = GradScaler()\n",
    "train_data = create_windows(EEG_tensor[:int(0.7 * len(EEG_tensor))], window_size, stride).detach().requires_grad_(True)\n",
    "train_dataset = EEGDataset(data=train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833459c-3640-4db4-a658-65ef6d125254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop and feature extraction\n",
    "kuramoto_features_list = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    # Moves batch to device and changes dtype to float16\n",
    "    batch = batch.to(device, dtype=torch.float16)\n",
    "\n",
    "    # Using autocast for the forward pass\n",
    "    with autocast():\n",
    "        theta, mean_coherence = kuramoto_model(batch)\n",
    "\n",
    "    kuramoto_features_list.append(mean_coherence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd979502-d47a-4338-8168-67f629d3a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined features\n",
    "kuramoto_features_tensor = torch.stack(kuramoto_features_list)\n",
    "all_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt'\n",
    "all_features = torch.load(all_features_path)\n",
    "combined_features = torch.cat([all_features, kuramoto_features_tensor.unsqueeze(1)], dim=1)\n",
    "combined_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/combined_features.pt'\n",
    "torch.save(combined_features, combined_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636c6bc-9261-471f-b690-7077137c8bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d843f16-33c5-41a2-a5bb-23aeebd33c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of EEG_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\",\n",
    "]\n",
    "\n",
    "# Load tensors and print their shapes\n",
    "loaded_tensors = {}\n",
    "for path in tensor_paths:\n",
    "    tensor_name = path.split(\"/\")[-1].replace(\".pth\", \"\")\n",
    "    tensor = torch.load(path)\n",
    "    print(f\"Shape of {tensor_name}: {tensor.shape}\")\n",
    "    loaded_tensors[tensor_name] = tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8dc5e-228b-45c0-b743-8d2d47a713ba",
   "metadata": {},
   "source": [
    "# Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f770cc1e-fd3f-48fd-88e6-727cc87e212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squeezed shape: torch.Size([32, 4227788])\n",
      "Permuted shape: torch.Size([4227788, 32])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n",
      "Batch output: torch.Size([500, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "dim_feedforward = 512\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 500  # truncated sequence length\n",
    "feature_dim = 32\n",
    "\n",
    "class EEGPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, sequence_length):\n",
    "        super(EEGPredictor, self).__init__()\n",
    "        self.feature_transform = nn.Linear(feature_dim, d_model)  # Transform from 32 to d_model\n",
    "        self.transformer_block = TransformerBlock(d_model, nhead, num_layers, dim_feedforward, seq_len)\n",
    "        self.prediction_head = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_transform(x)  # Add this line\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.prediction_head(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, sequence_length):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.pos_encoder = nn.Embedding(seq_len, d_model)\n",
    "        self.position = torch.arange(0, seq_len, dtype=torch.long).unsqueeze(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pos_encoder(self.position[:x.size(0), :])\n",
    "        x = x + pos_encoding\n",
    "        return self.transformer(x)\n",
    "redictor(d_model, nhead, num_layers, dim_feedforward, seq_len)\n",
    "\n",
    "# Prepare the EEG tensor, removing singleton dimensions\n",
    "eeg_tensor = loaded_tensors[\"EEG_tensor\"].squeeze()  # Likely becomes [32, 4227788]\n",
    "print(\"Squeezed shape:\", eeg_tensor.shape)\n",
    "\n",
    "# Since the tensor is 2D, permute using only two dimensions\n",
    "eeg_tensor = eeg_tensor.permute(1, 0)  # This would swap the dimensions making it [4227788, 32]\n",
    "print(\"Permuted shape:\", eeg_tensor.shape)\n",
    "\n",
    "num_batches = eeg_tensor.shape[0] // (batch_size * seq_len)\n",
    "\n",
    "# To store the transformer outputs\n",
    "transformer_outputs = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size * seq_len\n",
    "    end_idx = start_idx + (batch_size * seq_len)\n",
    "    \n",
    "    # Extract batch and reshape to [seq_len, batch_size, feature_dim]\n",
    "    batch = eeg_tensor[start_idx:end_idx, :].reshape(seq_len, batch_size, feature_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(batch)\n",
    "    print(\"Batch output:\", output.shape)\n",
    "    \n",
    "    # Save the transformer output for use in RNN\n",
    "    transformer_outputs.append(output.detach())\n",
    "\n",
    "# Convert list of outputs to a tensor (or keep as a list if that's easier for your application)\n",
    "transformer_outputs = torch.stack(transformer_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982d702-9951-408b-9e74-665019135379",
   "metadata": {},
   "source": [
    "# RNN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aba6ee57-1c00-42bf-8e6d-a7cf9650efb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m rnn_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m     19\u001b[0m rnn_num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerModel\u001b[49m(d_model, nhead, num_layers, dim_feedforward)\n\u001b[1;32m     22\u001b[0m rnn_model \u001b[38;5;241m=\u001b[39m RNNModel(d_model, rnn_hidden_size, rnn_num_layers)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Forward pass through Transformer\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Define an RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_input_size, rnn_hidden_size, rnn_num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=rnn_input_size, hidden_size=rnn_hidden_size, num_layers=rnn_num_layers)\n",
    "        self.prediction_head = nn.Linear(rnn_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.prediction_head(x[-1])\n",
    "        return x\n",
    "\n",
    "# Initialize the RNN model\n",
    "rnn_model = RNNModel(d_model, rnn_hidden_size=64, rnn_num_layers=1)\n",
    "\n",
    "# Use the transformer_outputs as input for the RNN model\n",
    "# You might need to reshape transformer_outputs to fit the expected input shape of the RNN\n",
    "rnn_output = rnn_model(transformer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03410b62-483c-4734-aa81-c0f209e867a6",
   "metadata": {},
   "source": [
    "# Complete EEG Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3a4cb-5393-4695-a3cc-fc5604ecab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteEEGPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward):\n",
    "        super(CompleteEEGPredictor, self).__init__()\n",
    "        \n",
    "        # Initialize all feature embedding networks\n",
    "        self.eeg_embedding = EEGEmbeddingNet()\n",
    "        self.rotation_embedding = RotationEmbeddingNet()\n",
    "        self.band_power_embedding = BandPowerEmbeddingNet()\n",
    "        self.dspm_embedding = DSPMEmbeddingNet()\n",
    "        self.fast_fourier_embedding = FastFourierEmbeddingNet()\n",
    "        self.higuchi_fractal_embedding = HiguchiFractalEmbeddingNet()\n",
    "        self.hurst_embedding = HurstEmbeddingNet()\n",
    "        self.mfdfa_concatd_embedding = MFDFAConcatdEmbeddingNet()\n",
    "        self.mfdfa_embedding = MFDFAEmbeddingNet()\n",
    "        self.short_time_fourier_embedding = ShortTimeFourierEmbeddingNet()\n",
    "        self.spectral_entropy_embedding = SpectralEntropyEmbeddingNet()\n",
    "        self.spectral_centroids_embedding = SpectralCentroidsEmbeddingNet()\n",
    "        self.freq_max_power_embedding = FreqMaxPowerEmbeddingNet()\n",
    "        self.spectral_edge_freqs_embedding = SpectralEdgeFreqsEmbeddingNet()\n",
    "        self.pairwise_measure_net = PairwiseMeasureNet(input_channels=32, output_channels=64)\n",
    "        \n",
    "        # Initialize Kuramoto layer\n",
    "        self.kuramoto = KuramotoLayer(oscillator_count=32, time_steps=100)\n",
    "        \n",
    "        # Initialize Transformer block\n",
    "        self.transformer_block = TransformerBlock(d_model, nhead, num_encoder_layers, dim_feedforward)\n",
    "        \n",
    "        # Initialize RNN block\n",
    "        self.rnn_block = RNNBlock(input_size=d_model, hidden_size=256, num_layers=2, dropout=0.5)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # Feature extraction using all the embedding networks\n",
    "        embeddings = [\n",
    "            self.eeg_embedding(src),\n",
    "            self.rotation_embedding(src),\n",
    "            self.band_power_embedding(src),\n",
    "            self.dspm_embedding(src),\n",
    "            self.fast_fourier_embedding(src),\n",
    "            self.higuchi_fractal_embedding(src),\n",
    "            self.hurst_embedding(src),\n",
    "            self.mfdfa_concatd_embedding(src),\n",
    "            self.mfdfa_embedding(src),\n",
    "            self.short_time_fourier_embedding(src),\n",
    "            self.spectral_entropy_embedding(src),\n",
    "            self.spectral_centroids_embedding(src),\n",
    "            self.freq_max_power_embedding(src),\n",
    "            self.spectral_edge_freqs_embedding(src),\n",
    "            self.pairwise_measure_net(src)\n",
    "        ]\n",
    "        \n",
    "        # Concatenating the embeddings\n",
    "        combined_features = torch.cat(embeddings, dim=-1)\n",
    "        \n",
    "        # Kuramoto layer\n",
    "        src_kuramoto = self.kuramoto(combined_features)\n",
    "        \n",
    "        # Transformer block\n",
    "        src_transformed = self.transformer_block(src_kuramoto)\n",
    "        \n",
    "        # RNN block\n",
    "        output = self.rnn_block(src_transformed)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6d9a9-557d-493b-a292-f0ed1e3531fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308a730-b820-4bdf-bada-51be5fadfd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808a72e-0026-4437-919d-089d256e655d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
