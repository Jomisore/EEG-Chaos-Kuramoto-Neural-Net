{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516441f8-ccaa-478a-a7e3-6db52f2a74ef",
   "metadata": {},
   "source": [
    "# Load Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43be465b-9c39-4493-9313-db7eef868865",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arnold_tongues_rotation_numbers_tensor: torch.Size([32, 300, 300])\n",
      "dspm_tensor: torch.Size([19, 18840, 10])\n",
      "higuchi_fractal_dimensions_tensor: torch.Size([1, 1, 4, 8])\n",
      "Hurst_tensor: torch.Size([1, 1, 32, 1])\n",
      "mfdfa_concatd_tensor: torch.Size([32, 1, 30, 2])\n",
      "mfdfa_tensor: torch.Size([9, 32, 10, 1])\n",
      "short_time_fourier_transform_tensor: torch.Size([32, 1001, 4229])\n",
      "transfer_entropy_granular_tensor: torch.Size([4, 4])\n",
      "transfer_entropy_hemispheric_avg_input_tensor: torch.Size([92, 92])\n",
      "transfer_entropy_regional_tensor: torch.Size([4, 4])\n",
      "spectral_entropy_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_centroids_tensor: torch.Size([1, 1, 32, 1])\n",
      "freq_max_power_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_edge_freqs_tensor: torch.Size([1, 1, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/arnold_tongues_rotation_numbers_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/dspm_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/higuchi_fractal_dimensions_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/Hurst_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_concatd_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/short_time_fourier_transform_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_granular_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_hemispheric_avg_input_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_regional_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_entropy_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_centroids_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/freq_max_power_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_edge_freqs_tensor.pt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store the tensors and another for their shapes\n",
    "tensors = {}\n",
    "tensor_shapes = {}\n",
    "\n",
    "# Load the tensors into a dictionary and collect their shapes\n",
    "for path in tensor_paths:\n",
    "    tensor_name = path.split('/')[-1].replace('.pt', '').replace('.pth', '')\n",
    "\n",
    "    # Remove the 'h' from the end, if it exists\n",
    "    if tensor_name.endswith(\"h\"):\n",
    "        tensor_name = tensor_name[:-1]\n",
    "\n",
    "    # Load the tensor\n",
    "    data = torch.load(path)\n",
    "    tensors[tensor_name] = data\n",
    "\n",
    "    # Check the type of the loaded data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor_shapes[tensor_name] = data.shape\n",
    "    elif isinstance(data, dict):  # Likely a state_dict\n",
    "        tensor_shapes[tensor_name] = \"state_dict (model parameters)\"\n",
    "    else:\n",
    "        tensor_shapes[tensor_name] = \"unknown type\"\n",
    "\n",
    "# Print the shapes of all loaded tensors\n",
    "for name, shape in tensor_shapes.items():\n",
    "    print(f\"{name}: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e8e026a-f6bb-4d54-8a3b-be607fa4f34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in arnold_tongues_rotation_numbers_tensor: 0.0%\n",
      "Percentage of Infs in arnold_tongues_rotation_numbers_tensor: 0.0%\n",
      "Percentage of NaNs in dspm_tensor: 0.0%\n",
      "Percentage of Infs in dspm_tensor: 0.0%\n",
      "Percentage of NaNs in higuchi_fractal_dimensions_tensor: 0.0%\n",
      "Percentage of Infs in higuchi_fractal_dimensions_tensor: 0.0%\n",
      "Percentage of NaNs in Hurst_tensor: 0.0%\n",
      "Percentage of Infs in Hurst_tensor: 0.0%\n",
      "Percentage of NaNs in mfdfa_concatd_tensor: 0.0%\n",
      "Percentage of Infs in mfdfa_concatd_tensor: 0.0%\n",
      "Percentage of NaNs in mfdfa_tensor: 0.0%\n",
      "Percentage of Infs in mfdfa_tensor: 0.0%\n",
      "Percentage of NaNs in short_time_fourier_transform_tensor: 0.0%\n",
      "Percentage of Infs in short_time_fourier_transform_tensor: 0.0%\n",
      "Percentage of NaNs in transfer_entropy_granular_tensor: 0.0%\n",
      "Percentage of Infs in transfer_entropy_granular_tensor: 0.0%\n",
      "Percentage of NaNs in transfer_entropy_hemispheric_avg_input_tensor: 0.0%\n",
      "Percentage of Infs in transfer_entropy_hemispheric_avg_input_tensor: 0.0%\n",
      "Percentage of NaNs in transfer_entropy_regional_tensor: 0.0%\n",
      "Percentage of Infs in transfer_entropy_regional_tensor: 0.0%\n",
      "Percentage of NaNs in spectral_entropy_tensor: 0.0%\n",
      "Percentage of Infs in spectral_entropy_tensor: 0.0%\n",
      "Percentage of NaNs in spectral_centroids_tensor: 0.0%\n",
      "Percentage of Infs in spectral_centroids_tensor: 0.0%\n",
      "Percentage of NaNs in freq_max_power_tensor: 0.0%\n",
      "Percentage of Infs in freq_max_power_tensor: 0.0%\n",
      "Percentage of NaNs in spectral_edge_freqs_tensor: 0.0%\n",
      "Percentage of Infs in spectral_edge_freqs_tensor: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Loop through the loaded tensors to check for NaNs and Infs\n",
    "for tensor_name, tensor_data in tensors.items():\n",
    "    # Only perform the checks if the data is a tensor\n",
    "    if isinstance(tensor_data, torch.Tensor):\n",
    "        total_elements = torch.numel(tensor_data)\n",
    "        \n",
    "        nans_count = torch.sum(torch.isnan(tensor_data)).item()\n",
    "        infs_count = torch.sum(torch.isinf(tensor_data)).item()\n",
    "        \n",
    "        nans_percentage = (nans_count / total_elements) * 100\n",
    "        infs_percentage = (infs_count / total_elements) * 100\n",
    "        \n",
    "        print(f\"Percentage of NaNs in {tensor_name}: {nans_percentage}%\")\n",
    "        print(f\"Percentage of Infs in {tensor_name}: {infs_percentage}%\")\n",
    "    else:\n",
    "        print(f\"{tensor_name} is not a tensor, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7667071-ad74-4a9e-ab1b-f29354a5816c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Match dimensions, reshape, and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f60a69-caf6-47d1-a2c1-b71536f18f65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tensor arnold_tongues_rotation_numbers_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor dspm_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor higuchi_fractal_dimensions_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor Hurst_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor mfdfa_concatd_tensor shape: torch.Size([32, 1, 32, 32])\n",
      "Processed tensor mfdfa_tensor shape: torch.Size([9, 1, 32, 32])\n",
      "Processed tensor short_time_fourier_transform_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor transfer_entropy_granular_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor transfer_entropy_hemispheric_avg_input_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor transfer_entropy_regional_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor spectral_entropy_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor spectral_centroids_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor freq_max_power_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor spectral_edge_freqs_tensor shape: torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_and_resize_tensor(tensor, target_shape):\n",
    "    # Add missing batch and channel dimensions\n",
    "    while len(tensor.shape) < 4:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Reduce the channel dimension to 1 by taking the mean along that axis\n",
    "    tensor = torch.mean(tensor, dim=1, keepdim=True)\n",
    "\n",
    "    # Normalize\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    if std != 0:\n",
    "        tensor = (tensor - mean) / std\n",
    "\n",
    "    # Reshape/resize to target_shape\n",
    "    tensor = F.interpolate(tensor, size=target_shape[2:], mode='bilinear', align_corners=True)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "target_shape = [1, 1, 32, 32]\n",
    "\n",
    "# List of all your tensors\n",
    "all_tensors = [tensors[key] for key in tensors]\n",
    "\n",
    "# Preprocess all tensors\n",
    "processed_tensors = [preprocess_and_resize_tensor(tensor, target_shape) for tensor in all_tensors]\n",
    "\n",
    "# Store preprocessed tensors back into the `tensors` dictionary\n",
    "for key, tensor in zip(tensors.keys(), processed_tensors):\n",
    "    tensors[key] = tensor\n",
    "\n",
    "# Print out the new shapes\n",
    "for key in tensors.keys():\n",
    "    print(f\"Processed tensor {key} shape: {tensors[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc365929-f568-48e7-af9e-aa60e517f841",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ff40f7-f03a-43bc-9afb-1ce7afd9f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tensor_names = [\n",
    "    'arnold_tongues_rotation_numbers_tensor',\n",
    "    'dspm_tensor',\n",
    "    'higuchi_fractal_dimensions_tensor',\n",
    "    'Hurst_tensor',\n",
    "    'mfdfa_concatd_tensor',\n",
    "    'mfdfa_tensor',\n",
    "    'short_time_fourier_transform_tensor',\n",
    "    'transfer_entropy_granular_tensor',\n",
    "    'transfer_entropy_hemispheric_avg_input_tensor',\n",
    "    'transfer_entropy_regional_tensor',\n",
    "    'spectral_entropy_tensor',\n",
    "    'spectral_centroids_tensor',\n",
    "    'freq_max_power_tensor',\n",
    "    'spectral_edge_freqs_tensor',\n",
    "]\n",
    "\n",
    "class BaseEmbeddingNet(nn.Module):\n",
    "    def __init__(self, input_channels, conv_output_channels, reduce_to_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, conv_output_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(conv_output_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_reduce = nn.Linear(conv_output_channels, reduce_to_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_reduce(x)\n",
    "        return x\n",
    "\n",
    "processed_tensors_dict = {name: tensor for name, tensor in zip(tensor_names, processed_tensors)}\n",
    "\n",
    "net_params = {name: {'input_channels': 1, 'conv_output_channels': 16, 'reduce_to_dim': 8} \n",
    "              for name in processed_tensors_dict.keys()}\n",
    "\n",
    "# Create BaseEmbeddingNets for each tensor\n",
    "embedding_nets = {name: BaseEmbeddingNet(**params) for name, params in net_params.items()}\n",
    "\n",
    "# Move networks to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for net in embedding_nets.values():\n",
    "    net.to(device)\n",
    "\n",
    "# Create custom dataset and dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = tensors\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __len__(self):\n",
    "        first_tensor = next(iter(self.tensors.values()))\n",
    "        return first_tensor.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        result = {}\n",
    "        for key, val in self.tensors.items():\n",
    "            if val.shape[0] > idx:\n",
    "                result[key] = val[idx]\n",
    "        return result\n",
    "\n",
    "def custom_collate(batch):\n",
    "    collated_batch = {}\n",
    "    all_keys = set([key for item in batch for key in item.keys()])\n",
    "    \n",
    "    for key in all_keys:\n",
    "        collated_batch[key] = torch.stack([item[key] for item in batch if key in item.keys()], dim=0)\n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "# Use processed_tensors for your CustomDataset\n",
    "dataset = CustomDataset(processed_tensors_dict)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=custom_collate)\n",
    "\n",
    "# Collect feature embeddings\n",
    "all_features = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "    features_list = [net(batch[key].to(device, dtype=torch.float32)) for key, net in embedding_nets.items()]\n",
    "    concatenated_features = torch.cat(features_list, dim=1)\n",
    "    all_features.append(concatenated_features.cpu().detach())\n",
    "\n",
    "# Convert list to tensor\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "\n",
    "# Save the feature embeddings\n",
    "save_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto'\n",
    "torch.save(all_features, f'{save_path}/all_features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56cafa-30b8-415d-823c-fafd212eecfa",
   "metadata": {},
   "source": [
    "# Kuramoto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b58e085-dc2d-4f47-91b1-f2709000a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484061c8-7040-4833-a73c-c9d3974b7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd4ba5d-a115-4eb9-beb4-98d6f37c95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint\n",
    "from torch.cuda.amp import autocast, GradScaler  # Importing the AMP utilities\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "EEG_data = np.load('/home/vincent/AAA_projects/MVCS/Neuroscience/eeg_data_with_channels.npy', allow_pickle=True)\n",
    "EEG_tensor = torch.FloatTensor(EEG_data)  # Assumes EEG_data is a NumPy ndarray\n",
    "\n",
    "# Function to create windows for time-series data\n",
    "def create_windows(data, window_size, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        windows.append(data[i:i+window_size])\n",
    "    return torch.stack(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040d257f-994d-409b-9875-849ff643ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "window_size = 50\n",
    "stride = 10\n",
    "\n",
    "# Add necessary transformations here to EEG_tensor if required\n",
    "EEG_tensor = EEG_tensor.clone().detach().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174b3288-f313-4b35-b037-e6eec6892a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hilbert_in_batches(data, batch_size):\n",
    "    n_batches = int(np.ceil(data.shape[0] / batch_size))\n",
    "    analytic_signal = np.zeros_like(data, dtype=np.complex64)  # change dtype as needed\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        analytic_signal[start_idx:end_idx, :] = hilbert(data[start_idx:end_idx, :])\n",
    "        \n",
    "    return analytic_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f0a84dd-6970-4888-8168-7489406b22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # Set as appropriate\n",
    "\n",
    "# Apply Hilbert transform in batches\n",
    "EEG_numpy = EEG_tensor.cpu().numpy()\n",
    "analytic_signal_batches = apply_hilbert_in_batches(EEG_numpy, batch_size)\n",
    "\n",
    "# Convert the angle to phases and move to GPU\n",
    "phases = torch.tensor(np.angle(analytic_signal_batches), dtype=torch.float16).to(device)\n",
    "\n",
    "# Load PLV matrix\n",
    "plv_matrix_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Phase Syncronization/plv_matrix.npy\"\n",
    "plv_matrix = torch.tensor(np.load(plv_matrix_path), dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b34c43-dc25-45be-b0d0-2fd9bfbdfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute_phase_diff_matrix function\n",
    "def compute_phase_diff_matrix(phases):\n",
    "    time, channels = phases.shape[:2]\n",
    "    phase_diff_matrix = torch.zeros(channels, channels, device=phases.device)\n",
    "    for i in range(channels):\n",
    "        for j in range(channels):\n",
    "            phase_diff_matrix[i, j] = torch.mean(phases[:, i] - phases[:, j])\n",
    "    return phase_diff_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed965961-347e-4c66-821a-9737b8b9262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)\n",
    "\n",
    "# EEG channel names\n",
    "eeg_channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                     'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                     'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "# Broad regions and corresponding channels\n",
    "regions = {\n",
    "    \"frontal\": ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8'],\n",
    "    \"temporal\": ['T7', 'T8'],\n",
    "    \"parietal\": ['CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8'],\n",
    "    \"occipital\": ['O1', 'Oz', 'O2']\n",
    "}\n",
    "\n",
    "# Precompute omega and phase_diff_matrix\n",
    "N = len(eeg_channel_names)\n",
    "omega = torch.mean(plv_matrix, dim=1).to(device)\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73de9cf1-96b2-4d6a-9e16-cf3211bbf35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the Kuramoto function to use PyTorch functions instead of NumPy\n",
    "def kuramoto_weighted_bias(t, y, omega, K):\n",
    "    weighted_sin = plv_matrix * torch.sin(y - y[:, None] - phase_diff_matrix)\n",
    "    dydt = omega + K / N * torch.sum(weighted_sin, axis=1)\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a59c75e-edd9-45f0-ad11-bd3ebb70922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuramotoODEFunc(nn.Module):\n",
    "    def __init__(self, omega, K, plv_matrix, phase_diff_matrix):\n",
    "        super(KuramotoODEFunc, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.K = K\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "    def forward(self, t, theta):\n",
    "        # Reshape to accommodate the additional time dimension.\n",
    "        theta = theta.view(-1, theta.shape[-1])\n",
    "        N = theta.shape[1]\n",
    "    \n",
    "        # Compute the phase differences without unsqueezing\n",
    "        theta_diff = theta[:, :, None] - theta[:, None, :]\n",
    "        phase_diff_with_matrix = theta_diff - self.phase_diff_matrix\n",
    "    \n",
    "        # Compute the weighted sine values\n",
    "        weighted_sin = self.plv_matrix * torch.sin(phase_diff_with_matrix)\n",
    "    \n",
    "        dtheta = self.omega + (self.K / N) * torch.sum(weighted_sin, dim=1)\n",
    "    \n",
    "        return dtheta.view(theta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8756d6-b965-433b-93e1-4d5660ffcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuramotoLayer(nn.Module):\n",
    "    def __init__(self, oscillator_count, time_steps, dt=0.01, plv_matrix=None, phase_diff_matrix=None):\n",
    "        super(KuramotoLayer, self).__init__()\n",
    "        self.oscillator_count = oscillator_count\n",
    "        self.time_steps = time_steps\n",
    "        self.dt = dt\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "        if plv_matrix is not None:\n",
    "            omega_init = torch.mean(plv_matrix, dim=1)\n",
    "            self.omega = nn.Parameter(omega_init, requires_grad=True)\n",
    "        else:\n",
    "            self.omega = nn.Parameter(torch.randn(oscillator_count), requires_grad=True)\n",
    "\n",
    "        self.K = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "\n",
    "    def custom_forward(self, *inputs):\n",
    "        initial_shape = inputs[0].shape  # Store the initial shape\n",
    "    \n",
    "        # Flatten the batch and time dimensions\n",
    "        inputs_flattened = inputs[0].reshape(-1, initial_shape[-1])\n",
    "        \n",
    "        ode_func = KuramotoODEFunc(self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        time_points = torch.arange(0, 10000 * self.dt, self.dt).to(device)  # Assume device is defined elsewhere\n",
    "        theta_flattened = odeint(ode_func, inputs_flattened, time_points, method='bosh3', rtol=1e-6, atol=1e-8)\n",
    "\n",
    "        # Reshape theta to its original shape\n",
    "        theta = theta_flattened.reshape(*initial_shape, -1)  # -1 will automatically compute the required size\n",
    "        return theta\n",
    "        \n",
    "    def forward(self, theta):\n",
    "        device = theta.device\n",
    "        self.plv_matrix = self.plv_matrix.to(device)\n",
    "        self.phase_diff_matrix = self.phase_diff_matrix.to(device)\n",
    "        theta = checkpoint(self.custom_forward, theta, self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        theta = theta.to(torch.float16)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    def forward_with_checkpoint(self, x):\n",
    "        x = x.to(device)\n",
    "        theta = checkpoint(self.custom_forward, x)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mean_coherence(theta):\n",
    "        N, _, _, _ = theta.shape\n",
    "        mean_coherence = torch.mean(torch.cos(theta[:, -1, :] - theta[:, -1, :].mean(dim=1).unsqueeze(1)))\n",
    "        return mean_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceaef628-97ee-44c6-9da2-3ae5655b9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all tensors are on the correct device\n",
    "phases = phases.to(device)\n",
    "\n",
    "# Compute natural frequencies and phase differences just once\n",
    "num_channels = len(eeg_channel_names)  # Get the number of channels\n",
    "#print(\"Theta shape: \", theta.shape)\n",
    "#print(\"Theta Unsqueeze(1) shape: \", theta.unsqueeze(1).shape)\n",
    "#print(\"Theta Unsqueeze(2) shape: \", theta.unsqueeze(2).shape)\n",
    "\n",
    "# Number of channels\n",
    "N = len(eeg_channel_names)\n",
    "\n",
    "# Initialize model and move to device\n",
    "kuramoto_model = KuramotoLayer(N, 12800, plv_matrix=plv_matrix, phase_diff_matrix=phase_diff_matrix).to(dtype=torch.float16).to(device)\n",
    "\n",
    "# Data Parallelism for multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    kuramoto_model = nn.DataParallel(kuramoto_model)\n",
    "\n",
    "scaler = GradScaler()\n",
    "train_data = create_windows(EEG_tensor[:int(0.7 * len(EEG_tensor))], window_size, stride).detach().requires_grad_(True)\n",
    "train_dataset = EEGDataset(data=train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833459c-3640-4db4-a658-65ef6d125254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop and feature extraction\n",
    "kuramoto_features_list = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    # Moves batch to device and changes dtype to float16\n",
    "    batch = batch.to(device, dtype=torch.float16)\n",
    "\n",
    "    # Using autocast for the forward pass\n",
    "    with autocast():\n",
    "        theta, mean_coherence = kuramoto_model(batch)\n",
    "\n",
    "    kuramoto_features_list.append(mean_coherence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd979502-d47a-4338-8168-67f629d3a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined features\n",
    "kuramoto_features_tensor = torch.stack(kuramoto_features_list)\n",
    "all_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt'\n",
    "all_features = torch.load(all_features_path)\n",
    "combined_features = torch.cat([all_features, kuramoto_features_tensor.unsqueeze(1)], dim=1)\n",
    "combined_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/combined_features.pt'\n",
    "torch.save(combined_features, combined_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636c6bc-9261-471f-b690-7077137c8bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f8dc5e-228b-45c0-b743-8d2d47a713ba",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e47c39-9fd7-4309-b2e8-87942b5a7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d843f16-33c5-41a2-a5bb-23aeebd33c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of EEG_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\",\n",
    "]\n",
    "\n",
    "# Dictionary to store the loaded tensors\n",
    "loaded_tensors = {}\n",
    "\n",
    "# Loop over the tensor paths to load and store them in the dictionary\n",
    "for path in tensor_paths:\n",
    "    # Extract the tensor name from the path (removing '.pth')\n",
    "    tensor_name = path.split(\"/\")[-1].replace(\".pth\", \"\")\n",
    "    \n",
    "    # Load the tensor\n",
    "    tensor = torch.load(path)\n",
    "    \n",
    "    # Store the tensor in the dictionary\n",
    "    loaded_tensors[tensor_name] = tensor\n",
    "    \n",
    "    # Print shape for verification\n",
    "    print(f\"Shape of {tensor_name}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d0e23a-0b70-407e-9d43-624b40f99b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in band_power_tensor: 0.0%\n",
      "Percentage of Infs in band_power_tensor: 0.0%\n",
      "Percentage of NaNs in EEG_tensor: 0.0%\n",
      "Percentage of Infs in EEG_tensor: 0.0%\n",
      "Percentage of NaNs in fast_fourier_transform_psd_tensor: 0.0%\n",
      "Percentage of Infs in fast_fourier_transform_psd_tensor: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Loop through the loaded tensors to check for NaNs and Infs\n",
    "for tensor_name, tensor_data in loaded_tensors.items():\n",
    "    # Assuming the loaded data is a tensor; if not, additional checks may be needed\n",
    "    total_elements = torch.numel(tensor_data)\n",
    "    \n",
    "    nans_count = torch.sum(torch.isnan(tensor_data)).item()\n",
    "    infs_count = torch.sum(torch.isinf(tensor_data)).item()\n",
    "    \n",
    "    nans_percentage = (nans_count / total_elements) * 100\n",
    "    infs_percentage = (infs_count / total_elements) * 100\n",
    "    \n",
    "    print(f\"Percentage of NaNs in {tensor_name}: {nans_percentage}%\")\n",
    "    print(f\"Percentage of Infs in {tensor_name}: {infs_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55fd27fe-7d47-425b-bf5b-b8aca009b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape band_power_tensor from [4227788, 32, 5] to [4227788, 32*5]\n",
    "band_power_tensor = loaded_tensors[\"band_power_tensor\"]  # Retrieve from dictionary\n",
    "band_power_tensor_reshaped = band_power_tensor.reshape(4227788, -1)  # -1 means auto-calculate the size\n",
    "\n",
    "# Reshape fast_fourier_transform_psd_tensor from [32, 4227788] to [4227788, 32]\n",
    "fast_fourier_transform_psd_tensor = loaded_tensors[\"fast_fourier_transform_psd_tensor\"]  # Retrieve from dictionary\n",
    "fast_fourier_transform_psd_tensor_reshaped = fast_fourier_transform_psd_tensor.permute(1, 0)\n",
    "\n",
    "# Reshape band_power_tensor from [4227788, 32, 5] to [4227788, 32*5]\n",
    "band_power_tensor_reshaped = band_power_tensor.reshape(4227788, -1)  # -1 means auto-calculate the size\n",
    "\n",
    "# Reshape fast_fourier_transform_psd_tensor from [32, 4227788] to [4227788, 32]\n",
    "fast_fourier_transform_psd_tensor = fast_fourier_transform_psd_tensor.permute(1, 0)\n",
    "\n",
    "# Concatenate along the feature dimension\n",
    "concatenated_tensor = torch.cat([band_power_tensor_reshaped, fast_fourier_transform_psd_tensor], dim=1)\n",
    "\n",
    "# Now concatenated_tensor has shape [4227788, (32*5)+32]\n",
    "# If 32*5+32 = feature_dim, you can directly use this tensor as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f770cc1e-fd3f-48fd-88e6-727cc87e212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squeezed shape: torch.Size([32, 4227788])\n",
      "Permuted shape: torch.Size([4227788, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize parameters\n",
    "batch_size = 64\n",
    "seq_len = 1000  \n",
    "feature_dim = concatenated_tensor.shape[1]  # Should be the second dimension of your concatenated tensor\n",
    "\n",
    "# Define the EEGPredictor class\n",
    "class EEGPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(EEGPredictor, self).__init__()\n",
    "        self.feature_transform = nn.Linear(feature_dim, d_model)\n",
    "        self.transformer_block = TransformerBlock(d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.prediction_head = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_transform(x)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.prediction_head(x)\n",
    "        return x\n",
    "\n",
    "# Define the TransformerBlock class\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.pos_encoder = nn.Embedding(seq_len, d_model)\n",
    "        self.position = torch.arange(0, seq_len, dtype=torch.long).unsqueeze(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pos_encoder(self.position[:x.size(0), :])\n",
    "        x = x + pos_encoding\n",
    "        return self.transformer(x)\n",
    "\n",
    "# Initialize the model\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "dim_feedforward = 512\n",
    "model = EEGPredictor(d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Prepare the EEG tensor, removing singleton dimensions\n",
    "eeg_tensor = loaded_tensors[\"EEG_tensor\"].squeeze()\n",
    "print(\"Squeezed shape:\", eeg_tensor.shape)\n",
    "\n",
    "# Since the tensor is 2D, permute using only two dimensions\n",
    "eeg_tensor = eeg_tensor.permute(1, 0)\n",
    "print(\"Permuted shape:\", eeg_tensor.shape)\n",
    "\n",
    "num_batches = eeg_tensor.shape[0] // (batch_size * seq_len)\n",
    "\n",
    "# To store the transformer outputs\n",
    "transformer_outputs = []\n",
    "\n",
    "# Concatenate band_power_tensor_reshaped and fast_fourier_transform_psd_tensor_reshaped\n",
    "concatenated_tensor = torch.cat((band_power_tensor_reshaped, fast_fourier_transform_psd_tensor_reshaped), dim=1)\n",
    "\n",
    "# Update feature_dim to the new size after concatenation\n",
    "feature_dim = concatenated_tensor.shape[1]\n",
    "\n",
    "# Re-initialize the model with the updated feature_dim\n",
    "model = EEGPredictor(d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = concatenated_tensor.shape[0] // (batch_size * seq_len)\n",
    "\n",
    "# Check if num_batches is a reasonable number\n",
    "if num_batches == 0:\n",
    "    print(\"The number of batches is zero. Check your batch_size and seq_len settings.\")\n",
    "else:\n",
    "    # To store the transformer outputs\n",
    "    transformer_outputs = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size * seq_len\n",
    "        end_idx = start_idx + (batch_size * seq_len)\n",
    "        # Extract batch and reshape to [seq_len, batch_size, feature_dim]\n",
    "        batch = concatenated_tensor[start_idx:end_idx, :].reshape(seq_len, batch_size, feature_dim)\n",
    "        # Forward pass\n",
    "        output = model(batch)\n",
    "        # Save the transformer output for use in RNN\n",
    "        transformer_outputs.append(output.detach())\n",
    "    \n",
    "    # Stack and save the outputs\n",
    "    transformer_outputs = torch.stack(transformer_outputs)\n",
    "    torch.save(transformer_outputs, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "632f82b6-adbc-4ae1-aecd-84b31989a6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in transformer_outputs: 0.023674242424242424%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(transformer_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(transformer_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(transformer_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in transformer_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in transformer_outputs: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c67252-b29f-4d81-868d-cd6be6c420cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Percentage of NaNs in transformer_outputs: 0.023674242424242424%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n",
      "After cleaning:\n",
      "Percentage of NaNs in transformer_outputs: 0.0%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in transformer_outputs before cleaning\n",
    "total_elements = torch.numel(transformer_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(transformer_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(transformer_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Before cleaning:\")\n",
    "print(f\"Percentage of NaNs in transformer_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in transformer_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Replace NaNs and Infs with zeros\n",
    "transformer_outputs[torch.isnan(transformer_outputs)] = 0\n",
    "transformer_outputs[torch.isinf(transformer_outputs)] = 0\n",
    "\n",
    "# Re-check for NaNs and Infs after cleaning\n",
    "nans_count = torch.sum(torch.isnan(transformer_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(transformer_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"After cleaning:\")\n",
    "print(f\"Percentage of NaNs in transformer_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in transformer_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Optionally, save the cleaned tensor\n",
    "torch.save(transformer_outputs, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982d702-9951-408b-9e74-665019135379",
   "metadata": {},
   "source": [
    "# RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0bb924-0223-4dde-9afa-f7fa96b1d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb646e7-bcf8-4a55-8dbd-f408fcf167ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all_features: torch.Size([1, 112])\n",
      "Shape of transformer_outputs: torch.Size([66, 1000, 64, 1])\n",
      "Shape of eeg_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load saved features\n",
    "feature_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt'\n",
    "transformer_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\"\n",
    "eeg_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\"\n",
    "band_power_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\"\n",
    "fast_fourier_transform_psd_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\"\n",
    "\n",
    "all_features = torch.load(feature_path)\n",
    "transformer_outputs = torch.load(transformer_outputs_path)\n",
    "eeg_tensor = torch.load(eeg_tensor_path)\n",
    "band_power_tensor = torch.load(band_power_path)\n",
    "fast_fourier_transform_psd_tensor = torch.load(fast_fourier_transform_psd_path)\n",
    "\n",
    "print(f\"Shape of all_features: {all_features.shape}\")\n",
    "print(f\"Shape of transformer_outputs: {transformer_outputs.shape}\")\n",
    "print(f\"Shape of eeg_tensor: {eeg_tensor.shape}\")\n",
    "print(f\"Shape of band_power_tensor: {band_power_tensor.shape}\")\n",
    "print(f\"Shape of fast_fourier_transform_psd_tensor: {fast_fourier_transform_psd_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558678df-e1e1-4d92-a703-19d5b0c9b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated time-aligned features shape: torch.Size([4227788, 224])\n"
     ]
    }
   ],
   "source": [
    "# Aligning the time axis\n",
    "time_length = 4227788  # replace with the length of the common time axis\n",
    "\n",
    "# Reshape `eeg_tensor` to align it with time_length\n",
    "eeg_tensor_reshaped = eeg_tensor.squeeze().transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Reshape `band_power_tensor` to align it with time_length\n",
    "band_power_tensor_reshaped = band_power_tensor.view(time_length, -1)  # [time_length, 32*5]\n",
    "\n",
    "# Reshape `fast_fourier_transform_psd_tensor` to align it with time_length\n",
    "fast_fourier_transform_psd_tensor_reshaped = fast_fourier_transform_psd_tensor.transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Concatenating time-aligned tensors\n",
    "concatenated_time_aligned_features = torch.cat(\n",
    "    (eeg_tensor_reshaped, band_power_tensor_reshaped, fast_fourier_transform_psd_tensor_reshaped), \n",
    "    dim=1\n",
    ")\n",
    "\n",
    "print(\"Concatenated time-aligned features shape:\", concatenated_time_aligned_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba6ee57-1c00-42bf-8e6d-a7cf9650efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model Definition\n",
    "class ConditionalRNN(nn.Module):\n",
    "    def __init__(self, time_aligned_feature_dim, hidden_size, global_feature_dim, transformer_feature_dim):\n",
    "        super(ConditionalRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn = nn.LSTM(time_aligned_feature_dim, hidden_size, batch_first=True)\n",
    "        self.global_feature_layer = nn.Linear(global_feature_dim, hidden_size)\n",
    "        self.transformer_feature_layer = nn.Linear(transformer_feature_dim, hidden_size)\n",
    "\n",
    "    def forward(self, time_aligned_features, global_features, transformer_outputs):\n",
    "        batch_size = time_aligned_features.size(0)\n",
    "        \n",
    "        h0 = self.global_feature_layer(global_features).unsqueeze(0).to(device, dtype=torch.float32)\n",
    "        c0 = torch.zeros_like(h0).to(device, dtype=torch.float32)\n",
    "\n",
    "        hidden_state = (h0, c0)\n",
    "        output_list = []\n",
    "\n",
    "        for t in range(time_aligned_features.size(1)):\n",
    "            current_data = time_aligned_features[:, t, :].to(device, dtype=torch.float32)\n",
    "            transformer_hidden = self.transformer_feature_layer(transformer_outputs[:, t, :]).to(device, dtype=torch.float32)\n",
    "            \n",
    "            hidden_state = (hidden_state[0] + transformer_hidden.unsqueeze(0), hidden_state[1])\n",
    "            output, hidden_state = self.rnn(current_data.unsqueeze(1), hidden_state)\n",
    "            output_list.append(output)\n",
    "\n",
    "        outputs = torch.cat(output_list, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Initialize the model and move to the device\n",
    "time_aligned_feature_dim = 224  \n",
    "hidden_size = 128\n",
    "global_feature_dim = 112\n",
    "transformer_feature_dim = 64  # 2 x 32 EEG channels\n",
    "\n",
    "model = ConditionalRNN(time_aligned_feature_dim, hidden_size, global_feature_dim, transformer_feature_dim).to(device)\n",
    "\n",
    "all_features = torch.load(feature_path).to(device)\n",
    "transformer_outputs = torch.load(transformer_outputs_path).to(device)\n",
    "\n",
    "# Pre-process the datasets\n",
    "expanded_all_features = all_features.expand(66, -1)  # Example, adjust as necessary\n",
    "time_feature_chunks = torch.split(concatenated_time_aligned_features, 1000)\n",
    "time_feature_chunks = time_feature_chunks[:66]  # Example, adjust as necessary\n",
    "aligned_time_features = torch.stack(time_feature_chunks)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dataset = TensorDataset(aligned_time_features, expanded_all_features, transformer_outputs.squeeze(-1))\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)  # Example batch size\n",
    "\n",
    "# Store outputs\n",
    "rnn_outputs = []\n",
    "\n",
    "# Run the model\n",
    "for i, (time_aligned_batch, global_features_batch, transformer_outputs_batch) in enumerate(dataloader):\n",
    "    outputs = model(time_aligned_batch, global_features_batch, transformer_outputs_batch)\n",
    "    rnn_outputs.append(outputs.detach())\n",
    "\n",
    "# Combine and save outputs\n",
    "rnn_outputs = torch.cat(rnn_outputs, dim=0)\n",
    "torch.save(rnn_outputs, '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c005cae7-e6be-4eb6-9db1-03337c7bb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in rnn_outputs: 0.0%\n",
      "Percentage of Infs in rnn_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "rnn_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth\"\n",
    "rnn_outputs = torch.load(rnn_outputs_path)\n",
    "\n",
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(rnn_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(rnn_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(rnn_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in rnn_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in rnn_outputs: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420a5095-e5ad-4fea-a830-a28cc02ab18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Percentage of NaNs in rnn_outputs: 0.0%\n",
      "Percentage of Infs in rnn_outputs: 0.0%\n",
      "After cleaning:\n",
      "Percentage of NaNs in rnn_outputs: 0.0%\n",
      "Percentage of Infs in rnn_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in rnn_outputs before cleaning\n",
    "total_elements = torch.numel(rnn_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(rnn_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(rnn_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Before cleaning:\")\n",
    "print(f\"Percentage of NaNs in rnn_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in rnn_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Replace NaNs and Infs with zeros\n",
    "rnn_outputs[torch.isnan(rnn_outputs)] = 0\n",
    "rnn_outputs[torch.isinf(rnn_outputs)] = 0\n",
    "\n",
    "# Re-check for NaNs and Infs after cleaning\n",
    "nans_count = torch.sum(torch.isnan(rnn_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(rnn_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"After cleaning:\")\n",
    "print(f\"Percentage of NaNs in rnn_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in rnn_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Optionally, save the cleaned tensor\n",
    "torch.save(rnn_outputs, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522cfb9-adcc-4979-a058-fbfda120cca2",
   "metadata": {},
   "source": [
    "# Final predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb81f6e7-8849-4af6-896d-80aac78e3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf2e0ec-2207-4be3-bf42-2768d6dd33fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of RNN_outputs: torch.Size([66, 1000, 128])\n",
      "Shape of transformer_outputs: torch.Size([66, 1000, 64, 1])\n",
      "Shape of eeg_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load saved features\n",
    "transformer_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\"\n",
    "eeg_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\"\n",
    "band_power_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\"\n",
    "fast_fourier_transform_psd_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\"\n",
    "RNN_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth\"\n",
    "\n",
    "transformer_outputs = torch.load(transformer_outputs_path)\n",
    "eeg_tensor = torch.load(eeg_tensor_path)\n",
    "band_power_tensor = torch.load(band_power_path)\n",
    "fast_fourier_transform_psd_tensor = torch.load(fast_fourier_transform_psd_path)\n",
    "RNN_outputs = torch.load(RNN_outputs_path)\n",
    "\n",
    "print(f\"Shape of RNN_outputs: {RNN_outputs.shape}\")\n",
    "print(f\"Shape of transformer_outputs: {transformer_outputs.shape}\")\n",
    "print(f\"Shape of eeg_tensor: {eeg_tensor.shape}\")\n",
    "print(f\"Shape of band_power_tensor: {band_power_tensor.shape}\")\n",
    "print(f\"Shape of fast_fourier_transform_psd_tensor: {fast_fourier_transform_psd_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb55a49-e5d0-4d01-9d10-a3733a9ab855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated time-aligned features shape: torch.Size([4227788, 224])\n"
     ]
    }
   ],
   "source": [
    "# Aligning the time axis\n",
    "time_length = 4227788  # replace with the length of the common time axis\n",
    "\n",
    "# Reshape `eeg_tensor` to align it with time_length\n",
    "eeg_tensor_reshaped = eeg_tensor.squeeze().transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Reshape `band_power_tensor` to align it with time_length\n",
    "band_power_tensor_reshaped = band_power_tensor.view(time_length, -1)  # [time_length, 32*5]\n",
    "\n",
    "# Reshape `fast_fourier_transform_psd_tensor` to align it with time_length\n",
    "fast_fourier_transform_psd_tensor_reshaped = fast_fourier_transform_psd_tensor.transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Concatenating time-aligned tensors\n",
    "concatenated_time_aligned_features = torch.cat(\n",
    "    (eeg_tensor_reshaped, band_power_tensor_reshaped, fast_fourier_transform_psd_tensor_reshaped), \n",
    "    dim=1\n",
    ")\n",
    "\n",
    "print(\"Concatenated time-aligned features shape:\", concatenated_time_aligned_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6268a2-499f-41c1-9bc6-15a756762782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in transformer_outputs: 0.0%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n",
      "Percentage of NaNs in eeg_tensor: 0.0%\n",
      "Percentage of Infs in eeg_tensor: 0.0%\n",
      "Percentage of NaNs in band_power_tensor: 0.0%\n",
      "Percentage of Infs in band_power_tensor: 0.0%\n",
      "Percentage of NaNs in fast_fourier_transform_psd_tensor: 0.0%\n",
      "Percentage of Infs in fast_fourier_transform_psd_tensor: 0.0%\n",
      "Percentage of NaNs in RNN_outputs: 0.0%\n",
      "Percentage of Infs in RNN_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "def check_tensor(tensor, name):\n",
    "    total_elements = torch.numel(tensor)\n",
    "    nans_count = torch.sum(torch.isnan(tensor)).item()\n",
    "    infs_count = torch.sum(torch.isinf(tensor)).item()\n",
    "    nans_percentage = (nans_count / total_elements) * 100\n",
    "    infs_percentage = (infs_count / total_elements) * 100\n",
    "    print(f\"Percentage of NaNs in {name}: {nans_percentage}%\")\n",
    "    print(f\"Percentage of Infs in {name}: {infs_percentage}%\")\n",
    "\n",
    "check_tensor(transformer_outputs, \"transformer_outputs\")\n",
    "check_tensor(eeg_tensor, \"eeg_tensor\")\n",
    "check_tensor(band_power_tensor, \"band_power_tensor\")\n",
    "check_tensor(fast_fourier_transform_psd_tensor, \"fast_fourier_transform_psd_tensor\")\n",
    "check_tensor(RNN_outputs, \"RNN_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8eaafc-bde4-4af0-a497-d22dfb59c07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in eeg_tensor_reshaped: 0.0%\n",
      "Percentage of Infs in eeg_tensor_reshaped: 0.0%\n",
      "Percentage of NaNs in band_power_tensor_reshaped: 0.0%\n",
      "Percentage of Infs in band_power_tensor_reshaped: 0.0%\n",
      "Percentage of NaNs in fast_fourier_transform_psd_tensor_reshaped: 0.0%\n",
      "Percentage of Infs in fast_fourier_transform_psd_tensor_reshaped: 0.0%\n",
      "Percentage of NaNs in concatenated_time_aligned_features: 0.0%\n",
      "Percentage of Infs in concatenated_time_aligned_features: 0.0%\n"
     ]
    }
   ],
   "source": [
    "check_tensor(eeg_tensor_reshaped, \"eeg_tensor_reshaped\")\n",
    "check_tensor(band_power_tensor_reshaped, \"band_power_tensor_reshaped\")\n",
    "check_tensor(fast_fourier_transform_psd_tensor_reshaped, \"fast_fourier_transform_psd_tensor_reshaped\")\n",
    "check_tensor(concatenated_time_aligned_features, \"concatenated_time_aligned_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf09775-dadf-414a-b6c5-db5464433b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in concatenated_time_aligned_features_outputs: 0.0%\n",
      "Percentage of Infs in concatenated_time_aligned_features_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(concatenated_time_aligned_features)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(concatenated_time_aligned_features)).item()\n",
    "infs_count = torch.sum(torch.isinf(concatenated_time_aligned_features)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in concatenated_time_aligned_features_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in concatenated_time_aligned_features_outputs: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab01f1ae-dcea-4e47-b458-fa37ccdc2321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4227788, 352])\n"
     ]
    }
   ],
   "source": [
    "# Ensure all tensors are on the CPU\n",
    "RNN_outputs = RNN_outputs.to('cpu')\n",
    "concatenated_time_aligned_features = concatenated_time_aligned_features.to('cpu')\n",
    "\n",
    "# Reduce the RNN outputs' time dimension by averaging\n",
    "RNN_outputs_reduced = torch.mean(RNN_outputs, dim=1)  # Shape: [66, 128]\n",
    "\n",
    "# Reshape `RNN_outputs` to align it with time_length\n",
    "RNN_outputs_reshaped = RNN_outputs.reshape(-1, RNN_outputs.shape[-1])  # [66*1000, 128] = [66000, 128]\n",
    "\n",
    "# Compute the number of repetitions needed to approximate the time_length\n",
    "n_repeats = time_length // RNN_outputs_reshaped.shape[0]\n",
    "remaining_rows = time_length % RNN_outputs_reshaped.shape[0]\n",
    "\n",
    "# Repeat the tensor for n_repeats times and add extra padding if needed\n",
    "RNN_outputs_expanded = RNN_outputs_reshaped.repeat(n_repeats, 1)\n",
    "if remaining_rows > 0:\n",
    "    extra_padding = RNN_outputs_reshaped[:remaining_rows]\n",
    "    RNN_outputs_expanded = torch.cat([RNN_outputs_expanded, extra_padding], dim=0)\n",
    "\n",
    "# Initialize a list to hold the smaller tensors\n",
    "final_input_tensor_list = []\n",
    "\n",
    "# Split and concatenate in chunks to reduce memory footprint\n",
    "split_size = 1000  # adjust as needed\n",
    "for i in range(0, time_length, split_size):\n",
    "    # Ensure the slice size matches for both tensors\n",
    "    slice_size = min(split_size, time_length - i)\n",
    "    temp_concat = torch.cat(\n",
    "        (concatenated_time_aligned_features[i:i + slice_size], \n",
    "         RNN_outputs_expanded[i:i + slice_size]), \n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "    # Append the tensor to the list\n",
    "    final_input_tensor_list.append(temp_concat)\n",
    "    \n",
    "    # Optionally, save this tensor to disk to free up memory\n",
    "    torch.save(temp_concat, f\"/home/vincent/AAA_projects/MVCS/Neuroscience/tempfiles/temp_concat_chunk_{i//split_size}.pt\")\n",
    "\n",
    "# Update feature_dim to the new size after concatenation\n",
    "feature_dim = final_input_tensor_list[0].shape[1]  # Taking shape from one of the chunks\n",
    "\n",
    "# Pre-allocate a zero tensor with the required size\n",
    "final_time_length = 4227788  # replace with your value\n",
    "final_feature_dim = feature_dim  # replace with your feature dimension\n",
    "\n",
    "# Pre-allocate on CPU\n",
    "final_input_tensor = torch.zeros((final_time_length, final_feature_dim))\n",
    "\n",
    "# Fill in the slices\n",
    "start_idx = 0\n",
    "for temp_tensor in final_input_tensor_list:\n",
    "    end_idx = start_idx + temp_tensor.shape[0]\n",
    "    final_input_tensor[start_idx:end_idx, :] = temp_tensor  # Tensor is already on CPU\n",
    "    start_idx = end_idx  # set start_idx for the next iteration\n",
    "\n",
    "# The tensor final_input_tensor should now have shape [4227788, feature_dim]\n",
    "print(final_input_tensor.shape)\n",
    "\n",
    "# Save the tensor\n",
    "torch.save(final_input_tensor, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/final_input_tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47da3217-a61d-4d24-be1f-caa8c95e1ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in temp_concat_chunk_4227: 0.0%\n",
      "Percentage of Infs in temp_concat_chunk_4227: 0.0%\n"
     ]
    }
   ],
   "source": [
    "check_tensor(temp_concat, f\"temp_concat_chunk_{i//split_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede5313c-2781-4926-a515-6a6d6898b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in final_input_tensor: 0.0%\n",
      "Percentage of Infs in final_input_tensor: 0.0%\n"
     ]
    }
   ],
   "source": [
    "final_input_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/final_input_tensor\"\n",
    "final_input_tensor = torch.load(final_input_tensor_path)\n",
    "\n",
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(final_input_tensor)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(final_input_tensor)).item()\n",
    "infs_count = torch.sum(torch.isinf(final_input_tensor)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in final_input_tensor: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in final_input_tensor: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea9271-d953-4eb2-bffc-966b3885c984",
   "metadata": {},
   "source": [
    "# prepare to train test validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10982ddf-5fe3-46d2-ba52-c68a870898bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35bb88da-9e74-43b7-8188-c88ad98ff212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4227788, 352])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the tensor\n",
    "final_input_tensor = torch.load(\"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/final_input_tensor\")\n",
    "\n",
    "# Verify that the tensor was loaded correctly\n",
    "print(final_input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4c76aa-8546-4dd9-8807-3b12e5d27dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([64, 1000, 352])\n",
      "torch.Size([64, 1000, 32])\n",
      "torch.Size([54, 1000, 352])\n",
      "torch.Size([54, 1000, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_mini_batches(tensor, seq_length, batch_size):\n",
    "    dataset_list = []\n",
    "    for i in range(0, tensor.shape[0] - seq_length, seq_length):\n",
    "        end_idx = min(i + seq_length, tensor.shape[0])\n",
    "        subset = tensor[i:end_idx]\n",
    "        dataset_list.append(subset)\n",
    "        \n",
    "    if len(dataset_list) == 0:\n",
    "        raise ValueError(\"dataset_list is empty. Check the tensor dimensions.\")\n",
    "        \n",
    "    combined_dataset = torch.stack(dataset_list)\n",
    "    tensor_dataset = TensorDataset(combined_dataset)\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "# Load EEG data (labels)\n",
    "eeg_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\"\n",
    "eeg_tensor = torch.load(eeg_tensor_path)\n",
    "eeg_data = eeg_tensor.squeeze().transpose(0, 1)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "total_data = len(eeg_data)\n",
    "train_split = int(0.8 * total_data)\n",
    "val_split = int(0.9 * total_data)\n",
    "\n",
    "train_data_Y = eeg_data[:train_split]\n",
    "val_data_Y = eeg_data[train_split:val_split]\n",
    "test_data_Y = eeg_data[val_split:]\n",
    "\n",
    "train_data_X = final_input_tensor[:train_split]\n",
    "val_data_X = final_input_tensor[train_split:val_split]\n",
    "test_data_X = final_input_tensor[val_split:]\n",
    "\n",
    "# Parameters\n",
    "seq_length = 1000\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_Y = create_mini_batches(train_data_Y, seq_length, batch_size)\n",
    "val_loader_Y = create_mini_batches(val_data_Y, seq_length, batch_size)\n",
    "test_loader_Y = create_mini_batches(test_data_Y, seq_length, batch_size)\n",
    "\n",
    "train_loader_X = create_mini_batches(train_data_X, seq_length, batch_size)\n",
    "val_loader_X = create_mini_batches(val_data_X, seq_length, batch_size)\n",
    "test_loader_X = create_mini_batches(test_data_X, seq_length, batch_size)\n",
    "\n",
    "for batch_idx, (batch_X, batch_Y) in enumerate(zip(train_loader_X, train_loader_Y)):\n",
    "    input_batch_X = batch_X[0]  # Extracting tensor from tuple\n",
    "    input_batch_Y = batch_Y[0]  # Extracting tensor from tuple\n",
    "    print(input_batch_X.shape)  # Should match the input feature size your model expects\n",
    "    print(input_batch_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0461b6-7fec-4356-9290-50de3f499541",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535b0531-a3a6-48d3-873f-f6a4fba6e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Add weight initialization in the constructor\n",
    "class EEGSeq2SeqPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(EEGSeq2SeqPredictor, self).__init__()\n",
    "\n",
    "        # Initialize fully connected layers for input dimension reduction\n",
    "        self.input_fc_X = nn.Linear(352, d_model)\n",
    "        self.input_fc_Y = nn.Linear(32, d_model)\n",
    "        \n",
    "        # Initialize Transformer Encoder and Decoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Initialize fully connected layer for output\n",
    "        self.fc = nn.Linear(d_model, 32)\n",
    "\n",
    "        # Correct the weight initialization\n",
    "        for p in self.parameters():  # Change 'model' to 'self'\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)  # corrected syntax\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # Dimension reduction\n",
    "        src = self.input_fc_X(src)\n",
    "        tgt = self.input_fc_Y(tgt)\n",
    "        \n",
    "        # Transformer Encoder-Decoder\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4c1e4-12fa-4bd1-8db2-21926973292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 256\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "dim_feedforward = 1024\n",
    "lr = 0.1  # Lower initial learning rate\n",
    "batch_size = 64\n",
    "seq_len = 1000\n",
    "num_epochs = 100\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cpu')\n",
    "model = EEGSeq2SeqPredictor(d_model, nhead, num_layers, dim_feedforward).to(device)\n",
    "\n",
    "# Initialize model weights\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        if param.dim() >= 2:\n",
    "            nn.init.kaiming_uniform_(param.data)\n",
    "    elif 'bias' in name:\n",
    "        nn.init.constant_(param.data, 0)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)\n",
    "\n",
    "# Placeholder for the best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Start of Epoch\", epoch + 1)  # Debugging line\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "\n",
    "    for (input_batch_X,), (input_batch_Y,) in zip(train_loader_X, train_loader_Y):\n",
    "        #print(\"Batch loaded\")  # Debugging line\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Debug: Check if input contains NaN or Inf\n",
    "        if torch.isnan(input_batch_X).any() or torch.isinf(input_batch_X).any():\n",
    "            print(\"NaN or Inf found in input, skipping batch\")\n",
    "            continue\n",
    "\n",
    "        outputs = model(input_batch_X, input_batch_Y[:-1])\n",
    "\n",
    "        # Debug: Check if output contains NaN or Inf\n",
    "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "            print(\"NaN or Inf found in model output, skipping batch\")\n",
    "            continue\n",
    "\n",
    "        loss = criterion(outputs, input_batch_Y[1:])\n",
    "\n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"NaN loss, stopping training\")\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Debug: Print the gradient norms\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if param.grad is not None:\n",
    "        #        print(f\"{name}: Gradient Norm: {torch.norm(param.grad)}\")\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "\n",
    "    for (input_batch_X,), (input_batch_Y,) in zip(val_loader_X, val_loader_Y):\n",
    "        input_batch_X, input_batch_Y = input_batch_X.to(device), input_batch_Y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_batch_X, input_batch_Y[:-1])\n",
    "            batch_loss = criterion(outputs, input_batch_Y[1:])\n",
    "            val_loss += batch_loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "\n",
    "    # Store the average losses for this epoch\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "\n",
    "    # Print current learning rate, average training loss, and average validation loss\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Current learning rate is: {param_group['lr']}\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Update learning rate scheduler based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Save the model if it's the best one so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/best_model.pth')\n",
    "\n",
    "    # Check if learning rate is too small\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr'] < 1e-10:\n",
    "            print(\"Learning rate too small, stopping training\")\n",
    "            break\n",
    "\n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"Stopping training due to NaN loss\")\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e00840-9c5b-4d4d-b73e-83b621eeaac0",
   "metadata": {},
   "source": [
    "# second version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09e10b7-db2c-4231-a2db-84d18c5fcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class EEGSeq2SeqPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(EEGSeq2SeqPredictor, self).__init__()\n",
    "\n",
    "        # Input transformations\n",
    "        self.input_batch_X = nn.Linear(352, d_model)\n",
    "        self.input_batch_Y = nn.Linear(32, d_model)\n",
    "\n",
    "        # Define encoder and decoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=0.1)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc = nn.Linear(d_model, 32)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # Apply transformations and positional encoding\n",
    "        src = self.input_batch_X(src)\n",
    "        tgt = self.input_batch_Y(tgt)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # Forward pass through encoder and decoder\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        \n",
    "        # Generate final output\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efca97cd-0051-474b-b257-06a4a27b285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 11,166,240 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 256\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "dim_feedforward = 1024\n",
    "lr = 0.1  \n",
    "batch_size = 64\n",
    "seq_len = 1000\n",
    "num_epochs = 100\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cpu')  # or 'cuda' if you have a GPU\n",
    "model = EEGSeq2SeqPredictor(d_model, nhead, num_layers, dim_feedforward).to(device)\n",
    "\n",
    "# Count parameters\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# Initialize model weights\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        if param.dim() >= 2:\n",
    "            nn.init.kaiming_uniform_(param.data)\n",
    "    elif 'bias' in name:\n",
    "        nn.init.constant_(param.data, 0)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Placeholder for the best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833d153-12f8-452e-8151-7f4f3d2ea16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Epoch 1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate time taken for each epoch\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start time tracking\n",
    "    print(f\"Start of Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "\n",
    "    # Training loop\n",
    "    for (input_batch_X,), (input_batch_Y,) in zip(train_loader_X, train_loader_Y):\n",
    "        input_batch_X, input_batch_Y = input_batch_X.to(device), input_batch_Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_batch_X, input_batch_Y[:-1])  # Encoding should be inside the model's forward()\n",
    "\n",
    "        # Debug: Check if input contains NaN or Inf\n",
    "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "            print(\"NaN or Inf found in model output, skipping batch\")\n",
    "            continue\n",
    "\n",
    "        loss = criterion(outputs, input_batch_Y[1:])\n",
    "\n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"NaN loss, stopping training\")\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    for (input_batch_X,), (input_batch_Y,) in zip(val_loader_X, val_loader_Y):\n",
    "        input_batch_X, input_batch_Y = input_batch_X.to(device), input_batch_Y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_batch_X, input_batch_Y[:-1])\n",
    "            batch_loss = criterion(outputs, input_batch_Y[1:])\n",
    "            val_loss += batch_loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "\n",
    "    end_time = time.time()  # End time tracking\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tAverage Training Loss: {avg_train_loss:.4f} | Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Current learning rate is: {param_group['lr']}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    if param_group['lr'] < 1e-10:\n",
    "        print(\"Learning rate too small, stopping training\")\n",
    "        break\n",
    "\n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"Stopping training due to NaN loss\")\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad479e-99ab-44ba-af5d-c8906423d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import pandas as pd  # for pretty printing\n",
    "\n",
    "# Load EEG data\n",
    "EEG_data = np.load('/home/vincent/AAA_projects/MVCS/Neuroscience/eeg_data_with_channels.npy', allow_pickle=True)\n",
    "\n",
    "# Standardize the EEG data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(EEG_data)\n",
    "\n",
    "# Assuming eeg_data is already a NumPy array\n",
    "# Use the inverse_transform function to transform the data back to the original scale\n",
    "scaled_EEG_data = scaler.transform(EEG_data)\n",
    "\n",
    "# Now if you inverse_transform this, it should match the original EEG_data\n",
    "eeg_data_original_scale = scaler.inverse_transform(scaled_EEG_data)\n",
    "\n",
    "# Convert back to PyTorch tensor\n",
    "eeg_data_original_scale_tensor = torch.tensor(eeg_data_original_scale, dtype=torch.float32)\n",
    "\n",
    "# Assuming 'outputs' is a PyTorch tensor (this was not defined in your original code)\n",
    "# Convert PyTorch tensor outputs to NumPy array for inverse transform\n",
    "outputs_numpy = outputs.detach().numpy()\n",
    "\n",
    "# Reshape it to 2D for inverse_transform\n",
    "reshaped_outputs = outputs_numpy.reshape(-1, 32)\n",
    "\n",
    "# Use the inverse_transform function to transform the data back to the original scale\n",
    "outputs_original_scale = scaler.inverse_transform(reshaped_outputs)\n",
    "\n",
    "# Reshape the output back to its original 3D form\n",
    "outputs_original_scale = outputs_original_scale.reshape(64, 1000, 32)\n",
    "\n",
    "outputs_original_scale_tensor = torch.tensor(outputs_original_scale, dtype=torch.float32)\n",
    "\n",
    "# Pretty print using Pandas DataFrame, assuming you have channel names as a list called channel_names\n",
    "channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "print(\"\\nFirst two rows of model target eeg data:\")\n",
    "print(pd.DataFrame(eeg_data_original_scale_tensor[:2, :].numpy(), columns=channel_names))\n",
    "\n",
    "print(\"\\nFirst two rows of inverse-transformed model predicted data:\")\n",
    "print(pd.DataFrame(outputs_original_scale_tensor[:2, 0, :].numpy(), columns=channel_names))  # assuming second dimension (1000) is seq_len\n",
    "\n",
    "print(\"\\nFirst two rows of original data:\")\n",
    "print(pd.DataFrame(EEG_data[:2, :], columns=channel_names))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0770fc9-94d2-40c2-b827-a5229f082e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "true_data_np = eeg_data_original_scale_tensor[:1000, 0].numpy()  # First channel\n",
    "predicted_data_np = outputs_numpy[:1000, 0]  # Already a numpy array, first channel\n",
    "# Configure Matplotlib to have a black background and light grey text\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "plt.rcParams['axes.edgecolor'] = 'lightgrey'\n",
    "plt.rcParams['axes.labelcolor'] = 'lightgrey'\n",
    "plt.rcParams['text.color'] = 'lightgrey'\n",
    "plt.rcParams['xtick.color'] = 'lightgrey'\n",
    "plt.rcParams['ytick.color'] = 'lightgrey'\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(30, 10), facecolor='black')\n",
    "\n",
    "# Plotting True EEG Data for the first channel\n",
    "plt.plot(true_data_np, color='orange', alpha=0.5, label='True EEG')\n",
    "\n",
    "# Plotting Predicted EEG Data for the first channel\n",
    "plt.plot(predicted_data_np, color='red', alpha=0.5, label='Predicted EEG')\n",
    "\n",
    "# Adding title, legend, and axis labels\n",
    "plt.title('EEG Data - First Channel')\n",
    "plt.xlabel('Time Points')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Adding grid lines for better visibility of scale\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, color='lightgrey')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706f5f7-3d79-4084-87d3-d19e4741699c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming eeg_data_original_scale_tensor, predicted_outputs_reshaped, and EEG_data are available\n",
    "\n",
    "# `eeg_data_original_scale_tensor` is converted to a numpy array\n",
    "# `predicted_outputs_reshaped` is already a numpy array\n",
    "true_data_np = eeg_data_original_scale_tensor.numpy()\n",
    "predicted_data_np = outputs_numpy  # No need to convert\n",
    "original_data_np = EEG_data\n",
    "\n",
    "print(\"True EEG - First Channel | Predicted EEG - First Channel | Original EEG - First Channel\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "for true_value, predicted_value, original_value in zip(true_data_np[:100, 0], predicted_data_np[:100, 0], original_data_np[:100, 0]):\n",
    "    print(f\"{true_value:20} | {predicted_value:20} | {original_value:20}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c3570-1195-414a-9fbe-da7dfe5402ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308a730-b820-4bdf-bada-51be5fadfd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your saved test dataset\n",
    "test_data = torch.load(test_dataset_path)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Final Evaluation on Test Data\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for time_data, global_data, trans_data, labels in test_loader:\n",
    "        outputs = model(time_data, global_data, trans_data)\n",
    "        test_loss += nn.MSELoss()(outputs, labels).item()\n",
    "        \n",
    "print(f'Final Test loss: {test_loss / len(test_loader)}')\n",
    "\n",
    "# If you want to save the model\n",
    "torch.save(model.state_dict(), '/path/to/save/final_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c2625-26bd-46c1-9551-82a2a7f794e3",
   "metadata": {},
   "source": [
    "# Real-time training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4139be6-99f2-48e0-9a20-10a3e70324d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record user EEG for one minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2a97f-1aeb-4ccb-915d-8f5ee6348f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let training finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492799b-fa41-4d6e-bcde-e666545a58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for one minute for new user EEG data and predict the next minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2719e-2713-4921-a483-eda1c8e6f316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
