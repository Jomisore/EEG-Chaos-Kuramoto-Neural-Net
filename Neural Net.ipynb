{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516441f8-ccaa-478a-a7e3-6db52f2a74ef",
   "metadata": {},
   "source": [
    "# Load Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43be465b-9c39-4493-9313-db7eef868865",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arnold_tongues_rotation_numbers_tensor: torch.Size([32, 300, 300])\n",
      "dspm_tensor: torch.Size([19, 18840, 10])\n",
      "higuchi_fractal_dimensions_tensor: torch.Size([1, 1, 4, 8])\n",
      "Hurst_tensor: torch.Size([1, 1, 32, 1])\n",
      "mfdfa_concatd_tensor: torch.Size([32, 1, 30, 2])\n",
      "mfdfa_tensor: torch.Size([9, 32, 10, 1])\n",
      "short_time_fourier_transform_tensor: torch.Size([32, 1001, 4229])\n",
      "transfer_entropy_granular_tensor: torch.Size([4, 4])\n",
      "transfer_entropy_hemispheric_avg_input_tensor: torch.Size([92, 92])\n",
      "transfer_entropy_regional_tensor: torch.Size([4, 4])\n",
      "spectral_entropy_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_centroids_tensor: torch.Size([1, 1, 32, 1])\n",
      "freq_max_power_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_edge_freqs_tensor: torch.Size([1, 1, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/arnold_tongues_rotation_numbers_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/dspm_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/higuchi_fractal_dimensions_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/Hurst_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_concatd_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/short_time_fourier_transform_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_granular_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_hemispheric_avg_input_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_regional_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_entropy_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_centroids_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/freq_max_power_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_edge_freqs_tensor.pt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store the tensors and another for their shapes\n",
    "tensors = {}\n",
    "tensor_shapes = {}\n",
    "\n",
    "# Load the tensors into a dictionary and collect their shapes\n",
    "for path in tensor_paths:\n",
    "    tensor_name = path.split('/')[-1].replace('.pt', '').replace('.pth', '')\n",
    "\n",
    "    # Remove the 'h' from the end, if it exists\n",
    "    if tensor_name.endswith(\"h\"):\n",
    "        tensor_name = tensor_name[:-1]\n",
    "\n",
    "    # Load the tensor\n",
    "    data = torch.load(path)\n",
    "    tensors[tensor_name] = data\n",
    "\n",
    "    # Check the type of the loaded data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor_shapes[tensor_name] = data.shape\n",
    "    elif isinstance(data, dict):  # Likely a state_dict\n",
    "        tensor_shapes[tensor_name] = \"state_dict (model parameters)\"\n",
    "    else:\n",
    "        tensor_shapes[tensor_name] = \"unknown type\"\n",
    "\n",
    "# Print the shapes of all loaded tensors\n",
    "for name, shape in tensor_shapes.items():\n",
    "    print(f\"{name}: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e8e026a-f6bb-4d54-8a3b-be607fa4f34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in arnold_tongues_rotation_numbers_tensor: 0.0%\n",
      "Percentage of Infs in arnold_tongues_rotation_numbers_tensor: 0.0%\n",
      "Percentage of NaNs in dspm_tensor: 0.0%\n",
      "Percentage of Infs in dspm_tensor: 0.0%\n",
      "Percentage of NaNs in higuchi_fractal_dimensions_tensor: 0.0%\n",
      "Percentage of Infs in higuchi_fractal_dimensions_tensor: 0.0%\n",
      "Percentage of NaNs in Hurst_tensor: 0.0%\n",
      "Percentage of Infs in Hurst_tensor: 0.0%\n",
      "Percentage of NaNs in mfdfa_concatd_tensor: 0.0%\n",
      "Percentage of Infs in mfdfa_concatd_tensor: 0.0%\n",
      "Percentage of NaNs in mfdfa_tensor: 0.0%\n",
      "Percentage of Infs in mfdfa_tensor: 0.0%\n",
      "Percentage of NaNs in short_time_fourier_transform_tensor: 0.0%\n",
      "Percentage of Infs in short_time_fourier_transform_tensor: 0.0%\n",
      "Percentage of NaNs in transfer_entropy_granular_tensor: 0.0%\n",
      "Percentage of Infs in transfer_entropy_granular_tensor: 0.0%\n",
      "Percentage of NaNs in transfer_entropy_hemispheric_avg_input_tensor: 0.0%\n",
      "Percentage of Infs in transfer_entropy_hemispheric_avg_input_tensor: 0.0%\n",
      "Percentage of NaNs in transfer_entropy_regional_tensor: 0.0%\n",
      "Percentage of Infs in transfer_entropy_regional_tensor: 0.0%\n",
      "Percentage of NaNs in spectral_entropy_tensor: 0.0%\n",
      "Percentage of Infs in spectral_entropy_tensor: 0.0%\n",
      "Percentage of NaNs in spectral_centroids_tensor: 0.0%\n",
      "Percentage of Infs in spectral_centroids_tensor: 0.0%\n",
      "Percentage of NaNs in freq_max_power_tensor: 0.0%\n",
      "Percentage of Infs in freq_max_power_tensor: 0.0%\n",
      "Percentage of NaNs in spectral_edge_freqs_tensor: 0.0%\n",
      "Percentage of Infs in spectral_edge_freqs_tensor: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Loop through the loaded tensors to check for NaNs and Infs\n",
    "for tensor_name, tensor_data in tensors.items():\n",
    "    # Only perform the checks if the data is a tensor\n",
    "    if isinstance(tensor_data, torch.Tensor):\n",
    "        total_elements = torch.numel(tensor_data)\n",
    "        \n",
    "        nans_count = torch.sum(torch.isnan(tensor_data)).item()\n",
    "        infs_count = torch.sum(torch.isinf(tensor_data)).item()\n",
    "        \n",
    "        nans_percentage = (nans_count / total_elements) * 100\n",
    "        infs_percentage = (infs_count / total_elements) * 100\n",
    "        \n",
    "        print(f\"Percentage of NaNs in {tensor_name}: {nans_percentage}%\")\n",
    "        print(f\"Percentage of Infs in {tensor_name}: {infs_percentage}%\")\n",
    "    else:\n",
    "        print(f\"{tensor_name} is not a tensor, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7667071-ad74-4a9e-ab1b-f29354a5816c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Match dimensions, reshape, and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f60a69-caf6-47d1-a2c1-b71536f18f65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tensor arnold_tongues_rotation_numbers_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor dspm_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor higuchi_fractal_dimensions_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor Hurst_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor mfdfa_concatd_tensor shape: torch.Size([32, 1, 32, 32])\n",
      "Processed tensor mfdfa_tensor shape: torch.Size([9, 1, 32, 32])\n",
      "Processed tensor short_time_fourier_transform_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor transfer_entropy_granular_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor transfer_entropy_hemispheric_avg_input_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor transfer_entropy_regional_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor spectral_entropy_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor spectral_centroids_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor freq_max_power_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor spectral_edge_freqs_tensor shape: torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_and_resize_tensor(tensor, target_shape):\n",
    "    # Add missing batch and channel dimensions\n",
    "    while len(tensor.shape) < 4:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Reduce the channel dimension to 1 by taking the mean along that axis\n",
    "    tensor = torch.mean(tensor, dim=1, keepdim=True)\n",
    "\n",
    "    # Normalize\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    if std != 0:\n",
    "        tensor = (tensor - mean) / std\n",
    "\n",
    "    # Reshape/resize to target_shape\n",
    "    tensor = F.interpolate(tensor, size=target_shape[2:], mode='bilinear', align_corners=True)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "target_shape = [1, 1, 32, 32]\n",
    "\n",
    "# List of all your tensors\n",
    "all_tensors = [tensors[key] for key in tensors]\n",
    "\n",
    "# Preprocess all tensors\n",
    "processed_tensors = [preprocess_and_resize_tensor(tensor, target_shape) for tensor in all_tensors]\n",
    "\n",
    "# Store preprocessed tensors back into the `tensors` dictionary\n",
    "for key, tensor in zip(tensors.keys(), processed_tensors):\n",
    "    tensors[key] = tensor\n",
    "\n",
    "# Print out the new shapes\n",
    "for key in tensors.keys():\n",
    "    print(f\"Processed tensor {key} shape: {tensors[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc365929-f568-48e7-af9e-aa60e517f841",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ff40f7-f03a-43bc-9afb-1ce7afd9f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tensor_names = [\n",
    "    'arnold_tongues_rotation_numbers_tensor',\n",
    "    'dspm_tensor',\n",
    "    'higuchi_fractal_dimensions_tensor',\n",
    "    'Hurst_tensor',\n",
    "    'mfdfa_concatd_tensor',\n",
    "    'mfdfa_tensor',\n",
    "    'short_time_fourier_transform_tensor',\n",
    "    'transfer_entropy_granular_tensor',\n",
    "    'transfer_entropy_hemispheric_avg_input_tensor',\n",
    "    'transfer_entropy_regional_tensor',\n",
    "    'spectral_entropy_tensor',\n",
    "    'spectral_centroids_tensor',\n",
    "    'freq_max_power_tensor',\n",
    "    'spectral_edge_freqs_tensor',\n",
    "]\n",
    "\n",
    "class BaseEmbeddingNet(nn.Module):\n",
    "    def __init__(self, input_channels, conv_output_channels, reduce_to_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, conv_output_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(conv_output_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_reduce = nn.Linear(conv_output_channels, reduce_to_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_reduce(x)\n",
    "        return x\n",
    "\n",
    "processed_tensors_dict = {name: tensor for name, tensor in zip(tensor_names, processed_tensors)}\n",
    "\n",
    "net_params = {name: {'input_channels': 1, 'conv_output_channels': 16, 'reduce_to_dim': 8} \n",
    "              for name in processed_tensors_dict.keys()}\n",
    "\n",
    "# Create BaseEmbeddingNets for each tensor\n",
    "embedding_nets = {name: BaseEmbeddingNet(**params) for name, params in net_params.items()}\n",
    "\n",
    "# Move networks to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for net in embedding_nets.values():\n",
    "    net.to(device)\n",
    "\n",
    "# Create custom dataset and dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = tensors\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __len__(self):\n",
    "        first_tensor = next(iter(self.tensors.values()))\n",
    "        return first_tensor.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        result = {}\n",
    "        for key, val in self.tensors.items():\n",
    "            if val.shape[0] > idx:\n",
    "                result[key] = val[idx]\n",
    "        return result\n",
    "\n",
    "def custom_collate(batch):\n",
    "    collated_batch = {}\n",
    "    all_keys = set([key for item in batch for key in item.keys()])\n",
    "    \n",
    "    for key in all_keys:\n",
    "        collated_batch[key] = torch.stack([item[key] for item in batch if key in item.keys()], dim=0)\n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "# Use processed_tensors for your CustomDataset\n",
    "dataset = CustomDataset(processed_tensors_dict)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=custom_collate)\n",
    "\n",
    "# Collect feature embeddings\n",
    "all_features = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "    features_list = [net(batch[key].to(device, dtype=torch.float32)) for key, net in embedding_nets.items()]\n",
    "    concatenated_features = torch.cat(features_list, dim=1)\n",
    "    all_features.append(concatenated_features.cpu().detach())\n",
    "\n",
    "# Convert list to tensor\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "\n",
    "# Save the feature embeddings\n",
    "save_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto'\n",
    "torch.save(all_features, f'{save_path}/all_features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56cafa-30b8-415d-823c-fafd212eecfa",
   "metadata": {},
   "source": [
    "# Kuramoto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b58e085-dc2d-4f47-91b1-f2709000a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484061c8-7040-4833-a73c-c9d3974b7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd4ba5d-a115-4eb9-beb4-98d6f37c95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint\n",
    "from torch.cuda.amp import autocast, GradScaler  # Importing the AMP utilities\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "EEG_data = np.load('/home/vincent/AAA_projects/MVCS/Neuroscience/eeg_data_with_channels.npy', allow_pickle=True)\n",
    "EEG_tensor = torch.FloatTensor(EEG_data)  # Assumes EEG_data is a NumPy ndarray\n",
    "\n",
    "# Function to create windows for time-series data\n",
    "def create_windows(data, window_size, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        windows.append(data[i:i+window_size])\n",
    "    return torch.stack(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040d257f-994d-409b-9875-849ff643ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "window_size = 50\n",
    "stride = 10\n",
    "\n",
    "# Add necessary transformations here to EEG_tensor if required\n",
    "EEG_tensor = EEG_tensor.clone().detach().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174b3288-f313-4b35-b037-e6eec6892a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hilbert_in_batches(data, batch_size):\n",
    "    n_batches = int(np.ceil(data.shape[0] / batch_size))\n",
    "    analytic_signal = np.zeros_like(data, dtype=np.complex64)  # change dtype as needed\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        analytic_signal[start_idx:end_idx, :] = hilbert(data[start_idx:end_idx, :])\n",
    "        \n",
    "    return analytic_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f0a84dd-6970-4888-8168-7489406b22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # Set as appropriate\n",
    "\n",
    "# Apply Hilbert transform in batches\n",
    "EEG_numpy = EEG_tensor.cpu().numpy()\n",
    "analytic_signal_batches = apply_hilbert_in_batches(EEG_numpy, batch_size)\n",
    "\n",
    "# Convert the angle to phases and move to GPU\n",
    "phases = torch.tensor(np.angle(analytic_signal_batches), dtype=torch.float16).to(device)\n",
    "\n",
    "# Load PLV matrix\n",
    "plv_matrix_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Phase Syncronization/plv_matrix.npy\"\n",
    "plv_matrix = torch.tensor(np.load(plv_matrix_path), dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b34c43-dc25-45be-b0d0-2fd9bfbdfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute_phase_diff_matrix function\n",
    "def compute_phase_diff_matrix(phases):\n",
    "    time, channels = phases.shape[:2]\n",
    "    phase_diff_matrix = torch.zeros(channels, channels, device=phases.device)\n",
    "    for i in range(channels):\n",
    "        for j in range(channels):\n",
    "            phase_diff_matrix[i, j] = torch.mean(phases[:, i] - phases[:, j])\n",
    "    return phase_diff_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed965961-347e-4c66-821a-9737b8b9262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)\n",
    "\n",
    "# EEG channel names\n",
    "eeg_channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                     'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                     'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "# Broad regions and corresponding channels\n",
    "regions = {\n",
    "    \"frontal\": ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8'],\n",
    "    \"temporal\": ['T7', 'T8'],\n",
    "    \"parietal\": ['CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8'],\n",
    "    \"occipital\": ['O1', 'Oz', 'O2']\n",
    "}\n",
    "\n",
    "# Precompute omega and phase_diff_matrix\n",
    "N = len(eeg_channel_names)\n",
    "omega = torch.mean(plv_matrix, dim=1).to(device)\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73de9cf1-96b2-4d6a-9e16-cf3211bbf35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the Kuramoto function to use PyTorch functions instead of NumPy\n",
    "def kuramoto_weighted_bias(t, y, omega, K):\n",
    "    weighted_sin = plv_matrix * torch.sin(y - y[:, None] - phase_diff_matrix)\n",
    "    dydt = omega + K / N * torch.sum(weighted_sin, axis=1)\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a59c75e-edd9-45f0-ad11-bd3ebb70922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuramotoODEFunc(nn.Module):\n",
    "    def __init__(self, omega, K, plv_matrix, phase_diff_matrix):\n",
    "        super(KuramotoODEFunc, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.K = K\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "    def forward(self, t, theta):\n",
    "        # Reshape to accommodate the additional time dimension.\n",
    "        theta = theta.view(-1, theta.shape[-1])\n",
    "        N = theta.shape[1]\n",
    "    \n",
    "        # Compute the phase differences without unsqueezing\n",
    "        theta_diff = theta[:, :, None] - theta[:, None, :]\n",
    "        phase_diff_with_matrix = theta_diff - self.phase_diff_matrix\n",
    "    \n",
    "        # Compute the weighted sine values\n",
    "        weighted_sin = self.plv_matrix * torch.sin(phase_diff_with_matrix)\n",
    "    \n",
    "        dtheta = self.omega + (self.K / N) * torch.sum(weighted_sin, dim=1)\n",
    "    \n",
    "        return dtheta.view(theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8756d6-b965-433b-93e1-4d5660ffcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuramotoLayer(nn.Module):\n",
    "    def __init__(self, oscillator_count, time_steps, dt=0.01, plv_matrix=None, phase_diff_matrix=None):\n",
    "        super(KuramotoLayer, self).__init__()\n",
    "        self.oscillator_count = oscillator_count\n",
    "        self.time_steps = time_steps\n",
    "        self.dt = dt\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "        if plv_matrix is not None:\n",
    "            omega_init = torch.mean(plv_matrix, dim=1)\n",
    "            self.omega = nn.Parameter(omega_init, requires_grad=True)\n",
    "        else:\n",
    "            self.omega = nn.Parameter(torch.randn(oscillator_count), requires_grad=True)\n",
    "\n",
    "        self.K = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "\n",
    "    def custom_forward(self, *inputs):\n",
    "        initial_shape = inputs[0].shape  # Store the initial shape\n",
    "    \n",
    "        # Flatten the batch and time dimensions\n",
    "        inputs_flattened = inputs[0].reshape(-1, initial_shape[-1])\n",
    "        \n",
    "        ode_func = KuramotoODEFunc(self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        time_points = torch.arange(0, 10000 * self.dt, self.dt).to(device)  # Assume device is defined elsewhere\n",
    "        theta_flattened = odeint(ode_func, inputs_flattened, time_points, method='bosh3', rtol=1e-6, atol=1e-8)\n",
    "\n",
    "        # Reshape theta to its original shape\n",
    "        theta = theta_flattened.reshape(*initial_shape, -1)  # -1 will automatically compute the required size\n",
    "        return theta\n",
    "        \n",
    "    def forward(self, theta):\n",
    "        device = theta.device\n",
    "        self.plv_matrix = self.plv_matrix.to(device)\n",
    "        self.phase_diff_matrix = self.phase_diff_matrix.to(device)\n",
    "        theta = checkpoint(self.custom_forward, theta, self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        theta = theta.to(torch.float16)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    def forward_with_checkpoint(self, x):\n",
    "        x = x.to(device)\n",
    "        theta = checkpoint(self.custom_forward, x)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mean_coherence(theta):\n",
    "        N, _, _, _ = theta.shape\n",
    "        mean_coherence = torch.mean(torch.cos(theta[:, -1, :] - theta[:, -1, :].mean(dim=1).unsqueeze(1)))\n",
    "        return mean_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceaef628-97ee-44c6-9da2-3ae5655b9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all tensors are on the correct device\n",
    "phases = phases.to(device)\n",
    "\n",
    "# Compute natural frequencies and phase differences just once\n",
    "num_channels = len(eeg_channel_names)  # Get the number of channels\n",
    "#print(\"Theta shape: \", theta.shape)\n",
    "#print(\"Theta Unsqueeze(1) shape: \", theta.unsqueeze(1).shape)\n",
    "#print(\"Theta Unsqueeze(2) shape: \", theta.unsqueeze(2).shape)\n",
    "\n",
    "# Number of channels\n",
    "N = len(eeg_channel_names)\n",
    "\n",
    "# Initialize model and move to device\n",
    "kuramoto_model = KuramotoLayer(N, 12800, plv_matrix=plv_matrix, phase_diff_matrix=phase_diff_matrix).to(dtype=torch.float16).to(device)\n",
    "\n",
    "# Data Parallelism for multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    kuramoto_model = nn.DataParallel(kuramoto_model)\n",
    "\n",
    "scaler = GradScaler()\n",
    "train_data = create_windows(EEG_tensor[:int(0.7 * len(EEG_tensor))], window_size, stride).detach().requires_grad_(True)\n",
    "train_dataset = EEGDataset(data=train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833459c-3640-4db4-a658-65ef6d125254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop and feature extraction\n",
    "kuramoto_features_list = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    # Moves batch to device and changes dtype to float16\n",
    "    batch = batch.to(device, dtype=torch.float16)\n",
    "\n",
    "    # Using autocast for the forward pass\n",
    "    with autocast():\n",
    "        theta, mean_coherence = kuramoto_model(batch)\n",
    "\n",
    "    kuramoto_features_list.append(mean_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd979502-d47a-4338-8168-67f629d3a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined features\n",
    "kuramoto_features_tensor = torch.stack(kuramoto_features_list)\n",
    "all_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt'\n",
    "all_features = torch.load(all_features_path)\n",
    "combined_features = torch.cat([all_features, kuramoto_features_tensor.unsqueeze(1)], dim=1)\n",
    "combined_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/combined_features.pt'\n",
    "torch.save(combined_features, combined_features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8dc5e-228b-45c0-b743-8d2d47a713ba",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e47c39-9fd7-4309-b2e8-87942b5a7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d843f16-33c5-41a2-a5bb-23aeebd33c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of EEG_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\",\n",
    "]\n",
    "\n",
    "# Dictionary to store the loaded tensors\n",
    "loaded_tensors = {}\n",
    "\n",
    "# Loop over the tensor paths to load and store them in the dictionary\n",
    "for path in tensor_paths:\n",
    "    # Extract the tensor name from the path (removing '.pth')\n",
    "    tensor_name = path.split(\"/\")[-1].replace(\".pth\", \"\")\n",
    "    \n",
    "    # Load the tensor\n",
    "    tensor = torch.load(path)\n",
    "    \n",
    "    # Store the tensor in the dictionary\n",
    "    loaded_tensors[tensor_name] = tensor\n",
    "    \n",
    "    # Print shape for verification\n",
    "    print(f\"Shape of {tensor_name}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d0e23a-0b70-407e-9d43-624b40f99b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in band_power_tensor: 0.0%\n",
      "Percentage of Infs in band_power_tensor: 0.0%\n",
      "Percentage of NaNs in EEG_tensor: 0.0%\n",
      "Percentage of Infs in EEG_tensor: 0.0%\n",
      "Percentage of NaNs in fast_fourier_transform_psd_tensor: 0.0%\n",
      "Percentage of Infs in fast_fourier_transform_psd_tensor: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Loop through the loaded tensors to check for NaNs and Infs\n",
    "for tensor_name, tensor_data in loaded_tensors.items():\n",
    "    # Assuming the loaded data is a tensor; if not, additional checks may be needed\n",
    "    total_elements = torch.numel(tensor_data)\n",
    "    \n",
    "    nans_count = torch.sum(torch.isnan(tensor_data)).item()\n",
    "    infs_count = torch.sum(torch.isinf(tensor_data)).item()\n",
    "    \n",
    "    nans_percentage = (nans_count / total_elements) * 100\n",
    "    infs_percentage = (infs_count / total_elements) * 100\n",
    "    \n",
    "    print(f\"Percentage of NaNs in {tensor_name}: {nans_percentage}%\")\n",
    "    print(f\"Percentage of Infs in {tensor_name}: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55fd27fe-7d47-425b-bf5b-b8aca009b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape band_power_tensor from [4227788, 32, 5] to [4227788, 32*5]\n",
    "band_power_tensor = loaded_tensors[\"band_power_tensor\"]  # Retrieve from dictionary\n",
    "band_power_tensor_reshaped = band_power_tensor.reshape(4227788, -1)  # -1 means auto-calculate the size\n",
    "\n",
    "# Reshape fast_fourier_transform_psd_tensor from [32, 4227788] to [4227788, 32]\n",
    "fast_fourier_transform_psd_tensor = loaded_tensors[\"fast_fourier_transform_psd_tensor\"]  # Retrieve from dictionary\n",
    "fast_fourier_transform_psd_tensor_reshaped = fast_fourier_transform_psd_tensor.permute(1, 0)\n",
    "\n",
    "# Reshape band_power_tensor from [4227788, 32, 5] to [4227788, 32*5]\n",
    "band_power_tensor_reshaped = band_power_tensor.reshape(4227788, -1)  # -1 means auto-calculate the size\n",
    "\n",
    "# Reshape fast_fourier_transform_psd_tensor from [32, 4227788] to [4227788, 32]\n",
    "fast_fourier_transform_psd_tensor = fast_fourier_transform_psd_tensor.permute(1, 0)\n",
    "\n",
    "# Concatenate along the feature dimension\n",
    "concatenated_tensor = torch.cat([band_power_tensor_reshaped, fast_fourier_transform_psd_tensor], dim=1)\n",
    "\n",
    "# Now concatenated_tensor has shape [4227788, (32*5)+32]\n",
    "# If 32*5+32 = feature_dim, you can directly use this tensor as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f770cc1e-fd3f-48fd-88e6-727cc87e212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squeezed shape: torch.Size([32, 4227788])\n",
      "Permuted shape: torch.Size([4227788, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize parameters\n",
    "batch_size = 64\n",
    "seq_len = 1000  \n",
    "feature_dim = concatenated_tensor.shape[1]  # Should be the second dimension of your concatenated tensor\n",
    "\n",
    "# Define the EEGPredictor class\n",
    "class EEGPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(EEGPredictor, self).__init__()\n",
    "        self.feature_transform = nn.Linear(feature_dim, d_model)\n",
    "        self.transformer_block = TransformerBlock(d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.prediction_head = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_transform(x)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.prediction_head(x)\n",
    "        return x\n",
    "\n",
    "# Define the TransformerBlock class\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.pos_encoder = nn.Embedding(seq_len, d_model)\n",
    "        self.position = torch.arange(0, seq_len, dtype=torch.long).unsqueeze(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pos_encoder(self.position[:x.size(0), :])\n",
    "        x = x + pos_encoding\n",
    "        return self.transformer(x)\n",
    "\n",
    "# Initialize the model\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "dim_feedforward = 512\n",
    "model = EEGPredictor(d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Prepare the EEG tensor, removing singleton dimensions\n",
    "eeg_tensor = loaded_tensors[\"EEG_tensor\"].squeeze()\n",
    "print(\"Squeezed shape:\", eeg_tensor.shape)\n",
    "\n",
    "# Since the tensor is 2D, permute using only two dimensions\n",
    "eeg_tensor = eeg_tensor.permute(1, 0)\n",
    "print(\"Permuted shape:\", eeg_tensor.shape)\n",
    "\n",
    "num_batches = eeg_tensor.shape[0] // (batch_size * seq_len)\n",
    "\n",
    "# To store the transformer outputs\n",
    "transformer_outputs = []\n",
    "\n",
    "# Concatenate band_power_tensor_reshaped and fast_fourier_transform_psd_tensor_reshaped\n",
    "concatenated_tensor = torch.cat((band_power_tensor_reshaped, fast_fourier_transform_psd_tensor_reshaped), dim=1)\n",
    "\n",
    "# Update feature_dim to the new size after concatenation\n",
    "feature_dim = concatenated_tensor.shape[1]\n",
    "\n",
    "# Re-initialize the model with the updated feature_dim\n",
    "model = EEGPredictor(d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = concatenated_tensor.shape[0] // (batch_size * seq_len)\n",
    "\n",
    "# Check if num_batches is a reasonable number\n",
    "if num_batches == 0:\n",
    "    print(\"The number of batches is zero. Check your batch_size and seq_len settings.\")\n",
    "else:\n",
    "    # To store the transformer outputs\n",
    "    transformer_outputs = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size * seq_len\n",
    "        end_idx = start_idx + (batch_size * seq_len)\n",
    "        # Extract batch and reshape to [seq_len, batch_size, feature_dim]\n",
    "        batch = concatenated_tensor[start_idx:end_idx, :].reshape(seq_len, batch_size, feature_dim)\n",
    "        # Forward pass\n",
    "        output = model(batch)\n",
    "        # Save the transformer output for use in RNN\n",
    "        transformer_outputs.append(output.detach())\n",
    "    \n",
    "    # Stack and save the outputs\n",
    "    transformer_outputs = torch.stack(transformer_outputs)\n",
    "    torch.save(transformer_outputs, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "632f82b6-adbc-4ae1-aecd-84b31989a6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in transformer_outputs: 0.023674242424242424%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(transformer_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(transformer_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(transformer_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in transformer_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in transformer_outputs: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c67252-b29f-4d81-868d-cd6be6c420cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Percentage of NaNs in transformer_outputs: 0.023674242424242424%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n",
      "After cleaning:\n",
      "Percentage of NaNs in transformer_outputs: 0.0%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in transformer_outputs before cleaning\n",
    "total_elements = torch.numel(transformer_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(transformer_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(transformer_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Before cleaning:\")\n",
    "print(f\"Percentage of NaNs in transformer_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in transformer_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Replace NaNs and Infs with zeros\n",
    "transformer_outputs[torch.isnan(transformer_outputs)] = 0\n",
    "transformer_outputs[torch.isinf(transformer_outputs)] = 0\n",
    "\n",
    "# Re-check for NaNs and Infs after cleaning\n",
    "nans_count = torch.sum(torch.isnan(transformer_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(transformer_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"After cleaning:\")\n",
    "print(f\"Percentage of NaNs in transformer_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in transformer_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Optionally, save the cleaned tensor\n",
    "torch.save(transformer_outputs, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982d702-9951-408b-9e74-665019135379",
   "metadata": {},
   "source": [
    "# RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0bb924-0223-4dde-9afa-f7fa96b1d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb646e7-bcf8-4a55-8dbd-f408fcf167ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all_features: torch.Size([1, 112])\n",
      "Shape of transformer_outputs: torch.Size([66, 1000, 64, 1])\n",
      "Shape of eeg_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load saved features\n",
    "feature_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt'\n",
    "transformer_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\"\n",
    "eeg_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\"\n",
    "band_power_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\"\n",
    "fast_fourier_transform_psd_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\"\n",
    "\n",
    "all_features = torch.load(feature_path)\n",
    "transformer_outputs = torch.load(transformer_outputs_path)\n",
    "eeg_tensor = torch.load(eeg_tensor_path)\n",
    "band_power_tensor = torch.load(band_power_path)\n",
    "fast_fourier_transform_psd_tensor = torch.load(fast_fourier_transform_psd_path)\n",
    "\n",
    "print(f\"Shape of all_features: {all_features.shape}\")\n",
    "print(f\"Shape of transformer_outputs: {transformer_outputs.shape}\")\n",
    "print(f\"Shape of eeg_tensor: {eeg_tensor.shape}\")\n",
    "print(f\"Shape of band_power_tensor: {band_power_tensor.shape}\")\n",
    "print(f\"Shape of fast_fourier_transform_psd_tensor: {fast_fourier_transform_psd_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558678df-e1e1-4d92-a703-19d5b0c9b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated time-aligned features shape: torch.Size([4227788, 224])\n"
     ]
    }
   ],
   "source": [
    "# Aligning the time axis\n",
    "time_length = 4227788  # replace with the length of the common time axis\n",
    "\n",
    "# Reshape `eeg_tensor` to align it with time_length\n",
    "eeg_tensor_reshaped = eeg_tensor.squeeze().transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Reshape `band_power_tensor` to align it with time_length\n",
    "band_power_tensor_reshaped = band_power_tensor.view(time_length, -1)  # [time_length, 32*5]\n",
    "\n",
    "# Reshape `fast_fourier_transform_psd_tensor` to align it with time_length\n",
    "fast_fourier_transform_psd_tensor_reshaped = fast_fourier_transform_psd_tensor.transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Concatenating time-aligned tensors\n",
    "concatenated_time_aligned_features = torch.cat(\n",
    "    (eeg_tensor_reshaped, band_power_tensor_reshaped, fast_fourier_transform_psd_tensor_reshaped), \n",
    "    dim=1\n",
    ")\n",
    "\n",
    "print(\"Concatenated time-aligned features shape:\", concatenated_time_aligned_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba6ee57-1c00-42bf-8e6d-a7cf9650efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model Definition\n",
    "class ConditionalRNN(nn.Module):\n",
    "    def __init__(self, time_aligned_feature_dim, hidden_size, global_feature_dim, transformer_feature_dim):\n",
    "        super(ConditionalRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn = nn.LSTM(time_aligned_feature_dim, hidden_size, batch_first=True)\n",
    "        self.global_feature_layer = nn.Linear(global_feature_dim, hidden_size)\n",
    "        self.transformer_feature_layer = nn.Linear(transformer_feature_dim, hidden_size)\n",
    "\n",
    "    def forward(self, time_aligned_features, global_features, transformer_outputs):\n",
    "        batch_size = time_aligned_features.size(0)\n",
    "        \n",
    "        h0 = self.global_feature_layer(global_features).unsqueeze(0).to(device, dtype=torch.float32)\n",
    "        c0 = torch.zeros_like(h0).to(device, dtype=torch.float32)\n",
    "\n",
    "        hidden_state = (h0, c0)\n",
    "        output_list = []\n",
    "\n",
    "        for t in range(time_aligned_features.size(1)):\n",
    "            current_data = time_aligned_features[:, t, :].to(device, dtype=torch.float32)\n",
    "            transformer_hidden = self.transformer_feature_layer(transformer_outputs[:, t, :]).to(device, dtype=torch.float32)\n",
    "            \n",
    "            hidden_state = (hidden_state[0] + transformer_hidden.unsqueeze(0), hidden_state[1])\n",
    "            output, hidden_state = self.rnn(current_data.unsqueeze(1), hidden_state)\n",
    "            output_list.append(output)\n",
    "\n",
    "        outputs = torch.cat(output_list, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Initialize the model and move to the device\n",
    "time_aligned_feature_dim = 224  \n",
    "hidden_size = 128\n",
    "global_feature_dim = 112\n",
    "transformer_feature_dim = 64  # 2 x 32 EEG channels\n",
    "\n",
    "model = ConditionalRNN(time_aligned_feature_dim, hidden_size, global_feature_dim, transformer_feature_dim).to(device)\n",
    "\n",
    "all_features = torch.load(feature_path).to(device)\n",
    "transformer_outputs = torch.load(transformer_outputs_path).to(device)\n",
    "\n",
    "# Pre-process the datasets\n",
    "expanded_all_features = all_features.expand(66, -1)  # Example, adjust as necessary\n",
    "time_feature_chunks = torch.split(concatenated_time_aligned_features, 1000)\n",
    "time_feature_chunks = time_feature_chunks[:66]  # Example, adjust as necessary\n",
    "aligned_time_features = torch.stack(time_feature_chunks)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dataset = TensorDataset(aligned_time_features, expanded_all_features, transformer_outputs.squeeze(-1))\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)  # Example batch size\n",
    "\n",
    "# Store outputs\n",
    "rnn_outputs = []\n",
    "\n",
    "# Run the model\n",
    "for i, (time_aligned_batch, global_features_batch, transformer_outputs_batch) in enumerate(dataloader):\n",
    "    outputs = model(time_aligned_batch, global_features_batch, transformer_outputs_batch)\n",
    "    rnn_outputs.append(outputs.detach())\n",
    "\n",
    "# Combine and save outputs\n",
    "rnn_outputs = torch.cat(rnn_outputs, dim=0)\n",
    "torch.save(rnn_outputs, '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c005cae7-e6be-4eb6-9db1-03337c7bb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in rnn_outputs: 0.0%\n",
      "Percentage of Infs in rnn_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "rnn_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth\"\n",
    "rnn_outputs = torch.load(rnn_outputs_path)\n",
    "\n",
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(rnn_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(rnn_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(rnn_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in rnn_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in rnn_outputs: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420a5095-e5ad-4fea-a830-a28cc02ab18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Percentage of NaNs in rnn_outputs: 0.0%\n",
      "Percentage of Infs in rnn_outputs: 0.0%\n",
      "After cleaning:\n",
      "Percentage of NaNs in rnn_outputs: 0.0%\n",
      "Percentage of Infs in rnn_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in rnn_outputs before cleaning\n",
    "total_elements = torch.numel(rnn_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(rnn_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(rnn_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Before cleaning:\")\n",
    "print(f\"Percentage of NaNs in rnn_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in rnn_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Replace NaNs and Infs with zeros\n",
    "rnn_outputs[torch.isnan(rnn_outputs)] = 0\n",
    "rnn_outputs[torch.isinf(rnn_outputs)] = 0\n",
    "\n",
    "# Re-check for NaNs and Infs after cleaning\n",
    "nans_count = torch.sum(torch.isnan(rnn_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(rnn_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"After cleaning:\")\n",
    "print(f\"Percentage of NaNs in rnn_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in rnn_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Optionally, save the cleaned tensor\n",
    "torch.save(rnn_outputs, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522cfb9-adcc-4979-a058-fbfda120cca2",
   "metadata": {},
   "source": [
    "# Final predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb81f6e7-8849-4af6-896d-80aac78e3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf2e0ec-2207-4be3-bf42-2768d6dd33fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of RNN_outputs: torch.Size([66, 1000, 128])\n",
      "Shape of transformer_outputs: torch.Size([66, 1000, 64, 1])\n",
      "Shape of eeg_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load saved features\n",
    "transformer_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\"\n",
    "eeg_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\"\n",
    "band_power_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\"\n",
    "fast_fourier_transform_psd_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\"\n",
    "RNN_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth\"\n",
    "\n",
    "transformer_outputs = torch.load(transformer_outputs_path)\n",
    "eeg_tensor = torch.load(eeg_tensor_path)\n",
    "band_power_tensor = torch.load(band_power_path)\n",
    "fast_fourier_transform_psd_tensor = torch.load(fast_fourier_transform_psd_path)\n",
    "RNN_outputs = torch.load(RNN_outputs_path)\n",
    "\n",
    "print(f\"Shape of RNN_outputs: {RNN_outputs.shape}\")\n",
    "print(f\"Shape of transformer_outputs: {transformer_outputs.shape}\")\n",
    "print(f\"Shape of eeg_tensor: {eeg_tensor.shape}\")\n",
    "print(f\"Shape of band_power_tensor: {band_power_tensor.shape}\")\n",
    "print(f\"Shape of fast_fourier_transform_psd_tensor: {fast_fourier_transform_psd_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb55a49-e5d0-4d01-9d10-a3733a9ab855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated time-aligned features shape: torch.Size([4227788, 224])\n"
     ]
    }
   ],
   "source": [
    "# Aligning the time axis\n",
    "time_length = 4227788  # replace with the length of the common time axis\n",
    "\n",
    "# Reshape `eeg_tensor` to align it with time_length\n",
    "eeg_tensor_reshaped = eeg_tensor.squeeze().transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Reshape `band_power_tensor` to align it with time_length\n",
    "band_power_tensor_reshaped = band_power_tensor.view(time_length, -1)  # [time_length, 32*5]\n",
    "\n",
    "# Reshape `fast_fourier_transform_psd_tensor` to align it with time_length\n",
    "fast_fourier_transform_psd_tensor_reshaped = fast_fourier_transform_psd_tensor.transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Concatenating time-aligned tensors\n",
    "concatenated_time_aligned_features = torch.cat(\n",
    "    (eeg_tensor_reshaped, band_power_tensor_reshaped, fast_fourier_transform_psd_tensor_reshaped), \n",
    "    dim=1\n",
    ")\n",
    "\n",
    "print(\"Concatenated time-aligned features shape:\", concatenated_time_aligned_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6268a2-499f-41c1-9bc6-15a756762782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in transformer_outputs: 0.0%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n",
      "Percentage of NaNs in eeg_tensor: 0.0%\n",
      "Percentage of Infs in eeg_tensor: 0.0%\n",
      "Percentage of NaNs in band_power_tensor: 0.0%\n",
      "Percentage of Infs in band_power_tensor: 0.0%\n",
      "Percentage of NaNs in fast_fourier_transform_psd_tensor: 0.0%\n",
      "Percentage of Infs in fast_fourier_transform_psd_tensor: 0.0%\n",
      "Percentage of NaNs in RNN_outputs: 0.0%\n",
      "Percentage of Infs in RNN_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "def check_tensor(tensor, name):\n",
    "    total_elements = torch.numel(tensor)\n",
    "    nans_count = torch.sum(torch.isnan(tensor)).item()\n",
    "    infs_count = torch.sum(torch.isinf(tensor)).item()\n",
    "    nans_percentage = (nans_count / total_elements) * 100\n",
    "    infs_percentage = (infs_count / total_elements) * 100\n",
    "    print(f\"Percentage of NaNs in {name}: {nans_percentage}%\")\n",
    "    print(f\"Percentage of Infs in {name}: {infs_percentage}%\")\n",
    "\n",
    "check_tensor(transformer_outputs, \"transformer_outputs\")\n",
    "check_tensor(eeg_tensor, \"eeg_tensor\")\n",
    "check_tensor(band_power_tensor, \"band_power_tensor\")\n",
    "check_tensor(fast_fourier_transform_psd_tensor, \"fast_fourier_transform_psd_tensor\")\n",
    "check_tensor(RNN_outputs, \"RNN_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8eaafc-bde4-4af0-a497-d22dfb59c07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in eeg_tensor_reshaped: 0.0%\n",
      "Percentage of Infs in eeg_tensor_reshaped: 0.0%\n",
      "Percentage of NaNs in band_power_tensor_reshaped: 0.0%\n",
      "Percentage of Infs in band_power_tensor_reshaped: 0.0%\n",
      "Percentage of NaNs in fast_fourier_transform_psd_tensor_reshaped: 0.0%\n",
      "Percentage of Infs in fast_fourier_transform_psd_tensor_reshaped: 0.0%\n",
      "Percentage of NaNs in concatenated_time_aligned_features: 0.0%\n",
      "Percentage of Infs in concatenated_time_aligned_features: 0.0%\n"
     ]
    }
   ],
   "source": [
    "check_tensor(eeg_tensor_reshaped, \"eeg_tensor_reshaped\")\n",
    "check_tensor(band_power_tensor_reshaped, \"band_power_tensor_reshaped\")\n",
    "check_tensor(fast_fourier_transform_psd_tensor_reshaped, \"fast_fourier_transform_psd_tensor_reshaped\")\n",
    "check_tensor(concatenated_time_aligned_features, \"concatenated_time_aligned_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf09775-dadf-414a-b6c5-db5464433b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in concatenated_time_aligned_features_outputs: 0.0%\n",
      "Percentage of Infs in concatenated_time_aligned_features_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(concatenated_time_aligned_features)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(concatenated_time_aligned_features)).item()\n",
    "infs_count = torch.sum(torch.isinf(concatenated_time_aligned_features)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in concatenated_time_aligned_features_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in concatenated_time_aligned_features_outputs: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab01f1ae-dcea-4e47-b458-fa37ccdc2321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4227788, 352])\n"
     ]
    }
   ],
   "source": [
    "# Ensure all tensors are on the CPU\n",
    "RNN_outputs = RNN_outputs.to('cpu')\n",
    "concatenated_time_aligned_features = concatenated_time_aligned_features.to('cpu')\n",
    "\n",
    "# Reduce the RNN outputs' time dimension by averaging\n",
    "RNN_outputs_reduced = torch.mean(RNN_outputs, dim=1)  # Shape: [66, 128]\n",
    "\n",
    "# Reshape `RNN_outputs` to align it with time_length\n",
    "RNN_outputs_reshaped = RNN_outputs.reshape(-1, RNN_outputs.shape[-1])  # [66*1000, 128] = [66000, 128]\n",
    "\n",
    "# Compute the number of repetitions needed to approximate the time_length\n",
    "n_repeats = time_length // RNN_outputs_reshaped.shape[0]\n",
    "remaining_rows = time_length % RNN_outputs_reshaped.shape[0]\n",
    "\n",
    "# Repeat the tensor for n_repeats times and add extra padding if needed\n",
    "RNN_outputs_expanded = RNN_outputs_reshaped.repeat(n_repeats, 1)\n",
    "if remaining_rows > 0:\n",
    "    extra_padding = RNN_outputs_reshaped[:remaining_rows]\n",
    "    RNN_outputs_expanded = torch.cat([RNN_outputs_expanded, extra_padding], dim=0)\n",
    "\n",
    "# Initialize a list to hold the smaller tensors\n",
    "final_input_tensor_list = []\n",
    "\n",
    "# Split and concatenate in chunks to reduce memory footprint\n",
    "split_size = 1000  # adjust as needed\n",
    "for i in range(0, time_length, split_size):\n",
    "    # Ensure the slice size matches for both tensors\n",
    "    slice_size = min(split_size, time_length - i)\n",
    "    temp_concat = torch.cat(\n",
    "        (concatenated_time_aligned_features[i:i + slice_size], \n",
    "         RNN_outputs_expanded[i:i + slice_size]), \n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "    # Append the tensor to the list\n",
    "    final_input_tensor_list.append(temp_concat)\n",
    "    \n",
    "    # Optionally, save this tensor to disk to free up memory\n",
    "    torch.save(temp_concat, f\"/home/vincent/AAA_projects/MVCS/Neuroscience/tempfiles/temp_concat_chunk_{i//split_size}.pt\")\n",
    "\n",
    "# Update feature_dim to the new size after concatenation\n",
    "feature_dim = final_input_tensor_list[0].shape[1]  # Taking shape from one of the chunks\n",
    "\n",
    "# Pre-allocate a zero tensor with the required size\n",
    "final_time_length = 4227788  # replace with your value\n",
    "final_feature_dim = feature_dim  # replace with your feature dimension\n",
    "\n",
    "# Pre-allocate on CPU\n",
    "final_input_tensor = torch.zeros((final_time_length, final_feature_dim))\n",
    "\n",
    "# Fill in the slices\n",
    "start_idx = 0\n",
    "for temp_tensor in final_input_tensor_list:\n",
    "    end_idx = start_idx + temp_tensor.shape[0]\n",
    "    final_input_tensor[start_idx:end_idx, :] = temp_tensor  # Tensor is already on CPU\n",
    "    start_idx = end_idx  # set start_idx for the next iteration\n",
    "\n",
    "# The tensor final_input_tensor should now have shape [4227788, feature_dim]\n",
    "print(final_input_tensor.shape)\n",
    "\n",
    "# Save the tensor\n",
    "torch.save(final_input_tensor, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/final_input_tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47da3217-a61d-4d24-be1f-caa8c95e1ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in temp_concat_chunk_4227: 0.0%\n",
      "Percentage of Infs in temp_concat_chunk_4227: 0.0%\n"
     ]
    }
   ],
   "source": [
    "check_tensor(temp_concat, f\"temp_concat_chunk_{i//split_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede5313c-2781-4926-a515-6a6d6898b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in final_input_tensor: 0.0%\n",
      "Percentage of Infs in final_input_tensor: 0.0%\n"
     ]
    }
   ],
   "source": [
    "final_input_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/final_input_tensor\"\n",
    "final_input_tensor = torch.load(final_input_tensor_path)\n",
    "\n",
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(final_input_tensor)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(final_input_tensor)).item()\n",
    "infs_count = torch.sum(torch.isinf(final_input_tensor)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in final_input_tensor: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in final_input_tensor: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea9271-d953-4eb2-bffc-966b3885c984",
   "metadata": {},
   "source": [
    "# prepare to train test validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10982ddf-5fe3-46d2-ba52-c68a870898bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35bb88da-9e74-43b7-8188-c88ad98ff212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4227788, 352])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the tensor\n",
    "final_input_tensor = torch.load(\"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/final_input_tensor\")\n",
    "\n",
    "# Verify that the tensor was loaded correctly\n",
    "print(final_input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4c76aa-8546-4dd9-8807-3b12e5d27dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one batch from train_loader_X DataLoader: torch.Size([64, 1000, 352])\n",
      "Shape of one batch from val_loader_X DataLoader: torch.Size([64, 1000, 352])\n",
      "Shape of one batch from test_loader_X DataLoader: torch.Size([64, 1000, 352])\n",
      "Shape of one batch from train_loader_Y DataLoader: torch.Size([64, 1000, 32])\n",
      "Shape of one batch from val_loader_Y DataLoader: torch.Size([64, 1000, 32])\n",
      "Shape of one batch from test_loader_Y DataLoader: torch.Size([64, 1000, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_mini_batches(tensor, seq_length, batch_size):\n",
    "    dataset_list = []\n",
    "    for i in range(0, tensor.shape[0] - seq_length, seq_length):\n",
    "        end_idx = min(i + seq_length, tensor.shape[0])\n",
    "        subset = tensor[i:end_idx]\n",
    "        dataset_list.append(subset)\n",
    "        \n",
    "    if len(dataset_list) == 0:\n",
    "        raise ValueError(\"dataset_list is empty. Check the tensor dimensions.\")\n",
    "        \n",
    "    combined_dataset = torch.stack(dataset_list)\n",
    "    tensor_dataset = TensorDataset(combined_dataset)\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "# Load EEG data (labels)\n",
    "eeg_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\"\n",
    "eeg_tensor = torch.load(eeg_tensor_path)\n",
    "eeg_data = eeg_tensor.squeeze().transpose(0, 1)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "total_data = len(eeg_data)\n",
    "train_split = int(0.8 * total_data)\n",
    "val_split = int(0.9 * total_data)\n",
    "\n",
    "train_data_Y = eeg_data[:train_split]\n",
    "val_data_Y = eeg_data[train_split:val_split]\n",
    "test_data_Y = eeg_data[val_split:]\n",
    "\n",
    "train_data_X = final_input_tensor[:train_split]\n",
    "val_data_X = final_input_tensor[train_split:val_split]\n",
    "test_data_X = final_input_tensor[val_split:]\n",
    "\n",
    "# Parameters\n",
    "seq_length = 1000\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_Y = create_mini_batches(train_data_Y, seq_length, batch_size)\n",
    "val_loader_Y = create_mini_batches(val_data_Y, seq_length, batch_size)\n",
    "test_loader_Y = create_mini_batches(test_data_Y, seq_length, batch_size)\n",
    "\n",
    "train_loader_X = create_mini_batches(train_data_X, seq_length, batch_size)\n",
    "val_loader_X = create_mini_batches(val_data_X, seq_length, batch_size)\n",
    "test_loader_X = create_mini_batches(test_data_X, seq_length, batch_size)\n",
    "\n",
    "\n",
    "# Print the shape of one batch for each DataLoader\n",
    "def print_one_batch_shape(data_loader, name):\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        input_batch = batch[0]  # Extracting tensor from tuple\n",
    "        print(f\"Shape of one batch from {name} DataLoader: {input_batch.shape}\")\n",
    "        break  # Stop after the first batch\n",
    "\n",
    "# Print shapes for X DataLoaders\n",
    "print_one_batch_shape(train_loader_X, \"train_loader_X\")\n",
    "print_one_batch_shape(val_loader_X, \"val_loader_X\")\n",
    "print_one_batch_shape(test_loader_X, \"test_loader_X\")\n",
    "\n",
    "# Print shapes for Y DataLoaders\n",
    "print_one_batch_shape(train_loader_Y, \"train_loader_Y\")\n",
    "print_one_batch_shape(val_loader_Y, \"val_loader_Y\")\n",
    "print_one_batch_shape(test_loader_Y, \"test_loader_Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0461b6-7fec-4356-9290-50de3f499541",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535b0531-a3a6-48d3-873f-f6a4fba6e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Add weight initialization in the constructor\n",
    "class EEGSeq2SeqPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(EEGSeq2SeqPredictor, self).__init__()\n",
    "\n",
    "        # Initialize fully connected layers for input dimension reduction\n",
    "        self.input_fc_X = nn.Linear(352, d_model)\n",
    "        self.input_fc_Y = nn.Linear(32, d_model)\n",
    "        \n",
    "        # Initialize Transformer Encoder and Decoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Initialize fully connected layer for output\n",
    "        self.fc = nn.Linear(d_model, 32)\n",
    "\n",
    "        # Correct the weight initialization\n",
    "        for p in self.parameters():  # Change 'model' to 'self'\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)  # corrected syntax\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # Dimension reduction\n",
    "        src = self.input_fc_X(src)\n",
    "        tgt = self.input_fc_Y(tgt)\n",
    "        \n",
    "        # Transformer Encoder-Decoder\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4c1e4-12fa-4bd1-8db2-21926973292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Enable anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 32\n",
    "nhead = 4\n",
    "num_layers = 6\n",
    "dim_feedforward = 128\n",
    "lr = 0.001  # Lower initial learning rate\n",
    "batch_size = 64\n",
    "seq_len = 1000\n",
    "num_epochs = 50\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cpu')\n",
    "model = EEGSeq2SeqPredictor(d_model, nhead, num_layers, dim_feedforward).to(device)\n",
    "\n",
    "# Count parameters\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# Initialize model weights\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        if param.dim() >= 2:\n",
    "            nn.init.kaiming_uniform_(param.data)\n",
    "    elif 'bias' in name:\n",
    "        nn.init.constant_(param.data, 0)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)\n",
    "\n",
    "# Placeholder for the best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Start of Epoch\", epoch + 1)  # Debugging line\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "\n",
    "    for (input_batch_X,), (input_batch_Y,) in zip(train_loader_X, train_loader_Y):\n",
    "        #print(\"Batch loaded\")  # Debugging line\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Debug: Check if input contains NaN or Inf\n",
    "        if torch.isnan(input_batch_X).any() or torch.isinf(input_batch_X).any():\n",
    "            print(\"NaN or Inf found in input, skipping batch\")\n",
    "            continue\n",
    "\n",
    "        outputs = model(input_batch_X, input_batch_Y[:-1])\n",
    "\n",
    "        # Debug: Check if output contains NaN or Inf\n",
    "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "            print(\"NaN or Inf found in model output, skipping batch\")\n",
    "            continue\n",
    "\n",
    "        loss = criterion(outputs, input_batch_Y[1:])\n",
    "\n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"NaN loss, stopping training\")\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Debug: Print the gradient norms\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if param.grad is not None:\n",
    "        #        print(f\"{name}: Gradient Norm: {torch.norm(param.grad)}\")\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "\n",
    "    for (input_batch_X,), (input_batch_Y,) in zip(val_loader_X, val_loader_Y):\n",
    "        input_batch_X, input_batch_Y = input_batch_X.to(device), input_batch_Y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_batch_X, input_batch_Y[:-1])\n",
    "            batch_loss = criterion(outputs, input_batch_Y[1:])\n",
    "            val_loss += batch_loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "\n",
    "    # Store the average losses for this epoch\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "\n",
    "    # Print current learning rate, average training loss, and average validation loss\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Current learning rate is: {param_group['lr']}\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Update learning rate scheduler based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Save the model if it's the best one so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/best_model.pth')\n",
    "\n",
    "    # Check if learning rate is too small\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr'] < 1e-10:\n",
    "            print(\"Learning rate too small, stopping training\")\n",
    "            break\n",
    "\n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"Stopping training due to NaN loss\")\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e35100ec-f2cc-4f99-8133-4ca9ed4f429e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EEGSeq2SeqPredictor.forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming your model and input size is already defined\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m352\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace input_size with the actual input size\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming attention_weights is a tensor containing the attention map\u001b[39;00m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(attention_weights\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: EEGSeq2SeqPredictor.forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Assuming your model and input size is already defined\n",
    "summary(model, input_size=(64, 1000, 352))  # Replace input_size with the actual input size\n",
    "# Assuming attention_weights is a tensor containing the attention map\n",
    "plt.imshow(attention_weights.cpu().detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Assuming attention_weights is a tensor containing the attention map\n",
    "plt.imshow(attention_weights.cpu().detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801f3a0-96d6-4648-a305-2526c99d764f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30444a6-652d-44f5-83f5-628b6dc5ab52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83e00840-9c5b-4d4d-b73e-83b621eeaac0",
   "metadata": {},
   "source": [
    "# second version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09e10b7-db2c-4231-a2db-84d18c5fcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class EEGSeq2SeqPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(EEGSeq2SeqPredictor, self).__init__()\n",
    "\n",
    "        # Input transformations\n",
    "        self.input_batch_X = nn.Linear(352, d_model)\n",
    "        self.input_batch_Y = nn.Linear(32, d_model)\n",
    "\n",
    "        # Define encoder and decoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=0.1)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc = nn.Linear(d_model, 32)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # Apply transformations and positional encoding\n",
    "        src = self.input_batch_X(src)\n",
    "        tgt = self.input_batch_Y(tgt)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # Forward pass through encoder and decoder\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        \n",
    "        # Generate final output\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efca97cd-0051-474b-b257-06a4a27b285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 726,112 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 32\n",
    "nhead = 4\n",
    "num_layers = 24\n",
    "dim_feedforward = 128\n",
    "lr = 0.1 \n",
    "batch_size = 64\n",
    "seq_len = 1000\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cpu')  # or 'cuda' if you have a GPU\n",
    "model = EEGSeq2SeqPredictor(d_model, nhead, num_layers, dim_feedforward).to(device)\n",
    "\n",
    "# Count parameters\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# Initialize model weights\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        if param.dim() >= 2:\n",
    "            nn.init.kaiming_uniform_(param.data)\n",
    "    elif 'bias' in name:\n",
    "        nn.init.constant_(param.data, 0)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Placeholder for the best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833d153-12f8-452e-8151-7f4f3d2ea16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Epoch 1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate time taken for each epoch\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start time tracking\n",
    "    print(f\"Start of Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "\n",
    "    # Training loop\n",
    "    for (input_batch_X,), (input_batch_Y,) in zip(train_loader_X, train_loader_Y):\n",
    "        input_batch_X, input_batch_Y = input_batch_X.to(device), input_batch_Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_batch_X, input_batch_Y[:-1])  # Encoding should be inside the model's forward()\n",
    "\n",
    "        # Debug: Check if input contains NaN or Inf\n",
    "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "            print(\"NaN or Inf found in model output, skipping batch\")\n",
    "            continue\n",
    "\n",
    "        loss = criterion(outputs, input_batch_Y[1:])\n",
    "\n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"NaN loss, stopping training\")\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    for (input_batch_X,), (input_batch_Y,) in zip(val_loader_X, val_loader_Y):\n",
    "        input_batch_X, input_batch_Y = input_batch_X.to(device), input_batch_Y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_batch_X, input_batch_Y[:-1])\n",
    "            batch_loss = criterion(outputs, input_batch_Y[1:])\n",
    "            val_loss += batch_loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "\n",
    "    end_time = time.time()  # End time tracking\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tAverage Training Loss: {avg_train_loss:.4f} | Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Current learning rate is: {param_group['lr']}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    if param_group['lr'] < 1e-10:\n",
    "        print(\"Learning rate too small, stopping training\")\n",
    "        break\n",
    "\n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"Stopping training due to NaN loss\")\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "543afb2f-4983-490b-9016-7a3a8e18bd04",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EEGSeq2SeqPredictor.forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming your model and input size is already defined\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m352\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace input_size with the actual input size\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming attention_weights is a tensor containing the attention map\u001b[39;00m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(attention_weights\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: EEGSeq2SeqPredictor.forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Assuming your model and input size is already defined\n",
    "summary(model, input_size=(64, 1000, 352))  # Replace input_size with the actual input size\n",
    "# Assuming attention_weights is a tensor containing the attention map\n",
    "plt.imshow(attention_weights.cpu().detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Assuming attention_weights is a tensor containing the attention map\n",
    "plt.imshow(attention_weights.cpu().detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f2de891-1149-4490-a301-9598a177eac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of outputs tensor: torch.Size([37, 1000, 32])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of outputs tensor:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad479e-99ab-44ba-af5d-c8906423d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load EEG data\n",
    "EEG_data = np.load('/home/vincent/AAA_projects/MVCS/Neuroscience/eeg_data_with_channels.npy', allow_pickle=True)\n",
    "\n",
    "# Get shape of EEG data\n",
    "n_timepoints, n_channels = EEG_data.shape\n",
    "\n",
    "# Standardize the EEG data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(EEG_data)\n",
    "scaled_EEG_data = scaler.transform(EEG_data)\n",
    "\n",
    "# Convert scaled EEG data back to PyTorch tensor\n",
    "eeg_data_scaled_tensor = torch.tensor(scaled_EEG_data, dtype=torch.float32)\n",
    "\n",
    "# Assuming 'outputs' is a PyTorch tensor (make sure it's defined)\n",
    "outputs_numpy = outputs.detach().numpy()\n",
    "\n",
    "# Reshape it to 2D for inverse_transform\n",
    "reshaped_outputs = outputs_numpy.reshape(-1, n_channels)\n",
    "\n",
    "# Use the inverse_transform function to transform the data back to the original scale\n",
    "outputs_original_scale = scaler.inverse_transform(reshaped_outputs)\n",
    "\n",
    "# Reshape the output back to its original form\n",
    "outputs_original_scale = outputs_original_scale.reshape(*outputs_numpy.shape)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "outputs_original_scale_tensor = torch.tensor(outputs_original_scale, dtype=torch.float32)\n",
    "\n",
    "# Concatenating the output batches to create a sequence of length 63*1000\n",
    "concatenated_outputs = outputs_original_scale_tensor.reshape(-1, n_channels)\n",
    "\n",
    "# Channel names (assuming you have them)\n",
    "channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                 'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                 'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "# Slicing to get only the first five rows of the concatenated output for all channels\n",
    "first_five_rows = concatenated_outputs[:5, :]\n",
    "\n",
    "print(\"\\nFirst five rows of inverse-transformed model predicted data after concatenation:\")\n",
    "print(pd.DataFrame(first_five_rows.numpy(), columns=channel_names))\n",
    "\n",
    "# First five rows of original data\n",
    "print(\"\\nFirst five rows of original data:\")\n",
    "print(pd.DataFrame(EEG_data[:5, :], columns=channel_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0770fc9-94d2-40c2-b827-a5229f082e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First ... data points of original data for the first channel\n",
    "true_data_np = EEG_data[:60000, 4]  # Already a numpy array, first channel\n",
    "\n",
    "# First ... data points of concatenated, inverse-transformed model predicted data for the first channel\n",
    "predicted_data_np = concatenated_outputs[:60000, 4]  # We use 'concatenated_outputs_np' array here\n",
    "\n",
    "# Configure Matplotlib to have a black background and light grey text\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "plt.rcParams['axes.edgecolor'] = 'lightgrey'\n",
    "plt.rcParams['axes.labelcolor'] = 'lightgrey'\n",
    "plt.rcParams['text.color'] = 'lightgrey'\n",
    "plt.rcParams['xtick.color'] = 'lightgrey'\n",
    "plt.rcParams['ytick.color'] = 'lightgrey'\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(30, 10), facecolor='black')\n",
    "\n",
    "# Plotting True EEG Data for the first channel\n",
    "plt.plot(true_data_np, color='white', alpha=0.8, label='True EEG')\n",
    "\n",
    "# Plotting Predicted EEG Data for the first channel\n",
    "plt.plot(predicted_data_np, color='red', alpha=0.5, label='Predicted EEG')\n",
    "\n",
    "# Adding title, legend, and axis labels\n",
    "plt.title('EEG Data - First Channel')\n",
    "plt.xlabel('Time Points')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Adding grid lines for better visibility of scale\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.5, color='lightgrey')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb75ab-7195-4854-ab51-0ec558ff8740",
   "metadata": {},
   "source": [
    "# postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a5c0829-ed40-4c02-949e-5af0920b92bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_data_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m predicted_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mpredicted_data_np\u001b[49m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChannel_1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m predicted_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoothed_Moving_Avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predicted_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChannel_1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gaussian_filter\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_data_np' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predicted_df = pd.DataFrame(predicted_data_np, columns=['Channel_1'])\n",
    "predicted_df['Smoothed_Moving_Avg'] = predicted_df['Channel_1'].rolling(window=5).mean()\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "smoothed_gaussian = gaussian_filter(predicted_data_np, sigma=2)\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "smoothed_savgol = savgol_filter(predicted_data_np, window_length=5, polyorder=2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Configure Matplotlib to have a black background and light grey text\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "plt.rcParams['axes.edgecolor'] = 'lightgrey'\n",
    "plt.rcParams['axes.labelcolor'] = 'lightgrey'\n",
    "plt.rcParams['text.color'] = 'lightgrey'\n",
    "plt.rcParams['xtick.color'] = 'lightgrey'\n",
    "plt.rcParams['ytick.color'] = 'lightgrey'\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(30, 10), facecolor='black')\n",
    "\n",
    "plt.plot(predicted_data_np, label=\"Predicted\", alpha=0.2)\n",
    "plt.plot(smoothed_savgol, label=\"Savitzky-Golay Predicted\", alpha=0.2)\n",
    "plt.plot(predicted_df['Smoothed_Moving_Avg'], label=\"Moving Average Predicted\", alpha=.3)\n",
    "plt.plot(smoothed_gaussian, color='red', label=\"Gaussian Smoothed Predicted\", alpha=0.9)\n",
    "plt.plot(true_data_np, color='white', alpha=0.5, label='True EEG')\n",
    "\n",
    "# Adding grid lines for better visibility of scale\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.5, color='lightgrey')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "eaaff876-1cbb-4806-a519-3c019e900154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BandPowers_x: Dictionary with keys: ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
      "combined_fft_psd_x: Dictionary with keys: ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
      "SpectralEntropy_x: Dictionary with keys: ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
      "welchs_x: (4,)\n",
      "STFT_x: Dictionary with keys: ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
      "PeakFrequencies_x: Dictionary with keys: ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
      "full_granularity_transfer_entropy_results: Dictionary with keys: ['Fp1_to_Fpz', 'Fp1_to_Fp2', 'Fp1_to_F7', 'Fp1_to_F3', 'Fp1_to_Fz', 'Fp1_to_F4', 'Fp1_to_F8', 'Fp1_to_FC5', 'Fp1_to_FC1', 'Fp1_to_FC2', 'Fp1_to_FC6', 'Fp1_to_M1', 'Fp1_to_T7', 'Fp1_to_C3', 'Fp1_to_Cz', 'Fp1_to_C4', 'Fp1_to_T8', 'Fp1_to_M2', 'Fp1_to_CP5', 'Fp1_to_CP1', 'Fp1_to_CP2', 'Fp1_to_CP6', 'Fp1_to_P7', 'Fp1_to_P3', 'Fp1_to_Pz', 'Fp1_to_P4', 'Fp1_to_P8', 'Fp1_to_POz', 'Fp1_to_O1', 'Fp1_to_Oz', 'Fp1_to_O2', 'Fpz_to_Fp1', 'Fpz_to_Fp2', 'Fpz_to_F7', 'Fpz_to_F3', 'Fpz_to_Fz', 'Fpz_to_F4', 'Fpz_to_F8', 'Fpz_to_FC5', 'Fpz_to_FC1', 'Fpz_to_FC2', 'Fpz_to_FC6', 'Fpz_to_M1', 'Fpz_to_T7', 'Fpz_to_C3', 'Fpz_to_Cz', 'Fpz_to_C4', 'Fpz_to_T8', 'Fpz_to_M2', 'Fpz_to_CP5', 'Fpz_to_CP1', 'Fpz_to_CP2', 'Fpz_to_CP6', 'Fpz_to_P7', 'Fpz_to_P3', 'Fpz_to_Pz', 'Fpz_to_P4', 'Fpz_to_P8', 'Fpz_to_POz', 'Fpz_to_O1', 'Fpz_to_Oz', 'Fpz_to_O2', 'Fp2_to_Fp1', 'Fp2_to_Fpz', 'Fp2_to_F7', 'Fp2_to_F3', 'Fp2_to_Fz', 'Fp2_to_F4', 'Fp2_to_F8', 'Fp2_to_FC5', 'Fp2_to_FC1', 'Fp2_to_FC2', 'Fp2_to_FC6', 'Fp2_to_M1', 'Fp2_to_T7', 'Fp2_to_C3', 'Fp2_to_Cz', 'Fp2_to_C4', 'Fp2_to_T8', 'Fp2_to_M2', 'Fp2_to_CP5', 'Fp2_to_CP1', 'Fp2_to_CP2', 'Fp2_to_CP6', 'Fp2_to_P7', 'Fp2_to_P3', 'Fp2_to_Pz', 'Fp2_to_P4', 'Fp2_to_P8', 'Fp2_to_POz', 'Fp2_to_O1', 'Fp2_to_Oz', 'Fp2_to_O2', 'F7_to_Fp1', 'F7_to_Fpz', 'F7_to_Fp2', 'F7_to_F3', 'F7_to_Fz', 'F7_to_F4', 'F7_to_F8', 'F7_to_FC5', 'F7_to_FC1', 'F7_to_FC2', 'F7_to_FC6', 'F7_to_M1', 'F7_to_T7', 'F7_to_C3', 'F7_to_Cz', 'F7_to_C4', 'F7_to_T8', 'F7_to_M2', 'F7_to_CP5', 'F7_to_CP1', 'F7_to_CP2', 'F7_to_CP6', 'F7_to_P7', 'F7_to_P3', 'F7_to_Pz', 'F7_to_P4', 'F7_to_P8', 'F7_to_POz', 'F7_to_O1', 'F7_to_Oz', 'F7_to_O2', 'F3_to_Fp1', 'F3_to_Fpz', 'F3_to_Fp2', 'F3_to_F7', 'F3_to_Fz', 'F3_to_F4', 'F3_to_F8', 'F3_to_FC5', 'F3_to_FC1', 'F3_to_FC2', 'F3_to_FC6', 'F3_to_M1', 'F3_to_T7', 'F3_to_C3', 'F3_to_Cz', 'F3_to_C4', 'F3_to_T8', 'F3_to_M2', 'F3_to_CP5', 'F3_to_CP1', 'F3_to_CP2', 'F3_to_CP6', 'F3_to_P7', 'F3_to_P3', 'F3_to_Pz', 'F3_to_P4', 'F3_to_P8', 'F3_to_POz', 'F3_to_O1', 'F3_to_Oz', 'F3_to_O2', 'Fz_to_Fp1', 'Fz_to_Fpz', 'Fz_to_Fp2', 'Fz_to_F7', 'Fz_to_F3', 'Fz_to_F4', 'Fz_to_F8', 'Fz_to_FC5', 'Fz_to_FC1', 'Fz_to_FC2', 'Fz_to_FC6', 'Fz_to_M1', 'Fz_to_T7', 'Fz_to_C3', 'Fz_to_Cz', 'Fz_to_C4', 'Fz_to_T8', 'Fz_to_M2', 'Fz_to_CP5', 'Fz_to_CP1', 'Fz_to_CP2', 'Fz_to_CP6', 'Fz_to_P7', 'Fz_to_P3', 'Fz_to_Pz', 'Fz_to_P4', 'Fz_to_P8', 'Fz_to_POz', 'Fz_to_O1', 'Fz_to_Oz', 'Fz_to_O2', 'F4_to_Fp1', 'F4_to_Fpz', 'F4_to_Fp2', 'F4_to_F7', 'F4_to_F3', 'F4_to_Fz', 'F4_to_F8', 'F4_to_FC5', 'F4_to_FC1', 'F4_to_FC2', 'F4_to_FC6', 'F4_to_M1', 'F4_to_T7', 'F4_to_C3', 'F4_to_Cz', 'F4_to_C4', 'F4_to_T8', 'F4_to_M2', 'F4_to_CP5', 'F4_to_CP1', 'F4_to_CP2', 'F4_to_CP6', 'F4_to_P7', 'F4_to_P3', 'F4_to_Pz', 'F4_to_P4', 'F4_to_P8', 'F4_to_POz', 'F4_to_O1', 'F4_to_Oz', 'F4_to_O2', 'F8_to_Fp1', 'F8_to_Fpz', 'F8_to_Fp2', 'F8_to_F7', 'F8_to_F3', 'F8_to_Fz', 'F8_to_F4', 'F8_to_FC5', 'F8_to_FC1', 'F8_to_FC2', 'F8_to_FC6', 'F8_to_M1', 'F8_to_T7', 'F8_to_C3', 'F8_to_Cz', 'F8_to_C4', 'F8_to_T8', 'F8_to_M2', 'F8_to_CP5', 'F8_to_CP1', 'F8_to_CP2', 'F8_to_CP6', 'F8_to_P7', 'F8_to_P3', 'F8_to_Pz', 'F8_to_P4', 'F8_to_P8', 'F8_to_POz', 'F8_to_O1', 'F8_to_Oz', 'F8_to_O2', 'FC5_to_Fp1', 'FC5_to_Fpz', 'FC5_to_Fp2', 'FC5_to_F7', 'FC5_to_F3', 'FC5_to_Fz', 'FC5_to_F4', 'FC5_to_F8', 'FC5_to_FC1', 'FC5_to_FC2', 'FC5_to_FC6', 'FC5_to_M1', 'FC5_to_T7', 'FC5_to_C3', 'FC5_to_Cz', 'FC5_to_C4', 'FC5_to_T8', 'FC5_to_M2', 'FC5_to_CP5', 'FC5_to_CP1', 'FC5_to_CP2', 'FC5_to_CP6', 'FC5_to_P7', 'FC5_to_P3', 'FC5_to_Pz', 'FC5_to_P4', 'FC5_to_P8', 'FC5_to_POz', 'FC5_to_O1', 'FC5_to_Oz', 'FC5_to_O2', 'FC1_to_Fp1', 'FC1_to_Fpz', 'FC1_to_Fp2', 'FC1_to_F7', 'FC1_to_F3', 'FC1_to_Fz', 'FC1_to_F4', 'FC1_to_F8', 'FC1_to_FC5', 'FC1_to_FC2', 'FC1_to_FC6', 'FC1_to_M1', 'FC1_to_T7', 'FC1_to_C3', 'FC1_to_Cz', 'FC1_to_C4', 'FC1_to_T8', 'FC1_to_M2', 'FC1_to_CP5', 'FC1_to_CP1', 'FC1_to_CP2', 'FC1_to_CP6', 'FC1_to_P7', 'FC1_to_P3', 'FC1_to_Pz', 'FC1_to_P4', 'FC1_to_P8', 'FC1_to_POz', 'FC1_to_O1', 'FC1_to_Oz', 'FC1_to_O2', 'FC2_to_Fp1', 'FC2_to_Fpz', 'FC2_to_Fp2', 'FC2_to_F7', 'FC2_to_F3', 'FC2_to_Fz', 'FC2_to_F4', 'FC2_to_F8', 'FC2_to_FC5', 'FC2_to_FC1', 'FC2_to_FC6', 'FC2_to_M1', 'FC2_to_T7', 'FC2_to_C3', 'FC2_to_Cz', 'FC2_to_C4', 'FC2_to_T8', 'FC2_to_M2', 'FC2_to_CP5', 'FC2_to_CP1', 'FC2_to_CP2', 'FC2_to_CP6', 'FC2_to_P7', 'FC2_to_P3', 'FC2_to_Pz', 'FC2_to_P4', 'FC2_to_P8', 'FC2_to_POz', 'FC2_to_O1', 'FC2_to_Oz', 'FC2_to_O2', 'FC6_to_Fp1', 'FC6_to_Fpz', 'FC6_to_Fp2', 'FC6_to_F7', 'FC6_to_F3', 'FC6_to_Fz', 'FC6_to_F4', 'FC6_to_F8', 'FC6_to_FC5', 'FC6_to_FC1', 'FC6_to_FC2', 'FC6_to_M1', 'FC6_to_T7', 'FC6_to_C3', 'FC6_to_Cz', 'FC6_to_C4', 'FC6_to_T8', 'FC6_to_M2', 'FC6_to_CP5', 'FC6_to_CP1', 'FC6_to_CP2', 'FC6_to_CP6', 'FC6_to_P7', 'FC6_to_P3', 'FC6_to_Pz', 'FC6_to_P4', 'FC6_to_P8', 'FC6_to_POz', 'FC6_to_O1', 'FC6_to_Oz', 'FC6_to_O2', 'M1_to_Fp1', 'M1_to_Fpz', 'M1_to_Fp2', 'M1_to_F7', 'M1_to_F3', 'M1_to_Fz', 'M1_to_F4', 'M1_to_F8', 'M1_to_FC5', 'M1_to_FC1', 'M1_to_FC2', 'M1_to_FC6', 'M1_to_T7', 'M1_to_C3', 'M1_to_Cz', 'M1_to_C4', 'M1_to_T8', 'M1_to_M2', 'M1_to_CP5', 'M1_to_CP1', 'M1_to_CP2', 'M1_to_CP6', 'M1_to_P7', 'M1_to_P3', 'M1_to_Pz', 'M1_to_P4', 'M1_to_P8', 'M1_to_POz', 'M1_to_O1', 'M1_to_Oz', 'M1_to_O2', 'T7_to_Fp1', 'T7_to_Fpz', 'T7_to_Fp2', 'T7_to_F7', 'T7_to_F3', 'T7_to_Fz', 'T7_to_F4', 'T7_to_F8', 'T7_to_FC5', 'T7_to_FC1', 'T7_to_FC2', 'T7_to_FC6', 'T7_to_M1', 'T7_to_C3', 'T7_to_Cz', 'T7_to_C4', 'T7_to_T8', 'T7_to_M2', 'T7_to_CP5', 'T7_to_CP1', 'T7_to_CP2', 'T7_to_CP6', 'T7_to_P7', 'T7_to_P3', 'T7_to_Pz', 'T7_to_P4', 'T7_to_P8', 'T7_to_POz', 'T7_to_O1', 'T7_to_Oz', 'T7_to_O2', 'C3_to_Fp1', 'C3_to_Fpz', 'C3_to_Fp2', 'C3_to_F7', 'C3_to_F3', 'C3_to_Fz', 'C3_to_F4', 'C3_to_F8', 'C3_to_FC5', 'C3_to_FC1', 'C3_to_FC2', 'C3_to_FC6', 'C3_to_M1', 'C3_to_T7', 'C3_to_Cz', 'C3_to_C4', 'C3_to_T8', 'C3_to_M2', 'C3_to_CP5', 'C3_to_CP1', 'C3_to_CP2', 'C3_to_CP6', 'C3_to_P7', 'C3_to_P3', 'C3_to_Pz', 'C3_to_P4', 'C3_to_P8', 'C3_to_POz', 'C3_to_O1', 'C3_to_Oz', 'C3_to_O2', 'Cz_to_Fp1', 'Cz_to_Fpz', 'Cz_to_Fp2', 'Cz_to_F7', 'Cz_to_F3', 'Cz_to_Fz', 'Cz_to_F4', 'Cz_to_F8', 'Cz_to_FC5', 'Cz_to_FC1', 'Cz_to_FC2', 'Cz_to_FC6', 'Cz_to_M1', 'Cz_to_T7', 'Cz_to_C3', 'Cz_to_C4', 'Cz_to_T8', 'Cz_to_M2', 'Cz_to_CP5', 'Cz_to_CP1', 'Cz_to_CP2', 'Cz_to_CP6', 'Cz_to_P7', 'Cz_to_P3', 'Cz_to_Pz', 'Cz_to_P4', 'Cz_to_P8', 'Cz_to_POz', 'Cz_to_O1', 'Cz_to_Oz', 'Cz_to_O2', 'C4_to_Fp1', 'C4_to_Fpz', 'C4_to_Fp2', 'C4_to_F7', 'C4_to_F3', 'C4_to_Fz', 'C4_to_F4', 'C4_to_F8', 'C4_to_FC5', 'C4_to_FC1', 'C4_to_FC2', 'C4_to_FC6', 'C4_to_M1', 'C4_to_T7', 'C4_to_C3', 'C4_to_Cz', 'C4_to_T8', 'C4_to_M2', 'C4_to_CP5', 'C4_to_CP1', 'C4_to_CP2', 'C4_to_CP6', 'C4_to_P7', 'C4_to_P3', 'C4_to_Pz', 'C4_to_P4', 'C4_to_P8', 'C4_to_POz', 'C4_to_O1', 'C4_to_Oz', 'C4_to_O2', 'T8_to_Fp1', 'T8_to_Fpz', 'T8_to_Fp2', 'T8_to_F7', 'T8_to_F3', 'T8_to_Fz', 'T8_to_F4', 'T8_to_F8', 'T8_to_FC5', 'T8_to_FC1', 'T8_to_FC2', 'T8_to_FC6', 'T8_to_M1', 'T8_to_T7', 'T8_to_C3', 'T8_to_Cz', 'T8_to_C4', 'T8_to_M2', 'T8_to_CP5', 'T8_to_CP1', 'T8_to_CP2', 'T8_to_CP6', 'T8_to_P7', 'T8_to_P3', 'T8_to_Pz', 'T8_to_P4', 'T8_to_P8', 'T8_to_POz', 'T8_to_O1', 'T8_to_Oz', 'T8_to_O2', 'M2_to_Fp1', 'M2_to_Fpz', 'M2_to_Fp2', 'M2_to_F7', 'M2_to_F3', 'M2_to_Fz', 'M2_to_F4', 'M2_to_F8', 'M2_to_FC5', 'M2_to_FC1', 'M2_to_FC2', 'M2_to_FC6', 'M2_to_M1', 'M2_to_T7', 'M2_to_C3', 'M2_to_Cz', 'M2_to_C4', 'M2_to_T8', 'M2_to_CP5', 'M2_to_CP1', 'M2_to_CP2', 'M2_to_CP6', 'M2_to_P7', 'M2_to_P3', 'M2_to_Pz', 'M2_to_P4', 'M2_to_P8', 'M2_to_POz', 'M2_to_O1', 'M2_to_Oz', 'M2_to_O2', 'CP5_to_Fp1', 'CP5_to_Fpz', 'CP5_to_Fp2', 'CP5_to_F7', 'CP5_to_F3', 'CP5_to_Fz', 'CP5_to_F4', 'CP5_to_F8', 'CP5_to_FC5', 'CP5_to_FC1', 'CP5_to_FC2', 'CP5_to_FC6', 'CP5_to_M1', 'CP5_to_T7', 'CP5_to_C3', 'CP5_to_Cz', 'CP5_to_C4', 'CP5_to_T8', 'CP5_to_M2', 'CP5_to_CP1', 'CP5_to_CP2', 'CP5_to_CP6', 'CP5_to_P7', 'CP5_to_P3', 'CP5_to_Pz', 'CP5_to_P4', 'CP5_to_P8', 'CP5_to_POz', 'CP5_to_O1', 'CP5_to_Oz', 'CP5_to_O2', 'CP1_to_Fp1', 'CP1_to_Fpz', 'CP1_to_Fp2', 'CP1_to_F7', 'CP1_to_F3', 'CP1_to_Fz', 'CP1_to_F4', 'CP1_to_F8', 'CP1_to_FC5', 'CP1_to_FC1', 'CP1_to_FC2', 'CP1_to_FC6', 'CP1_to_M1', 'CP1_to_T7', 'CP1_to_C3', 'CP1_to_Cz', 'CP1_to_C4', 'CP1_to_T8', 'CP1_to_M2', 'CP1_to_CP5', 'CP1_to_CP2', 'CP1_to_CP6', 'CP1_to_P7', 'CP1_to_P3', 'CP1_to_Pz', 'CP1_to_P4', 'CP1_to_P8', 'CP1_to_POz', 'CP1_to_O1', 'CP1_to_Oz', 'CP1_to_O2', 'CP2_to_Fp1', 'CP2_to_Fpz', 'CP2_to_Fp2', 'CP2_to_F7', 'CP2_to_F3', 'CP2_to_Fz', 'CP2_to_F4', 'CP2_to_F8', 'CP2_to_FC5', 'CP2_to_FC1', 'CP2_to_FC2', 'CP2_to_FC6', 'CP2_to_M1', 'CP2_to_T7', 'CP2_to_C3', 'CP2_to_Cz', 'CP2_to_C4', 'CP2_to_T8', 'CP2_to_M2', 'CP2_to_CP5', 'CP2_to_CP1', 'CP2_to_CP6', 'CP2_to_P7', 'CP2_to_P3', 'CP2_to_Pz', 'CP2_to_P4', 'CP2_to_P8', 'CP2_to_POz', 'CP2_to_O1', 'CP2_to_Oz', 'CP2_to_O2', 'CP6_to_Fp1', 'CP6_to_Fpz', 'CP6_to_Fp2', 'CP6_to_F7', 'CP6_to_F3', 'CP6_to_Fz', 'CP6_to_F4', 'CP6_to_F8', 'CP6_to_FC5', 'CP6_to_FC1', 'CP6_to_FC2', 'CP6_to_FC6', 'CP6_to_M1', 'CP6_to_T7', 'CP6_to_C3', 'CP6_to_Cz', 'CP6_to_C4', 'CP6_to_T8', 'CP6_to_M2', 'CP6_to_CP5', 'CP6_to_CP1', 'CP6_to_CP2', 'CP6_to_P7', 'CP6_to_P3', 'CP6_to_Pz', 'CP6_to_P4', 'CP6_to_P8', 'CP6_to_POz', 'CP6_to_O1', 'CP6_to_Oz', 'CP6_to_O2', 'P7_to_Fp1', 'P7_to_Fpz', 'P7_to_Fp2', 'P7_to_F7', 'P7_to_F3', 'P7_to_Fz', 'P7_to_F4', 'P7_to_F8', 'P7_to_FC5', 'P7_to_FC1', 'P7_to_FC2', 'P7_to_FC6', 'P7_to_M1', 'P7_to_T7', 'P7_to_C3', 'P7_to_Cz', 'P7_to_C4', 'P7_to_T8', 'P7_to_M2', 'P7_to_CP5', 'P7_to_CP1', 'P7_to_CP2', 'P7_to_CP6', 'P7_to_P3', 'P7_to_Pz', 'P7_to_P4', 'P7_to_P8', 'P7_to_POz', 'P7_to_O1', 'P7_to_Oz', 'P7_to_O2', 'P3_to_Fp1', 'P3_to_Fpz', 'P3_to_Fp2', 'P3_to_F7', 'P3_to_F3', 'P3_to_Fz', 'P3_to_F4', 'P3_to_F8', 'P3_to_FC5', 'P3_to_FC1', 'P3_to_FC2', 'P3_to_FC6', 'P3_to_M1', 'P3_to_T7', 'P3_to_C3', 'P3_to_Cz', 'P3_to_C4', 'P3_to_T8', 'P3_to_M2', 'P3_to_CP5', 'P3_to_CP1', 'P3_to_CP2', 'P3_to_CP6', 'P3_to_P7', 'P3_to_Pz', 'P3_to_P4', 'P3_to_P8', 'P3_to_POz', 'P3_to_O1', 'P3_to_Oz', 'P3_to_O2', 'Pz_to_Fp1', 'Pz_to_Fpz', 'Pz_to_Fp2', 'Pz_to_F7', 'Pz_to_F3', 'Pz_to_Fz', 'Pz_to_F4', 'Pz_to_F8', 'Pz_to_FC5', 'Pz_to_FC1', 'Pz_to_FC2', 'Pz_to_FC6', 'Pz_to_M1', 'Pz_to_T7', 'Pz_to_C3', 'Pz_to_Cz', 'Pz_to_C4', 'Pz_to_T8', 'Pz_to_M2', 'Pz_to_CP5', 'Pz_to_CP1', 'Pz_to_CP2', 'Pz_to_CP6', 'Pz_to_P7', 'Pz_to_P3', 'Pz_to_P4', 'Pz_to_P8', 'Pz_to_POz', 'Pz_to_O1', 'Pz_to_Oz', 'Pz_to_O2', 'P4_to_Fp1', 'P4_to_Fpz', 'P4_to_Fp2', 'P4_to_F7', 'P4_to_F3', 'P4_to_Fz', 'P4_to_F4', 'P4_to_F8', 'P4_to_FC5', 'P4_to_FC1', 'P4_to_FC2', 'P4_to_FC6', 'P4_to_M1', 'P4_to_T7', 'P4_to_C3', 'P4_to_Cz', 'P4_to_C4', 'P4_to_T8', 'P4_to_M2', 'P4_to_CP5', 'P4_to_CP1', 'P4_to_CP2', 'P4_to_CP6', 'P4_to_P7', 'P4_to_P3', 'P4_to_Pz', 'P4_to_P8', 'P4_to_POz', 'P4_to_O1', 'P4_to_Oz', 'P4_to_O2', 'P8_to_Fp1', 'P8_to_Fpz', 'P8_to_Fp2', 'P8_to_F7', 'P8_to_F3', 'P8_to_Fz', 'P8_to_F4', 'P8_to_F8', 'P8_to_FC5', 'P8_to_FC1', 'P8_to_FC2', 'P8_to_FC6', 'P8_to_M1', 'P8_to_T7', 'P8_to_C3', 'P8_to_Cz', 'P8_to_C4', 'P8_to_T8', 'P8_to_M2', 'P8_to_CP5', 'P8_to_CP1', 'P8_to_CP2', 'P8_to_CP6', 'P8_to_P7', 'P8_to_P3', 'P8_to_Pz', 'P8_to_P4', 'P8_to_POz', 'P8_to_O1', 'P8_to_Oz', 'P8_to_O2', 'POz_to_Fp1', 'POz_to_Fpz', 'POz_to_Fp2', 'POz_to_F7', 'POz_to_F3', 'POz_to_Fz', 'POz_to_F4', 'POz_to_F8', 'POz_to_FC5', 'POz_to_FC1', 'POz_to_FC2', 'POz_to_FC6', 'POz_to_M1', 'POz_to_T7', 'POz_to_C3', 'POz_to_Cz', 'POz_to_C4', 'POz_to_T8', 'POz_to_M2', 'POz_to_CP5', 'POz_to_CP1', 'POz_to_CP2', 'POz_to_CP6', 'POz_to_P7', 'POz_to_P3', 'POz_to_Pz', 'POz_to_P4', 'POz_to_P8', 'POz_to_O1', 'POz_to_Oz', 'POz_to_O2', 'O1_to_Fp1', 'O1_to_Fpz', 'O1_to_Fp2', 'O1_to_F7', 'O1_to_F3', 'O1_to_Fz', 'O1_to_F4', 'O1_to_F8', 'O1_to_FC5', 'O1_to_FC1', 'O1_to_FC2', 'O1_to_FC6', 'O1_to_M1', 'O1_to_T7', 'O1_to_C3', 'O1_to_Cz', 'O1_to_C4', 'O1_to_T8', 'O1_to_M2', 'O1_to_CP5', 'O1_to_CP1', 'O1_to_CP2', 'O1_to_CP6', 'O1_to_P7', 'O1_to_P3', 'O1_to_Pz', 'O1_to_P4', 'O1_to_P8', 'O1_to_POz', 'O1_to_Oz', 'O1_to_O2', 'Oz_to_Fp1', 'Oz_to_Fpz', 'Oz_to_Fp2', 'Oz_to_F7', 'Oz_to_F3', 'Oz_to_Fz', 'Oz_to_F4', 'Oz_to_F8', 'Oz_to_FC5', 'Oz_to_FC1', 'Oz_to_FC2', 'Oz_to_FC6', 'Oz_to_M1', 'Oz_to_T7', 'Oz_to_C3', 'Oz_to_Cz', 'Oz_to_C4', 'Oz_to_T8', 'Oz_to_M2', 'Oz_to_CP5', 'Oz_to_CP1', 'Oz_to_CP2', 'Oz_to_CP6', 'Oz_to_P7', 'Oz_to_P3', 'Oz_to_Pz', 'Oz_to_P4', 'Oz_to_P8', 'Oz_to_POz', 'Oz_to_O1', 'Oz_to_O2', 'O2_to_Fp1', 'O2_to_Fpz', 'O2_to_Fp2', 'O2_to_F7', 'O2_to_F3', 'O2_to_Fz', 'O2_to_F4', 'O2_to_F8', 'O2_to_FC5', 'O2_to_FC1', 'O2_to_FC2', 'O2_to_FC6', 'O2_to_M1', 'O2_to_T7', 'O2_to_C3', 'O2_to_Cz', 'O2_to_C4', 'O2_to_T8', 'O2_to_M2', 'O2_to_CP5', 'O2_to_CP1', 'O2_to_CP2', 'O2_to_CP6', 'O2_to_P7', 'O2_to_P3', 'O2_to_Pz', 'O2_to_P4', 'O2_to_P8', 'O2_to_POz', 'O2_to_O1', 'O2_to_Oz']\n",
      "regional_transfer_entropy_results: Dictionary with keys: ['Frontal_to_Temporal', 'Frontal_to_Parietal', 'Frontal_to_Occipital', 'Temporal_to_Frontal', 'Temporal_to_Parietal', 'Temporal_to_Occipital', 'Parietal_to_Frontal', 'Parietal_to_Temporal', 'Parietal_to_Occipital', 'Occipital_to_Frontal', 'Occipital_to_Temporal', 'Occipital_to_Parietal']\n",
      "transfer_entropy_hemispheric_avg: (8454,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# List of npy paths\n",
    "npy_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Spectral Analysis/BandPowers_x.npy\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Spectral Analysis/combined_fft_psd_x.npy\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Spectral Analysis/SpectralEntropy_x.npy\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Spectral Analysis/SpectralCentroids_x.npy\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Spectral Analysis/welchs_x.npy\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Spectral Analysis/STFT_x.npy\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Spectral Analysis/PeakFrequencies_x.npy\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Transfer Entropy/full_granularity_transfer_entropy_results.npy\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Transfer Entropy/regional_transfer_entropy_results.npy\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Transfer Entropy/transfer_entropy_hemispheric_avg.npy\"\n",
    "]\n",
    "\n",
    "# Initialize dictionaries to store the npy files and their shapes\n",
    "npys = {}\n",
    "npys_shapes = {}\n",
    "\n",
    "# Load the npy files into a dictionary and collect their shapes\n",
    "for path in npy_paths:\n",
    "    npy_name = path.split('/')[-1].replace('.npy', '')\n",
    "\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    if data.shape == ():  # Check for zero-dimensional arrays\n",
    "        data = data.item()  # Convert to dictionary or scalar\n",
    "            \n",
    "    npys[npy_name] = data\n",
    "\n",
    "    # Handle the shape differently based on the type of data\n",
    "    if isinstance(data, dict):\n",
    "        npys_shapes[npy_name] = \"Dictionary with keys: \" + str(list(data.keys()))\n",
    "    else:\n",
    "        npys_shapes[npy_name] = data.shape\n",
    "\n",
    "# Print the shapes of all loaded npy files\n",
    "for name, shape in npys_shapes.items():\n",
    "    print(f\"{name}: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360e4160-74b7-4052-ae68-64a70f90040d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_data_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m     18\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 20\u001b[0m predicted_data_np \u001b[38;5;241m=\u001b[39m \u001b[43mpredicted_data_np\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Spectral entropy threshold\u001b[39;00m\n\u001b[1;32m     23\u001b[0m spectral_entropy_threshold \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(\u001b[38;5;28mlist\u001b[39m(npys[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpectralEntropy_x\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;241m75\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_data_np' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def butter_bandstop_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='bandstop')\n",
    "    \n",
    "    if len(data) > max(33, 3 * max(len(a), len(b))):  # Change 33 to a variable if this value changes\n",
    "        y = filtfilt(b, a, data)\n",
    "        return y\n",
    "    else:\n",
    "        print(f\"Data too short to apply band-stop filter for frequency band {lowcut}-{highcut} Hz.\")\n",
    "        return data\n",
    "\n",
    "fs = 1000\n",
    "\n",
    "predicted_data_np = predicted_data_np\n",
    "\n",
    "# Spectral entropy threshold\n",
    "spectral_entropy_threshold = np.percentile(list(npys['SpectralEntropy_x'].values()), 75)\n",
    "\n",
    "# Transfer entropy threshold\n",
    "transfer_entropy_values = []\n",
    "for val in npys['full_granularity_transfer_entropy_results'].values():\n",
    "    if isinstance(val, dict):\n",
    "        transfer_entropy_values.extend(val.values())\n",
    "    else:\n",
    "        transfer_entropy_values.append(val)\n",
    "transfer_entropy_threshold = np.percentile(transfer_entropy_values, 75)\n",
    "\n",
    "# Spectral entropy as an array for slicing\n",
    "spectral_entropy_array = np.array([val for key, val in sorted(npys['SpectralEntropy_x'].items())])\n",
    "\n",
    "# Define frequency bands\n",
    "frequency_bands = [(i, i + 1) for i in range(int(fs / 2))]\n",
    "\n",
    "# Check if predicted_data_np is long enough\n",
    "if len(predicted_data_np) > 33:  # Change 33 to a variable if this value changes\n",
    "    for low, high in frequency_bands:\n",
    "        spectral_subarray = spectral_entropy_array[low:high] if low < len(spectral_entropy_array) else np.array([0])\n",
    "        transfer_subarray = np.array(transfer_entropy_values[low:high]) if low < len(transfer_entropy_values) else np.array([0])\n",
    "        current_spectral_entropy = np.mean(spectral_subarray)\n",
    "        current_transfer_entropy = np.mean(transfer_subarray)\n",
    "        \n",
    "        if current_spectral_entropy > spectral_entropy_threshold or current_transfer_entropy > transfer_entropy_threshold:\n",
    "            predicted_data_np = butter_bandstop_filter(predicted_data_np, low, high, fs)\n",
    "else:\n",
    "    print(\"predicted_data_np is not long enough for filtering.\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.plot(predicted_data_np, label=\"Predicted (Filtered)\", alpha=0.5)\n",
    "plt.plot(true_data_np, label=\"True\", alpha=0.5)\n",
    "plt.plot(true_data_np, color='white', alpha=0.5, label='True EEG')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "189188b4-55bf-4526-b446-cc338f73d600",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m _, original_signal \u001b[38;5;241m=\u001b[39m istft(stft_data_original, fs\u001b[38;5;241m=\u001b[39mfs)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Using real predicted time-domain data\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m predicted_signal \u001b[38;5;241m=\u001b[39m \u001b[43mpredicted_data_time_domain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Create a new modified signal based on some condition between original and predicted data\u001b[39;00m\n\u001b[1;32m     38\u001b[0m modified_signal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(original_signal, predicted_signal)\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import istft, stft\n",
    "import os\n",
    "\n",
    "# Define function to load data from npy file\n",
    "def load_npy(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} does not exist.\")\n",
    "    return np.load(file_path, allow_pickle=True).item()\n",
    "\n",
    "# Load the real STFT data\n",
    "stft_data_dict = load_npy(\"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Spectral Analysis/STFT_x.npy\")\n",
    "\n",
    "# Load the real predicted time-domain data\n",
    "predicted_data_time_domain = smoothed_gaussian\n",
    "\n",
    "# Load the real true EEG data\n",
    "true_data_np = true_data_np\n",
    "\n",
    "# Define the sampling frequency and channel names\n",
    "fs = 1000  # Replace this with the actual sampling frequency\n",
    "channel_names = list(stft_data_dict.keys())\n",
    "\n",
    "# Initialize a dictionary to store the modified time-domain signals for each channel\n",
    "predicted_data_modified_stft = {}\n",
    "\n",
    "# Loop through each EEG channel\n",
    "for channel in channel_names:\n",
    "    stft_log_original = stft_data_dict[channel]\n",
    "    stft_data_original = np.power(10, stft_log_original / 10)\n",
    "    _, original_signal = istft(stft_data_original, fs=fs)\n",
    "    \n",
    "    # Using real predicted time-domain data\n",
    "    predicted_signal = predicted_data_time_domain[channel]\n",
    "    \n",
    "    # Create a new modified signal based on some condition between original and predicted data\n",
    "    modified_signal = np.maximum(original_signal, predicted_signal)\n",
    "    \n",
    "    _, _, Zxx_modified = stft(modified_signal, fs=fs, nperseg=fs*2)\n",
    "    stft_log_modified = 10 * np.log10(np.abs(Zxx_modified))\n",
    "    predicted_data_modified_stft[channel] = stft_log_modified\n",
    "\n",
    "# At this point, predicted_data_modified_stft contains the modified STFT data for each channel\n",
    "# Configure Matplotlib to have a black background and light grey text\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "plt.rcParams['axes.edgecolor'] = 'lightgrey'\n",
    "plt.rcParams['axes.labelcolor'] = 'lightgrey'\n",
    "plt.rcParams['text.color'] = 'lightgrey'\n",
    "plt.rcParams['xtick.color'] = 'lightgrey'\n",
    "plt.rcParams['ytick.color'] = 'lightgrey'\n",
    "\n",
    "plt.plot(smoothed_gaussian, color='red', label=\"predicted_data_modified_stft\", alpha=0.9)\n",
    "plt.plot(true_data_np, color='white', alpha=0.5, label='True EEG')\n",
    "\n",
    "# Adding grid lines for better visibility of scale\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, color='lightgrey')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8cfe9b06-e372-459f-a797-b8604d5dad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 40.27901945  43.29048063  43.28785317 ...  42.36033325  42.33499974\n",
      "   37.06853644]\n",
      " [ 39.35955148  40.28244249  40.27744403 ...  39.34691137  39.42114273\n",
      "   36.52675937]\n",
      " [ 36.54919092   8.603578     3.26600537 ...  12.23843448  20.45885866\n",
      "   34.89871106]\n",
      " ...\n",
      " [ 10.2731061  -12.98425529 -11.03289046 ... -15.63056347  -0.42828091\n",
      "    8.90193404]\n",
      " [ 10.2638421  -10.41895443 -18.98203293 ... -14.64773741  -0.17291408\n",
      "    8.90253612]\n",
      " [ 10.26053239 -10.29006623 -13.99129227 ... -16.59998548  -0.38464733\n",
      "    8.90347357]]\n"
     ]
    }
   ],
   "source": [
    "print(STFT_x[list(STFT_x.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a7ed6-9c58-44c6-b0f1-876255c32c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Assume `spectral_centroid_true` is the Spectral Centroid frequency for the true data\n",
    "spectral_centroid_true = 50.0  # Replace with the actual value\n",
    "predicted_data_modified_centroid = butter_bandpass_filter(predicted_data_np, spectral_centroid_true - 5, spectral_centroid_true + 5, fs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951053be-2db8-4566-afee-934753e4c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "# Assume `max_power_frequency_true` is the Max Power Frequency for the true data\n",
    "max_power_frequency_true = 60.0  # Replace with the actual value\n",
    "\n",
    "# FFT of the predicted data\n",
    "fft_predicted = fft(predicted_data_np)\n",
    "\n",
    "# Boost the max power frequency\n",
    "fft_predicted[int(max_power_frequency_true):(int(max_power_frequency_true) + 2)] *= 1.5\n",
    "fft_predicted[-(int(max_power_frequency_true) + 2):-int(max_power_frequency_true)] *= 1.5\n",
    "\n",
    "# Inverse FFT to get time-domain signal\n",
    "predicted_data_modified_maxpower = np.fft.ifft(fft_predicted)\n",
    "\n",
    "# Since the output might be complex, take the real part\n",
    "predicted_data_modified_maxpower = np.real(predicted_data_modified_maxpower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e53ff6-bb45-452e-a9b7-006126b90185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have loaded these metrics into variables like so:\n",
    "# higuchi_fractal_dimensions, hurst_exponents, arnold_tongues_rotation\n",
    "\n",
    "# Initialize a dictionary to store the modified time-domain signals for each channel\n",
    "predicted_data_modified_stft = {}\n",
    "\n",
    "# Loop through each EEG channel\n",
    "for channel in channel_names:\n",
    "\n",
    "    # Extract the original STFT data for the current channel\n",
    "    stft_log_original = stft_data_dict[channel]\n",
    "\n",
    "    # Convert the dB data back to power\n",
    "    stft_data_original = np.power(10, stft_log_original / 10)\n",
    "\n",
    "    # Inverse STFT to get the original time-domain signal\n",
    "    _, original_signal = istft(stft_data_original, fs=fs)\n",
    "\n",
    "    # Extract the predicted time-domain data for the current channel\n",
    "    predicted_signal = predicted_data_time_domain.get(channel, np.zeros_like(original_signal))  # Dummy zeros for now\n",
    "\n",
    "    # Access the metrics for the current channel\n",
    "    higuchi = higuchi_fractal_dimensions.get(channel, None)\n",
    "    hurst = hurst_exponents.get(channel, None)\n",
    "    arnold = arnold_tongues_rotation.get(channel, None)\n",
    "\n",
    "    # Create a new modified signal based on some condition between original and predicted data\n",
    "    # Use higuchi, hurst, and arnold to influence how the signals are combined\n",
    "    # This is where your advanced logic will go. For now, we use a dummy operation:\n",
    "    modified_signal = np.maximum(original_signal, predicted_signal)  # This should be replaced by your actual logic\n",
    "\n",
    "    # Calculate STFT of the modified signal\n",
    "    _, _, Zxx_modified = stft(modified_signal, fs=fs, nperseg=fs*2)\n",
    "\n",
    "    # Store the modified STFT data\n",
    "    stft_log_modified = 10 * np.log10(np.abs(Zxx_modified))\n",
    "    predicted_data_modified_stft[channel] = stft_log_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a36755-aae0-42a3-a75c-fa1e7930aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming spectral_edge_frequencies is a dictionary with spectral edge frequencies for each EEG channel\n",
    "\n",
    "# Initialize a dictionary to store the modified time-domain signals for each channel\n",
    "predicted_data_modified_stft = {}\n",
    "\n",
    "# Loop through each EEG channel\n",
    "for channel in channel_names:\n",
    "\n",
    "    # Extract the original STFT data for the current channel\n",
    "    stft_log_original = stft_data_dict[channel]\n",
    "    \n",
    "    # Convert the dB data back to power\n",
    "    stft_data_original = np.power(10, stft_log_original / 10)\n",
    "    \n",
    "    # Inverse STFT to get the original time-domain signal\n",
    "    _, original_signal = istft(stft_data_original, fs=fs)\n",
    "    \n",
    "    # Extract the predicted time-domain data for the current channel\n",
    "    predicted_signal = predicted_data_time_domain.get(channel, np.zeros_like(original_signal))  # Dummy zeros for now\n",
    "\n",
    "    # Access the spectral edge frequency for the current channel\n",
    "    edge_frequency = spectral_edge_frequencies.get(channel, None)\n",
    "\n",
    "    # Assume edge_frequency is between 0 and Nyquist frequency\n",
    "    # Perform logic to adjust predicted STFT using spectral edge frequency\n",
    "    # Here we are just making an example, actual logic may be far more complex\n",
    "    _, _, Zxx_predicted = stft(predicted_signal, fs=fs, nperseg=fs*2)\n",
    "    frequencies, _, _ = stft(predicted_signal, fs=fs, nperseg=fs*2)\n",
    "\n",
    "    if edge_frequency:\n",
    "        mask = frequencies < edge_frequency\n",
    "        Zxx_predicted[mask, :] = np.zeros_like(Zxx_predicted[mask, :])\n",
    "\n",
    "    # Here, you could have advanced logic to adjust the predicted_signal based on spectral edge frequency\n",
    "\n",
    "    # Inverse STFT to get the modified time-domain signal\n",
    "    _, modified_signal = istft(Zxx_predicted, fs=fs)\n",
    "    \n",
    "    # Calculate STFT of the modified signal\n",
    "    _, _, Zxx_modified = stft(modified_signal, fs=fs, nperseg=fs*2)\n",
    "\n",
    "    # Store the modified STFT data\n",
    "    stft_log_modified = 10 * np.log10(np.abs(Zxx_modified))\n",
    "    predicted_data_modified_stft[channel] = stft_log_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82e3c3-c891-46ed-864b-ebfffc35c7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706f5f7-3d79-4084-87d3-d19e4741699c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming eeg_data_original_scale_tensor, predicted_outputs_reshaped, and EEG_data are available\n",
    "\n",
    "# `eeg_data_original_scale_tensor` is converted to a numpy array\n",
    "# `predicted_outputs_reshaped` is already a numpy array\n",
    "true_data_np = eeg_data_original_scale_tensor.numpy()\n",
    "predicted_data_np = outputs_numpy  # No need to convert\n",
    "original_data_np = EEG_data\n",
    "\n",
    "print(\"True EEG - First Channel | Predicted EEG - First Channel | Original EEG - First Channel\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "for true_value, predicted_value, original_value in zip(true_data_np[:100, 0], predicted_data_np[:100, 0], original_data_np[:100, 0]):\n",
    "    print(f\"{true_value:20} | {predicted_value:20} | {original_value:20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c3570-1195-414a-9fbe-da7dfe5402ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308a730-b820-4bdf-bada-51be5fadfd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your saved test dataset\n",
    "test_data = torch.load(test_dataset_path)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Final Evaluation on Test Data\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for time_data, global_data, trans_data, labels in test_loader:\n",
    "        outputs = model(time_data, global_data, trans_data)\n",
    "        test_loss += nn.MSELoss()(outputs, labels).item()\n",
    "        \n",
    "print(f'Final Test loss: {test_loss / len(test_loader)}')\n",
    "\n",
    "# If you want to save the model\n",
    "torch.save(model.state_dict(), '/path/to/save/final_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c2625-26bd-46c1-9551-82a2a7f794e3",
   "metadata": {},
   "source": [
    "# Real-time training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4139be6-99f2-48e0-9a20-10a3e70324d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record user EEG for one minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2a97f-1aeb-4ccb-915d-8f5ee6348f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let training finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492799b-fa41-4d6e-bcde-e666545a58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for one minute of new user EEG data and predict the next minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2719e-2713-4921-a483-eda1c8e6f316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
