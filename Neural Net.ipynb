{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516441f8-ccaa-478a-a7e3-6db52f2a74ef",
   "metadata": {},
   "source": [
    "# Load Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2a006-4eab-47fa-bf4f-e645f3bdb3ba",
   "metadata": {},
   "source": [
    "gradient clipping, learning rate scheduling, advanced regularization techniques,\n",
    " Bidirectional RNNs, attention mechanisms, transformer architectures, oscillators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52b7943e-7c23-49d8-bc4d-d65dfd8c603e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/EEG_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/arnold_tongues_rotation_numbers_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/bandpowers_embedding_net.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/band_power_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/dspm_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/fast_fourier_transform_psd_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/higuchi_fractal_dimensions_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/Hurst_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_concatd_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/short_time_fourier_transform_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_granular_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_hemispheric_avg_input_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_regional_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_entropy_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_centroids_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/freq_max_power_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_edge_freqs_tensor.pt\",\n",
    "]\n",
    "\n",
    "# Load the tensors into a dictionary\n",
    "tensors = {path.split('/')[-1].replace('.pt', ''): torch.load(path) for path in tensor_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43be465b-9c39-4493-9313-db7eef868865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG_tensorh: torch.Size([1, 4227788, 1, 32])\n",
      "arnold_tongues_rotation_numbers_tensor: torch.Size([32, 300, 300])\n",
      "bandpowers_embedding_neth: state_dict (model parameters)\n",
      "band_power_tensorh: torch.Size([4227788, 32, 5])\n",
      "dspm_tensor: torch.Size([19, 18840, 10])\n",
      "fast_fourier_transform_psd_tensorh: torch.Size([32, 4227788])\n",
      "higuchi_fractal_dimensions_tensor: torch.Size([1, 1, 4, 8])\n",
      "Hurst_tensorh: torch.Size([1, 1, 32, 1])\n",
      "mfdfa_concatd_tensorh: torch.Size([32, 1, 30, 2])\n",
      "mfdfa_tensorh: torch.Size([9, 32, 10, 1])\n",
      "short_time_fourier_transform_tensorh: torch.Size([32, 1001, 4229])\n",
      "transfer_entropy_granular_tensor: torch.Size([4, 4])\n",
      "transfer_entropy_hemispheric_avg_input_tensor: torch.Size([92, 92])\n",
      "transfer_entropy_regional_tensor: torch.Size([4, 4])\n",
      "spectral_entropy_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_centroids_tensor: torch.Size([1, 1, 32, 1])\n",
      "freq_max_power_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_edge_freqs_tensor: torch.Size([1, 1, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load each tensor and print its shape\n",
    "tensor_shapes = {}\n",
    "for path in tensor_paths:\n",
    "    tensor_name = path.split(\"/\")[-1].replace(\".pt\", \"\").replace(\".pth\", \"\")\n",
    "    data = torch.load(path)\n",
    "    \n",
    "    # Check if the loaded data is a state dictionary or a tensor\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor_shapes[tensor_name] = data.shape\n",
    "    elif isinstance(data, dict):  # Likely a state_dict\n",
    "        tensor_shapes[tensor_name] = \"state_dict (model parameters)\"\n",
    "    else:\n",
    "        tensor_shapes[tensor_name] = \"unknown type\"\n",
    "\n",
    "# Print the tensor shapes\n",
    "for name, shape in tensor_shapes.items():\n",
    "    print(f\"{name}: {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0ec40-2261-41fc-82e1-13d73334d058",
   "metadata": {},
   "source": [
    "# Train Test Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ba273-0591-476a-aa5c-07fb403af45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc365929-f568-48e7-af9e-aa60e517f841",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff40f7-f03a-43bc-9afb-1ce7afd9f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from apex import amp\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "import nvidia.dali.ops as ops\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "class SimplePipeline(Pipeline):\n",
    "    def __init__(self, batch_size, num_threads, device_id):\n",
    "        super(SimplePipeline, self).__init__(batch_size, num_threads, device_id, seed=12)\n",
    "        self.input = ops.FileReader(device=\"cpu\", file_root=\"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN\")\n",
    "        self.decode = ops.ImageDecoder(device=\"cpu\", output_type=types.RGB)\n",
    "\n",
    "    def define_graph(self):\n",
    "        jpegs, labels = self.input()\n",
    "        images = self.decode(jpegs)\n",
    "        return (images, labels)\n",
    "\n",
    "# Replace these with your data specifics\n",
    "batch_size = 128\n",
    "pipe = SimplePipeline(batch_size=batch_size, num_threads=2, device_id=0)\n",
    "pipe.build()\n",
    "\n",
    "class EEGPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGPredictor, self).__init__()\n",
    "        \n",
    "class BaseEmbeddingNet(nn.Module):\n",
    "    def __init__(self, input_channels, conv_output_channels, height, width):\n",
    "        super(BaseEmbeddingNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, conv_output_channels, 3)\n",
    "        self.batch_norm = nn.BatchNorm2d(conv_output_channels)\n",
    "        self.fc1 = nn.Linear(conv_output_channels * (height - 2) * (width - 2), 512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch_norm(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(self.dropout(x)))\n",
    "        return x\n",
    "\n",
    "class EEGEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(1, 64, 4227788, 32)\n",
    "\n",
    "class RotationEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(32, 64, 300, 300)\n",
    "\n",
    "class BandPowerEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(32, 64, 4227788, 5)\n",
    "\n",
    "class DSPMEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(19, 64, 18840, 10)\n",
    "\n",
    "class FastFourierEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(32, 64, 4227788, 32)\n",
    "\n",
    "class HiguchiFractalEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(1, 64, 4, 8)\n",
    "\n",
    "class HurstEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(1, 64, 32, 1)\n",
    "\n",
    "class MFDFAConcatdEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(32, 64, 30, 2)\n",
    "\n",
    "class MFDFAEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(9, 64, 32, 10)\n",
    "\n",
    "class ShortTimeFourierEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(32, 64, 1001, 4229)\n",
    "\n",
    "class SpectralEntropyEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(1, 64, 32, 1)\n",
    "\n",
    "class SpectralCentroidsEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(1, 64, 32, 1)\n",
    "\n",
    "class FreqMaxPowerEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(1, 64, 32, 1)\n",
    "\n",
    "class SpectralEdgeFreqsEmbeddingNet(BaseEmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(1, 64, 32, 1)\n",
    "\n",
    "class PairwiseMeasureNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EEGPredictor()\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "accumulation_steps = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        num_epochs = 50\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Use AMP for loss scaling\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        \n",
    "        # Don't step optimizer for every batch. Only step every accumulation_steps\n",
    "        if (i+1) % accumulation_steps == 0:             \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88f8ba-0bd5-45f7-9a84-deae168e531d",
   "metadata": {},
   "source": [
    "# Tranformer Kuramoto RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b1d88-c6cf-43af-bb96-3ddc5840f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdiffeq import odeint\n",
    "import torch.nn as nn\n",
    "\n",
    "class CompleteEEGPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward):\n",
    "        super(CompleteEEGPredictor, self).__init__()\n",
    "\n",
    "        # EEG Embedding\n",
    "        self.eeg_embedding = EEGEmbeddingNet()\n",
    "        \n",
    "class KuramotoODEFunc(nn.Module):\n",
    "    def __init__(self, omega, K):\n",
    "        super(KuramotoODEFunc, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, t, theta):\n",
    "        N = theta.shape[1]\n",
    "        dtheta = self.omega + (self.K / N) * torch.sum(torch.sin(theta - theta.unsqueeze(1)), dim=1)\n",
    "        return dtheta\n",
    "       class KuramotoLayer(nn.Module):\n",
    "    def __init__(self, oscillator_count, time_steps, dt=0.01):\n",
    "        super(KuramotoLayer, self).__init__()\n",
    "        self.oscillator_count = oscillator_count\n",
    "        self.time_steps = time_steps\n",
    "        self.dt = dt\n",
    "        \n",
    "        self.omega = nn.Parameter(torch.randn(oscillator_count))\n",
    "        self.K = nn.Parameter(torch.tensor(1.0))  # global coupling strength\n",
    "\n",
    "    def forward(self, x):\n",
    "        ode_func = KuramotoODEFunc(self.omega, self.K)\n",
    "        time_points = torch.arange(0, self.time_steps * self.dt, self.dt)\n",
    "        theta = odeint(ode_func, x, time_points)\n",
    "        return theta\n",
    "        \n",
    "        # Transformer model\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward),\n",
    "            num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        # Positional Encoding (assuming a sequence length of 4227788 for simplicity; adjust as needed)\n",
    "        self.pos_encoder = nn.Embedding(4227788, d_model) \n",
    "        self.position = torch.arange(4227788).unsqueeze(1)\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.LSTM(input_size=d_model, hidden_size=256, num_layers=2, batch_first=True, dropout=0.5)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc_out = nn.Linear(256, 32) # 32 channels of EEG data\n",
    "\n",
    "    def forward(self, src, tgt=None):\n",
    "        # src is the input EEG sequence, tgt is the target sequence (can be None during inference)\n",
    "        \n",
    "        # Embedding\n",
    "        src_emb = self.eeg_embedding(src)\n",
    "        \n",
    "        # Kuramoto Layer\n",
    "        src_kuramoto = self.kuramoto(src_emb)\n",
    "        \n",
    "        # Adding positional encoding\n",
    "        src_kuramoto = src_kuramoto + self.pos_encoder(self.position).permute(1, 0, 2)\n",
    "\n",
    "        # Transformer\n",
    "        transformer_out = self.transformer(src_kuramoto)\n",
    "        \n",
    "        # RNN\n",
    "        rnn_out, _ = self.rnn(transformer_out)\n",
    "        \n",
    "        # Decoding the output to the desired size\n",
    "        return self.fc_out(rnn_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
