{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516441f8-ccaa-478a-a7e3-6db52f2a74ef",
   "metadata": {},
   "source": [
    "# Load Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43be465b-9c39-4493-9313-db7eef868865",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arnold_tongues_rotation_numbers_tensor: torch.Size([32, 300, 300])\n",
      "dspm_tensor: torch.Size([19, 18840, 10])\n",
      "higuchi_fractal_dimensions_tensor: torch.Size([1, 1, 4, 8])\n",
      "Hurst_tensor: torch.Size([1, 1, 32, 1])\n",
      "mfdfa_concatd_tensor: torch.Size([32, 1, 30, 2])\n",
      "mfdfa_tensor: torch.Size([9, 32, 10, 1])\n",
      "short_time_fourier_transform_tensor: torch.Size([32, 1001, 4229])\n",
      "transfer_entropy_granular_tensor: torch.Size([4, 4])\n",
      "transfer_entropy_hemispheric_avg_input_tensor: torch.Size([92, 92])\n",
      "transfer_entropy_regional_tensor: torch.Size([4, 4])\n",
      "spectral_entropy_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_centroids_tensor: torch.Size([1, 1, 32, 1])\n",
      "freq_max_power_tensor: torch.Size([1, 1, 32, 1])\n",
      "spectral_edge_freqs_tensor: torch.Size([1, 1, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/arnold_tongues_rotation_numbers_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/dspm_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/higuchi_fractal_dimensions_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/Hurst_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_concatd_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/mfdfa_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/short_time_fourier_transform_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_granular_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_hemispheric_avg_input_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/transfer_entropy_regional_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_entropy_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_centroids_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/freq_max_power_tensor.pt\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/CNN/spectral_edge_freqs_tensor.pt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store the tensors and another for their shapes\n",
    "tensors = {}\n",
    "tensor_shapes = {}\n",
    "\n",
    "# Load the tensors into a dictionary and collect their shapes\n",
    "for path in tensor_paths:\n",
    "    tensor_name = path.split('/')[-1].replace('.pt', '').replace('.pth', '')\n",
    "\n",
    "    # Remove the 'h' from the end, if it exists\n",
    "    if tensor_name.endswith(\"h\"):\n",
    "        tensor_name = tensor_name[:-1]\n",
    "\n",
    "    # Load the tensor\n",
    "    data = torch.load(path)\n",
    "    tensors[tensor_name] = data\n",
    "\n",
    "    # Check the type of the loaded data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor_shapes[tensor_name] = data.shape\n",
    "    elif isinstance(data, dict):  # Likely a state_dict\n",
    "        tensor_shapes[tensor_name] = \"state_dict (model parameters)\"\n",
    "    else:\n",
    "        tensor_shapes[tensor_name] = \"unknown type\"\n",
    "\n",
    "# Print the shapes of all loaded tensors\n",
    "for name, shape in tensor_shapes.items():\n",
    "    print(f\"{name}: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e8e026a-f6bb-4d54-8a3b-be607fa4f34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in arnold_tongues_rotation_numbers_tensor: 0.0%\n",
      "Percentage of Infs in arnold_tongues_rotation_numbers_tensor: 0.0%\n",
      "Percentage of NaNs in dspm_tensor: 0.0%\n",
      "Percentage of Infs in dspm_tensor: 0.0%\n",
      "Percentage of NaNs in higuchi_fractal_dimensions_tensor: 0.0%\n",
      "Percentage of Infs in higuchi_fractal_dimensions_tensor: 0.0%\n",
      "Percentage of NaNs in Hurst_tensor: 0.0%\n",
      "Percentage of Infs in Hurst_tensor: 0.0%\n",
      "Percentage of NaNs in mfdfa_concatd_tensor: 0.0%\n",
      "Percentage of Infs in mfdfa_concatd_tensor: 0.0%\n",
      "Percentage of NaNs in mfdfa_tensor: 0.0%\n",
      "Percentage of Infs in mfdfa_tensor: 0.0%\n",
      "Percentage of NaNs in short_time_fourier_transform_tensor: 0.0%\n",
      "Percentage of Infs in short_time_fourier_transform_tensor: 0.0%\n",
      "Percentage of NaNs in transfer_entropy_granular_tensor: 0.0%\n",
      "Percentage of Infs in transfer_entropy_granular_tensor: 0.0%\n",
      "Percentage of NaNs in transfer_entropy_hemispheric_avg_input_tensor: 0.0%\n",
      "Percentage of Infs in transfer_entropy_hemispheric_avg_input_tensor: 0.0%\n",
      "Percentage of NaNs in transfer_entropy_regional_tensor: 0.0%\n",
      "Percentage of Infs in transfer_entropy_regional_tensor: 0.0%\n",
      "Percentage of NaNs in spectral_entropy_tensor: 0.0%\n",
      "Percentage of Infs in spectral_entropy_tensor: 0.0%\n",
      "Percentage of NaNs in spectral_centroids_tensor: 0.0%\n",
      "Percentage of Infs in spectral_centroids_tensor: 0.0%\n",
      "Percentage of NaNs in freq_max_power_tensor: 0.0%\n",
      "Percentage of Infs in freq_max_power_tensor: 0.0%\n",
      "Percentage of NaNs in spectral_edge_freqs_tensor: 0.0%\n",
      "Percentage of Infs in spectral_edge_freqs_tensor: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Loop through the loaded tensors to check for NaNs and Infs\n",
    "for tensor_name, tensor_data in tensors.items():\n",
    "    # Only perform the checks if the data is a tensor\n",
    "    if isinstance(tensor_data, torch.Tensor):\n",
    "        total_elements = torch.numel(tensor_data)\n",
    "        \n",
    "        nans_count = torch.sum(torch.isnan(tensor_data)).item()\n",
    "        infs_count = torch.sum(torch.isinf(tensor_data)).item()\n",
    "        \n",
    "        nans_percentage = (nans_count / total_elements) * 100\n",
    "        infs_percentage = (infs_count / total_elements) * 100\n",
    "        \n",
    "        print(f\"Percentage of NaNs in {tensor_name}: {nans_percentage}%\")\n",
    "        print(f\"Percentage of Infs in {tensor_name}: {infs_percentage}%\")\n",
    "    else:\n",
    "        print(f\"{tensor_name} is not a tensor, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7667071-ad74-4a9e-ab1b-f29354a5816c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Match dimensions, reshape, and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f60a69-caf6-47d1-a2c1-b71536f18f65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tensor arnold_tongues_rotation_numbers_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor dspm_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor higuchi_fractal_dimensions_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor Hurst_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor mfdfa_concatd_tensor shape: torch.Size([32, 1, 32, 32])\n",
      "Processed tensor mfdfa_tensor shape: torch.Size([9, 1, 32, 32])\n",
      "Processed tensor short_time_fourier_transform_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor transfer_entropy_granular_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor transfer_entropy_hemispheric_avg_input_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor transfer_entropy_regional_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor spectral_entropy_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor spectral_centroids_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor freq_max_power_tensor shape: torch.Size([1, 1, 32, 32])\n",
      "Processed tensor spectral_edge_freqs_tensor shape: torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_and_resize_tensor(tensor, target_shape):\n",
    "    # Add missing batch and channel dimensions\n",
    "    while len(tensor.shape) < 4:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    # Reduce the channel dimension to 1 by taking the mean along that axis\n",
    "    tensor = torch.mean(tensor, dim=1, keepdim=True)\n",
    "\n",
    "    # Normalize\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    if std != 0:\n",
    "        tensor = (tensor - mean) / std\n",
    "\n",
    "    # Reshape/resize to target_shape\n",
    "    tensor = F.interpolate(tensor, size=target_shape[2:], mode='bilinear', align_corners=True)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "target_shape = [1, 1, 32, 32]\n",
    "\n",
    "# List of all your tensors\n",
    "all_tensors = [tensors[key] for key in tensors]\n",
    "\n",
    "# Preprocess all tensors\n",
    "processed_tensors = [preprocess_and_resize_tensor(tensor, target_shape) for tensor in all_tensors]\n",
    "\n",
    "# Store preprocessed tensors back into the `tensors` dictionary\n",
    "for key, tensor in zip(tensors.keys(), processed_tensors):\n",
    "    tensors[key] = tensor\n",
    "\n",
    "# Print out the new shapes\n",
    "for key in tensors.keys():\n",
    "    print(f\"Processed tensor {key} shape: {tensors[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc365929-f568-48e7-af9e-aa60e517f841",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ff40f7-f03a-43bc-9afb-1ce7afd9f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tensor_names = [\n",
    "    'arnold_tongues_rotation_numbers_tensor',\n",
    "    'dspm_tensor',\n",
    "    'higuchi_fractal_dimensions_tensor',\n",
    "    'Hurst_tensor',\n",
    "    'mfdfa_concatd_tensor',\n",
    "    'mfdfa_tensor',\n",
    "    'short_time_fourier_transform_tensor',\n",
    "    'transfer_entropy_granular_tensor',\n",
    "    'transfer_entropy_hemispheric_avg_input_tensor',\n",
    "    'transfer_entropy_regional_tensor',\n",
    "    'spectral_entropy_tensor',\n",
    "    'spectral_centroids_tensor',\n",
    "    'freq_max_power_tensor',\n",
    "    'spectral_edge_freqs_tensor',\n",
    "]\n",
    "\n",
    "class BaseEmbeddingNet(nn.Module):\n",
    "    def __init__(self, input_channels, conv_output_channels, reduce_to_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, conv_output_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(conv_output_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_reduce = nn.Linear(conv_output_channels, reduce_to_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_reduce(x)\n",
    "        return x\n",
    "\n",
    "processed_tensors_dict = {name: tensor for name, tensor in zip(tensor_names, processed_tensors)}\n",
    "\n",
    "net_params = {name: {'input_channels': 1, 'conv_output_channels': 16, 'reduce_to_dim': 8} \n",
    "              for name in processed_tensors_dict.keys()}\n",
    "\n",
    "# Create BaseEmbeddingNets for each tensor\n",
    "embedding_nets = {name: BaseEmbeddingNet(**params) for name, params in net_params.items()}\n",
    "\n",
    "# Move networks to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for net in embedding_nets.values():\n",
    "    net.to(device)\n",
    "\n",
    "# Create custom dataset and dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = tensors\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __len__(self):\n",
    "        first_tensor = next(iter(self.tensors.values()))\n",
    "        return first_tensor.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        result = {}\n",
    "        for key, val in self.tensors.items():\n",
    "            if val.shape[0] > idx:\n",
    "                result[key] = val[idx]\n",
    "        return result\n",
    "\n",
    "def custom_collate(batch):\n",
    "    collated_batch = {}\n",
    "    all_keys = set([key for item in batch for key in item.keys()])\n",
    "    \n",
    "    for key in all_keys:\n",
    "        collated_batch[key] = torch.stack([item[key] for item in batch if key in item.keys()], dim=0)\n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "# Use processed_tensors for your CustomDataset\n",
    "dataset = CustomDataset(processed_tensors_dict)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=custom_collate)\n",
    "\n",
    "# Collect feature embeddings\n",
    "all_features = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "    features_list = [net(batch[key].to(device, dtype=torch.float32)) for key, net in embedding_nets.items()]\n",
    "    concatenated_features = torch.cat(features_list, dim=1)\n",
    "    all_features.append(concatenated_features.cpu().detach())\n",
    "\n",
    "# Convert list to tensor\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "\n",
    "# Save the feature embeddings\n",
    "save_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto'\n",
    "torch.save(all_features, f'{save_path}/all_features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56cafa-30b8-415d-823c-fafd212eecfa",
   "metadata": {},
   "source": [
    "# Kuramoto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b58e085-dc2d-4f47-91b1-f2709000a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484061c8-7040-4833-a73c-c9d3974b7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd4ba5d-a115-4eb9-beb4-98d6f37c95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint\n",
    "from torch.cuda.amp import autocast, GradScaler  # Importing the AMP utilities\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "EEG_data = np.load('/home/vincent/AAA_projects/MVCS/Neuroscience/eeg_data_with_channels.npy', allow_pickle=True)\n",
    "EEG_tensor = torch.FloatTensor(EEG_data)  # Assumes EEG_data is a NumPy ndarray\n",
    "\n",
    "# Function to create windows for time-series data\n",
    "def create_windows(data, window_size, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        windows.append(data[i:i+window_size])\n",
    "    return torch.stack(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040d257f-994d-409b-9875-849ff643ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "window_size = 50\n",
    "stride = 10\n",
    "\n",
    "# Add necessary transformations here to EEG_tensor if required\n",
    "EEG_tensor = EEG_tensor.clone().detach().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174b3288-f313-4b35-b037-e6eec6892a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hilbert_in_batches(data, batch_size):\n",
    "    n_batches = int(np.ceil(data.shape[0] / batch_size))\n",
    "    analytic_signal = np.zeros_like(data, dtype=np.complex64)  # change dtype as needed\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        analytic_signal[start_idx:end_idx, :] = hilbert(data[start_idx:end_idx, :])\n",
    "        \n",
    "    return analytic_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f0a84dd-6970-4888-8168-7489406b22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # Set as appropriate\n",
    "\n",
    "# Apply Hilbert transform in batches\n",
    "EEG_numpy = EEG_tensor.cpu().numpy()\n",
    "analytic_signal_batches = apply_hilbert_in_batches(EEG_numpy, batch_size)\n",
    "\n",
    "# Convert the angle to phases and move to GPU\n",
    "phases = torch.tensor(np.angle(analytic_signal_batches), dtype=torch.float16).to(device)\n",
    "\n",
    "# Load PLV matrix\n",
    "plv_matrix_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Analysis/Phase Syncronization/plv_matrix.npy\"\n",
    "plv_matrix = torch.tensor(np.load(plv_matrix_path), dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b34c43-dc25-45be-b0d0-2fd9bfbdfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute_phase_diff_matrix function\n",
    "def compute_phase_diff_matrix(phases):\n",
    "    time, channels = phases.shape[:2]\n",
    "    phase_diff_matrix = torch.zeros(channels, channels, device=phases.device)\n",
    "    for i in range(channels):\n",
    "        for j in range(channels):\n",
    "            phase_diff_matrix[i, j] = torch.mean(phases[:, i] - phases[:, j])\n",
    "    return phase_diff_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed965961-347e-4c66-821a-9737b8b9262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)\n",
    "\n",
    "# EEG channel names\n",
    "eeg_channel_names = ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "                     'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6',\n",
    "                     'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1', 'Oz', 'O2']\n",
    "\n",
    "# Broad regions and corresponding channels\n",
    "regions = {\n",
    "    \"frontal\": ['Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8'],\n",
    "    \"temporal\": ['T7', 'T8'],\n",
    "    \"parietal\": ['CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8'],\n",
    "    \"occipital\": ['O1', 'Oz', 'O2']\n",
    "}\n",
    "\n",
    "# Precompute omega and phase_diff_matrix\n",
    "N = len(eeg_channel_names)\n",
    "omega = torch.mean(plv_matrix, dim=1).to(device)\n",
    "phase_diff_matrix = compute_phase_diff_matrix(phases).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73de9cf1-96b2-4d6a-9e16-cf3211bbf35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the Kuramoto function to use PyTorch functions instead of NumPy\n",
    "def kuramoto_weighted_bias(t, y, omega, K):\n",
    "    weighted_sin = plv_matrix * torch.sin(y - y[:, None] - phase_diff_matrix)\n",
    "    dydt = omega + K / N * torch.sum(weighted_sin, axis=1)\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a59c75e-edd9-45f0-ad11-bd3ebb70922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuramotoODEFunc(nn.Module):\n",
    "    def __init__(self, omega, K, plv_matrix, phase_diff_matrix):\n",
    "        super(KuramotoODEFunc, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.K = K\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "    def forward(self, t, theta):\n",
    "        # Reshape to accommodate the additional time dimension.\n",
    "        theta = theta.view(-1, theta.shape[-1])\n",
    "        N = theta.shape[1]\n",
    "    \n",
    "        # Compute the phase differences without unsqueezing\n",
    "        theta_diff = theta[:, :, None] - theta[:, None, :]\n",
    "        phase_diff_with_matrix = theta_diff - self.phase_diff_matrix\n",
    "    \n",
    "        # Compute the weighted sine values\n",
    "        weighted_sin = self.plv_matrix * torch.sin(phase_diff_with_matrix)\n",
    "    \n",
    "        dtheta = self.omega + (self.K / N) * torch.sum(weighted_sin, dim=1)\n",
    "    \n",
    "        return dtheta.view(theta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8756d6-b965-433b-93e1-4d5660ffcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuramotoLayer(nn.Module):\n",
    "    def __init__(self, oscillator_count, time_steps, dt=0.01, plv_matrix=None, phase_diff_matrix=None):\n",
    "        super(KuramotoLayer, self).__init__()\n",
    "        self.oscillator_count = oscillator_count\n",
    "        self.time_steps = time_steps\n",
    "        self.dt = dt\n",
    "        self.plv_matrix = plv_matrix\n",
    "        self.phase_diff_matrix = phase_diff_matrix\n",
    "\n",
    "        if plv_matrix is not None:\n",
    "            omega_init = torch.mean(plv_matrix, dim=1)\n",
    "            self.omega = nn.Parameter(omega_init, requires_grad=True)\n",
    "        else:\n",
    "            self.omega = nn.Parameter(torch.randn(oscillator_count), requires_grad=True)\n",
    "\n",
    "        self.K = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "\n",
    "    def custom_forward(self, *inputs):\n",
    "        initial_shape = inputs[0].shape  # Store the initial shape\n",
    "    \n",
    "        # Flatten the batch and time dimensions\n",
    "        inputs_flattened = inputs[0].reshape(-1, initial_shape[-1])\n",
    "        \n",
    "        ode_func = KuramotoODEFunc(self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        time_points = torch.arange(0, 10000 * self.dt, self.dt).to(device)  # Assume device is defined elsewhere\n",
    "        theta_flattened = odeint(ode_func, inputs_flattened, time_points, method='bosh3', rtol=1e-6, atol=1e-8)\n",
    "\n",
    "        # Reshape theta to its original shape\n",
    "        theta = theta_flattened.reshape(*initial_shape, -1)  # -1 will automatically compute the required size\n",
    "        return theta\n",
    "        \n",
    "    def forward(self, theta):\n",
    "        device = theta.device\n",
    "        self.plv_matrix = self.plv_matrix.to(device)\n",
    "        self.phase_diff_matrix = self.phase_diff_matrix.to(device)\n",
    "        theta = checkpoint(self.custom_forward, theta, self.omega, self.K, self.plv_matrix, self.phase_diff_matrix)\n",
    "        theta = theta.to(torch.float16)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    def forward_with_checkpoint(self, x):\n",
    "        x = x.to(device)\n",
    "        theta = checkpoint(self.custom_forward, x)\n",
    "        mean_coherence = self.calculate_mean_coherence(theta)\n",
    "        return theta, mean_coherence\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mean_coherence(theta):\n",
    "        N, _, _, _ = theta.shape\n",
    "        mean_coherence = torch.mean(torch.cos(theta[:, -1, :] - theta[:, -1, :].mean(dim=1).unsqueeze(1)))\n",
    "        return mean_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceaef628-97ee-44c6-9da2-3ae5655b9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all tensors are on the correct device\n",
    "phases = phases.to(device)\n",
    "\n",
    "# Compute natural frequencies and phase differences just once\n",
    "num_channels = len(eeg_channel_names)  # Get the number of channels\n",
    "#print(\"Theta shape: \", theta.shape)\n",
    "#print(\"Theta Unsqueeze(1) shape: \", theta.unsqueeze(1).shape)\n",
    "#print(\"Theta Unsqueeze(2) shape: \", theta.unsqueeze(2).shape)\n",
    "\n",
    "# Number of channels\n",
    "N = len(eeg_channel_names)\n",
    "\n",
    "# Initialize model and move to device\n",
    "kuramoto_model = KuramotoLayer(N, 12800, plv_matrix=plv_matrix, phase_diff_matrix=phase_diff_matrix).to(dtype=torch.float16).to(device)\n",
    "\n",
    "# Data Parallelism for multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    kuramoto_model = nn.DataParallel(kuramoto_model)\n",
    "\n",
    "scaler = GradScaler()\n",
    "train_data = create_windows(EEG_tensor[:int(0.7 * len(EEG_tensor))], window_size, stride).detach().requires_grad_(True)\n",
    "train_dataset = EEGDataset(data=train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833459c-3640-4db4-a658-65ef6d125254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop and feature extraction\n",
    "kuramoto_features_list = []\n",
    "for i, batch in enumerate(train_loader):\n",
    "    # Moves batch to device and changes dtype to float16\n",
    "    batch = batch.to(device, dtype=torch.float16)\n",
    "\n",
    "    # Using autocast for the forward pass\n",
    "    with autocast():\n",
    "        theta, mean_coherence = kuramoto_model(batch)\n",
    "\n",
    "    kuramoto_features_list.append(mean_coherence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd979502-d47a-4338-8168-67f629d3a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined features\n",
    "kuramoto_features_tensor = torch.stack(kuramoto_features_list)\n",
    "all_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt'\n",
    "all_features = torch.load(all_features_path)\n",
    "combined_features = torch.cat([all_features, kuramoto_features_tensor.unsqueeze(1)], dim=1)\n",
    "combined_features_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/combined_features.pt'\n",
    "torch.save(combined_features, combined_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636c6bc-9261-471f-b690-7077137c8bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f8dc5e-228b-45c0-b743-8d2d47a713ba",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e47c39-9fd7-4309-b2e8-87942b5a7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d843f16-33c5-41a2-a5bb-23aeebd33c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of EEG_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List of tensor paths\n",
    "tensor_paths = [\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\",\n",
    "    \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\",\n",
    "]\n",
    "\n",
    "# Dictionary to store the loaded tensors\n",
    "loaded_tensors = {}\n",
    "\n",
    "# Loop over the tensor paths to load and store them in the dictionary\n",
    "for path in tensor_paths:\n",
    "    # Extract the tensor name from the path (removing '.pth')\n",
    "    tensor_name = path.split(\"/\")[-1].replace(\".pth\", \"\")\n",
    "    \n",
    "    # Load the tensor\n",
    "    tensor = torch.load(path)\n",
    "    \n",
    "    # Store the tensor in the dictionary\n",
    "    loaded_tensors[tensor_name] = tensor\n",
    "    \n",
    "    # Print shape for verification\n",
    "    print(f\"Shape of {tensor_name}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28d0e23a-0b70-407e-9d43-624b40f99b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in band_power_tensor: 0.0%\n",
      "Percentage of Infs in band_power_tensor: 0.0%\n",
      "Percentage of NaNs in EEG_tensor: 0.0%\n",
      "Percentage of Infs in EEG_tensor: 0.0%\n",
      "Percentage of NaNs in fast_fourier_transform_psd_tensor: 0.0%\n",
      "Percentage of Infs in fast_fourier_transform_psd_tensor: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Loop through the loaded tensors to check for NaNs and Infs\n",
    "for tensor_name, tensor_data in loaded_tensors.items():\n",
    "    # Assuming the loaded data is a tensor; if not, additional checks may be needed\n",
    "    total_elements = torch.numel(tensor_data)\n",
    "    \n",
    "    nans_count = torch.sum(torch.isnan(tensor_data)).item()\n",
    "    infs_count = torch.sum(torch.isinf(tensor_data)).item()\n",
    "    \n",
    "    nans_percentage = (nans_count / total_elements) * 100\n",
    "    infs_percentage = (infs_count / total_elements) * 100\n",
    "    \n",
    "    print(f\"Percentage of NaNs in {tensor_name}: {nans_percentage}%\")\n",
    "    print(f\"Percentage of Infs in {tensor_name}: {infs_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fd27fe-7d47-425b-bf5b-b8aca009b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape band_power_tensor from [4227788, 32, 5] to [4227788, 32*5]\n",
    "band_power_tensor = loaded_tensors[\"band_power_tensor\"]  # Retrieve from dictionary\n",
    "band_power_tensor_reshaped = band_power_tensor.reshape(4227788, -1)  # -1 means auto-calculate the size\n",
    "\n",
    "# Reshape fast_fourier_transform_psd_tensor from [32, 4227788] to [4227788, 32]\n",
    "fast_fourier_transform_psd_tensor = loaded_tensors[\"fast_fourier_transform_psd_tensor\"]  # Retrieve from dictionary\n",
    "fast_fourier_transform_psd_tensor_reshaped = fast_fourier_transform_psd_tensor.permute(1, 0)\n",
    "\n",
    "# Reshape band_power_tensor from [4227788, 32, 5] to [4227788, 32*5]\n",
    "band_power_tensor_reshaped = band_power_tensor.reshape(4227788, -1)  # -1 means auto-calculate the size\n",
    "\n",
    "# Reshape fast_fourier_transform_psd_tensor from [32, 4227788] to [4227788, 32]\n",
    "fast_fourier_transform_psd_tensor = fast_fourier_transform_psd_tensor.permute(1, 0)\n",
    "\n",
    "# Concatenate along the feature dimension\n",
    "concatenated_tensor = torch.cat([band_power_tensor_reshaped, fast_fourier_transform_psd_tensor], dim=1)\n",
    "\n",
    "# Now concatenated_tensor has shape [4227788, (32*5)+32]\n",
    "# If 32*5+32 = feature_dim, you can directly use this tensor as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f770cc1e-fd3f-48fd-88e6-727cc87e212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squeezed shape: torch.Size([32, 4227788])\n",
      "Permuted shape: torch.Size([4227788, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize parameters\n",
    "batch_size = 64\n",
    "seq_len = 1000  \n",
    "feature_dim = concatenated_tensor.shape[1]  # Should be the second dimension of your concatenated tensor\n",
    "\n",
    "# Define the EEGPredictor class\n",
    "class EEGPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(EEGPredictor, self).__init__()\n",
    "        self.feature_transform = nn.Linear(feature_dim, d_model)\n",
    "        self.transformer_block = TransformerBlock(d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.prediction_head = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_transform(x)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.prediction_head(x)\n",
    "        return x\n",
    "\n",
    "# Define the TransformerBlock class\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.pos_encoder = nn.Embedding(seq_len, d_model)\n",
    "        self.position = torch.arange(0, seq_len, dtype=torch.long).unsqueeze(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pos_encoder(self.position[:x.size(0), :])\n",
    "        x = x + pos_encoding\n",
    "        return self.transformer(x)\n",
    "\n",
    "# Initialize the model\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 2\n",
    "dim_feedforward = 512\n",
    "model = EEGPredictor(d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Prepare the EEG tensor, removing singleton dimensions\n",
    "eeg_tensor = loaded_tensors[\"EEG_tensor\"].squeeze()\n",
    "print(\"Squeezed shape:\", eeg_tensor.shape)\n",
    "\n",
    "# Since the tensor is 2D, permute using only two dimensions\n",
    "eeg_tensor = eeg_tensor.permute(1, 0)\n",
    "print(\"Permuted shape:\", eeg_tensor.shape)\n",
    "\n",
    "num_batches = eeg_tensor.shape[0] // (batch_size * seq_len)\n",
    "\n",
    "# To store the transformer outputs\n",
    "transformer_outputs = []\n",
    "\n",
    "# Concatenate band_power_tensor_reshaped and fast_fourier_transform_psd_tensor_reshaped\n",
    "concatenated_tensor = torch.cat((band_power_tensor_reshaped, fast_fourier_transform_psd_tensor_reshaped), dim=1)\n",
    "\n",
    "# Update feature_dim to the new size after concatenation\n",
    "feature_dim = concatenated_tensor.shape[1]\n",
    "\n",
    "# Re-initialize the model with the updated feature_dim\n",
    "model = EEGPredictor(d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = concatenated_tensor.shape[0] // (batch_size * seq_len)\n",
    "\n",
    "# Check if num_batches is a reasonable number\n",
    "if num_batches == 0:\n",
    "    print(\"The number of batches is zero. Check your batch_size and seq_len settings.\")\n",
    "else:\n",
    "    # To store the transformer outputs\n",
    "    transformer_outputs = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size * seq_len\n",
    "        end_idx = start_idx + (batch_size * seq_len)\n",
    "        # Extract batch and reshape to [seq_len, batch_size, feature_dim]\n",
    "        batch = concatenated_tensor[start_idx:end_idx, :].reshape(seq_len, batch_size, feature_dim)\n",
    "        # Forward pass\n",
    "        output = model(batch)\n",
    "        # Save the transformer output for use in RNN\n",
    "        transformer_outputs.append(output.detach())\n",
    "    \n",
    "    # Stack and save the outputs\n",
    "    transformer_outputs = torch.stack(transformer_outputs)\n",
    "    torch.save(transformer_outputs, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "632f82b6-adbc-4ae1-aecd-84b31989a6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in transformer_outputs: 0.023674242424242424%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(transformer_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(transformer_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(transformer_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in transformer_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in transformer_outputs: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c67252-b29f-4d81-868d-cd6be6c420cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Percentage of NaNs in transformer_outputs: 0.023674242424242424%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n",
      "After cleaning:\n",
      "Percentage of NaNs in transformer_outputs: 0.0%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in transformer_outputs before cleaning\n",
    "total_elements = torch.numel(transformer_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(transformer_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(transformer_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Before cleaning:\")\n",
    "print(f\"Percentage of NaNs in transformer_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in transformer_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Replace NaNs and Infs with zeros\n",
    "transformer_outputs[torch.isnan(transformer_outputs)] = 0\n",
    "transformer_outputs[torch.isinf(transformer_outputs)] = 0\n",
    "\n",
    "# Re-check for NaNs and Infs after cleaning\n",
    "nans_count = torch.sum(torch.isnan(transformer_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(transformer_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"After cleaning:\")\n",
    "print(f\"Percentage of NaNs in transformer_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in transformer_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Optionally, save the cleaned tensor\n",
    "torch.save(transformer_outputs, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982d702-9951-408b-9e74-665019135379",
   "metadata": {},
   "source": [
    "# RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb646e7-bcf8-4a55-8dbd-f408fcf167ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all_features: torch.Size([1, 112])\n",
      "Shape of transformer_outputs: torch.Size([66, 1000, 64, 1])\n",
      "Shape of eeg_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load saved features\n",
    "feature_path = '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Kuramoto/all_features.pt'\n",
    "transformer_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\"\n",
    "eeg_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\"\n",
    "band_power_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\"\n",
    "fast_fourier_transform_psd_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\"\n",
    "\n",
    "all_features = torch.load(feature_path)\n",
    "transformer_outputs = torch.load(transformer_outputs_path)\n",
    "eeg_tensor = torch.load(eeg_tensor_path)\n",
    "band_power_tensor = torch.load(band_power_path)\n",
    "fast_fourier_transform_psd_tensor = torch.load(fast_fourier_transform_psd_path)\n",
    "\n",
    "print(f\"Shape of all_features: {all_features.shape}\")\n",
    "print(f\"Shape of transformer_outputs: {transformer_outputs.shape}\")\n",
    "print(f\"Shape of eeg_tensor: {eeg_tensor.shape}\")\n",
    "print(f\"Shape of band_power_tensor: {band_power_tensor.shape}\")\n",
    "print(f\"Shape of fast_fourier_transform_psd_tensor: {fast_fourier_transform_psd_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558678df-e1e1-4d92-a703-19d5b0c9b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated time-aligned features shape: torch.Size([4227788, 224])\n"
     ]
    }
   ],
   "source": [
    "# Aligning the time axis\n",
    "time_length = 4227788  # replace with the length of the common time axis\n",
    "\n",
    "# Reshape `eeg_tensor` to align it with time_length\n",
    "eeg_tensor_reshaped = eeg_tensor.squeeze().transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Reshape `band_power_tensor` to align it with time_length\n",
    "band_power_tensor_reshaped = band_power_tensor.view(time_length, -1)  # [time_length, 32*5]\n",
    "\n",
    "# Reshape `fast_fourier_transform_psd_tensor` to align it with time_length\n",
    "fast_fourier_transform_psd_tensor_reshaped = fast_fourier_transform_psd_tensor.transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Concatenating time-aligned tensors\n",
    "concatenated_time_aligned_features = torch.cat(\n",
    "    (eeg_tensor_reshaped, band_power_tensor_reshaped, fast_fourier_transform_psd_tensor_reshaped), \n",
    "    dim=1\n",
    ")\n",
    "\n",
    "print(\"Concatenated time-aligned features shape:\", concatenated_time_aligned_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba6ee57-1c00-42bf-8e6d-a7cf9650efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model Definition\n",
    "class ConditionalRNN(nn.Module):\n",
    "    def __init__(self, time_aligned_feature_dim, hidden_size, global_feature_dim, transformer_feature_dim):\n",
    "        super(ConditionalRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn = nn.LSTM(time_aligned_feature_dim, hidden_size, batch_first=True)\n",
    "        self.global_feature_layer = nn.Linear(global_feature_dim, hidden_size)\n",
    "        self.transformer_feature_layer = nn.Linear(transformer_feature_dim, hidden_size)\n",
    "\n",
    "    def forward(self, time_aligned_features, global_features, transformer_outputs):\n",
    "        batch_size = time_aligned_features.size(0)\n",
    "        \n",
    "        h0 = self.global_feature_layer(global_features).unsqueeze(0).to(device, dtype=torch.float32)\n",
    "        c0 = torch.zeros_like(h0).to(device, dtype=torch.float32)\n",
    "\n",
    "        hidden_state = (h0, c0)\n",
    "        output_list = []\n",
    "\n",
    "        for t in range(time_aligned_features.size(1)):\n",
    "            current_data = time_aligned_features[:, t, :].to(device, dtype=torch.float32)\n",
    "            transformer_hidden = self.transformer_feature_layer(transformer_outputs[:, t, :]).to(device, dtype=torch.float32)\n",
    "            \n",
    "            hidden_state = (hidden_state[0] + transformer_hidden.unsqueeze(0), hidden_state[1])\n",
    "            output, hidden_state = self.rnn(current_data.unsqueeze(1), hidden_state)\n",
    "            output_list.append(output)\n",
    "\n",
    "        outputs = torch.cat(output_list, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Initialize the model and move to the device\n",
    "time_aligned_feature_dim = 224  \n",
    "hidden_size = 128\n",
    "global_feature_dim = 112\n",
    "transformer_feature_dim = 64  # 2 x 32 EEG channels\n",
    "\n",
    "model = ConditionalRNN(time_aligned_feature_dim, hidden_size, global_feature_dim, transformer_feature_dim).to(device)\n",
    "\n",
    "all_features = torch.load(feature_path).to(device)\n",
    "transformer_outputs = torch.load(transformer_outputs_path).to(device)\n",
    "\n",
    "# Pre-process the datasets\n",
    "expanded_all_features = all_features.expand(66, -1)  # Example, adjust as necessary\n",
    "time_feature_chunks = torch.split(concatenated_time_aligned_features, 1000)\n",
    "time_feature_chunks = time_feature_chunks[:66]  # Example, adjust as necessary\n",
    "aligned_time_features = torch.stack(time_feature_chunks)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dataset = TensorDataset(aligned_time_features, expanded_all_features, transformer_outputs.squeeze(-1))\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)  # Example batch size\n",
    "\n",
    "# Store outputs\n",
    "rnn_outputs = []\n",
    "\n",
    "# Run the model\n",
    "for i, (time_aligned_batch, global_features_batch, transformer_outputs_batch) in enumerate(dataloader):\n",
    "    outputs = model(time_aligned_batch, global_features_batch, transformer_outputs_batch)\n",
    "    rnn_outputs.append(outputs.detach())\n",
    "\n",
    "# Combine and save outputs\n",
    "rnn_outputs = torch.cat(rnn_outputs, dim=0)\n",
    "torch.save(rnn_outputs, '/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c005cae7-e6be-4eb6-9db1-03337c7bb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in rnn_outputs: 1.5151515151515151%\n",
      "Percentage of Infs in rnn_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "rnn_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth\"\n",
    "rnn_outputs = torch.load(rnn_outputs_path)\n",
    "\n",
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(rnn_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(rnn_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(rnn_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in rnn_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in rnn_outputs: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "420a5095-e5ad-4fea-a830-a28cc02ab18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Percentage of NaNs in rnn_outputs: 0.0%\n",
      "Percentage of Infs in rnn_outputs: 0.0%\n",
      "After cleaning:\n",
      "Percentage of NaNs in rnn_outputs: 0.0%\n",
      "Percentage of Infs in rnn_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in rnn_outputs before cleaning\n",
    "total_elements = torch.numel(rnn_outputs)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(rnn_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(rnn_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Before cleaning:\")\n",
    "print(f\"Percentage of NaNs in rnn_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in rnn_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Replace NaNs and Infs with zeros\n",
    "rnn_outputs[torch.isnan(rnn_outputs)] = 0\n",
    "rnn_outputs[torch.isinf(rnn_outputs)] = 0\n",
    "\n",
    "# Re-check for NaNs and Infs after cleaning\n",
    "nans_count = torch.sum(torch.isnan(rnn_outputs)).item()\n",
    "infs_count = torch.sum(torch.isinf(rnn_outputs)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"After cleaning:\")\n",
    "print(f\"Percentage of NaNs in rnn_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in rnn_outputs: {infs_percentage}%\")\n",
    "\n",
    "# Optionally, save the cleaned tensor\n",
    "torch.save(rnn_outputs, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522cfb9-adcc-4979-a058-fbfda120cca2",
   "metadata": {},
   "source": [
    "# Final predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf2e0ec-2207-4be3-bf42-2768d6dd33fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of RNN_outputs: torch.Size([66, 1000, 128])\n",
      "Shape of transformer_outputs: torch.Size([66, 1000, 64, 1])\n",
      "Shape of eeg_tensor: torch.Size([1, 32, 1, 4227788])\n",
      "Shape of band_power_tensor: torch.Size([4227788, 32, 5])\n",
      "Shape of fast_fourier_transform_psd_tensor: torch.Size([32, 4227788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load saved features\n",
    "transformer_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/RNN/transformer.pth\"\n",
    "eeg_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\"\n",
    "band_power_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/band_power_tensor.pth\"\n",
    "fast_fourier_transform_psd_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/fast_fourier_transform_psd_tensor.pth\"\n",
    "RNN_outputs_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/rnn_outputs.pth\"\n",
    "\n",
    "transformer_outputs = torch.load(transformer_outputs_path)\n",
    "eeg_tensor = torch.load(eeg_tensor_path)\n",
    "band_power_tensor = torch.load(band_power_path)\n",
    "fast_fourier_transform_psd_tensor = torch.load(fast_fourier_transform_psd_path)\n",
    "RNN_outputs = torch.load(RNN_outputs_path)\n",
    "\n",
    "print(f\"Shape of RNN_outputs: {RNN_outputs.shape}\")\n",
    "print(f\"Shape of transformer_outputs: {transformer_outputs.shape}\")\n",
    "print(f\"Shape of eeg_tensor: {eeg_tensor.shape}\")\n",
    "print(f\"Shape of band_power_tensor: {band_power_tensor.shape}\")\n",
    "print(f\"Shape of fast_fourier_transform_psd_tensor: {fast_fourier_transform_psd_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb55a49-e5d0-4d01-9d10-a3733a9ab855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated time-aligned features shape: torch.Size([4227788, 224])\n"
     ]
    }
   ],
   "source": [
    "# Aligning the time axis\n",
    "time_length = 4227788  # replace with the length of the common time axis\n",
    "\n",
    "# Reshape `eeg_tensor` to align it with time_length\n",
    "eeg_tensor_reshaped = eeg_tensor.squeeze().transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Reshape `band_power_tensor` to align it with time_length\n",
    "band_power_tensor_reshaped = band_power_tensor.view(time_length, -1)  # [time_length, 32*5]\n",
    "\n",
    "# Reshape `fast_fourier_transform_psd_tensor` to align it with time_length\n",
    "fast_fourier_transform_psd_tensor_reshaped = fast_fourier_transform_psd_tensor.transpose(0, 1)  # [time_length, 32]\n",
    "\n",
    "# Concatenating time-aligned tensors\n",
    "concatenated_time_aligned_features = torch.cat(\n",
    "    (eeg_tensor_reshaped, band_power_tensor_reshaped, fast_fourier_transform_psd_tensor_reshaped), \n",
    "    dim=1\n",
    ")\n",
    "\n",
    "print(\"Concatenated time-aligned features shape:\", concatenated_time_aligned_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6268a2-499f-41c1-9bc6-15a756762782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in transformer_outputs: 0.0%\n",
      "Percentage of Infs in transformer_outputs: 0.0%\n",
      "Percentage of NaNs in eeg_tensor: 0.0%\n",
      "Percentage of Infs in eeg_tensor: 0.0%\n",
      "Percentage of NaNs in band_power_tensor: 0.0%\n",
      "Percentage of Infs in band_power_tensor: 0.0%\n",
      "Percentage of NaNs in fast_fourier_transform_psd_tensor: 0.0%\n",
      "Percentage of Infs in fast_fourier_transform_psd_tensor: 0.0%\n",
      "Percentage of NaNs in RNN_outputs: 0.0%\n",
      "Percentage of Infs in RNN_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "def check_tensor(tensor, name):\n",
    "    total_elements = torch.numel(tensor)\n",
    "    nans_count = torch.sum(torch.isnan(tensor)).item()\n",
    "    infs_count = torch.sum(torch.isinf(tensor)).item()\n",
    "    nans_percentage = (nans_count / total_elements) * 100\n",
    "    infs_percentage = (infs_count / total_elements) * 100\n",
    "    print(f\"Percentage of NaNs in {name}: {nans_percentage}%\")\n",
    "    print(f\"Percentage of Infs in {name}: {infs_percentage}%\")\n",
    "\n",
    "check_tensor(transformer_outputs, \"transformer_outputs\")\n",
    "check_tensor(eeg_tensor, \"eeg_tensor\")\n",
    "check_tensor(band_power_tensor, \"band_power_tensor\")\n",
    "check_tensor(fast_fourier_transform_psd_tensor, \"fast_fourier_transform_psd_tensor\")\n",
    "check_tensor(RNN_outputs, \"RNN_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e8eaafc-bde4-4af0-a497-d22dfb59c07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in eeg_tensor_reshaped: 0.0%\n",
      "Percentage of Infs in eeg_tensor_reshaped: 0.0%\n",
      "Percentage of NaNs in band_power_tensor_reshaped: 0.0%\n",
      "Percentage of Infs in band_power_tensor_reshaped: 0.0%\n",
      "Percentage of NaNs in fast_fourier_transform_psd_tensor_reshaped: 0.0%\n",
      "Percentage of Infs in fast_fourier_transform_psd_tensor_reshaped: 0.0%\n",
      "Percentage of NaNs in concatenated_time_aligned_features: 0.0%\n",
      "Percentage of Infs in concatenated_time_aligned_features: 0.0%\n"
     ]
    }
   ],
   "source": [
    "check_tensor(eeg_tensor_reshaped, \"eeg_tensor_reshaped\")\n",
    "check_tensor(band_power_tensor_reshaped, \"band_power_tensor_reshaped\")\n",
    "check_tensor(fast_fourier_transform_psd_tensor_reshaped, \"fast_fourier_transform_psd_tensor_reshaped\")\n",
    "check_tensor(concatenated_time_aligned_features, \"concatenated_time_aligned_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf09775-dadf-414a-b6c5-db5464433b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in concatenated_time_aligned_features_outputs: 0.0%\n",
      "Percentage of Infs in concatenated_time_aligned_features_outputs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(concatenated_time_aligned_features)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(concatenated_time_aligned_features)).item()\n",
    "infs_count = torch.sum(torch.isinf(concatenated_time_aligned_features)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in concatenated_time_aligned_features_outputs: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in concatenated_time_aligned_features_outputs: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab01f1ae-dcea-4e47-b458-fa37ccdc2321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     final_input_tensor_list\u001b[38;5;241m.\u001b[39mappend(temp_concat)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Optionally, save this tensor to disk to free up memory\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_concat_chunk_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43msplit_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Update feature_dim to the new size after concatenation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m feature_dim \u001b[38;5;241m=\u001b[39m final_input_tensor_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Taking shape from one of the chunks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure all tensors are on the CPU\n",
    "RNN_outputs = RNN_outputs.to('cpu')\n",
    "concatenated_time_aligned_features = concatenated_time_aligned_features.to('cpu')\n",
    "\n",
    "# Reduce the RNN outputs' time dimension by averaging\n",
    "RNN_outputs_reduced = torch.mean(RNN_outputs, dim=1)  # Shape: [66, 128]\n",
    "\n",
    "# Reshape `RNN_outputs` to align it with time_length\n",
    "RNN_outputs_reshaped = RNN_outputs.reshape(-1, RNN_outputs.shape[-1])  # [66*1000, 128] = [66000, 128]\n",
    "\n",
    "# Compute the number of repetitions needed to approximate the time_length\n",
    "n_repeats = time_length // RNN_outputs_reshaped.shape[0]\n",
    "remaining_rows = time_length % RNN_outputs_reshaped.shape[0]\n",
    "\n",
    "# Repeat the tensor for n_repeats times and add extra padding if needed\n",
    "RNN_outputs_expanded = RNN_outputs_reshaped.repeat(n_repeats, 1)\n",
    "if remaining_rows > 0:\n",
    "    extra_padding = RNN_outputs_reshaped[:remaining_rows]\n",
    "    RNN_outputs_expanded = torch.cat([RNN_outputs_expanded, extra_padding], dim=0)\n",
    "\n",
    "# Initialize a list to hold the smaller tensors\n",
    "final_input_tensor_list = []\n",
    "\n",
    "# Split and concatenate in chunks to reduce memory footprint\n",
    "split_size = 1000  # adjust as needed\n",
    "for i in range(0, time_length, split_size):\n",
    "    # Ensure the slice size matches for both tensors\n",
    "    slice_size = min(split_size, time_length - i)\n",
    "    temp_concat = torch.cat(\n",
    "        (concatenated_time_aligned_features[i:i + slice_size], \n",
    "         RNN_outputs_expanded[i:i + slice_size]), \n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "    # Append the tensor to the list\n",
    "    final_input_tensor_list.append(temp_concat)\n",
    "    \n",
    "    # Optionally, save this tensor to disk to free up memory\n",
    "    torch.save(temp_concat, f\"temp_concat_chunk_{i//split_size}.pt\")\n",
    "\n",
    "# Update feature_dim to the new size after concatenation\n",
    "feature_dim = final_input_tensor_list[0].shape[1]  # Taking shape from one of the chunks\n",
    "\n",
    "# Pre-allocate a zero tensor with the required size\n",
    "final_time_length = 4227788  # replace with your value\n",
    "final_feature_dim = feature_dim  # replace with your feature dimension\n",
    "\n",
    "# Pre-allocate on CPU\n",
    "final_input_tensor = torch.zeros((final_time_length, final_feature_dim))\n",
    "\n",
    "# Fill in the slices\n",
    "start_idx = 0\n",
    "for temp_tensor in final_input_tensor_list:\n",
    "    end_idx = start_idx + temp_tensor.shape[0]\n",
    "    final_input_tensor[start_idx:end_idx, :] = temp_tensor  # Tensor is already on CPU\n",
    "    start_idx = end_idx  # set start_idx for the next iteration\n",
    "\n",
    "# The tensor final_input_tensor should now have shape [4227788, feature_dim]\n",
    "print(final_input_tensor.shape)\n",
    "\n",
    "# Save the tensor\n",
    "torch.save(final_input_tensor, \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/final_input_tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47da3217-a61d-4d24-be1f-caa8c95e1ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in temp_concat_chunk_82: 0.0%\n",
      "Percentage of Infs in temp_concat_chunk_82: 0.0%\n"
     ]
    }
   ],
   "source": [
    "check_tensor(temp_concat, f\"temp_concat_chunk_{i//split_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede5313c-2781-4926-a515-6a6d6898b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaNs in final_input_tensor: 0.0%\n",
      "Percentage of Infs in final_input_tensor: 0.0%\n"
     ]
    }
   ],
   "source": [
    "final_input_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/final_input_tensor\"\n",
    "final_input_tensor = torch.load(final_input_tensor_path)\n",
    "\n",
    "# Check for NaNs and Infs in transformer_outputs\n",
    "total_elements = torch.numel(final_input_tensor)\n",
    "\n",
    "nans_count = torch.sum(torch.isnan(final_input_tensor)).item()\n",
    "infs_count = torch.sum(torch.isinf(final_input_tensor)).item()\n",
    "\n",
    "nans_percentage = (nans_count / total_elements) * 100\n",
    "infs_percentage = (infs_count / total_elements) * 100\n",
    "\n",
    "print(f\"Percentage of NaNs in final_input_tensor: {nans_percentage}%\")\n",
    "print(f\"Percentage of Infs in final_input_tensor: {infs_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea9271-d953-4eb2-bffc-966b3885c984",
   "metadata": {},
   "source": [
    "# prepare to train test validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35bb88da-9e74-43b7-8188-c88ad98ff212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4227788, 352])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the tensor\n",
    "final_input_tensor = torch.load(\"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Test Validation/final_input_tensor\")\n",
    "\n",
    "# Verify that the tensor was loaded correctly\n",
    "print(final_input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4c76aa-8546-4dd9-8807-3b12e5d27dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG data: torch.Size([4227788, 32])\n",
      "Shape of input tensor: torch.Size([4227788, 352])\n"
     ]
    }
   ],
   "source": [
    "def create_mini_batches(tensor, seq_length, batch_size):\n",
    "    dataset_list = []\n",
    "    for i in range(0, tensor.shape[0] - seq_length, seq_length):\n",
    "        end_idx = min(i + seq_length, tensor.shape[0])\n",
    "        subset = tensor[i:end_idx]\n",
    "        dataset_list.append(subset)\n",
    "        \n",
    "    if len(dataset_list) == 0:\n",
    "        raise ValueError(\"dataset_list is empty. Check the tensor dimensions.\")\n",
    "        \n",
    "    combined_dataset = torch.stack(dataset_list)\n",
    "    tensor_dataset = TensorDataset(combined_dataset)\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "# Load EEG data (labels)\n",
    "eeg_tensor_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Transformer/EEG_tensor.pth\"\n",
    "eeg_tensor = torch.load(eeg_tensor_path)\n",
    "eeg_data = eeg_tensor.squeeze().transpose(0, 1)  # Squeeze singleton dimensions and transpose\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Shape of EEG data:\", eeg_data.shape)\n",
    "print(\"Shape of input tensor:\", final_input_tensor.shape)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "total_data = len(eeg_data)\n",
    "train_split = int(0.8 * total_data)\n",
    "val_split = int(0.9 * total_data)\n",
    "\n",
    "train_data_Y = eeg_data[:train_split]\n",
    "val_data_Y = eeg_data[train_split:val_split]\n",
    "test_data_Y = eeg_data[val_split:]\n",
    "\n",
    "train_data_X = final_input_tensor[:train_split]\n",
    "val_data_X = final_input_tensor[train_split:val_split]\n",
    "test_data_X = final_input_tensor[val_split:]\n",
    "\n",
    "# Parameters\n",
    "seq_length = 1000\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders for EEG data (train, validation, and test sets)\n",
    "train_loader_Y = create_mini_batches(train_data_Y, seq_length, batch_size)\n",
    "val_loader_Y = create_mini_batches(val_data_Y, seq_length, batch_size)\n",
    "test_loader_Y = create_mini_batches(test_data_Y, seq_length, batch_size)\n",
    "\n",
    "# Create DataLoaders for the features (train, validation, and test sets)\n",
    "train_loader_X = create_mini_batches(train_data_X, seq_length, batch_size)\n",
    "val_loader_X = create_mini_batches(val_data_X, seq_length, batch_size)\n",
    "test_loader_X = create_mini_batches(test_data_X, seq_length, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0461b6-7fec-4356-9290-50de3f499541",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "535b0531-a3a6-48d3-873f-f6a4fba6e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Add weight initialization in the constructor\n",
    "class EEGSeq2SeqPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(EEGSeq2SeqPredictor, self).__init__()\n",
    "\n",
    "        # Initialize fully connected layers for input dimension reduction\n",
    "        self.input_fc_X = nn.Linear(352, d_model)\n",
    "        self.input_fc_Y = nn.Linear(32, d_model)\n",
    "        \n",
    "        # Initialize Transformer Encoder and Decoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Initialize fully connected layer for output\n",
    "        self.fc = nn.Linear(d_model, 32)\n",
    "\n",
    "        # Correct the weight initialization\n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)  # corrected syntax\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # Dimension reduction\n",
    "        src = self.input_fc_X(src)\n",
    "        tgt = self.input_fc_Y(tgt)\n",
    "        \n",
    "        # Transformer Encoder-Decoder\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4833d153-12f8-452e-8151-7f4f3d2ea16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model outputs:  tensor([[[ 1.0120e-01,  1.8821e-01, -5.1884e-01,  ..., -1.4553e-01,\n",
      "           2.1883e-01,  5.5842e-01],\n",
      "         [ 4.7765e-01,  1.1960e-01, -3.1487e-01,  ..., -2.0871e-01,\n",
      "          -3.3689e-01,  3.8879e-01],\n",
      "         [ 2.6454e-01,  1.5813e-01, -5.5123e-01,  ..., -3.2001e-03,\n",
      "          -2.8601e-01,  8.1808e-01],\n",
      "         ...,\n",
      "         [ 1.5453e-01,  3.6097e-01, -3.4341e-01,  ..., -2.3486e-01,\n",
      "          -2.3411e-01,  4.9173e-01],\n",
      "         [ 3.8978e-01, -1.5079e-01, -3.4597e-01,  ..., -2.3643e-01,\n",
      "          -1.5611e-01,  4.7639e-01],\n",
      "         [ 3.0984e-01,  6.1088e-02, -3.7082e-01,  ..., -2.3686e-01,\n",
      "          -1.3976e-01,  7.2975e-01]],\n",
      "\n",
      "        [[ 1.0452e-01,  3.8901e-01, -9.2303e-01,  ...,  3.2515e-01,\n",
      "           9.2133e-02,  4.0405e-01],\n",
      "         [ 5.1112e-01,  2.3498e-01, -6.3515e-01,  ...,  1.9485e-01,\n",
      "          -3.4492e-01, -4.1567e-02],\n",
      "         [ 2.1674e-02,  6.0943e-01, -9.1323e-01,  ...,  5.2123e-01,\n",
      "           1.5939e-01,  5.0572e-01],\n",
      "         ...,\n",
      "         [ 1.0403e-01,  4.6930e-01, -8.1028e-01,  ...,  2.9563e-01,\n",
      "           9.6945e-02, -1.3744e-01],\n",
      "         [ 3.7482e-01,  9.0790e-02, -8.1846e-01,  ...,  1.1586e-01,\n",
      "           4.6928e-03,  1.2475e-01],\n",
      "         [ 7.6059e-02,  4.1332e-01, -8.5755e-01,  ...,  5.2897e-01,\n",
      "           3.1636e-01,  2.9991e-01]],\n",
      "\n",
      "        [[ 1.7331e-01,  2.0849e-01, -4.5954e-01,  ..., -4.9779e-01,\n",
      "          -1.2181e-01,  7.2474e-01],\n",
      "         [ 3.4826e-01,  2.1959e-01, -5.1514e-01,  ..., -1.1534e-01,\n",
      "           1.9343e-02,  3.6401e-01],\n",
      "         [-1.0756e-01,  5.0113e-02, -5.3800e-01,  ..., -3.8497e-01,\n",
      "          -2.1511e-01,  6.3654e-01],\n",
      "         ...,\n",
      "         [ 3.1175e-01,  2.9819e-01, -4.8631e-01,  ..., -3.6945e-01,\n",
      "          -2.1735e-01,  7.4185e-01],\n",
      "         [ 1.9048e-01,  1.6735e-02, -6.4390e-01,  ..., -5.7936e-02,\n",
      "          -1.1347e-01,  5.2388e-01],\n",
      "         [ 4.8919e-04, -2.4169e-02, -6.2875e-01,  ..., -1.7039e-01,\n",
      "          -9.6422e-02,  8.6781e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.2910e-02,  1.9335e-01, -6.9465e-01,  ..., -4.5523e-02,\n",
      "          -9.6104e-03,  6.8879e-01],\n",
      "         [ 4.5282e-01,  5.9258e-02, -5.5593e-01,  ..., -1.5513e-01,\n",
      "          -3.9727e-01,  5.5637e-01],\n",
      "         [ 9.7288e-02,  1.8407e-01, -8.7389e-01,  ..., -3.2115e-02,\n",
      "          -1.8896e-01,  8.5169e-01],\n",
      "         ...,\n",
      "         [ 1.6604e-02,  1.7301e-01, -6.2088e-01,  ..., -4.0143e-01,\n",
      "          -1.5728e-01,  4.6504e-01],\n",
      "         [ 1.4995e-01,  4.8700e-02, -7.5122e-01,  ..., -1.3990e-01,\n",
      "          -1.3268e-02,  4.9030e-01],\n",
      "         [ 8.0730e-02,  3.8784e-01, -6.2569e-01,  ..., -1.7824e-01,\n",
      "          -1.2820e-01,  7.9020e-01]],\n",
      "\n",
      "        [[ 2.0753e-02,  2.3654e-01, -5.3966e-01,  ..., -1.0764e-01,\n",
      "           6.5753e-02,  6.1134e-01],\n",
      "         [ 1.8730e-01, -4.0937e-02, -4.1518e-01,  ..., -3.7913e-01,\n",
      "          -5.0258e-01,  4.3652e-01],\n",
      "         [ 2.1198e-01,  1.8218e-01, -4.6006e-01,  ..., -1.2853e-01,\n",
      "          -1.3724e-01,  8.1304e-01],\n",
      "         ...,\n",
      "         [-2.1619e-03,  5.4652e-02, -8.2711e-01,  ..., -2.8701e-01,\n",
      "          -1.4827e-01,  4.7408e-01],\n",
      "         [ 7.7022e-02,  2.0173e-02, -1.7239e-01,  ...,  2.3477e-02,\n",
      "          -2.9857e-02,  2.5978e-01],\n",
      "         [ 4.3209e-01,  2.1634e-01, -5.4327e-01,  ..., -2.8489e-01,\n",
      "           5.2567e-02,  9.1144e-01]],\n",
      "\n",
      "        [[-3.0211e-01,  2.3195e-01, -8.7007e-01,  ...,  1.2022e-01,\n",
      "           1.8831e-01,  7.4704e-02],\n",
      "         [ 4.9313e-02,  3.5189e-01, -8.3549e-01,  ...,  1.5680e-01,\n",
      "          -3.2003e-01,  2.1871e-01],\n",
      "         [-3.7029e-01,  3.4550e-01, -9.6802e-01,  ...,  3.6261e-01,\n",
      "           1.9712e-01,  2.3691e-01],\n",
      "         ...,\n",
      "         [-1.8784e-01,  2.8341e-01, -7.2692e-01,  ..., -9.8100e-02,\n",
      "           2.2012e-01,  9.3905e-02],\n",
      "         [-1.5036e-01,  1.9987e-01, -9.5883e-01,  ...,  4.4333e-01,\n",
      "           1.8074e-01,  2.4910e-01],\n",
      "         [-1.5271e-01,  2.6662e-01, -9.9107e-01,  ...,  1.9889e-01,\n",
      "           3.3211e-01,  4.7529e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.4490444660186768\n",
      "Model outputs:  tensor([[[-1.4818e-01,  5.8852e-01, -3.2415e-01,  ...,  1.5807e-01,\n",
      "           4.1769e-01, -1.3307e-01],\n",
      "         [-2.6266e-01,  7.1876e-01, -4.8170e-01,  ...,  2.1702e-01,\n",
      "           3.1375e-01, -2.4717e-01],\n",
      "         [-2.2259e-01,  7.5237e-01, -4.0352e-01,  ...,  8.5581e-02,\n",
      "           5.5402e-01, -3.0532e-01],\n",
      "         ...,\n",
      "         [-2.7567e-01,  4.0550e-01, -7.6597e-01,  ...,  9.9399e-02,\n",
      "           3.1136e-01,  2.4898e-01],\n",
      "         [-3.5843e-01,  9.9736e-01, -5.3286e-01,  ...,  3.1076e-01,\n",
      "           3.3261e-01, -5.8448e-02],\n",
      "         [-2.4692e-01,  6.8547e-01, -4.2129e-01,  ..., -7.8032e-02,\n",
      "           3.6566e-01,  9.5787e-02]],\n",
      "\n",
      "        [[-1.1138e-01,  7.1668e-01, -8.2131e-01,  ..., -5.6231e-01,\n",
      "           1.2963e-01,  7.9697e-02],\n",
      "         [-8.5385e-02,  6.5768e-01, -6.8399e-01,  ..., -3.4297e-01,\n",
      "           8.3515e-03,  2.8110e-02],\n",
      "         [-7.7436e-02,  5.0590e-01, -7.0431e-01,  ..., -2.7733e-01,\n",
      "           3.8776e-01,  1.0681e-01],\n",
      "         ...,\n",
      "         [-6.8164e-02,  6.6257e-01, -1.0472e+00,  ..., -5.5056e-01,\n",
      "           1.5402e-01,  1.9819e-01],\n",
      "         [-1.2822e-01,  6.3294e-01, -1.0268e+00,  ..., -4.0053e-01,\n",
      "           4.9259e-02,  4.5689e-01],\n",
      "         [-1.8347e-01,  5.0645e-01, -8.4373e-01,  ..., -3.9166e-01,\n",
      "          -1.4622e-02,  6.6531e-02]],\n",
      "\n",
      "        [[ 2.6125e-01,  2.8889e-01, -6.2309e-01,  ..., -7.4290e-01,\n",
      "           5.1463e-01,  2.2639e-01],\n",
      "         [ 4.7611e-01,  2.0688e-01, -3.8986e-01,  ..., -8.4387e-01,\n",
      "           4.0194e-01,  3.3264e-01],\n",
      "         [ 4.0794e-01,  5.0889e-01, -1.5832e-01,  ..., -6.6342e-01,\n",
      "           8.4534e-01,  3.1104e-01],\n",
      "         ...,\n",
      "         [ 3.3219e-01,  5.6398e-01, -8.5319e-01,  ..., -7.3535e-01,\n",
      "           6.0796e-01,  7.5266e-01],\n",
      "         [ 1.4889e-01,  5.8094e-01, -7.5273e-01,  ..., -7.8422e-01,\n",
      "           5.0625e-01,  7.7341e-01],\n",
      "         [-1.7262e-02,  7.2257e-01, -3.3005e-01,  ..., -5.9036e-01,\n",
      "           8.6393e-01,  1.9438e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.3559e-01,  8.8950e-01, -6.7431e-01,  ..., -3.2684e-01,\n",
      "           3.1745e-01, -1.3144e-01],\n",
      "         [ 1.5896e-01,  7.0859e-01, -8.1844e-01,  ..., -3.1674e-01,\n",
      "           9.2841e-02, -1.3241e-01],\n",
      "         [ 6.3042e-02,  6.7856e-01, -7.4978e-01,  ..., -1.1051e-01,\n",
      "           4.6482e-01, -4.6082e-03],\n",
      "         ...,\n",
      "         [ 1.6633e-02,  8.0270e-01, -1.1279e+00,  ..., -1.6028e-01,\n",
      "          -1.6428e-01,  3.1049e-01],\n",
      "         [ 1.4946e-01,  4.6571e-01, -9.0466e-01,  ..., -2.5913e-01,\n",
      "           2.1191e-01,  2.4610e-01],\n",
      "         [ 5.3448e-02,  3.4908e-01, -8.2595e-01,  ..., -4.3883e-01,\n",
      "           8.4738e-02,  3.1325e-01]],\n",
      "\n",
      "        [[ 3.9286e-01,  2.8376e-01, -2.0585e-01,  ..., -3.6598e-01,\n",
      "           2.6205e-01, -6.4328e-01],\n",
      "         [ 3.9076e-01,  3.5753e-01, -9.2303e-02,  ..., -3.1362e-01,\n",
      "           1.3840e-01, -7.4445e-01],\n",
      "         [-8.6375e-02,  4.2935e-01, -7.3809e-02,  ...,  5.9590e-02,\n",
      "           3.3915e-01, -9.1578e-01],\n",
      "         ...,\n",
      "         [ 6.5556e-02,  3.6435e-01, -7.6468e-01,  ..., -9.8275e-01,\n",
      "           4.1128e-01,  3.8501e-01],\n",
      "         [ 3.2022e-01,  2.8867e-01, -6.6582e-01,  ..., -8.2258e-01,\n",
      "           3.1237e-01,  3.4190e-02],\n",
      "         [ 4.3665e-01,  8.5385e-02, -4.9064e-01,  ..., -5.8520e-01,\n",
      "           3.7170e-01, -2.6444e-01]],\n",
      "\n",
      "        [[ 2.0466e-02,  6.6343e-01, -8.2817e-01,  ..., -3.6082e-01,\n",
      "           1.9542e-01, -1.1537e-01],\n",
      "         [-7.0588e-02,  3.2355e-01, -6.4131e-01,  ..., -5.4895e-01,\n",
      "           2.3747e-01,  1.1886e-01],\n",
      "         [ 6.8540e-02,  5.5697e-01, -6.2291e-01,  ..., -3.1160e-01,\n",
      "           3.2968e-01, -5.3699e-02],\n",
      "         ...,\n",
      "         [ 8.0985e-02,  6.2478e-01, -1.0537e+00,  ..., -2.0428e-01,\n",
      "           5.1323e-02,  2.7883e-01],\n",
      "         [-1.7267e-01,  6.6172e-01, -8.9289e-01,  ..., -4.4366e-01,\n",
      "           1.0988e-04,  4.3368e-01],\n",
      "         [ 1.4533e-01,  8.7785e-01, -7.2279e-01,  ..., -2.1375e-01,\n",
      "           2.6190e-01,  2.1958e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3530495166778564\n",
      "Model outputs:  tensor([[[ 2.5948e-01,  7.9857e-02, -5.7319e-04,  ...,  1.9617e-01,\n",
      "          -1.1109e-01, -6.8460e-01],\n",
      "         [ 3.7242e-01,  1.9323e-01, -1.0126e-01,  ...,  5.5569e-02,\n",
      "           1.9051e-01, -6.2105e-01],\n",
      "         [ 1.7368e-01,  2.5985e-01,  1.6724e-02,  ...,  4.2750e-01,\n",
      "          -4.7209e-02, -4.3259e-01],\n",
      "         ...,\n",
      "         [ 3.5017e-01,  3.9131e-02, -3.9195e-01,  ...,  3.9266e-01,\n",
      "          -5.7075e-02, -3.8165e-01],\n",
      "         [ 6.4759e-01,  2.3456e-02,  1.1654e-01,  ...,  7.0980e-01,\n",
      "          -2.9785e-02, -4.8477e-01],\n",
      "         [ 1.9222e-01,  2.9493e-01, -1.0054e-01,  ...,  6.5958e-02,\n",
      "          -4.2688e-02, -5.1490e-01]],\n",
      "\n",
      "        [[-1.0426e-01,  6.9089e-01, -1.1193e+00,  ...,  6.8871e-02,\n",
      "           1.8298e-01,  2.8255e-01],\n",
      "         [-1.9183e-01,  6.9557e-01, -7.9545e-01,  ..., -2.5533e-03,\n",
      "           1.8802e-02, -5.1992e-02],\n",
      "         [-2.0545e-01,  9.5080e-01, -7.3152e-01,  ...,  9.1829e-02,\n",
      "           5.4048e-02, -2.4808e-01],\n",
      "         ...,\n",
      "         [-1.1177e-01,  4.0285e-01, -1.0992e+00,  ...,  2.0928e-01,\n",
      "           1.3222e-02,  7.8407e-01],\n",
      "         [ 1.9827e-01,  5.9923e-01, -8.4514e-01,  ...,  4.0230e-01,\n",
      "          -3.2752e-03,  3.0587e-01],\n",
      "         [-2.4932e-01,  6.5162e-01, -8.0387e-01,  ..., -1.6940e-01,\n",
      "           5.9195e-02,  3.0691e-01]],\n",
      "\n",
      "        [[ 2.0210e-01,  4.4479e-01, -7.6939e-01,  ..., -2.2280e-01,\n",
      "           4.4964e-02,  4.6110e-01],\n",
      "         [-2.0402e-01,  4.0427e-01, -4.2327e-01,  ..., -5.3288e-01,\n",
      "           1.7324e-01,  1.0603e-01],\n",
      "         [-1.9627e-01,  8.0379e-01, -3.8635e-01,  ..., -1.7612e-01,\n",
      "           1.6063e-01,  2.6786e-02],\n",
      "         ...,\n",
      "         [ 1.6052e-02,  1.7009e-01, -8.6492e-01,  ...,  1.1522e-01,\n",
      "           5.1917e-02,  7.8754e-01],\n",
      "         [ 2.8971e-01,  3.4917e-01, -8.0743e-01,  ..., -4.0533e-02,\n",
      "           1.1551e-01,  5.4274e-01],\n",
      "         [-3.9597e-01,  4.9185e-01, -6.8465e-01,  ..., -3.7794e-01,\n",
      "           2.1763e-01,  2.5188e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.7451e-01,  1.1800e-01, -1.6992e-01,  ...,  1.5740e-01,\n",
      "          -6.3384e-02, -4.2729e-01],\n",
      "         [ 4.0902e-01,  3.0073e-01, -1.2469e-01,  ...,  4.2706e-01,\n",
      "          -8.8474e-02, -5.8146e-01],\n",
      "         [ 1.2509e-02,  5.6467e-01,  4.0337e-02,  ...,  1.1116e-01,\n",
      "          -2.2349e-01, -6.3352e-01],\n",
      "         ...,\n",
      "         [ 2.6973e-01,  1.6052e-01, -2.5442e-01,  ...,  5.2534e-01,\n",
      "          -6.4719e-02, -1.9516e-01],\n",
      "         [ 7.9452e-01, -1.0808e-01,  3.9755e-02,  ...,  5.7104e-01,\n",
      "          -6.5260e-02, -4.6756e-01],\n",
      "         [ 1.6623e-01,  2.1101e-01, -1.4852e-01,  ...,  4.2781e-01,\n",
      "          -1.3373e-01, -6.7662e-01]],\n",
      "\n",
      "        [[-5.6324e-01,  7.4606e-01, -1.5481e-01,  ...,  5.0074e-01,\n",
      "           4.1063e-01, -5.2442e-02],\n",
      "         [-3.9000e-01,  6.7347e-01, -2.3394e-01,  ...,  3.2696e-01,\n",
      "           2.8553e-01, -2.6046e-01],\n",
      "         [-4.6523e-01,  7.8018e-01, -3.4360e-01,  ...,  2.3343e-01,\n",
      "           6.1254e-01, -9.0378e-02],\n",
      "         ...,\n",
      "         [-4.7744e-01,  5.4909e-01, -4.7151e-01,  ...,  5.5656e-01,\n",
      "           2.9575e-01,  9.3498e-02],\n",
      "         [-1.9474e-01,  3.7892e-01, -1.5083e-01,  ...,  6.1270e-01,\n",
      "           4.7134e-01, -8.4354e-02],\n",
      "         [-4.8030e-01,  7.9973e-01, -4.9980e-01,  ...,  2.6783e-01,\n",
      "           5.0262e-01,  9.2795e-02]],\n",
      "\n",
      "        [[-4.3690e-01,  9.1213e-01, -5.5734e-01,  ...,  2.2659e-01,\n",
      "           4.6723e-01,  3.2141e-01],\n",
      "         [-4.0898e-01,  6.6531e-01, -5.3921e-01,  ..., -1.6041e-01,\n",
      "           4.7694e-01,  1.8443e-01],\n",
      "         [-3.8804e-01,  1.0147e+00, -6.1622e-01,  ...,  1.6161e-01,\n",
      "           5.7738e-01,  9.6820e-02],\n",
      "         ...,\n",
      "         [-4.0861e-01,  6.4409e-01, -7.0662e-01,  ...,  1.5537e-01,\n",
      "           3.1823e-01,  3.9038e-01],\n",
      "         [-4.1803e-01,  5.5655e-01, -4.9771e-01,  ...,  4.3784e-01,\n",
      "           2.0062e-01,  4.8534e-01],\n",
      "         [-5.6345e-01,  5.0813e-01, -6.6965e-01,  ...,  1.6653e-01,\n",
      "           4.5411e-01,  3.9949e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2447108030319214\n",
      "Model outputs:  tensor([[[-0.3314,  0.5133, -0.1499,  ...,  0.6178,  0.2573, -0.0598],\n",
      "         [-0.4317,  0.3588, -0.3181,  ...,  0.7219,  0.1981,  0.0290],\n",
      "         [-0.2744,  0.3647, -0.4618,  ...,  0.6953,  0.4463,  0.2068],\n",
      "         ...,\n",
      "         [-0.2549,  0.2787, -0.3258,  ...,  0.6657,  0.4088,  0.0615],\n",
      "         [-0.4876,  0.6559, -0.2830,  ...,  0.7084,  0.3394,  0.1318],\n",
      "         [-0.4147,  0.5739, -0.1982,  ...,  0.4435,  0.5978,  0.1780]],\n",
      "\n",
      "        [[ 0.0065, -0.0962, -0.4714,  ..., -0.6313,  0.1081,  0.2834],\n",
      "         [ 0.3683, -0.0409, -0.4900,  ..., -0.5931,  0.3586,  0.5551],\n",
      "         [ 0.4189,  0.3301, -0.2890,  ..., -0.7689,  0.3509,  0.5244],\n",
      "         ...,\n",
      "         [ 0.1673,  0.1062, -0.7924,  ..., -0.1225, -0.0236, -0.1165],\n",
      "         [ 0.1989,  0.1446, -0.5225,  ..., -0.3612,  0.1615, -0.0436],\n",
      "         [ 0.1324,  0.3247, -0.4351,  ..., -0.4899, -0.1128,  0.0023]],\n",
      "\n",
      "        [[-0.4128,  0.7038, -0.2801,  ...,  0.8270,  0.6346, -0.1101],\n",
      "         [-0.1367,  0.2378, -0.1316,  ...,  0.6077,  0.2763, -0.3549],\n",
      "         [-0.2690,  0.6914, -0.3537,  ...,  0.8986,  0.2267,  0.2113],\n",
      "         ...,\n",
      "         [-0.3266,  0.3780, -0.3225,  ...,  0.6723,  0.3874,  0.2317],\n",
      "         [-0.4457,  0.7767, -0.0733,  ...,  0.5110,  0.2540, -0.1174],\n",
      "         [-0.3408,  0.7846, -0.1909,  ...,  0.4618,  0.2576,  0.2733]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1108,  0.3784, -0.7059,  ...,  0.1802, -0.0504,  0.0459],\n",
      "         [ 0.2668,  0.4808, -0.7571,  ...,  0.6262,  0.1041,  0.3599],\n",
      "         [ 0.0380,  0.5464, -0.8255,  ...,  0.3272,  0.0184,  0.3156],\n",
      "         ...,\n",
      "         [ 0.3643,  0.5445, -0.6686,  ...,  0.3365,  0.2235,  0.2253],\n",
      "         [-0.3748,  0.7810, -0.8450,  ...,  0.1674,  0.0534,  0.1055],\n",
      "         [-0.3080,  0.6949, -0.9856,  ...,  0.1330, -0.0591,  0.2184]],\n",
      "\n",
      "        [[-0.0786,  0.5859, -1.0445,  ...,  0.1068, -0.1020,  0.5613],\n",
      "         [-0.1993,  0.2543, -1.0492,  ...,  0.4367,  0.1575,  0.1593],\n",
      "         [-0.3870,  0.5229, -0.7776,  ...,  0.5702, -0.0151,  0.3929],\n",
      "         ...,\n",
      "         [-0.2024,  0.4901, -1.0492,  ...,  0.2228,  0.3689,  0.3232],\n",
      "         [-0.3261,  0.6176, -1.1200,  ...,  0.2430,  0.2433, -0.2000],\n",
      "         [-0.4325,  0.8115, -0.9867,  ...,  0.1989,  0.3328,  0.1899]],\n",
      "\n",
      "        [[ 0.0368,  0.3092, -0.5801,  ..., -0.3768, -0.1745,  0.3220],\n",
      "         [ 0.2572, -0.0794, -0.3945,  ..., -0.5579,  0.3701,  0.0022],\n",
      "         [ 0.2701, -0.1191, -0.4136,  ..., -0.4003,  0.2295,  0.4047],\n",
      "         ...,\n",
      "         [ 0.2373, -0.0739, -0.6694,  ...,  0.0170, -0.2123,  0.0921],\n",
      "         [-0.1787,  0.2342, -0.4884,  ..., -0.4412,  0.0419, -0.2016],\n",
      "         [ 0.1998,  0.1887, -0.4780,  ..., -0.5564, -0.0581,  0.1387]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1848710775375366\n",
      "Model outputs:  tensor([[[-0.3566,  0.6290, -0.8154,  ...,  0.1756,  0.2889,  0.0956],\n",
      "         [-0.4839,  0.5121, -1.0031,  ..., -0.0963,  0.3063,  0.2804],\n",
      "         [-0.1966,  0.4760, -0.3206,  ...,  0.2785,  0.2956,  0.1024],\n",
      "         ...,\n",
      "         [-0.7256,  0.6807, -0.8247,  ...,  0.5632, -0.0470,  0.5286],\n",
      "         [-0.3989,  0.6172, -0.9613,  ...,  0.3268,  0.2497,  0.4395],\n",
      "         [-0.4812,  0.7345, -0.4389,  ...,  0.3598,  0.3158,  0.3439]],\n",
      "\n",
      "        [[-0.1773,  0.4281, -0.0141,  ...,  0.0826,  0.4129,  0.0611],\n",
      "         [-0.3140,  0.5294, -0.2422,  ...,  0.4284,  0.3485, -0.1358],\n",
      "         [-0.2547,  0.4952, -0.3294,  ...,  0.0187,  0.5719,  0.0138],\n",
      "         ...,\n",
      "         [-0.1847,  0.3363, -0.2191,  ...,  0.5826,  0.0537,  0.0776],\n",
      "         [-0.0563,  0.6898, -0.6026,  ...,  0.3988,  0.2118,  0.2106],\n",
      "         [-0.4219,  0.6607, -0.5283,  ...,  0.5592,  0.5118, -0.1879]],\n",
      "\n",
      "        [[-0.6320,  0.5735, -0.0501,  ..., -0.0667,  0.1901,  0.1760],\n",
      "         [-0.5211,  0.6321, -0.3395,  ..., -0.0496,  0.2640,  0.4206],\n",
      "         [-0.4823,  0.6140, -0.3806,  ...,  0.1818,  0.2234,  0.2917],\n",
      "         ...,\n",
      "         [-0.3913,  0.6242, -0.4944,  ...,  0.0550, -0.0051,  0.4178],\n",
      "         [-0.4193,  0.2103, -0.5448,  ...,  0.1059,  0.1992,  0.5435],\n",
      "         [-0.5752,  0.4876, -0.2875,  ..., -0.0929,  0.2164,  0.5933]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0037,  0.2340, -0.1338,  ..., -0.7041, -0.0992,  0.6810],\n",
      "         [ 0.1542,  0.1680, -0.0138,  ..., -0.7322, -0.2297,  0.4799],\n",
      "         [ 0.1306,  0.3218, -0.2579,  ..., -0.9460, -0.0419,  0.4700],\n",
      "         ...,\n",
      "         [-0.1997,  0.3448, -0.2694,  ..., -0.6124, -0.4531,  0.7839],\n",
      "         [ 0.2474,  0.0827, -0.3576,  ..., -0.4221, -0.2682,  0.8679],\n",
      "         [ 0.3473,  0.0811, -0.1690,  ..., -0.8110, -0.1660,  0.8013]],\n",
      "\n",
      "        [[-0.5170,  0.3658, -0.2053,  ...,  0.0498,  0.2771,  0.1578],\n",
      "         [-0.1190,  0.5286, -0.1763,  ...,  0.2083,  0.2805,  0.1437],\n",
      "         [-0.2157,  0.5790, -0.2588,  ...,  0.2425,  0.2351, -0.1272],\n",
      "         ...,\n",
      "         [-0.5592,  0.4265, -0.6140,  ...,  0.4199,  0.2427,  0.3021],\n",
      "         [-0.3962,  0.4980, -0.3606,  ...,  0.1690,  0.3143,  0.2999],\n",
      "         [-0.4025,  0.5157, -0.3752,  ...,  0.2561,  0.5684,  0.1905]],\n",
      "\n",
      "        [[-0.0192,  0.5141, -0.7529,  ..., -0.0331,  0.4329,  0.2101],\n",
      "         [-0.4362,  0.6430, -1.0416,  ...,  0.2994,  0.4256,  0.0363],\n",
      "         [-0.1634,  0.7156, -0.3422,  ...,  0.2082,  0.1999, -0.1143],\n",
      "         ...,\n",
      "         [-0.4416,  0.6534, -1.0365,  ...,  0.7648, -0.0362,  0.4595],\n",
      "         [-0.1941,  0.7232, -1.2807,  ...,  0.4300,  0.3854,  0.2555],\n",
      "         [-0.4711,  0.8758, -0.8720,  ...,  0.2753,  0.2860,  0.3206]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0491459369659424\n",
      "Model outputs:  tensor([[[ 0.2018,  0.3980, -0.5834,  ..., -0.4159,  0.0533, -0.2054],\n",
      "         [ 0.0563,  0.6720, -0.3858,  ..., -0.1625,  0.1322,  0.0861],\n",
      "         [ 0.3687,  0.4993, -0.3453,  ..., -0.4875,  0.3223,  0.0662],\n",
      "         ...,\n",
      "         [ 0.3552,  0.5074, -0.3430,  ..., -0.6322, -0.1303,  0.1207],\n",
      "         [ 0.1725,  0.7767, -0.4680,  ..., -0.4603,  0.2469, -0.0671],\n",
      "         [ 0.1307,  0.6536, -0.2250,  ..., -0.5800,  0.3611, -0.0666]],\n",
      "\n",
      "        [[ 0.0871,  0.6211, -0.7171,  ..., -0.0047,  0.2811, -0.2004],\n",
      "         [-0.0907,  0.8845, -0.5494,  ...,  0.2615,  0.3054, -0.0747],\n",
      "         [-0.2161,  0.6995, -0.4719,  ...,  0.1673,  0.3522, -0.2393],\n",
      "         ...,\n",
      "         [-0.0199,  0.6462, -0.6848,  ...,  0.2108,  0.3846, -0.2102],\n",
      "         [-0.1353,  0.7811, -0.6597,  ..., -0.2683,  0.2209, -0.2414],\n",
      "         [ 0.0636,  0.9494, -0.5236,  ...,  0.0954,  0.2735, -0.2481]],\n",
      "\n",
      "        [[ 0.0141,  0.9355, -0.4742,  ..., -0.0291,  0.3253, -0.2471],\n",
      "         [-0.1814,  0.8387, -0.6412,  ...,  0.4236,  0.4594, -0.2305],\n",
      "         [-0.0564,  0.5866, -0.4555,  ...,  0.2379,  0.3891, -0.2228],\n",
      "         ...,\n",
      "         [-0.0310,  0.8866, -0.6131,  ..., -0.0086,  0.1094, -0.3711],\n",
      "         [-0.0678,  0.7417, -0.4877,  ...,  0.1903,  0.2616, -0.1472],\n",
      "         [-0.3314,  0.9816, -0.5146,  ..., -0.0552,  0.2745, -0.4427]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5974,  0.2735, -0.2015,  ..., -0.9649,  0.6410,  0.2013],\n",
      "         [ 0.5235,  0.2328, -0.0114,  ..., -1.0057,  0.3754,  0.0334],\n",
      "         [ 0.6466,  0.2522, -0.3160,  ..., -0.7553,  0.5444,  0.2588],\n",
      "         ...,\n",
      "         [ 0.1352,  0.1041, -0.1206,  ..., -0.9007,  0.3385, -0.2173],\n",
      "         [ 0.3596, -0.1302, -0.0641,  ..., -0.8448,  0.3353,  0.0042],\n",
      "         [ 0.5672,  0.2485, -0.0385,  ..., -0.9578,  0.3192, -0.1187]],\n",
      "\n",
      "        [[ 0.4094,  0.2643, -0.3055,  ..., -0.8382, -0.2348,  0.2132],\n",
      "         [ 0.3564,  0.4514, -0.1704,  ..., -0.5581, -0.0861,  0.2473],\n",
      "         [ 0.1724,  0.2791, -0.2565,  ..., -0.5498,  0.2239,  0.2925],\n",
      "         ...,\n",
      "         [ 0.3094,  0.1089, -0.0596,  ..., -0.7264,  0.0897, -0.0679],\n",
      "         [ 0.3374,  0.2305, -0.1111,  ..., -0.6372,  0.1275, -0.0566],\n",
      "         [ 0.3194,  0.5177, -0.3599,  ..., -0.4704, -0.0794,  0.2742]],\n",
      "\n",
      "        [[ 0.7205,  0.1686, -0.1450,  ..., -0.6090, -0.1504,  0.3831],\n",
      "         [ 0.3421,  0.4731, -0.2353,  ..., -0.5599,  0.0016,  0.2889],\n",
      "         [ 0.2927,  0.4011, -0.1477,  ..., -0.5720,  0.0859,  0.0083],\n",
      "         ...,\n",
      "         [ 0.0672,  0.2485, -0.0615,  ..., -0.7023,  0.0249, -0.1260],\n",
      "         [ 0.3723,  0.1427, -0.0283,  ..., -0.9813, -0.1196,  0.1559],\n",
      "         [ 0.3887,  0.3574, -0.1889,  ..., -0.9158,  0.3098,  0.1182]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3224687576293945\n",
      "Model outputs:  tensor([[[-0.2997,  0.3562, -0.9901,  ...,  0.3359,  0.1709,  0.2791],\n",
      "         [-0.0498,  0.3661, -0.6259,  ...,  0.3610,  0.0112, -0.0263],\n",
      "         [-0.2387,  0.4859, -0.8730,  ...,  0.7049,  0.4726, -0.0073],\n",
      "         ...,\n",
      "         [-0.3073,  0.4372, -0.7243,  ...,  0.5261,  0.4346,  0.2109],\n",
      "         [-0.1586,  0.6492, -0.6155,  ...,  0.3731,  0.2212, -0.2287],\n",
      "         [-0.3238,  0.7174, -0.5866,  ...,  0.2232,  0.4393, -0.0701]],\n",
      "\n",
      "        [[-0.3011,  0.6196, -0.9621,  ...,  0.6381,  0.3638, -0.0079],\n",
      "         [ 0.1051,  0.5472, -0.5598,  ...,  0.4638, -0.0098, -0.1633],\n",
      "         [ 0.0353,  0.5345, -0.8219,  ...,  0.6332,  0.2450,  0.2757],\n",
      "         ...,\n",
      "         [-0.1058,  0.2997, -0.8132,  ...,  0.6392,  0.4551,  0.1445],\n",
      "         [-0.4228,  0.5925, -0.9196,  ...,  0.4670,  0.3015, -0.1554],\n",
      "         [-0.2564,  0.6908, -0.7173,  ...,  0.3800,  0.3033, -0.1398]],\n",
      "\n",
      "        [[ 0.2241,  0.2010, -0.3598,  ..., -0.7746,  0.3257,  0.0651],\n",
      "         [ 0.4090,  0.1215, -0.2972,  ..., -0.7894,  0.1719,  0.1672],\n",
      "         [ 0.0211,  0.1275, -0.3332,  ..., -0.6281,  0.2908,  0.2392],\n",
      "         ...,\n",
      "         [-0.0774,  0.2094, -0.5180,  ..., -0.1197,  0.2228,  0.4058],\n",
      "         [-0.0315,  0.5016, -0.7719,  ..., -0.3454,  0.2889,  0.4406],\n",
      "         [-0.0070,  0.4616, -0.4186,  ..., -0.2689,  0.3413,  0.3299]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1814,  0.1563, -0.6724,  ..., -0.5347, -0.2436,  0.7174],\n",
      "         [ 0.0281,  0.3319, -0.2273,  ..., -0.2054, -0.1469,  0.5194],\n",
      "         [ 0.0768,  0.3192, -0.3512,  ..., -0.3461,  0.1769,  0.6769],\n",
      "         ...,\n",
      "         [-0.1395,  0.4091, -0.5612,  ..., -0.0764, -0.0836,  0.6374],\n",
      "         [ 0.1411,  0.3290, -0.5871,  ..., -0.5629, -0.0148,  0.5598],\n",
      "         [-0.0074,  0.3443, -0.4286,  ..., -0.5677, -0.0023,  0.4131]],\n",
      "\n",
      "        [[-0.1025,  0.6486, -0.4320,  ...,  0.9478,  0.5027,  0.0382],\n",
      "         [-0.0688,  0.3011, -0.0308,  ...,  0.4740, -0.1034, -0.1222],\n",
      "         [-0.0347,  0.3534, -0.4367,  ...,  0.7407,  0.3093,  0.2523],\n",
      "         ...,\n",
      "         [-0.2314,  0.3915, -0.2500,  ...,  0.9050,  0.4635,  0.1413],\n",
      "         [-0.4535,  0.5814, -0.1086,  ...,  0.4811,  0.1588, -0.2736],\n",
      "         [-0.3233,  0.7073,  0.0958,  ...,  0.5553,  0.5333, -0.0738]],\n",
      "\n",
      "        [[-0.2859,  0.3822, -0.3805,  ...,  0.6167,  0.0541,  0.2616],\n",
      "         [-0.4288,  0.3389, -0.3916,  ...,  0.4196,  0.4830, -0.2556],\n",
      "         [-0.4674,  0.6811, -0.2991,  ...,  0.6570,  0.6150,  0.0764],\n",
      "         ...,\n",
      "         [-0.2733,  0.3863, -0.2114,  ...,  0.6958,  0.6341, -0.0791],\n",
      "         [-0.4172,  0.4397, -0.1212,  ...,  0.5465,  0.1948, -0.1222],\n",
      "         [-0.4048,  0.5434, -0.3469,  ...,  0.2989,  0.5785, -0.0200]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.422472357749939\n",
      "Model outputs:  tensor([[[ 0.1188,  0.4128, -0.7273,  ...,  0.2075,  0.0645,  0.4625],\n",
      "         [-0.0014, -0.0066, -0.6731,  ..., -0.1241, -0.2916,  0.6201],\n",
      "         [ 0.1532,  0.0638, -0.7244,  ...,  0.2549, -0.3555,  0.5639],\n",
      "         ...,\n",
      "         [-0.2775,  0.2850, -0.6037,  ...,  0.2403, -0.2660,  0.2975],\n",
      "         [-0.1785,  0.0784, -0.9232,  ...,  0.2074, -0.0808,  0.6269],\n",
      "         [ 0.1664,  0.0159, -0.4363,  ...,  0.1905,  0.0074,  0.5265]],\n",
      "\n",
      "        [[-0.1369,  0.1277, -0.4413,  ..., -0.3631,  0.3294,  0.9011],\n",
      "         [ 0.2006,  0.1581, -0.3517,  ..., -0.5117,  0.7274,  0.7873],\n",
      "         [-0.0731,  0.1012, -0.2791,  ..., -0.2563,  0.1237,  0.7158],\n",
      "         ...,\n",
      "         [-0.0657, -0.1071, -0.3485,  ..., -0.3613,  0.1891,  0.4880],\n",
      "         [-0.1640,  0.0216, -0.7607,  ..., -0.3916,  0.6119,  0.8318],\n",
      "         [ 0.1319,  0.0721, -0.1507,  ..., -0.5468,  0.6541,  0.9740]],\n",
      "\n",
      "        [[-0.4296,  0.4834, -0.9018,  ...,  0.7230, -0.1728,  0.3294],\n",
      "         [ 0.0837,  0.3888, -0.9619,  ...,  0.6062,  0.1336,  0.3218],\n",
      "         [-0.0924,  0.0873, -0.7455,  ...,  0.7416, -0.3586,  0.2892],\n",
      "         ...,\n",
      "         [-0.1725,  0.3837, -0.9118,  ...,  0.7953, -0.2355,  0.3298],\n",
      "         [-0.0107,  0.2030, -0.7887,  ...,  0.5950,  0.2800,  0.4535],\n",
      "         [-0.0279,  0.1947, -0.6154,  ...,  0.4953,  0.1723,  0.4090]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4346,  0.3694, -0.3004,  ...,  0.9314,  0.4727, -0.0641],\n",
      "         [-0.2076,  0.4428, -0.3363,  ...,  0.9134,  0.5474,  0.0889],\n",
      "         [-0.1404, -0.0155, -0.1759,  ...,  0.8329, -0.1375,  0.0599],\n",
      "         ...,\n",
      "         [-0.4120,  0.2053, -0.1314,  ...,  0.5652,  0.0297,  0.0841],\n",
      "         [-0.2255,  0.2686, -0.5994,  ...,  1.1909,  0.7020, -0.0400],\n",
      "         [-0.4157,  0.0836, -0.1852,  ...,  0.8493,  0.3397,  0.0221]],\n",
      "\n",
      "        [[-0.6142,  0.6029, -0.7818,  ...,  0.9270,  0.2446,  0.2483],\n",
      "         [-0.4753,  0.4962, -0.7935,  ...,  0.7144,  0.0658,  0.3906],\n",
      "         [-0.0778,  0.1763, -0.7723,  ...,  0.7847, -0.2061,  0.3421],\n",
      "         ...,\n",
      "         [-0.3464,  0.3257, -0.5657,  ...,  0.5235, -0.2189,  0.1062],\n",
      "         [-0.3052,  0.3357, -0.8447,  ...,  0.6963,  0.1851,  0.7856],\n",
      "         [-0.1959,  0.4158, -0.8210,  ...,  0.8688,  0.3206,  0.2055]],\n",
      "\n",
      "        [[ 0.1759,  0.2925, -0.5699,  ..., -0.0891, -0.2573,  0.7119],\n",
      "         [ 0.0747,  0.3601, -0.7839,  ..., -0.1873, -0.1898,  0.5016],\n",
      "         [-0.0370,  0.0242, -0.4501,  ..., -0.0902, -0.4154,  0.8279],\n",
      "         ...,\n",
      "         [ 0.1324,  0.1310, -0.7650,  ..., -0.0064, -0.3222,  0.7235],\n",
      "         [ 0.0397,  0.1600, -0.6630,  ..., -0.1001, -0.2092,  0.8110],\n",
      "         [-0.1177,  0.2321, -0.4894,  ..., -0.2217, -0.1188,  0.7534]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1508506536483765\n",
      "Model outputs:  tensor([[[ 1.1244e-01,  2.9986e-01, -7.6607e-01,  ...,  1.6829e-01,\n",
      "           1.9852e-01,  2.6731e-01],\n",
      "         [-1.7466e-01,  3.8444e-03, -1.0694e+00,  ...,  5.2017e-01,\n",
      "          -2.1164e-01,  5.0011e-01],\n",
      "         [-9.6288e-02,  4.1078e-01, -7.5237e-01,  ...,  1.1397e-01,\n",
      "           1.2910e-01, -5.0636e-02],\n",
      "         ...,\n",
      "         [-3.2339e-01,  3.8087e-01, -6.9130e-01,  ...,  2.4273e-01,\n",
      "           6.4842e-02,  3.5274e-03],\n",
      "         [ 1.6737e-02,  2.0869e-01, -7.9105e-01,  ...,  1.6938e-01,\n",
      "          -1.0066e-01,  2.8456e-01],\n",
      "         [-6.2274e-02, -9.2615e-03, -9.9039e-01,  ...,  5.2955e-01,\n",
      "           1.6271e-01,  1.2447e-01]],\n",
      "\n",
      "        [[ 4.1685e-01,  9.6865e-02, -4.0521e-01,  ..., -3.4692e-01,\n",
      "           1.4175e-01,  4.1579e-01],\n",
      "         [ 1.9820e-01, -2.4400e-01, -4.7822e-01,  ..., -6.9797e-02,\n",
      "           1.3278e-01,  5.7641e-01],\n",
      "         [ 6.0860e-01,  1.9969e-01, -6.7508e-01,  ..., -6.9096e-01,\n",
      "          -1.5592e-01,  7.9667e-01],\n",
      "         ...,\n",
      "         [ 5.6561e-01,  1.3688e-01, -4.0499e-01,  ..., -3.9346e-01,\n",
      "          -1.7239e-01,  5.4036e-01],\n",
      "         [ 3.6802e-01,  9.4416e-02, -6.3670e-01,  ..., -2.2857e-01,\n",
      "           1.5027e-01,  7.6560e-01],\n",
      "         [ 2.2273e-01,  2.2023e-01, -2.6202e-01,  ..., -1.0765e-01,\n",
      "          -2.3970e-01,  5.5252e-01]],\n",
      "\n",
      "        [[ 6.0859e-01,  3.6500e-02, -4.0082e-01,  ..., -4.8855e-01,\n",
      "           8.3809e-02,  5.7532e-01],\n",
      "         [ 4.5619e-01,  1.5020e-02, -5.6564e-01,  ..., -1.2585e-01,\n",
      "          -2.9343e-01,  7.8602e-01],\n",
      "         [ 4.1455e-01,  1.9845e-01, -4.9466e-01,  ..., -6.2702e-01,\n",
      "          -1.3720e-01,  6.2944e-01],\n",
      "         ...,\n",
      "         [ 2.6812e-01,  3.1945e-01, -4.0241e-01,  ..., -5.5296e-01,\n",
      "          -5.3624e-02,  4.5254e-01],\n",
      "         [ 3.0267e-01,  2.8748e-01, -4.9286e-01,  ..., -3.6305e-01,\n",
      "           3.7880e-02,  7.2191e-01],\n",
      "         [ 1.4663e-01,  1.6573e-01, -6.2244e-01,  ..., -3.2947e-02,\n",
      "           1.6857e-02,  7.5352e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.1706e-01,  3.9296e-01, -5.8842e-01,  ...,  4.5351e-01,\n",
      "           2.5487e-01,  2.2056e-02],\n",
      "         [-1.5118e-01,  1.6784e-01, -5.0063e-01,  ...,  2.5307e-01,\n",
      "           3.7810e-01,  4.6701e-01],\n",
      "         [-2.5426e-01,  6.6540e-01, -1.0385e+00,  ...,  2.4454e-01,\n",
      "           4.7116e-02,  1.6599e-01],\n",
      "         ...,\n",
      "         [-6.1731e-01,  3.6762e-01, -1.9440e-01,  ...,  1.9282e-01,\n",
      "           3.0172e-01, -8.3465e-02],\n",
      "         [-3.5894e-01,  2.7835e-01, -4.0929e-01,  ...,  3.2797e-01,\n",
      "           2.5487e-01,  1.4160e-01],\n",
      "         [-3.6613e-01,  4.1855e-01, -9.7685e-01,  ...,  6.8893e-01,\n",
      "           1.8047e-01,  3.3048e-01]],\n",
      "\n",
      "        [[-2.8704e-01,  5.0589e-01, -1.7832e-01,  ...,  6.3495e-02,\n",
      "          -7.1224e-02, -1.1269e-02],\n",
      "         [-2.2859e-01,  2.3622e-01, -6.1705e-01,  ...,  1.4075e-01,\n",
      "           5.4899e-01,  2.6540e-01],\n",
      "         [-5.7108e-01,  4.2997e-01, -5.2992e-01,  ...,  1.1176e-01,\n",
      "          -3.1831e-02,  2.0759e-01],\n",
      "         ...,\n",
      "         [-5.0088e-01,  3.7602e-01, -3.5106e-01,  ..., -1.2864e-01,\n",
      "           2.5371e-01,  6.6140e-02],\n",
      "         [-3.3536e-01, -3.6236e-02, -5.3795e-01,  ..., -6.2976e-04,\n",
      "           2.7201e-01,  3.3221e-01],\n",
      "         [-4.2519e-01,  1.3974e-01, -6.7090e-01,  ...,  4.1555e-01,\n",
      "           8.4275e-02,  3.3932e-01]],\n",
      "\n",
      "        [[-2.3439e-01,  5.0342e-01, -8.1436e-01,  ...,  4.3413e-01,\n",
      "           2.1163e-01, -2.4493e-01],\n",
      "         [-1.5712e-01,  4.0959e-01, -1.0523e+00,  ...,  6.3851e-01,\n",
      "           3.3262e-01,  1.2175e-01],\n",
      "         [-5.8587e-01,  6.1510e-01, -7.5042e-01,  ...,  4.7393e-01,\n",
      "           1.5755e-02,  3.5336e-02],\n",
      "         ...,\n",
      "         [-4.1112e-01,  6.5968e-01, -4.2939e-01,  ...,  2.0791e-01,\n",
      "           5.0005e-01, -1.4535e-01],\n",
      "         [-1.8331e-01,  3.0233e-01, -8.5611e-01,  ...,  4.3909e-01,\n",
      "           3.4793e-01,  2.9646e-01],\n",
      "         [-1.1009e-01,  1.5062e-01, -7.7855e-01,  ...,  8.3759e-01,\n",
      "           2.5097e-01,  2.2987e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.9659029245376587\n",
      "Model outputs:  tensor([[[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 4.8473e-01,  3.4741e-01, -7.8277e-01,  ...,  4.2114e-01,\n",
      "           2.2213e-02,  2.5107e-01],\n",
      "         [ 6.8658e-02,  1.8841e-01, -7.6884e-01,  ...,  6.8422e-01,\n",
      "           1.7050e-01,  4.9457e-01],\n",
      "         ...,\n",
      "         [-1.3469e-01,  4.9786e-01, -7.2512e-01,  ...,  5.0712e-01,\n",
      "           1.9513e-02,  6.2893e-01],\n",
      "         [-8.7274e-03,  3.8908e-01, -1.0446e+00,  ...,  6.2182e-01,\n",
      "           1.4493e-04,  2.2236e-01],\n",
      "         [ 4.6503e-02,  5.9982e-01, -1.0063e+00,  ...,  4.9280e-01,\n",
      "           1.4629e-01,  4.4890e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [-3.4950e-02,  1.5199e-01, -7.9386e-01,  ...,  5.4756e-01,\n",
      "           3.0777e-01,  9.3935e-02],\n",
      "         [ 4.0109e-02,  4.5275e-01, -7.8792e-01,  ...,  7.7894e-01,\n",
      "           3.7533e-01,  3.1883e-01],\n",
      "         ...,\n",
      "         [-1.4214e-01,  4.9762e-01, -6.6056e-01,  ...,  7.0039e-01,\n",
      "           5.1364e-01,  4.0239e-01],\n",
      "         [-2.4492e-01,  6.6831e-01, -8.1142e-01,  ...,  5.6823e-01,\n",
      "           3.7900e-01,  2.1966e-01],\n",
      "         [-3.1325e-01,  6.4358e-01, -8.2166e-01,  ...,  6.5453e-01,\n",
      "           4.9486e-01,  1.4248e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 5.9654e-01,  1.3563e-01, -4.8622e-01,  ..., -9.3528e-02,\n",
      "          -1.2828e-01,  6.9641e-01],\n",
      "         [ 1.7433e-01,  1.3935e-01, -5.3391e-01,  ...,  1.9207e-01,\n",
      "          -6.0631e-02,  4.2948e-01],\n",
      "         ...,\n",
      "         [ 2.7320e-01, -1.4415e-02, -4.1250e-01,  ..., -1.5866e-01,\n",
      "          -6.6752e-02,  6.3603e-01],\n",
      "         [ 2.9471e-01,  2.8545e-01, -4.3140e-01,  ..., -3.1168e-01,\n",
      "           2.3017e-01,  3.6717e-01],\n",
      "         [ 2.5266e-01,  3.6925e-01, -8.5030e-01,  ..., -1.1415e-01,\n",
      "          -3.1184e-02,  7.4608e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 3.4372e-01,  4.8013e-01, -1.8345e-01,  ..., -3.7335e-02,\n",
      "          -2.9507e-01,  5.4992e-01],\n",
      "         [ 1.7015e-01,  3.2084e-01, -5.3051e-01,  ...,  2.2265e-01,\n",
      "          -2.4618e-01,  5.4653e-01],\n",
      "         ...,\n",
      "         [ 2.0832e-01,  1.6017e-01, -6.6873e-01,  ..., -1.2912e-02,\n",
      "           5.5805e-02,  7.3389e-01],\n",
      "         [ 1.4781e-01,  2.1692e-01, -5.2772e-01,  ..., -2.1134e-01,\n",
      "           1.9169e-02,  5.0993e-01],\n",
      "         [ 6.2990e-02,  2.3746e-01, -7.1551e-01,  ...,  1.3711e-01,\n",
      "           2.8162e-01,  4.6027e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 4.3080e-02,  4.6523e-01, -7.8096e-01,  ...,  2.0214e-01,\n",
      "          -7.3622e-02,  3.2580e-01],\n",
      "         [-1.1080e-01,  1.7211e-01, -6.6497e-01,  ...,  3.1106e-01,\n",
      "          -6.3605e-03,  2.7753e-01],\n",
      "         ...,\n",
      "         [-2.5965e-01,  2.6988e-01, -9.4719e-01,  ...,  5.5662e-01,\n",
      "           2.1647e-01,  5.6429e-01],\n",
      "         [-1.4361e-01,  6.1145e-01, -9.7285e-01,  ...,  2.5647e-01,\n",
      "           8.8901e-02,  2.9413e-02],\n",
      "         [-2.4240e-02,  4.4370e-01, -1.0010e+00,  ...,  1.9885e-01,\n",
      "           1.0221e-01,  2.0044e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 3.3920e-01,  6.4867e-01, -7.4294e-01,  ...,  4.2413e-01,\n",
      "          -7.6286e-03,  7.8752e-02],\n",
      "         [ 1.6937e-02,  3.6909e-01, -9.5447e-01,  ...,  4.4214e-01,\n",
      "           3.3271e-01,  3.0800e-01],\n",
      "         ...,\n",
      "         [ 9.5839e-02,  5.1702e-01, -9.0899e-01,  ...,  1.6513e-01,\n",
      "           3.5277e-02,  5.8571e-01],\n",
      "         [ 2.1070e-01,  6.5197e-01, -8.5056e-01,  ...,  2.8099e-01,\n",
      "           1.1011e-01,  2.7635e-01],\n",
      "         [-3.0304e-02,  3.6576e-01, -6.8395e-01,  ...,  1.8283e-01,\n",
      "           1.9408e-01,  1.2960e-01]]], grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [1/10], Loss: 1.9659\n",
      "Model outputs:  tensor([[[-0.0974,  0.3669, -0.5922,  ...,  0.2686,  0.2901, -0.0082],\n",
      "         [-0.5202,  0.3580, -0.7054,  ...,  0.7931,  0.4753,  0.3043],\n",
      "         [-0.5399,  0.6759, -0.4036,  ...,  0.3987,  0.4270, -0.0937],\n",
      "         ...,\n",
      "         [-0.5121,  0.6715, -0.5066,  ...,  0.5851,  0.3641,  0.1767],\n",
      "         [-0.5248,  0.3262, -0.5148,  ...,  0.8992,  0.7018,  0.1915],\n",
      "         [-0.6853,  0.7033, -0.6395,  ...,  0.6861,  0.4417,  0.3551]],\n",
      "\n",
      "        [[-0.2717,  0.0807,  0.0974,  ...,  0.2297,  0.1012, -0.2170],\n",
      "         [-0.2303,  0.2863,  0.1098,  ...,  0.6572,  0.5366,  0.1346],\n",
      "         [-0.5314,  0.8313, -0.2113,  ...,  0.5505,  0.6057,  0.0779],\n",
      "         ...,\n",
      "         [-0.4146,  0.6565, -0.3418,  ...,  0.6469,  0.3997,  0.0576],\n",
      "         [-0.5435,  0.5644, -0.0189,  ...,  0.8872,  0.4703, -0.0852],\n",
      "         [-0.2830,  0.4544, -0.1994,  ...,  0.5176,  0.2056,  0.0460]],\n",
      "\n",
      "        [[-0.2810,  0.3932, -0.6608,  ..., -0.0306,  0.3487,  0.0355],\n",
      "         [-0.6212,  0.1670, -0.5471,  ...,  0.6224,  0.2771,  0.4559],\n",
      "         [-0.6598,  0.8046, -0.2939,  ...,  0.1689,  0.6037,  0.3996],\n",
      "         ...,\n",
      "         [-0.4539,  0.6839, -0.3050,  ...,  0.2062,  0.4779,  0.4722],\n",
      "         [-0.4930,  0.6432, -0.3691,  ...,  0.4004,  0.4057,  0.5258],\n",
      "         [-0.4970,  0.4297, -0.3020,  ...,  0.1814,  0.1003,  0.5135]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3470, -0.1449, -0.1882,  ..., -0.6260, -0.0057,  0.6274],\n",
      "         [ 0.0954, -0.1275, -0.4287,  ..., -0.2641,  0.0051,  0.8393],\n",
      "         [-0.0526,  0.0700, -0.2701,  ..., -0.3984, -0.2878,  0.5660],\n",
      "         ...,\n",
      "         [ 0.0156, -0.0399, -0.4146,  ..., -0.6949, -0.3541,  0.5407],\n",
      "         [ 0.1673,  0.4073, -0.3435,  ...,  0.0464, -0.0737,  0.7144],\n",
      "         [ 0.1397,  0.2813, -0.2801,  ..., -0.3959, -0.1533,  0.7748]],\n",
      "\n",
      "        [[-0.0283,  0.3675, -0.5522,  ..., -0.4454, -0.0423,  0.0271],\n",
      "         [-0.2466,  0.4016, -0.8698,  ...,  0.2812,  0.0081,  0.7524],\n",
      "         [-0.4972,  0.6585, -0.4809,  ..., -0.3195,  0.1298,  0.3705],\n",
      "         ...,\n",
      "         [-0.2844,  0.4746, -0.7326,  ...,  0.2059,  0.4227,  0.2713],\n",
      "         [-0.1496,  0.3223, -0.4412,  ...,  0.1106,  0.5091,  0.4921],\n",
      "         [-0.2819,  0.6367, -0.5841,  ..., -0.1285,  0.0779,  0.2663]],\n",
      "\n",
      "        [[-0.5272,  0.3487, -0.1334,  ...,  0.2129,  0.1576, -0.0418],\n",
      "         [-0.4196,  0.0941, -0.2251,  ...,  0.5635,  0.2313,  0.2444],\n",
      "         [-0.6960,  0.7521,  0.0123,  ...,  0.4269,  0.4179, -0.0422],\n",
      "         ...,\n",
      "         [-0.1473,  0.5855, -0.1011,  ...,  0.4822,  0.4230, -0.0118],\n",
      "         [-0.1906,  0.6906, -0.1591,  ...,  0.7898,  0.7052,  0.0839],\n",
      "         [-0.5216,  0.6475, -0.1338,  ...,  0.7406,  0.3220,  0.2242]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8907816410064697\n",
      "Model outputs:  tensor([[[-3.7713e-01,  4.3667e-01, -1.1249e+00,  ...,  7.0876e-02,\n",
      "           1.3823e-01,  3.2551e-01],\n",
      "         [-5.0693e-01,  4.4001e-01, -1.0534e+00,  ...,  3.2375e-01,\n",
      "           2.1602e-01,  3.5003e-01],\n",
      "         [-1.1461e-02,  5.6888e-01, -9.5320e-01,  ...,  2.5476e-02,\n",
      "           3.2062e-01,  2.0030e-01],\n",
      "         ...,\n",
      "         [-2.9893e-01,  1.8656e-01, -6.9843e-01,  ..., -1.6778e-01,\n",
      "           2.5443e-01,  2.6501e-01],\n",
      "         [-3.0648e-01,  6.4854e-01, -8.8621e-01,  ..., -2.6154e-01,\n",
      "           3.1626e-01,  9.8656e-02],\n",
      "         [-2.4201e-01,  5.8678e-01, -5.7767e-01,  ..., -2.1140e-01,\n",
      "           2.2178e-01,  1.3398e-01]],\n",
      "\n",
      "        [[ 5.0427e-02,  3.4688e-01, -5.2568e-01,  ..., -4.0344e-01,\n",
      "          -3.1922e-01,  3.7126e-01],\n",
      "         [-1.4038e-01,  3.1676e-02, -4.6447e-01,  ..., -1.1845e-01,\n",
      "           3.2042e-02,  4.9009e-01],\n",
      "         [ 1.8112e-01,  2.8927e-01, -4.0193e-01,  ..., -5.3360e-01,\n",
      "           2.2285e-01,  4.4275e-01],\n",
      "         ...,\n",
      "         [-2.0957e-01,  5.0089e-02, -4.8999e-01,  ..., -2.7539e-01,\n",
      "          -1.2493e-01,  7.4595e-01],\n",
      "         [ 1.0952e-01,  1.4683e-01, -4.3039e-01,  ..., -6.8487e-01,\n",
      "          -1.5722e-03,  3.4428e-01],\n",
      "         [ 1.9528e-01,  4.8971e-01, -4.5410e-01,  ..., -5.6542e-01,\n",
      "           1.2641e-01,  4.1886e-01]],\n",
      "\n",
      "        [[-1.0547e-01,  7.6658e-01, -8.9258e-01,  ...,  4.8567e-01,\n",
      "           9.0592e-02, -5.5481e-02],\n",
      "         [-2.1252e-01,  3.8936e-01, -7.8816e-01,  ...,  8.8307e-01,\n",
      "           4.8815e-01,  1.0642e-01],\n",
      "         [-2.4184e-01,  7.6509e-01, -9.4381e-01,  ...,  5.4380e-01,\n",
      "           2.5689e-01, -1.2494e-01],\n",
      "         ...,\n",
      "         [-3.0010e-01,  3.2716e-01, -1.0004e+00,  ...,  4.4898e-01,\n",
      "           1.2817e-01,  3.7394e-01],\n",
      "         [-6.4439e-03,  8.3821e-01, -7.8003e-01,  ...,  5.9306e-01,\n",
      "           3.1585e-01,  1.0321e-02],\n",
      "         [-2.3086e-01,  1.1366e+00, -6.0703e-01,  ...,  4.5299e-01,\n",
      "           4.1614e-01, -4.0594e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.0344e-02,  3.4276e-01, -6.0689e-01,  ..., -2.2095e-01,\n",
      "           9.1492e-02,  4.8223e-01],\n",
      "         [-4.2688e-02,  1.4792e-01, -6.5689e-01,  ..., -1.9389e-01,\n",
      "          -1.6681e-01,  5.7612e-01],\n",
      "         [ 1.9195e-01,  4.6139e-01, -5.9134e-01,  ..., -4.4555e-01,\n",
      "           1.0085e-02,  3.2381e-01],\n",
      "         ...,\n",
      "         [-1.9844e-01, -1.6617e-02, -5.4310e-01,  ..., -3.6552e-01,\n",
      "          -1.3635e-01,  4.7653e-01],\n",
      "         [ 1.0217e-01,  3.4950e-01, -4.1415e-01,  ..., -6.7273e-01,\n",
      "           7.1575e-02,  3.3367e-01],\n",
      "         [-1.9561e-01,  4.8159e-01, -4.8506e-01,  ..., -5.3754e-01,\n",
      "           1.6198e-01,  2.1662e-01]],\n",
      "\n",
      "        [[-1.3764e-01,  6.1263e-01, -9.8359e-01,  ...,  4.6313e-01,\n",
      "           1.0573e-01, -1.3847e-01],\n",
      "         [-3.5539e-01,  4.5150e-01, -1.0842e+00,  ...,  7.9943e-01,\n",
      "          -9.3642e-04, -6.0520e-02],\n",
      "         [-1.7450e-01,  8.1322e-01, -9.3713e-01,  ...,  5.4304e-01,\n",
      "           5.3282e-01, -2.0408e-02],\n",
      "         ...,\n",
      "         [-4.5857e-01,  4.3334e-01, -9.1997e-01,  ...,  5.2660e-01,\n",
      "          -2.2323e-02,  5.5067e-02],\n",
      "         [-1.7773e-01,  7.7389e-01, -7.6952e-01,  ...,  4.5548e-01,\n",
      "           1.9466e-01, -4.9098e-02],\n",
      "         [-1.9170e-01,  1.0401e+00, -5.1080e-01,  ...,  3.9131e-01,\n",
      "           3.0377e-01, -1.9915e-01]],\n",
      "\n",
      "        [[-1.9405e-01,  6.1019e-01, -1.1539e+00,  ..., -5.3348e-02,\n",
      "           4.4391e-02,  2.0033e-01],\n",
      "         [ 4.6256e-02,  2.2246e-01, -6.6621e-01,  ...,  2.3901e-01,\n",
      "           6.2845e-02,  2.8367e-01],\n",
      "         [-2.7139e-01,  8.8929e-01, -9.9339e-01,  ...,  6.6021e-02,\n",
      "           2.0115e-01, -1.1796e-01],\n",
      "         ...,\n",
      "         [-2.7668e-01,  1.5695e-01, -6.8617e-01,  ...,  1.0025e-01,\n",
      "           9.2685e-02,  3.1272e-01],\n",
      "         [-2.2189e-01,  6.4229e-01, -8.3016e-01,  ...,  3.1154e-02,\n",
      "           2.6500e-01,  1.8415e-01],\n",
      "         [-1.3571e-01,  7.9361e-01, -7.2852e-01,  ..., -3.0146e-01,\n",
      "           3.9492e-01,  1.7878e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.4642285108566284\n",
      "Model outputs:  tensor([[[ 0.0236,  0.4841, -0.3586,  ...,  0.0406,  0.4417,  0.0142],\n",
      "         [-0.2781,  0.4866, -0.1938,  ...,  0.3509,  0.4172,  0.0399],\n",
      "         [ 0.0109,  0.0945, -0.4622,  ...,  0.5238,  0.5189, -0.1023],\n",
      "         ...,\n",
      "         [ 0.0924,  0.4206, -0.3651,  ...,  0.5861,  0.5358,  0.1859],\n",
      "         [-0.4014,  0.6025, -0.2134,  ...,  0.8407,  0.5077, -0.0291],\n",
      "         [-0.1049, -0.1440,  0.0144,  ...,  0.5188,  0.2673,  0.1781]],\n",
      "\n",
      "        [[-0.3911,  0.3556, -0.6201,  ...,  0.0956,  0.1562, -0.0768],\n",
      "         [-0.5835,  0.7301, -0.5162,  ..., -0.0343,  0.4936,  0.0506],\n",
      "         [ 0.2464,  0.0105, -0.6355,  ...,  0.6351,  0.6114,  0.0281],\n",
      "         ...,\n",
      "         [-0.3159,  0.3716, -0.6957,  ...,  0.4547,  0.2860,  0.2100],\n",
      "         [-0.3898,  0.6607, -0.7550,  ...,  0.3946,  0.7657, -0.0992],\n",
      "         [-0.5579,  0.0297, -0.6066,  ...,  0.1852,  0.4581,  0.1559]],\n",
      "\n",
      "        [[-0.3612,  0.5981, -0.2752,  ...,  0.1615,  0.5431, -0.1796],\n",
      "         [-0.0778,  0.3282, -0.2878,  ..., -0.0011,  0.3491, -0.0609],\n",
      "         [-0.0942, -0.0933, -0.2180,  ...,  0.6108,  0.6888, -0.2242],\n",
      "         ...,\n",
      "         [-0.4845,  0.3693, -0.5236,  ...,  0.3560,  0.4780,  0.1070],\n",
      "         [-0.4275,  0.8488,  0.1255,  ...,  0.5142,  0.3950,  0.0194],\n",
      "         [-0.5954,  0.2211, -0.2600,  ...,  0.4388,  0.1918, -0.0444]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.4334,  0.0892, -0.5630,  ..., -0.4493, -0.0560,  0.4356],\n",
      "         [ 0.2717,  0.1229, -0.5500,  ..., -0.4195,  0.0381,  0.3019],\n",
      "         [ 0.2912, -0.1352, -0.4348,  ...,  0.0224,  0.0445,  0.6551],\n",
      "         ...,\n",
      "         [ 0.1903, -0.0102, -0.7039,  ..., -0.1935, -0.0686,  0.6548],\n",
      "         [ 0.3223,  0.4849, -0.8152,  ..., -0.2295, -0.1057,  0.3830],\n",
      "         [ 0.0901, -0.2887, -0.5036,  ..., -0.2989, -0.1126,  0.5076]],\n",
      "\n",
      "        [[ 0.4545,  0.3083, -0.4154,  ..., -0.6216,  0.5302,  0.2757],\n",
      "         [ 0.3869,  0.3085, -0.3516,  ..., -0.6236,  0.3753,  0.3606],\n",
      "         [ 0.7509,  0.0845, -0.7161,  ..., -0.1032,  0.5251,  0.4907],\n",
      "         ...,\n",
      "         [ 0.3737,  0.4996, -0.5865,  ..., -0.4698,  0.4565,  0.3666],\n",
      "         [ 0.4269,  0.0401, -0.3426,  ..., -0.4412,  0.6568,  0.3225],\n",
      "         [ 0.2303, -0.1818, -0.3460,  ..., -0.3138,  0.4417,  0.5252]],\n",
      "\n",
      "        [[ 0.3735,  0.0167, -0.1074,  ...,  0.1857, -0.0602, -0.5545],\n",
      "         [ 0.2835,  0.1645, -0.1221,  ...,  0.2618,  0.1567, -0.6576],\n",
      "         [ 0.5030, -0.3500, -0.1143,  ...,  0.6155,  0.0936, -0.3888],\n",
      "         ...,\n",
      "         [ 0.4036,  0.0046, -0.0430,  ...,  0.5307,  0.1349, -0.2588],\n",
      "         [ 0.2880,  0.1559, -0.0719,  ...,  0.2857,  0.1783, -0.4390],\n",
      "         [ 0.2216, -0.3130, -0.0828,  ...,  0.4740, -0.1652, -0.3009]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.836827039718628\n",
      "Model outputs:  tensor([[[-0.3449, -0.0078, -0.4539,  ..., -0.1839, -0.1420,  0.7655],\n",
      "         [ 0.1731,  0.2220, -0.2803,  ..., -0.4118, -0.2083,  0.5238],\n",
      "         [ 0.0758,  0.2300, -0.2727,  ..., -0.4630, -0.2694,  0.6710],\n",
      "         ...,\n",
      "         [ 0.3002,  0.1000, -0.1773,  ..., -0.1312, -0.2152,  0.4847],\n",
      "         [ 0.4819, -0.0805, -0.2688,  ..., -0.4984, -0.1050,  0.6860],\n",
      "         [-0.0814, -0.3547, -0.2438,  ..., -0.4029, -0.4242,  0.9131]],\n",
      "\n",
      "        [[-0.1372,  0.1198, -0.1295,  ..., -0.3573, -0.0913,  0.7032],\n",
      "         [ 0.0521,  0.1100, -0.0972,  ..., -0.4908, -0.1399,  0.4839],\n",
      "         [-0.1025,  0.1982, -0.3867,  ..., -0.5530, -0.2523,  0.4214],\n",
      "         ...,\n",
      "         [ 0.2492,  0.1107, -0.2638,  ...,  0.0658, -0.0245,  0.2406],\n",
      "         [ 0.2504, -0.1524, -0.2944,  ..., -0.1550, -0.1564,  0.6632],\n",
      "         [ 0.3382,  0.1385, -0.3637,  ..., -0.0514, -0.1387,  0.8158]],\n",
      "\n",
      "        [[-0.1968,  0.3186, -0.9473,  ...,  0.1253, -0.0744,  0.6207],\n",
      "         [-0.2317,  0.2849, -0.6196,  ...,  0.2228, -0.3266,  0.3328],\n",
      "         [-0.5476,  0.4265, -0.9004,  ...,  0.0802,  0.2042,  0.3628],\n",
      "         ...,\n",
      "         [ 0.0627,  0.2592, -0.6166,  ...,  0.3798,  0.2630,  0.1043],\n",
      "         [-0.1094,  0.4104, -0.6507,  ...,  0.2287, -0.0478,  0.1002],\n",
      "         [-0.3149,  0.0608, -0.5714,  ...,  0.3219, -0.1090,  0.2159]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0763, -0.1008, -0.4649,  ..., -0.3816, -0.0125,  0.7062],\n",
      "         [-0.0393, -0.0169, -0.5228,  ..., -0.2374, -0.3160,  0.5699],\n",
      "         [-0.2961,  0.3110, -0.3694,  ..., -0.5287, -0.3179,  0.6067],\n",
      "         ...,\n",
      "         [ 0.2972,  0.1055, -0.2464,  ..., -0.1249,  0.2174,  0.5091],\n",
      "         [ 0.4217,  0.0265, -0.1886,  ..., -0.4849, -0.1639,  0.5941],\n",
      "         [ 0.1583, -0.0156, -0.5229,  ..., -0.0818,  0.2520,  0.4882]],\n",
      "\n",
      "        [[-0.1670,  0.2178, -0.5823,  ..., -0.2480, -0.2545,  0.7477],\n",
      "         [-0.0334,  0.0854, -0.2260,  ..., -0.4091, -0.0139,  0.6221],\n",
      "         [-0.2113,  0.2549, -0.7299,  ..., -0.2301, -0.1568,  0.7106],\n",
      "         ...,\n",
      "         [ 0.2139,  0.1640, -0.3564,  ..., -0.1253, -0.0551,  0.2540],\n",
      "         [ 0.2633,  0.0435, -0.1794,  ..., -0.4615,  0.0155,  0.7837],\n",
      "         [-0.0539, -0.0135, -0.2771,  ..., -0.0714, -0.2324,  0.4775]],\n",
      "\n",
      "        [[-0.0069,  0.0133, -0.5476,  ..., -0.3743, -0.2053,  0.8092],\n",
      "         [-0.0689,  0.0033, -0.5502,  ..., -0.2416, -0.1835,  0.5673],\n",
      "         [ 0.1377,  0.1623, -0.3518,  ..., -0.4375, -0.2999,  0.3549],\n",
      "         ...,\n",
      "         [ 0.2240, -0.1932, -0.3048,  ..., -0.2625, -0.0504,  0.6511],\n",
      "         [ 0.2896, -0.0786, -0.2728,  ..., -0.2348, -0.1580,  0.5410],\n",
      "         [ 0.0538, -0.1600, -0.3920,  ..., -0.2352,  0.0415,  0.5143]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.204529881477356\n",
      "Model outputs:  tensor([[[-0.4756,  0.4279, -0.5259,  ...,  0.7967,  0.4250,  0.1918],\n",
      "         [-0.3694,  0.4794, -0.4599,  ...,  0.7975,  0.2396, -0.0718],\n",
      "         [-0.3716,  0.6139, -0.7149,  ...,  0.7986,  0.4213,  0.1516],\n",
      "         ...,\n",
      "         [-0.0947,  0.3527, -0.4868,  ...,  1.0503,  0.4133,  0.2477],\n",
      "         [-0.2890,  0.2174, -0.1435,  ...,  0.9037,  0.5015,  0.2886],\n",
      "         [-0.2546,  0.2860, -0.1039,  ...,  0.9452,  0.6268,  0.0970]],\n",
      "\n",
      "        [[-0.2139,  0.4684, -1.1727,  ...,  0.9472,  0.1151,  0.2946],\n",
      "         [-0.1717,  0.7073, -0.9905,  ...,  0.8081,  0.3958,  0.1026],\n",
      "         [-0.1479,  0.5678, -1.0716,  ...,  0.8587,  0.1192,  0.1871],\n",
      "         ...,\n",
      "         [-0.2930,  0.4127, -1.0284,  ...,  0.8446,  0.0080,  0.3810],\n",
      "         [-0.0799,  0.3503, -0.9030,  ...,  1.0083,  0.3800,  0.1938],\n",
      "         [-0.4854,  0.3604, -0.8121,  ...,  1.1099,  0.4829,  0.1981]],\n",
      "\n",
      "        [[ 0.0900,  0.2866, -0.7053,  ...,  0.0685,  0.1245,  0.7636],\n",
      "         [-0.2090,  0.1572, -0.6410,  ..., -0.3085, -0.2966,  0.4437],\n",
      "         [ 0.1039,  0.1840, -0.3684,  ..., -0.0555, -0.2471,  0.7072],\n",
      "         ...,\n",
      "         [ 0.1844,  0.0549, -0.7453,  ..., -0.0244, -0.3896,  0.6263],\n",
      "         [ 0.1670,  0.2300, -0.5396,  ...,  0.2130,  0.0440,  0.7603],\n",
      "         [-0.0591,  0.3496, -0.4433,  ...,  0.0130, -0.1877,  0.6441]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1348,  0.1211, -0.7861,  ..., -0.0058,  0.0527,  0.6003],\n",
      "         [-0.0015,  0.3235, -0.8827,  ..., -0.2160, -0.3013,  0.4998],\n",
      "         [-0.2315,  0.1887, -0.9001,  ..., -0.0452,  0.0874,  0.5856],\n",
      "         ...,\n",
      "         [-0.1641,  0.2261, -0.7573,  ...,  0.0830, -0.0125,  0.5568],\n",
      "         [-0.2172,  0.2931, -0.7663,  ...,  0.0443, -0.0981,  0.6773],\n",
      "         [-0.1187,  0.2805, -0.6099,  ...,  0.1972,  0.3668,  0.7124]],\n",
      "\n",
      "        [[-0.0363,  0.1421, -1.1293,  ...,  0.1208, -0.1186,  0.7763],\n",
      "         [-0.1125,  0.3254, -0.8142,  ..., -0.0134, -0.1472,  0.3653],\n",
      "         [-0.0616,  0.5626, -0.8165,  ..., -0.0615, -0.1492,  0.2665],\n",
      "         ...,\n",
      "         [-0.1006, -0.0305, -0.8161,  ...,  0.1378, -0.0888,  0.3916],\n",
      "         [-0.0235,  0.1250, -0.4615,  ..., -0.0050,  0.1437,  0.5941],\n",
      "         [-0.0521,  0.0832, -0.6672,  ...,  0.1771,  0.0301,  0.5203]],\n",
      "\n",
      "        [[-0.4047,  0.4538, -0.8622,  ...,  0.8999,  0.2978,  0.5478],\n",
      "         [-0.4859,  0.5585, -0.2060,  ...,  0.6940,  0.4687,  0.0749],\n",
      "         [-0.3564,  0.5606, -0.5483,  ...,  0.5912,  0.4775,  0.1357],\n",
      "         ...,\n",
      "         [-0.4642,  0.2034, -0.4142,  ...,  0.7926,  0.3326,  0.1841],\n",
      "         [-0.4522,  0.1754, -0.3138,  ...,  0.8811,  0.5206,  0.1434],\n",
      "         [-0.3630,  0.1555, -0.4249,  ...,  0.7365,  0.2235,  0.2082]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1333789825439453\n",
      "Model outputs:  tensor([[[-0.3448,  0.4752, -0.3381,  ...,  0.3310,  0.5229,  0.0573],\n",
      "         [-0.3499,  0.5346, -0.0422,  ...,  0.4960,  0.4195, -0.0977],\n",
      "         [-0.0993,  0.6994,  0.0748,  ..., -0.0778,  0.2161,  0.0124],\n",
      "         ...,\n",
      "         [-0.4794,  0.5300, -0.2113,  ...,  0.7208,  0.3619,  0.0556],\n",
      "         [-0.2321,  0.3118, -0.2677,  ...,  0.6473,  0.5820,  0.2443],\n",
      "         [-0.4147,  0.5863, -0.1005,  ...,  0.5854,  0.5767, -0.1472]],\n",
      "\n",
      "        [[-0.0365,  0.3564, -0.3855,  ..., -0.7128,  0.8059,  0.4621],\n",
      "         [-0.0659,  0.6518, -0.4637,  ..., -0.9617,  0.5352,  0.4579],\n",
      "         [ 0.0493,  0.3207, -0.3130,  ..., -1.0051,  0.8473,  0.3683],\n",
      "         ...,\n",
      "         [ 0.3180,  0.1542, -0.6156,  ..., -0.5972,  0.7961,  0.1151],\n",
      "         [ 0.1635,  0.1649, -0.4620,  ..., -0.7147,  0.7462,  0.3865],\n",
      "         [ 0.2402,  0.2344, -0.4179,  ..., -0.8074,  0.8765,  0.6856]],\n",
      "\n",
      "        [[ 0.2257,  0.3117, -0.6125,  ..., -0.3081,  0.2497,  0.7063],\n",
      "         [ 0.1017,  0.5378, -0.4195,  ..., -0.2820,  0.2436,  0.0590],\n",
      "         [ 0.0452,  0.5480, -0.4254,  ..., -0.6509,  0.1627,  0.0516],\n",
      "         ...,\n",
      "         [ 0.0773,  0.4837, -0.7155,  ..., -0.0986,  0.3791,  0.3111],\n",
      "         [ 0.1408,  0.5822, -0.6690,  ..., -0.2413,  0.0306,  0.7128],\n",
      "         [ 0.2321,  0.3192, -0.6927,  ..., -0.5893, -0.0088,  0.2414]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3861,  0.5846, -0.7748,  ...,  0.1033,  0.5459, -0.0567],\n",
      "         [ 0.0382,  0.5242, -0.6130,  ..., -0.2972,  0.0717,  0.2576],\n",
      "         [-0.0973,  0.3546, -0.4806,  ..., -0.2829,  0.2505, -0.1534],\n",
      "         ...,\n",
      "         [-0.1967,  0.6717, -0.9632,  ..., -0.0183,  0.3022,  0.1200],\n",
      "         [ 0.0305,  0.4607, -0.8635,  ...,  0.1856,  0.1170,  0.5624],\n",
      "         [-0.3790,  0.5235, -0.9065,  ...,  0.2242, -0.0739,  0.5364]],\n",
      "\n",
      "        [[-0.3788,  0.7688, -0.2876,  ...,  0.4171,  0.5453,  0.1781],\n",
      "         [-0.5047,  0.5862, -0.0920,  ...,  0.4124,  0.5095, -0.2417],\n",
      "         [-0.3492,  0.6581, -0.1133,  ...,  0.1193,  0.3602,  0.0079],\n",
      "         ...,\n",
      "         [-0.2500,  0.5520, -0.4191,  ...,  0.4537,  0.3082, -0.0647],\n",
      "         [-0.3318,  0.4839, -0.3372,  ...,  0.6053,  0.4173, -0.0416],\n",
      "         [-0.3858,  0.5212, -0.2246,  ...,  0.4714,  0.4989,  0.0412]],\n",
      "\n",
      "        [[ 0.2368,  0.4200, -0.4910,  ..., -0.4626,  0.7426,  0.1850],\n",
      "         [ 0.3179,  0.2319, -0.3661,  ..., -0.5110,  0.4319,  0.3780],\n",
      "         [ 0.2981,  0.4199, -0.4591,  ..., -0.6857,  0.3226,  0.3562],\n",
      "         ...,\n",
      "         [ 0.7026,  0.4441, -0.5166,  ..., -0.4414,  0.6161,  0.2755],\n",
      "         [ 0.3862,  0.2247, -0.5553,  ..., -0.1703,  0.4483,  0.6012],\n",
      "         [ 0.5482,  0.0544, -0.4818,  ..., -0.2382,  0.3674,  0.7141]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.470111608505249\n",
      "Model outputs:  tensor([[[ 9.8351e-02,  8.0780e-01, -9.8746e-01,  ...,  2.6363e-01,\n",
      "           5.3000e-01,  4.8858e-02],\n",
      "         [-2.6330e-01,  5.3148e-01, -6.4417e-01,  ...,  9.9128e-02,\n",
      "           3.1465e-01, -2.6657e-01],\n",
      "         [-1.8283e-01,  2.0725e-01, -5.3494e-01,  ...,  1.8668e-01,\n",
      "           2.1720e-01, -3.0576e-01],\n",
      "         ...,\n",
      "         [-3.0633e-01,  6.1363e-01, -7.5595e-01,  ...,  1.9448e-01,\n",
      "           3.6043e-01, -9.7200e-02],\n",
      "         [ 1.4344e-01,  4.1841e-01, -8.8152e-01,  ...,  1.8486e-01,\n",
      "           3.0036e-01, -5.7822e-02],\n",
      "         [-1.9208e-01,  5.6742e-01, -6.1038e-01,  ...,  4.7201e-01,\n",
      "           8.1644e-02,  5.0490e-02]],\n",
      "\n",
      "        [[-3.3203e-01,  5.2042e-01, -4.4893e-01,  ...,  1.9357e-01,\n",
      "           3.2905e-01,  1.0229e-01],\n",
      "         [-3.4352e-01,  6.2695e-01, -2.5072e-01,  ...,  4.0374e-01,\n",
      "           4.1055e-01, -1.5836e-01],\n",
      "         [-4.6848e-01,  1.4947e-01, -1.4907e-01,  ..., -1.4415e-02,\n",
      "           2.6949e-01, -1.3548e-01],\n",
      "         ...,\n",
      "         [-1.1417e-01,  6.0159e-01, -2.0653e-01,  ...,  3.0915e-01,\n",
      "           3.0870e-01, -1.8936e-02],\n",
      "         [-1.6132e-01,  2.5824e-01, -3.9455e-01,  ...,  3.8056e-01,\n",
      "           2.2725e-01, -3.2345e-01],\n",
      "         [-2.2945e-01,  5.0294e-01, -5.7582e-01,  ...,  1.8955e-01,\n",
      "           2.0249e-01, -6.9920e-02]],\n",
      "\n",
      "        [[ 2.1564e-01,  1.9454e-01, -1.9111e-01,  ..., -8.0574e-01,\n",
      "           6.4749e-01,  1.6022e-01],\n",
      "         [ 4.1675e-01,  2.2736e-01, -1.0049e-01,  ..., -9.5914e-01,\n",
      "           6.2682e-01,  4.0244e-01],\n",
      "         [-3.8814e-02, -2.0366e-01, -5.6349e-02,  ..., -1.1014e+00,\n",
      "           5.3751e-01, -8.6042e-02],\n",
      "         ...,\n",
      "         [ 4.7935e-01,  1.7139e-01, -2.8232e-01,  ..., -8.7124e-01,\n",
      "           5.5671e-01,  1.3303e-01],\n",
      "         [ 3.0234e-01, -2.8950e-02, -2.5463e-01,  ..., -9.1014e-01,\n",
      "           7.6483e-01,  1.0095e-01],\n",
      "         [ 2.7188e-01,  4.1228e-01, -6.3787e-01,  ..., -6.8896e-01,\n",
      "           7.3718e-01,  2.9989e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.8706e-02,  6.7955e-01, -5.8655e-01,  ...,  4.1717e-01,\n",
      "           3.1634e-01, -1.3060e-01],\n",
      "         [-3.5340e-01,  6.9267e-01, -2.3372e-01,  ...,  3.9307e-01,\n",
      "           4.1603e-01, -3.4797e-01],\n",
      "         [-6.5927e-02,  1.4308e-01,  4.9756e-02,  ...,  1.5213e-01,\n",
      "           2.9798e-01, -5.1577e-01],\n",
      "         ...,\n",
      "         [-2.2217e-01,  5.1761e-01, -1.6405e-01,  ...,  4.1833e-01,\n",
      "           2.3990e-01, -2.4202e-01],\n",
      "         [-2.8779e-02,  3.2396e-01, -1.6224e-01,  ...,  3.0439e-01,\n",
      "           4.3189e-01, -2.4190e-01],\n",
      "         [-1.6709e-01,  5.8145e-01, -8.8411e-02,  ...,  4.1150e-01,\n",
      "           4.6708e-01, -2.2942e-01]],\n",
      "\n",
      "        [[ 1.8333e-01,  1.8165e-01, -8.8584e-01,  ..., -2.0676e-01,\n",
      "           1.9447e-01, -1.9077e-01],\n",
      "         [ 6.7378e-02,  3.2783e-01, -4.2835e-01,  ..., -3.2258e-01,\n",
      "           1.2369e-01, -3.6243e-01],\n",
      "         [ 3.0334e-01,  2.3870e-02, -2.7081e-01,  ..., -3.6869e-01,\n",
      "           1.0983e-03, -3.6333e-01],\n",
      "         ...,\n",
      "         [ 2.8964e-01,  1.9423e-01, -8.4987e-02,  ..., -4.5260e-01,\n",
      "           4.6080e-02, -2.9057e-01],\n",
      "         [ 2.2112e-01, -2.3919e-01, -3.5300e-01,  ..., -2.5094e-01,\n",
      "           1.8910e-01,  2.3110e-02],\n",
      "         [ 1.1311e-01,  3.4315e-01, -6.6657e-01,  ..., -1.0725e-01,\n",
      "           1.4001e-01, -3.4969e-01]],\n",
      "\n",
      "        [[-4.8153e-02,  5.2385e-01, -4.1056e-01,  ..., -1.9847e-02,\n",
      "           2.9210e-01,  2.0112e-01],\n",
      "         [-3.9691e-01,  6.5973e-01, -4.7491e-01,  ..., -3.1675e-01,\n",
      "           1.4842e-01,  3.0004e-01],\n",
      "         [-2.1486e-01,  2.2670e-01, -1.1738e-01,  ..., -3.1982e-01,\n",
      "           3.3541e-01, -2.6221e-01],\n",
      "         ...,\n",
      "         [-2.5123e-01,  5.8881e-01, -4.1062e-01,  ...,  1.0631e-01,\n",
      "           2.6725e-01,  3.1842e-02],\n",
      "         [-4.4453e-01,  6.2728e-01, -5.1297e-01,  ...,  8.1179e-02,\n",
      "           2.9802e-01,  2.5971e-01],\n",
      "         [-3.4811e-01,  5.6581e-01, -5.0385e-01,  ...,  1.4915e-01,\n",
      "           2.0649e-01,  1.1998e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.5414024591445923\n",
      "Model outputs:  tensor([[[ 1.6125e-01,  1.2199e-01, -2.6871e-02,  ...,  9.1884e-01,\n",
      "           5.6853e-01,  1.9498e-01],\n",
      "         [ 4.2149e-01,  6.3394e-02, -1.7900e-01,  ...,  7.0192e-01,\n",
      "           2.7368e-01, -1.3526e-01],\n",
      "         [ 3.6566e-01, -1.3104e-01, -1.8189e-01,  ...,  5.5338e-01,\n",
      "           2.6490e-01,  7.4122e-02],\n",
      "         ...,\n",
      "         [-2.4290e-01,  4.2728e-01, -4.2768e-01,  ...,  6.8355e-01,\n",
      "           4.6211e-01,  8.7641e-02],\n",
      "         [-3.2216e-02,  5.8409e-01, -3.8903e-01,  ...,  9.9857e-01,\n",
      "           2.0588e-01,  4.0647e-02],\n",
      "         [-1.7004e-01,  2.9224e-01, -5.9700e-01,  ...,  9.0273e-01,\n",
      "           2.7192e-01,  2.1088e-01]],\n",
      "\n",
      "        [[ 5.6301e-01,  2.1037e-01, -3.9651e-01,  ...,  1.5151e-01,\n",
      "          -1.8994e-01,  5.7143e-01],\n",
      "         [ 4.5410e-01,  1.4177e-01, -5.5152e-01,  ..., -7.6861e-02,\n",
      "          -1.8856e-01,  5.0275e-01],\n",
      "         [ 4.2204e-01,  1.1390e-01, -2.0216e-01,  ...,  9.4534e-02,\n",
      "           4.5256e-02,  5.6407e-01],\n",
      "         ...,\n",
      "         [ 3.1324e-01, -7.8842e-03, -5.7693e-01,  ..., -6.5455e-02,\n",
      "           3.4488e-02,  6.9895e-01],\n",
      "         [ 1.8353e-01,  7.6776e-02, -4.8229e-01,  ..., -1.2256e-01,\n",
      "          -1.1504e-01,  6.0279e-01],\n",
      "         [ 1.7643e-01, -3.3802e-02, -5.2103e-01,  ..., -1.2923e-01,\n",
      "          -1.0317e-01,  8.9071e-01]],\n",
      "\n",
      "        [[ 4.2019e-02,  2.4960e-01, -8.9315e-02,  ...,  6.6060e-01,\n",
      "           5.0407e-01, -2.0409e-04],\n",
      "         [ 2.9376e-01, -2.7248e-01, -4.6797e-01,  ...,  4.8463e-01,\n",
      "           3.1641e-01, -1.4932e-01],\n",
      "         [ 5.1447e-01,  5.4808e-02, -1.5072e-01,  ...,  7.2606e-01,\n",
      "           2.4789e-01,  1.4671e-01],\n",
      "         ...,\n",
      "         [-1.0485e-01,  4.0001e-01, -4.8290e-01,  ...,  9.4330e-01,\n",
      "           3.8233e-01,  1.5822e-01],\n",
      "         [-1.0158e-01,  5.5258e-01, -2.8243e-01,  ...,  7.9498e-01,\n",
      "           4.3218e-01, -2.1206e-01],\n",
      "         [-6.0122e-02,  2.1828e-01, -3.4812e-01,  ...,  8.0420e-01,\n",
      "           3.2600e-01,  2.7678e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.5326e-01, -6.0286e-02, -3.7585e-01,  ..., -4.1913e-02,\n",
      "          -1.2641e-01,  6.8478e-01],\n",
      "         [ 3.7208e-01, -1.4145e-01, -5.7659e-01,  ...,  3.1428e-01,\n",
      "          -2.4232e-01,  6.1391e-01],\n",
      "         [ 5.1701e-01, -4.1306e-02, -1.8473e-01,  ...,  9.4545e-02,\n",
      "          -1.2186e-01,  6.4602e-01],\n",
      "         ...,\n",
      "         [ 2.4008e-01,  2.6792e-01, -7.0103e-01,  ...,  7.2526e-02,\n",
      "          -1.1240e-01,  8.5946e-01],\n",
      "         [ 2.6873e-01,  3.4956e-01, -5.7389e-01,  ..., -2.3709e-01,\n",
      "          -1.0049e-01,  3.4126e-01],\n",
      "         [ 2.0326e-01, -2.1691e-03, -5.9185e-01,  ...,  1.0314e-02,\n",
      "          -1.5746e-01,  8.1992e-01]],\n",
      "\n",
      "        [[ 4.8132e-01,  3.2923e-02, -5.1580e-01,  ..., -2.3064e-01,\n",
      "          -6.7170e-02,  5.9136e-01],\n",
      "         [ 3.5401e-01, -4.4198e-02, -3.1477e-01,  ..., -8.8813e-02,\n",
      "          -4.1953e-01,  6.7970e-01],\n",
      "         [ 3.8276e-01, -4.1027e-02, -1.2913e-01,  ..., -3.0600e-03,\n",
      "          -4.7354e-01,  7.9883e-01],\n",
      "         ...,\n",
      "         [ 7.0822e-02,  9.9255e-02, -4.5765e-01,  ..., -1.6728e-01,\n",
      "          -2.8122e-01,  8.3478e-01],\n",
      "         [ 2.0933e-01,  1.3226e-01, -5.6924e-01,  ..., -2.2000e-01,\n",
      "          -1.1709e-01,  7.2663e-01],\n",
      "         [ 2.3032e-02,  1.3687e-02, -4.5895e-01,  ..., -1.8791e-01,\n",
      "          -3.1632e-01,  6.7844e-01]],\n",
      "\n",
      "        [[ 3.5483e-01,  1.3897e-01, -4.0368e-01,  ...,  8.5357e-02,\n",
      "          -1.7638e-01,  5.5637e-01],\n",
      "         [ 6.5197e-01, -1.8189e-01, -4.7468e-01,  ..., -4.3396e-02,\n",
      "          -1.6982e-01,  5.3421e-01],\n",
      "         [ 5.0093e-01,  1.0654e-01, -3.6758e-01,  ...,  1.0327e-01,\n",
      "          -2.6768e-01,  4.6466e-01],\n",
      "         ...,\n",
      "         [ 3.0952e-01,  1.8779e-02, -6.1732e-01,  ...,  8.4659e-02,\n",
      "           1.0710e-01,  7.9205e-01],\n",
      "         [ 1.3584e-01,  2.4395e-01, -5.5763e-01,  ..., -6.1684e-02,\n",
      "          -2.5852e-01,  6.8749e-01],\n",
      "         [ 3.1704e-01,  1.3892e-01, -5.6824e-01,  ...,  9.4816e-02,\n",
      "           5.7015e-02,  6.0297e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.5626964569091797\n",
      "Model outputs:  tensor([[[-0.4069,  0.2980, -0.7868,  ...,  0.4390, -0.0333,  0.1745],\n",
      "         [-0.1604,  0.2421, -1.0256,  ...,  0.4568,  0.1109, -0.0066],\n",
      "         [-0.3177,  0.6424, -1.0292,  ...,  0.4847,  0.1006,  0.0773],\n",
      "         ...,\n",
      "         [-0.1071,  0.3364, -0.7265,  ...,  0.6146,  0.4847,  0.2328],\n",
      "         [-0.0723,  0.3823, -0.6671,  ...,  0.3636, -0.0419,  0.1139],\n",
      "         [-0.0982,  0.6911, -0.9955,  ...,  0.4368,  0.2251, -0.0801]],\n",
      "\n",
      "        [[ 0.1261,  0.0899, -0.4280,  ..., -0.2273, -0.2422,  0.7524],\n",
      "         [ 0.1252,  0.0208, -0.3806,  ..., -0.2741, -0.0954,  0.6541],\n",
      "         [ 0.1760,  0.0071, -0.4398,  ..., -0.3076, -0.2275,  0.3119],\n",
      "         ...,\n",
      "         [-0.0303,  0.1300, -0.3752,  ..., -0.3601, -0.1415,  0.8162],\n",
      "         [ 0.3197,  0.2205, -0.3765,  ..., -0.1567, -0.0408,  0.3843],\n",
      "         [-0.0767,  0.3756, -0.4294,  ..., -0.0930, -0.1310,  0.4508]],\n",
      "\n",
      "        [[-0.2089,  0.4288, -0.8037,  ..., -0.1069, -0.2362,  0.4570],\n",
      "         [ 0.1651, -0.0448, -0.6772,  ..., -0.2244, -0.1739,  0.8570],\n",
      "         [ 0.2127,  0.3991, -0.5815,  ..., -0.2267,  0.0827,  0.3069],\n",
      "         ...,\n",
      "         [ 0.0626,  0.3751, -0.5431,  ..., -0.0784, -0.0371,  0.5943],\n",
      "         [ 0.4241,  0.0679, -0.5421,  ..., -0.1908, -0.1689,  0.5627],\n",
      "         [ 0.1893,  0.4439, -0.9776,  ...,  0.0241,  0.4288,  0.3751]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7647,  0.6359, -0.6239,  ...,  0.2222,  0.1435,  0.2106],\n",
      "         [-0.5349,  0.2347, -0.4981,  ...,  0.3022,  0.2267,  0.5624],\n",
      "         [-0.7781,  0.4868, -0.2024,  ..., -0.1377,  0.2553,  0.2467],\n",
      "         ...,\n",
      "         [-0.4309,  0.0980, -0.5125,  ...,  0.3497,  0.5920,  0.3334],\n",
      "         [-0.1853,  0.5050, -0.4096,  ...,  0.4717,  0.3591,  0.2968],\n",
      "         [-0.7158,  0.9098, -0.3982,  ...,  0.3318,  0.4842,  0.1757]],\n",
      "\n",
      "        [[-0.1084,  0.5684, -0.8188,  ...,  0.2887, -0.2644,  0.1914],\n",
      "         [ 0.0251,  0.3368, -0.8983,  ...,  0.1085,  0.0644,  0.3252],\n",
      "         [-0.2819,  0.4229, -0.8473,  ...,  0.2555,  0.0639,  0.1924],\n",
      "         ...,\n",
      "         [ 0.1995,  0.1020, -0.9433,  ...,  0.2877, -0.0793,  0.3578],\n",
      "         [ 0.1242,  0.3183, -0.9205,  ...,  0.2523,  0.0895,  0.2400],\n",
      "         [ 0.0564,  0.3209, -0.9660,  ...,  0.2629, -0.0677,  0.1684]],\n",
      "\n",
      "        [[-0.5639,  0.6148, -0.8129,  ...,  0.4127,  0.5250,  0.6243],\n",
      "         [-0.4711,  0.3486, -0.5495,  ...,  0.2519,  0.3502,  0.5251],\n",
      "         [-0.4790,  0.5999, -0.3940,  ...,  0.3187,  0.3633,  0.5021],\n",
      "         ...,\n",
      "         [-0.3181, -0.0131, -0.2776,  ...,  0.4244,  0.4467,  0.4072],\n",
      "         [-0.3272,  0.4123, -0.0760,  ...,  0.2339,  0.4503,  0.3467],\n",
      "         [-0.7634,  0.9121, -0.8618,  ...,  0.2964,  0.5569,  0.3873]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.362847089767456\n",
      "Model outputs:  tensor([[[ 0.2831, -0.1257, -0.3316,  ...,  0.2580, -0.0244, -0.3222],\n",
      "         [ 0.4852, -0.0293, -0.2298,  ...,  0.2142, -0.0448, -0.2081],\n",
      "         [ 0.3635, -0.2240, -0.0454,  ...,  0.2148,  0.0752, -0.6242],\n",
      "         ...,\n",
      "         [ 0.4222, -0.3062, -0.0827,  ..., -0.0720, -0.0710, -0.0776],\n",
      "         [ 0.3349, -0.1605, -0.3251,  ...,  0.0065,  0.2158, -0.4224],\n",
      "         [ 0.5570, -0.0264, -0.1700,  ...,  0.2846,  0.0856, -0.2681]],\n",
      "\n",
      "        [[-0.2476,  0.2680, -1.0908,  ...,  0.4641,  0.1578,  0.1328],\n",
      "         [-0.2602,  0.6323, -0.6380,  ...,  0.4503,  0.1378,  0.0710],\n",
      "         [-0.1534,  0.2920, -0.9793,  ...,  0.4870,  0.3565,  0.1639],\n",
      "         ...,\n",
      "         [-0.1317,  0.3940, -0.8335,  ...,  0.4112, -0.0733,  0.3171],\n",
      "         [-0.2516,  0.3785, -0.8677,  ...,  0.4846,  0.1066,  0.5418],\n",
      "         [-0.1372,  0.3472, -0.8685,  ...,  0.4625, -0.1145,  0.5832]],\n",
      "\n",
      "        [[-0.2275,  0.6320, -1.0888,  ...,  0.8451,  0.1055,  0.4598],\n",
      "         [-0.2094,  0.6898, -0.9746,  ...,  0.7735,  0.3661,  0.1667],\n",
      "         [-0.0938,  0.3036, -0.6636,  ...,  0.5861,  0.1076,  0.1853],\n",
      "         ...,\n",
      "         [-0.2144,  0.4474, -1.3058,  ...,  0.7151,  0.1032,  0.6300],\n",
      "         [-0.0899,  0.5454, -0.7402,  ...,  0.8379,  0.1770,  0.1888],\n",
      "         [ 0.1471,  0.0418, -0.8541,  ...,  1.1011,  0.1821,  0.1804]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3059,  0.4966, -0.4880,  ...,  0.8918,  0.2639,  0.0180],\n",
      "         [-0.6462,  0.5899, -0.1658,  ...,  0.5792,  0.2531, -0.2319],\n",
      "         [-0.4904,  0.3859, -0.1528,  ...,  0.6631,  0.0432, -0.1170],\n",
      "         ...,\n",
      "         [-0.4379,  0.3412, -0.3575,  ...,  0.6062,  0.0791, -0.0041],\n",
      "         [-0.2680,  0.1736, -0.3429,  ...,  0.7241,  0.4903,  0.0964],\n",
      "         [-0.1328,  0.2427, -0.2233,  ...,  0.6634,  0.0722,  0.3103]],\n",
      "\n",
      "        [[ 0.1718, -0.0058, -0.4590,  ..., -0.2640, -0.0048,  0.7083],\n",
      "         [ 0.1310,  0.2857, -0.4419,  ..., -0.4007, -0.1266,  0.5873],\n",
      "         [ 0.0829,  0.0497, -0.6770,  ..., -0.4936, -0.2735,  0.7251],\n",
      "         ...,\n",
      "         [ 0.1731, -0.0126, -0.5609,  ..., -0.3752, -0.5770,  0.9129],\n",
      "         [ 0.0240,  0.0881, -0.6193,  ..., -0.3011, -0.2940,  0.9810],\n",
      "         [ 0.3120, -0.1325, -0.5074,  ..., -0.1077, -0.3492,  0.4716]],\n",
      "\n",
      "        [[ 0.1300,  0.3631, -0.6313,  ..., -0.2070, -0.1829,  0.6251],\n",
      "         [ 0.0126,  0.3436, -0.5156,  ..., -0.3256, -0.1152,  0.6146],\n",
      "         [ 0.0705,  0.0312, -0.4923,  ..., -0.1530,  0.1109,  0.6981],\n",
      "         ...,\n",
      "         [ 0.2667, -0.1270, -0.3140,  ..., -0.0043, -0.1871,  0.7860],\n",
      "         [ 0.0749, -0.0785, -0.6590,  ..., -0.1598, -0.4879,  0.5912],\n",
      "         [ 0.2279, -0.0519, -0.5483,  ...,  0.0432,  0.0445,  0.9182]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.954474687576294\n",
      "Model outputs:  tensor([[[-0.1659, -0.2010, -0.7887,  ...,  0.0162, -0.4400,  0.5332],\n",
      "         [-0.0940, -0.1004, -0.7198,  ..., -0.1133, -0.5270,  0.7382],\n",
      "         [ 0.0166, -0.0411, -0.4486,  ..., -0.2122, -0.3318,  0.6857],\n",
      "         ...,\n",
      "         [ 0.2329, -0.1761, -0.5124,  ..., -0.1161, -0.2357,  1.0623],\n",
      "         [ 0.1565,  0.0375, -0.5800,  ..., -0.3201, -0.2299,  0.8331],\n",
      "         [ 0.4125,  0.2082, -0.6591,  ..., -0.1279,  0.0758,  0.6655]],\n",
      "\n",
      "        [[ 0.2524,  0.0480, -0.5624,  ..., -0.2385,  0.6798,  0.7565],\n",
      "         [ 0.0295,  0.3276, -0.8178,  ..., -0.2037,  0.6864,  0.6729],\n",
      "         [ 0.0446,  0.1539, -0.7212,  ..., -0.5407,  0.5121,  0.7015],\n",
      "         ...,\n",
      "         [ 0.0551, -0.1221, -0.3197,  ..., -0.2798,  0.5092,  0.6664],\n",
      "         [-0.0561, -0.0467, -0.5872,  ..., -0.4167,  0.4367,  0.8746],\n",
      "         [ 0.1404, -0.0640, -0.5838,  ..., -0.4425,  0.4799,  1.0985]],\n",
      "\n",
      "        [[ 0.4185, -0.3951, -0.2554,  ..., -0.0212, -0.0614, -0.2318],\n",
      "         [ 0.3272, -0.1027, -0.2132,  ...,  0.2637, -0.0870, -0.2472],\n",
      "         [ 0.3166,  0.0611, -0.2082,  ...,  0.2435, -0.0128, -0.1952],\n",
      "         ...,\n",
      "         [ 0.4415, -0.0752, -0.4472,  ..., -0.1671,  0.5548,  0.2425],\n",
      "         [ 0.3443, -0.0015, -0.3903,  ..., -0.2842,  0.2705, -0.1852],\n",
      "         [ 0.3614,  0.0963, -0.5228,  ..., -0.0935,  0.1665, -0.1619]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0600, -0.1164, -0.4979,  ..., -0.1999, -0.0826,  0.5834],\n",
      "         [-0.0119,  0.1027, -0.8237,  ..., -0.1409, -0.0393,  0.5226],\n",
      "         [ 0.2752,  0.1214, -0.7464,  ...,  0.0138, -0.0152,  0.5586],\n",
      "         ...,\n",
      "         [ 0.0358, -0.0355, -0.4033,  ...,  0.1373, -0.0901,  0.7332],\n",
      "         [-0.0290,  0.2123, -0.6242,  ..., -0.1754, -0.1674,  0.7696],\n",
      "         [ 0.0487,  0.1243, -0.5188,  ..., -0.2748,  0.1655,  0.7865]],\n",
      "\n",
      "        [[ 0.1627, -0.1740, -0.4708,  ...,  0.7311,  0.4863,  0.3619],\n",
      "         [-0.2747,  0.1458, -0.1132,  ...,  0.9426, -0.0116,  0.0920],\n",
      "         [-0.0436,  0.1546, -0.4798,  ...,  0.8835, -0.0247,  0.3527],\n",
      "         ...,\n",
      "         [-0.0236,  0.3113,  0.0538,  ...,  0.9141,  0.2553,  0.1692],\n",
      "         [-0.0812,  0.3711, -0.2837,  ...,  0.7387,  0.1328,  0.3354],\n",
      "         [-0.1726,  0.2036, -0.3648,  ...,  0.7049,  0.1617,  0.2999]],\n",
      "\n",
      "        [[ 0.2611, -0.1792, -0.7150,  ...,  0.1804, -0.2242,  0.6191],\n",
      "         [ 0.2036,  0.0571, -0.5821,  ..., -0.3000, -0.0368,  0.4644],\n",
      "         [ 0.0125,  0.2875, -0.9560,  ..., -0.0308,  0.1899,  0.4711],\n",
      "         ...,\n",
      "         [-0.0220, -0.0313, -0.4243,  ...,  0.1350, -0.0174,  0.5532],\n",
      "         [ 0.1369,  0.0106, -0.5872,  ..., -0.3037, -0.2320,  0.7099],\n",
      "         [ 0.2115, -0.0235, -0.7296,  ..., -0.1993, -0.0431,  0.6973]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.382524847984314\n",
      "Model outputs:  tensor([[[-0.2202,  0.0080, -0.6939,  ...,  0.3844,  0.4488,  0.5175],\n",
      "         [-0.2339,  0.2605, -0.5689,  ...,  0.4538,  0.4273,  0.3842],\n",
      "         [-0.5291,  0.4454, -0.4957,  ...,  0.1066,  0.3998,  0.0931],\n",
      "         ...,\n",
      "         [-0.4827,  0.5635, -0.7476,  ...,  0.5140,  0.2332,  0.0977],\n",
      "         [-0.5168,  0.2216, -0.4761,  ...,  0.5880,  0.2907,  0.4044],\n",
      "         [-0.3959,  0.4882, -0.6071,  ...,  0.2528,  0.2703,  0.1369]],\n",
      "\n",
      "        [[-0.1168,  0.2390, -0.7184,  ...,  0.7209,  0.4322,  0.3143],\n",
      "         [-0.1477,  0.0626, -0.8041,  ...,  0.7179,  0.6684,  0.1761],\n",
      "         [-0.3413,  0.3270, -0.5453,  ...,  0.1023,  0.3038, -0.0098],\n",
      "         ...,\n",
      "         [-0.2866,  0.4639, -0.6616,  ...,  0.5287,  0.4870,  0.3274],\n",
      "         [-0.5143,  0.3514, -0.6273,  ...,  0.7272,  0.3289,  0.3127],\n",
      "         [-0.6017,  0.7564, -0.8435,  ...,  0.2838,  0.2000,  0.2246]],\n",
      "\n",
      "        [[-0.1507,  0.0480, -0.3775,  ...,  0.7767,  0.4850,  0.3859],\n",
      "         [-0.1048,  0.2004, -0.3075,  ...,  0.7416,  0.3529,  0.2952],\n",
      "         [-0.1713,  0.5047, -0.2933,  ...,  0.3553,  0.3297,  0.0753],\n",
      "         ...,\n",
      "         [-0.2317,  0.5726, -0.2103,  ...,  0.6242,  0.4813,  0.1771],\n",
      "         [-0.5798,  0.3702, -0.1318,  ...,  0.6659,  0.1096,  0.0201],\n",
      "         [-0.6087,  0.5906, -0.2019,  ...,  0.6996,  0.3491, -0.2134]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3282,  0.1037, -0.7947,  ...,  0.6232,  0.2515,  0.3864],\n",
      "         [-0.2742,  0.1110, -0.9383,  ...,  0.7684,  0.5830,  0.4167],\n",
      "         [-0.4102,  0.2013, -0.7786,  ...,  0.3552,  0.3633,  0.2911],\n",
      "         ...,\n",
      "         [-0.4172,  0.3546, -0.7423,  ...,  0.7964,  0.3134,  0.3268],\n",
      "         [-0.3754,  0.3484, -0.8236,  ...,  0.9253,  0.4715,  0.3099],\n",
      "         [-0.6809,  0.5272, -1.0213,  ...,  0.4568,  0.2510,  0.2066]],\n",
      "\n",
      "        [[-0.1527, -0.0790, -0.9526,  ...,  0.0958,  0.2131,  0.6362],\n",
      "         [-0.2983,  0.0952, -0.9411,  ...,  0.3439,  0.2557,  0.4755],\n",
      "         [-0.2916,  0.3659, -0.6750,  ...,  0.0640,  0.2881,  0.2215],\n",
      "         ...,\n",
      "         [-0.0920,  0.2518, -0.5965,  ...,  0.1895,  0.2012,  0.2735],\n",
      "         [-0.2579, -0.0381, -0.8140,  ...,  0.0025,  0.0239,  0.3356],\n",
      "         [-0.4382,  0.4818, -0.8322,  ..., -0.2417,  0.1249,  0.3346]],\n",
      "\n",
      "        [[-0.2253,  0.1392, -0.7982,  ...,  1.0415,  0.4483,  0.3113],\n",
      "         [ 0.0810,  0.0351, -0.5152,  ...,  0.8120,  0.5302,  0.0054],\n",
      "         [-0.1671,  0.3774, -0.4941,  ...,  0.6433,  0.4868, -0.0242],\n",
      "         ...,\n",
      "         [-0.1362,  0.3266, -0.1889,  ...,  0.8116,  0.3593, -0.2274],\n",
      "         [ 0.0732,  0.2970, -0.2840,  ...,  0.9706,  0.3374,  0.0772],\n",
      "         [-0.1370,  0.5107, -0.3854,  ...,  0.8483,  0.5147, -0.2084]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.4698797464370728\n",
      "Model outputs:  tensor([[[-0.0862,  0.3588, -0.5521,  ..., -0.0524, -0.2604,  0.8151],\n",
      "         [ 0.0983,  0.2981, -0.9105,  ..., -0.1831, -0.1042,  0.6112],\n",
      "         [ 0.0341,  0.2283, -0.5785,  ...,  0.1249, -0.2209,  0.4160],\n",
      "         ...,\n",
      "         [-0.3658,  0.2509, -0.5186,  ..., -0.5070, -0.1342,  0.4268],\n",
      "         [ 0.0817,  0.1397, -0.6923,  ..., -0.2663, -0.0838,  0.5635],\n",
      "         [ 0.1194,  0.3237, -0.3852,  ..., -0.3460, -0.1602,  0.5479]],\n",
      "\n",
      "        [[-0.1888,  0.2017, -0.7078,  ..., -0.1561, -0.0985,  0.8663],\n",
      "         [-0.2309,  0.2867, -0.6925,  ..., -0.1114, -0.0488,  0.7668],\n",
      "         [ 0.3413, -0.1216, -0.4315,  ...,  0.1256, -0.0446,  0.8377],\n",
      "         ...,\n",
      "         [-0.0829,  0.2923, -0.1460,  ..., -0.5175, -0.2401,  0.5673],\n",
      "         [ 0.1275,  0.0959, -0.6291,  ..., -0.0485, -0.2977,  0.4098],\n",
      "         [ 0.1191,  0.1648, -0.4299,  ..., -0.3318,  0.0345,  0.4597]],\n",
      "\n",
      "        [[ 0.0581,  0.1572, -0.5362,  ..., -0.2941, -0.2676,  0.8125],\n",
      "         [ 0.1769,  0.3067, -0.6239,  ...,  0.1324, -0.2022,  0.5869],\n",
      "         [ 0.1839, -0.3160, -0.4284,  ...,  0.0312, -0.0343,  0.7241],\n",
      "         ...,\n",
      "         [-0.1019,  0.1756, -0.0945,  ..., -0.4495, -0.4557,  0.3382],\n",
      "         [ 0.1669,  0.0414, -0.6346,  ..., -0.0486, -0.1269,  0.6779],\n",
      "         [-0.0681,  0.1670, -0.6723,  ..., -0.4454, -0.3866,  0.5872]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4111,  0.4303, -0.3112,  ...,  0.7207,  0.1828,  0.3816],\n",
      "         [-0.1871,  0.3528, -0.1968,  ...,  0.2975,  0.4222,  0.1558],\n",
      "         [-0.1458,  0.1175, -0.1560,  ...,  0.6413,  0.3075,  0.2628],\n",
      "         ...,\n",
      "         [-0.6685,  0.5673,  0.0265,  ...,  0.0322,  0.0102,  0.0090],\n",
      "         [-0.4019,  0.2329, -0.2808,  ...,  0.4862,  0.0576,  0.3277],\n",
      "         [-0.5490,  0.7255, -0.1627,  ...,  0.5173,  0.3260,  0.2012]],\n",
      "\n",
      "        [[-0.2873,  0.4853, -0.3426,  ...,  0.6316,  0.2908,  0.1955],\n",
      "         [-0.0499,  0.5795, -0.3113,  ...,  0.7086,  0.4173,  0.2656],\n",
      "         [-0.0054,  0.0908, -0.2622,  ...,  0.9264,  0.4670,  0.2884],\n",
      "         ...,\n",
      "         [-0.5229,  0.6554,  0.0988,  ...,  0.4003,  0.0449, -0.0772],\n",
      "         [-0.2583,  0.4424, -0.1093,  ...,  0.5743,  0.2515,  0.1984],\n",
      "         [-0.4131,  0.5641, -0.0531,  ...,  0.6063,  0.4060, -0.0628]],\n",
      "\n",
      "        [[-0.3352,  0.3285, -1.1400,  ...,  0.6587,  0.1430,  0.3967],\n",
      "         [-0.1217,  0.4110, -1.0407,  ...,  0.6498,  0.1208,  0.1450],\n",
      "         [-0.0393,  0.1888, -0.7812,  ...,  0.9162,  0.0566,  0.1675],\n",
      "         ...,\n",
      "         [-0.3847,  0.5586, -0.6536,  ...,  0.2260,  0.4187,  0.1748],\n",
      "         [-0.0759,  0.4262, -0.8836,  ...,  0.6173,  0.2969, -0.0623],\n",
      "         [-0.5086,  0.5560, -0.9123,  ...,  0.5986,  0.2400,  0.2293]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.7274410724639893\n",
      "Model outputs:  tensor([[[ 2.0313e-01,  2.0507e-01, -3.1996e-01,  ..., -2.4452e-01,\n",
      "          -8.1574e-02,  6.1734e-01],\n",
      "         [ 2.3026e-01,  4.2375e-01, -7.1747e-01,  ..., -2.2941e-02,\n",
      "          -1.7282e-01,  1.0422e+00],\n",
      "         [ 4.3573e-01,  7.8698e-02, -5.1028e-01,  ..., -3.1495e-01,\n",
      "          -3.3957e-02,  7.5671e-01],\n",
      "         ...,\n",
      "         [ 3.9150e-01,  7.6132e-02, -3.0455e-01,  ..., -1.7365e-01,\n",
      "          -2.1412e-01,  6.1033e-01],\n",
      "         [ 4.5344e-01,  3.5301e-02, -5.8524e-01,  ..., -3.1413e-01,\n",
      "          -8.6103e-02,  5.2258e-01],\n",
      "         [ 5.8803e-01, -9.7412e-02, -4.4205e-01,  ..., -3.4483e-01,\n",
      "          -1.1159e-01,  3.0620e-01]],\n",
      "\n",
      "        [[-2.3179e-01,  2.9349e-01, -1.3368e-01,  ...,  7.4670e-01,\n",
      "           5.8206e-01,  1.1566e-01],\n",
      "         [-3.1864e-01,  5.7518e-01, -3.4837e-01,  ...,  1.0445e+00,\n",
      "           5.7825e-01,  1.5748e-01],\n",
      "         [-1.9871e-01,  5.6946e-01, -3.2187e-01,  ...,  7.0707e-01,\n",
      "           4.9161e-01, -1.1359e-01],\n",
      "         ...,\n",
      "         [ 6.1965e-04,  3.5787e-01, -2.8242e-01,  ...,  4.8827e-01,\n",
      "           1.8283e-01,  4.8722e-02],\n",
      "         [-3.2835e-01,  2.6625e-01, -1.9903e-01,  ...,  6.2412e-01,\n",
      "           6.8849e-01,  7.3799e-02],\n",
      "         [-3.9239e-02,  2.1251e-01, -3.5616e-01,  ...,  2.6778e-01,\n",
      "           2.1739e-01, -2.7605e-01]],\n",
      "\n",
      "        [[ 1.2028e-01,  2.5089e-01, -2.0523e-01,  ..., -2.1789e-01,\n",
      "          -4.1817e-02,  6.8197e-01],\n",
      "         [-2.6505e-02,  5.6335e-01, -7.3037e-01,  ..., -1.2292e-01,\n",
      "          -1.9867e-01,  4.9906e-01],\n",
      "         [ 2.8617e-01,  3.7840e-01, -4.7801e-01,  ..., -4.4599e-01,\n",
      "          -7.3469e-03,  6.0843e-01],\n",
      "         ...,\n",
      "         [ 7.5489e-02,  1.8870e-01, -2.4697e-01,  ..., -4.0669e-01,\n",
      "          -7.8159e-02,  4.4307e-01],\n",
      "         [ 2.4794e-01,  2.3746e-02, -4.3864e-01,  ..., -3.2319e-01,\n",
      "          -1.1200e-01,  7.9168e-01],\n",
      "         [ 4.1493e-01,  6.2719e-02, -1.6811e-01,  ..., -4.0804e-01,\n",
      "          -2.0100e-01,  2.6860e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0492e-01,  2.2387e-01, -1.8843e-01,  ..., -2.6000e-01,\n",
      "           1.6622e-01,  5.4266e-01],\n",
      "         [ 3.4349e-01,  1.9739e-01, -2.5388e-01,  ..., -1.5763e-01,\n",
      "           1.2326e-01,  7.0979e-01],\n",
      "         [-5.8349e-02,  6.1687e-01, -5.6850e-01,  ..., -4.3752e-01,\n",
      "           5.4142e-02,  5.1649e-01],\n",
      "         ...,\n",
      "         [ 3.0572e-01,  2.0325e-01, -3.2516e-01,  ..., -3.9224e-01,\n",
      "          -9.4532e-02,  6.2225e-01],\n",
      "         [ 1.1562e-01,  1.2911e-01, -3.2097e-01,  ..., -2.0470e-01,\n",
      "           2.4133e-01,  5.9051e-01],\n",
      "         [ 3.7689e-01,  2.0584e-01, -1.3967e-01,  ..., -5.1530e-01,\n",
      "          -1.0026e-01,  4.8319e-01]],\n",
      "\n",
      "        [[ 7.0418e-01, -3.3653e-01, -1.1141e-02,  ...,  6.2370e-01,\n",
      "           1.5251e-01, -6.6566e-01],\n",
      "         [ 3.5508e-01, -2.8701e-02, -5.6918e-02,  ...,  6.1166e-01,\n",
      "           1.9118e-01, -4.5739e-01],\n",
      "         [ 2.7649e-01,  1.4102e-01, -1.6062e-01,  ...,  4.2118e-01,\n",
      "           1.9731e-01, -4.5102e-01],\n",
      "         ...,\n",
      "         [ 6.7281e-01, -2.2038e-01,  1.2568e-01,  ...,  4.4660e-01,\n",
      "          -1.1099e-02, -2.7332e-01],\n",
      "         [ 6.1952e-01,  6.6139e-02,  5.5298e-02,  ...,  2.8444e-01,\n",
      "           3.0930e-01, -2.4588e-01],\n",
      "         [ 5.3039e-01, -1.9928e-01,  1.1988e-01,  ...,  2.2393e-01,\n",
      "          -6.5928e-02, -6.2106e-01]],\n",
      "\n",
      "        [[-4.4389e-01,  2.3259e-01, -1.3945e-01,  ...,  7.5655e-01,\n",
      "           3.9542e-01, -7.6558e-02],\n",
      "         [-4.4318e-01,  5.5369e-01, -5.3032e-01,  ...,  6.4842e-01,\n",
      "           4.1174e-01,  1.6218e-01],\n",
      "         [-4.3002e-01,  5.3375e-01, -3.7126e-01,  ...,  4.6854e-01,\n",
      "           3.3354e-01, -1.5219e-02],\n",
      "         ...,\n",
      "         [-3.5579e-01,  7.5221e-01, -4.3402e-01,  ...,  5.2095e-01,\n",
      "           3.0152e-01,  9.6688e-02],\n",
      "         [-4.2725e-01,  3.7120e-01, -3.3641e-01,  ...,  6.2385e-01,\n",
      "           4.8142e-01,  7.7895e-02],\n",
      "         [-7.2081e-02,  1.4143e-01, -2.3916e-01,  ...,  6.3733e-01,\n",
      "           3.3295e-01, -1.7415e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.271168828010559\n",
      "Model outputs:  tensor([[[-0.1436,  0.4266, -1.0801,  ...,  0.6642,  0.3426, -0.0832],\n",
      "         [-0.2995,  0.2639, -0.7216,  ...,  0.5383,  0.3618,  0.0500],\n",
      "         [-0.0740,  0.2788, -0.7206,  ...,  0.5268, -0.0567, -0.2000],\n",
      "         ...,\n",
      "         [-0.2224,  0.0568, -0.4222,  ..., -0.7462, -0.1684,  0.3388],\n",
      "         [-0.1961,  0.0823, -0.0284,  ..., -0.8114,  0.5833, -0.0368],\n",
      "         [-0.0080,  0.1475, -0.4670,  ..., -0.7018,  0.4199,  0.4300]],\n",
      "\n",
      "        [[ 0.2160,  0.1639, -0.0033,  ...,  0.3396,  0.0095, -0.5965],\n",
      "         [ 0.2157, -0.1337, -0.0243,  ...,  0.1731,  0.1397, -0.3126],\n",
      "         [ 0.2786, -0.1709,  0.0638,  ...,  0.2072,  0.0669, -0.6651],\n",
      "         ...,\n",
      "         [-0.1191, -0.0231, -0.0786,  ...,  0.5686, -0.4943, -0.4727],\n",
      "         [ 0.1917,  0.2948,  0.1249,  ...,  0.3566, -0.0722, -0.7950],\n",
      "         [ 0.0499,  0.2449,  0.0886,  ...,  0.2364, -0.1267, -0.4967]],\n",
      "\n",
      "        [[-0.0849,  0.5447, -0.8733,  ...,  0.1851,  0.6279,  0.4397],\n",
      "         [-0.7293,  0.1876, -0.6845,  ...,  0.4952,  0.5028,  0.0892],\n",
      "         [-0.2488,  0.4149, -0.6211,  ...,  0.4319,  0.2620,  0.2519],\n",
      "         ...,\n",
      "         [-0.6178,  0.4255, -0.9789,  ...,  0.5479, -0.3026,  0.1372],\n",
      "         [-0.6267,  0.5045, -0.8609,  ...,  0.2680,  0.3610,  0.0524],\n",
      "         [-0.7356,  0.7540, -0.7307,  ...,  0.3996,  0.0041,  0.2206]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1036,  0.2198, -0.7698,  ..., -0.1655,  0.1220,  0.5429],\n",
      "         [ 0.2196,  0.2645, -0.5410,  ..., -0.1346,  0.3494,  0.5591],\n",
      "         [ 0.2495,  0.2287, -0.6327,  ..., -0.3972, -0.2334,  0.4182],\n",
      "         ...,\n",
      "         [-0.2991,  0.1677, -0.7871,  ..., -0.2779, -0.4349,  0.3537],\n",
      "         [ 0.0297,  0.0073, -0.5410,  ..., -0.3939,  0.0771,  0.4846],\n",
      "         [ 0.0441,  0.3165, -0.5375,  ..., -0.3760, -0.1456,  0.3823]],\n",
      "\n",
      "        [[ 0.0621,  0.1299, -0.6239,  ..., -0.3935,  0.9393,  0.9984],\n",
      "         [-0.1654,  0.1990, -0.3093,  ..., -0.2835,  0.6561,  0.8016],\n",
      "         [ 0.1191,  0.1383, -0.4603,  ..., -0.6122,  0.6172,  0.8711],\n",
      "         ...,\n",
      "         [-0.3013,  0.0424, -0.4229,  ..., -0.7318, -0.0596,  0.9496],\n",
      "         [-0.1286,  0.2467, -0.8522,  ..., -0.7076,  0.7772,  0.8360],\n",
      "         [ 0.0390, -0.0482, -0.5115,  ..., -0.6364,  0.7100,  0.7942]],\n",
      "\n",
      "        [[ 0.1282,  0.1323, -0.6548,  ..., -0.3682, -0.1087,  0.5932],\n",
      "         [-0.0518,  0.0561, -0.4339,  ..., -0.0590, -0.0793,  0.6508],\n",
      "         [ 0.2214,  0.2505, -0.7050,  ..., -0.4812, -0.0800,  0.5773],\n",
      "         ...,\n",
      "         [-0.1588, -0.0818, -0.6336,  ..., -0.2910, -0.7474,  0.6206],\n",
      "         [ 0.2741,  0.2991, -0.3935,  ..., -0.5188, -0.3381,  0.4167],\n",
      "         [ 0.0842,  0.1380, -0.5084,  ..., -0.2575, -0.2644,  0.5735]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.615311622619629\n",
      "Model outputs:  tensor([[[-0.0266,  0.3428, -0.2441,  ..., -0.3877,  0.1151, -0.0732],\n",
      "         [-0.4527,  0.6377, -0.3047,  ..., -0.3562,  0.1264,  0.0446],\n",
      "         [-0.2735,  0.4054, -0.2831,  ..., -0.2710,  0.2387,  0.2787],\n",
      "         ...,\n",
      "         [ 0.0431,  0.3211, -0.4649,  ..., -0.4586, -0.0571, -0.0453],\n",
      "         [-0.1406,  0.3640, -0.4299,  ..., -0.4425,  0.1804,  0.2370],\n",
      "         [ 0.1343,  0.4657, -0.1260,  ..., -0.6289, -0.0563,  0.3828]],\n",
      "\n",
      "        [[-0.1392,  0.5613, -0.1112,  ...,  0.2791,  0.5256, -0.0941],\n",
      "         [-0.4247,  1.0082, -0.1531,  ...,  0.5584,  0.5984, -0.2870],\n",
      "         [-0.2420,  0.8636, -0.4120,  ...,  0.6012,  0.8605, -0.0782],\n",
      "         ...,\n",
      "         [-0.2602,  0.5967, -0.5879,  ...,  0.0942,  0.6879, -0.3866],\n",
      "         [-0.2923,  0.6826, -0.2897,  ...,  0.3785,  0.5373, -0.2487],\n",
      "         [-0.5921,  0.8007, -0.5093,  ...,  0.5265,  0.5398, -0.3328]],\n",
      "\n",
      "        [[ 0.3201,  0.2048, -0.4523,  ..., -0.3276,  0.7754, -0.4620],\n",
      "         [ 0.2107,  0.4723, -0.5867,  ..., -0.2548,  0.3875, -0.2191],\n",
      "         [ 0.0519,  0.2172, -0.5101,  ..., -0.1686,  0.7079, -0.2886],\n",
      "         ...,\n",
      "         [-0.2867,  0.8777, -0.6342,  ...,  0.2733,  0.6755, -0.0466],\n",
      "         [-0.5963,  0.8218, -0.6840,  ...,  0.3904,  0.7568,  0.1020],\n",
      "         [-0.4688,  0.9639, -0.6239,  ...,  0.4635,  0.7562,  0.1165]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0761,  0.5090, -0.4202,  ..., -0.3898,  0.3444,  0.2154],\n",
      "         [ 0.1460,  0.2934, -0.3151,  ..., -0.2975, -0.1037,  0.2436],\n",
      "         [ 0.0734,  0.4703, -0.6970,  ..., -0.0745,  0.1578,  0.5109],\n",
      "         ...,\n",
      "         [ 0.0646,  0.5454, -0.6611,  ..., -0.2490,  0.0926,  0.0608],\n",
      "         [ 0.0039,  0.3494, -0.2250,  ..., -0.3161,  0.1836,  0.1563],\n",
      "         [-0.0983,  0.3118, -0.3922,  ..., -0.4048, -0.0163,  0.3361]],\n",
      "\n",
      "        [[-0.4119,  0.8464, -0.1558,  ...,  0.4729,  0.4838, -0.3257],\n",
      "         [-0.2949,  1.0074, -0.3550,  ...,  0.5793,  0.6036, -0.1805],\n",
      "         [-0.3523,  1.0906, -0.3899,  ...,  0.7906,  0.7268, -0.0071],\n",
      "         ...,\n",
      "         [-0.1604,  0.8453, -0.3661,  ...,  0.0839,  0.4805, -0.0381],\n",
      "         [-0.2060,  0.8708, -0.4697,  ...,  0.3491,  0.2762, -0.0957],\n",
      "         [-0.2523,  0.8260, -0.2608,  ...,  0.2487,  0.2904, -0.0098]],\n",
      "\n",
      "        [[ 0.4164,  0.3079, -0.1027,  ...,  0.5870,  0.1007, -0.7520],\n",
      "         [-0.0053,  0.2850, -0.0865,  ...,  0.4881,  0.0412, -0.5072],\n",
      "         [ 0.1961,  0.3803, -0.1073,  ...,  0.2394,  0.0507, -0.5681],\n",
      "         ...,\n",
      "         [ 0.0333,  0.2426, -0.0616,  ...,  0.2678, -0.1026, -0.8588],\n",
      "         [ 0.1346,  0.3204, -0.1639,  ...,  0.2068,  0.2407, -0.7440],\n",
      "         [ 0.1276,  0.5820, -0.2010,  ...,  0.2668,  0.0924, -0.6191]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1791949272155762\n",
      "Model outputs:  tensor([[[ 2.9361e-01,  3.0169e-01, -6.6280e-01,  ..., -1.2803e-01,\n",
      "          -1.2834e-02,  9.2571e-01],\n",
      "         [ 2.0068e-01,  1.0330e-01, -1.5222e-01,  ..., -7.6769e-02,\n",
      "          -2.9993e-01,  8.7571e-01],\n",
      "         [ 6.5556e-03,  3.6122e-01, -3.5939e-01,  ..., -4.5604e-01,\n",
      "           1.9993e-01,  4.4274e-01],\n",
      "         ...,\n",
      "         [ 1.5874e-01,  1.1116e-01,  8.0187e-02,  ..., -4.5859e-01,\n",
      "          -4.6270e-01,  8.1744e-01],\n",
      "         [-4.8320e-03, -3.6013e-02, -5.8769e-01,  ..., -2.2609e-01,\n",
      "          -5.4610e-01,  8.8147e-01],\n",
      "         [-2.8802e-01,  4.8422e-03, -2.8368e-01,  ..., -3.1854e-01,\n",
      "          -3.8720e-01,  8.1359e-01]],\n",
      "\n",
      "        [[-6.4205e-01,  2.7611e-01, -1.1752e+00,  ...,  2.9646e-01,\n",
      "          -2.1294e-01,  3.5445e-01],\n",
      "         [-4.0454e-01, -1.7560e-01, -6.9822e-01,  ...,  7.5646e-02,\n",
      "           1.2488e-01,  3.8778e-01],\n",
      "         [-4.8032e-01,  7.6355e-01, -9.6684e-01,  ...,  1.3155e-01,\n",
      "           3.2439e-01,  2.2667e-01],\n",
      "         ...,\n",
      "         [-3.9264e-01,  7.1871e-01, -4.5113e-01,  ...,  7.5543e-02,\n",
      "           2.1012e-01,  8.9908e-02],\n",
      "         [-3.2989e-01, -5.7338e-02, -7.7646e-01,  ...,  8.9950e-02,\n",
      "          -1.7575e-02,  3.5376e-01],\n",
      "         [-5.8326e-01,  3.4684e-01, -1.0983e+00,  ...,  9.7654e-02,\n",
      "          -8.4178e-03,  3.1739e-01]],\n",
      "\n",
      "        [[-2.2956e-01,  3.0062e-01, -7.9074e-01,  ...,  2.5310e-01,\n",
      "           4.2700e-02,  4.3214e-01],\n",
      "         [-3.0217e-01,  1.8969e-01, -8.0507e-01,  ...,  4.3797e-01,\n",
      "           3.7992e-01,  3.4252e-01],\n",
      "         [-2.0083e-01,  6.7905e-01, -7.5305e-01,  ...,  1.8011e-01,\n",
      "          -7.1294e-02,  9.7868e-02],\n",
      "         ...,\n",
      "         [-4.3570e-01,  4.9159e-01, -5.2724e-01,  ...,  1.8030e-01,\n",
      "          -1.8829e-01,  2.2627e-02],\n",
      "         [-3.7307e-01,  2.4030e-01, -1.0697e+00,  ...,  4.5253e-01,\n",
      "           7.8549e-02,  5.9888e-01],\n",
      "         [-2.6610e-01,  4.1031e-01, -9.0442e-01,  ...,  3.1632e-01,\n",
      "           9.2685e-02,  4.4805e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.9433e-01,  2.0467e-01, -4.9917e-01,  ..., -5.3076e-02,\n",
      "          -2.5229e-01,  8.1481e-01],\n",
      "         [-1.6222e-02,  7.0444e-02, -1.7990e-01,  ..., -1.2695e-01,\n",
      "          -2.0060e-01,  8.8812e-01],\n",
      "         [-4.3917e-02,  5.2419e-01, -2.5036e-01,  ..., -3.0633e-01,\n",
      "           2.1488e-01,  1.7366e-01],\n",
      "         ...,\n",
      "         [-6.3774e-04,  2.0391e-01, -1.3485e-01,  ..., -3.5744e-01,\n",
      "          -4.2343e-01,  4.2379e-01],\n",
      "         [-2.3287e-02,  1.7800e-01, -5.5447e-01,  ..., -9.3682e-02,\n",
      "          -4.3023e-01,  7.1890e-01],\n",
      "         [-1.3323e-01,  2.7498e-01, -7.0822e-01,  ..., -2.9417e-02,\n",
      "           8.1408e-02,  7.9632e-01]],\n",
      "\n",
      "        [[ 3.7153e-01,  8.3098e-02, -6.0530e-01,  ..., -1.9350e-01,\n",
      "          -2.8387e-01,  8.6194e-01],\n",
      "         [-5.4759e-02, -1.3109e-01, -4.4392e-01,  ..., -1.9287e-02,\n",
      "          -6.9868e-02,  7.2008e-01],\n",
      "         [ 3.4357e-01,  1.9348e-01, -4.0756e-01,  ..., -1.9240e-01,\n",
      "          -2.3740e-01,  4.8081e-01],\n",
      "         ...,\n",
      "         [-1.4556e-01,  3.1860e-01, -1.9200e-01,  ..., -4.9166e-01,\n",
      "          -3.8851e-01,  3.6934e-01],\n",
      "         [-9.1215e-02, -8.1214e-03, -5.5317e-01,  ...,  1.1621e-01,\n",
      "          -4.9617e-01,  9.3915e-01],\n",
      "         [-1.2010e-01, -1.3821e-02, -6.3022e-01,  ..., -4.6932e-01,\n",
      "          -3.3546e-01,  8.9640e-01]],\n",
      "\n",
      "        [[ 4.3851e-01, -1.6485e-01, -5.8028e-01,  ..., -3.2063e-01,\n",
      "           3.9659e-01,  3.8959e-01],\n",
      "         [ 1.1940e-01, -2.2827e-01, -9.6652e-02,  ...,  3.8832e-02,\n",
      "           2.1349e-01, -1.2460e-01],\n",
      "         [ 1.8254e-01,  7.2902e-03, -1.2971e-01,  ...,  2.3708e-02,\n",
      "          -9.8142e-02, -6.3312e-01],\n",
      "         ...,\n",
      "         [-5.9732e-03,  2.4677e-01, -1.9369e-01,  ..., -6.0112e-01,\n",
      "           5.6031e-01,  4.8400e-01],\n",
      "         [ 4.6474e-02, -3.4484e-02, -5.3440e-01,  ..., -2.7861e-01,\n",
      "           5.8412e-01,  4.1951e-01],\n",
      "         [-1.1398e-02,  1.9156e-01, -3.7767e-01,  ..., -3.5741e-01,\n",
      "           5.4736e-01,  2.5101e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9500721096992493\n",
      "Model outputs:  tensor([[[ 0.0311,  0.2598, -0.5973,  ..., -0.2173,  0.1423,  0.5091],\n",
      "         [ 0.0772,  0.2178, -0.4974,  ..., -0.1271,  0.2294,  0.1723],\n",
      "         [-0.0337,  0.2847, -0.6392,  ..., -0.0167,  0.2907,  0.0356],\n",
      "         ...,\n",
      "         [-0.1908,  0.6314, -0.7990,  ..., -0.0281,  0.3055,  0.0419],\n",
      "         [ 0.0929,  0.1191, -0.6783,  ...,  0.3535,  0.3731,  0.3377],\n",
      "         [ 0.0042,  0.1971, -0.8518,  ...,  0.1104, -0.0508,  0.5037]],\n",
      "\n",
      "        [[-0.4435,  0.3086, -0.3313,  ...,  0.1272,  0.3435,  0.0477],\n",
      "         [-0.4141,  0.5747, -0.3038,  ...,  0.2172,  0.4528, -0.1524],\n",
      "         [-0.5078,  0.3363, -0.3642,  ...,  0.1657,  0.5987, -0.1473],\n",
      "         ...,\n",
      "         [-0.4944,  0.5234, -0.3081,  ...,  0.3608,  0.3043,  0.1541],\n",
      "         [-0.4323,  0.2088, -0.2795,  ...,  0.5770,  0.2805,  0.1193],\n",
      "         [-0.5739,  0.5222, -0.2883,  ...,  0.4823,  0.4211,  0.2234]],\n",
      "\n",
      "        [[-0.3611,  0.4671, -0.6935,  ..., -0.3456, -0.0427,  0.3739],\n",
      "         [-0.4000,  0.4459, -0.7508,  ..., -0.2935,  0.3650,  0.3893],\n",
      "         [-0.4518,  0.6666, -0.4639,  ...,  0.0259,  0.1547,  0.0172],\n",
      "         ...,\n",
      "         [-0.5103,  0.5517, -0.7656,  ..., -0.1677,  0.3742,  0.2803],\n",
      "         [-0.0551,  0.0567, -0.6916,  ...,  0.1173,  0.2991,  0.4798],\n",
      "         [-0.5625,  0.2983, -0.7843,  ...,  0.0799,  0.2231,  0.2170]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0784, -0.1075, -0.5413,  ..., -0.4278, -0.0290,  0.4898],\n",
      "         [ 0.0553,  0.0768, -0.3180,  ..., -0.3074,  0.0513,  0.3533],\n",
      "         [-0.1059,  0.2939, -0.4746,  ..., -0.6222,  0.2152,  0.7145],\n",
      "         ...,\n",
      "         [ 0.0815,  0.1314, -0.5637,  ..., -0.3700,  0.1736,  0.4753],\n",
      "         [-0.1358, -0.0917, -0.3707,  ..., -0.2847,  0.0052,  0.7192],\n",
      "         [-0.0477, -0.1294, -0.9283,  ..., -0.5710, -0.1551,  0.7005]],\n",
      "\n",
      "        [[ 0.2068, -0.0316, -0.4348,  ..., -0.4418, -0.2066,  0.7916],\n",
      "         [ 0.1080,  0.1824, -0.2338,  ..., -0.7634, -0.1909,  0.5310],\n",
      "         [-0.0840,  0.1341, -0.3326,  ..., -0.5989, -0.1780,  0.5743],\n",
      "         ...,\n",
      "         [ 0.2523,  0.2575, -0.3885,  ..., -0.2401,  0.1139,  0.3773],\n",
      "         [ 0.0065, -0.1076, -0.1399,  ..., -0.3993, -0.0447,  0.6590],\n",
      "         [ 0.0638,  0.2226, -0.5039,  ..., -0.4404, -0.2313,  0.7757]],\n",
      "\n",
      "        [[ 0.1608,  0.2419, -0.7577,  ..., -0.2854,  0.1379,  0.4133],\n",
      "         [ 0.1976,  0.4020, -0.6310,  ..., -0.1823,  0.2953,  0.3223],\n",
      "         [-0.1663,  0.2676, -0.3196,  ..., -0.1407,  0.2275,  0.2061],\n",
      "         ...,\n",
      "         [-0.1378,  0.4366, -0.7070,  ..., -0.0826,  0.1655,  0.4286],\n",
      "         [-0.2289,  0.0715, -0.4800,  ...,  0.1580,  0.3164,  0.1290],\n",
      "         [-0.0081,  0.4294, -0.6166,  ..., -0.1246, -0.0724,  0.3018]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.263318657875061\n",
      "Model outputs:  tensor([[[-0.1384, -0.1010, -0.4577,  ..., -0.3241, -0.5798,  0.5511],\n",
      "         [ 0.3030, -0.1419, -0.5581,  ..., -0.1333, -0.2480,  0.6699],\n",
      "         [ 0.2437,  0.2461, -0.0716,  ..., -0.5053, -0.3268,  0.2821],\n",
      "         ...,\n",
      "         [ 0.0206,  0.0499, -0.5934,  ..., -0.0563, -0.1913,  0.8463],\n",
      "         [ 0.1503, -0.0593, -0.4623,  ..., -0.0825, -0.1599,  0.5948],\n",
      "         [-0.0639,  0.2169, -0.4028,  ..., -0.3080, -0.3962,  0.6745]],\n",
      "\n",
      "        [[-0.3684,  0.1062, -0.0782,  ...,  0.6292,  0.1983,  0.0313],\n",
      "         [-0.1069,  0.1811, -0.0311,  ...,  0.6124,  0.4421, -0.1708],\n",
      "         [-0.2954,  0.3111,  0.5138,  ...,  0.7106, -0.1013, -0.1159],\n",
      "         ...,\n",
      "         [-0.2226, -0.0080, -0.4146,  ...,  0.8885,  0.2166,  0.1051],\n",
      "         [-0.3123,  0.5104, -0.2278,  ...,  0.9813,  0.3924,  0.0595],\n",
      "         [-0.5676,  0.4763, -0.3115,  ...,  0.7204,  0.0739,  0.0685]],\n",
      "\n",
      "        [[ 0.1847, -0.0585, -0.5252,  ..., -0.7766,  0.5700,  0.5953],\n",
      "         [ 0.0596, -0.0219, -0.2586,  ..., -0.4224,  0.8461,  0.8050],\n",
      "         [ 0.0579,  0.1831,  0.0663,  ..., -0.5650,  0.6568,  0.6365],\n",
      "         ...,\n",
      "         [-0.0858,  0.2257, -0.8365,  ..., -0.2493,  0.4647,  0.9647],\n",
      "         [ 0.1078,  0.2352, -0.4916,  ..., -0.4512,  0.6945,  0.7696],\n",
      "         [-0.3644,  0.3514, -0.4792,  ..., -0.4289,  0.3320,  1.0816]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3848,  0.1784, -0.1995,  ...,  0.6894,  0.1446,  0.0690],\n",
      "         [-0.1139,  0.0201,  0.0955,  ...,  0.4627,  0.2328,  0.0457],\n",
      "         [-0.1169,  0.4538,  0.4558,  ...,  0.5089,  0.3185, -0.0247],\n",
      "         ...,\n",
      "         [-0.2728, -0.1671, -0.3383,  ...,  0.7170,  0.3049,  0.2081],\n",
      "         [-0.2698,  0.4389, -0.2447,  ...,  0.8343,  0.5334,  0.0137],\n",
      "         [-0.7750,  0.5118, -0.1624,  ...,  0.5752, -0.0177,  0.1599]],\n",
      "\n",
      "        [[-0.0700,  0.3420, -0.6307,  ...,  0.4912, -0.0532,  0.1502],\n",
      "         [-0.0024,  0.2255, -0.5199,  ...,  0.6610,  0.1788,  0.1460],\n",
      "         [-0.0987,  0.4878, -0.0800,  ...,  0.5925, -0.0684,  0.0474],\n",
      "         ...,\n",
      "         [-0.1267,  0.2160, -1.1142,  ...,  1.0704,  0.3184,  0.5470],\n",
      "         [-0.1279,  0.4189, -0.7819,  ...,  0.6696,  0.0768,  0.4104],\n",
      "         [-0.4330,  0.6306, -0.8503,  ...,  0.8922, -0.3839,  0.1954]],\n",
      "\n",
      "        [[ 0.1276,  0.1677, -0.3094,  ..., -0.3150, -0.1503,  0.5579],\n",
      "         [ 0.3150, -0.1863, -0.2442,  ..., -0.2495, -0.1837,  0.4680],\n",
      "         [ 0.1060,  0.0474,  0.2434,  ..., -0.2854, -0.0975,  0.5832],\n",
      "         ...,\n",
      "         [-0.1114,  0.2575, -0.7828,  ...,  0.1946, -0.1534,  0.7540],\n",
      "         [ 0.4452,  0.2000, -0.6165,  ..., -0.0703, -0.0031,  0.7727],\n",
      "         [ 0.0802,  0.1656, -0.7761,  ..., -0.0836, -0.2657,  0.7483]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.4462138414382935\n",
      "Model outputs:  tensor([[[-0.4055,  0.5894, -0.4410,  ...,  0.0022,  0.5402, -0.0718],\n",
      "         [-0.2439,  0.6447, -0.5128,  ..., -0.2297,  0.2328, -0.1033],\n",
      "         [-0.6588,  0.7290, -0.7044,  ...,  0.0426,  0.7015,  0.0428],\n",
      "         ...,\n",
      "         [-0.7655,  0.6167, -0.7249,  ...,  0.0497,  0.4077, -0.0370],\n",
      "         [-0.1905,  0.5662, -0.5791,  ..., -0.1962,  0.5858,  0.0778],\n",
      "         [-0.6152,  0.4951, -0.6380,  ..., -0.3206,  0.3362,  0.0782]],\n",
      "\n",
      "        [[ 0.3517, -0.0248, -0.0046,  ..., -0.5541, -0.1600,  0.4479],\n",
      "         [ 0.4210,  0.0053, -0.0077,  ..., -0.5773, -0.0570,  0.1385],\n",
      "         [ 0.1008,  0.1237, -0.2272,  ..., -0.7362,  0.1117,  0.3251],\n",
      "         ...,\n",
      "         [ 0.2033,  0.0933, -0.0955,  ..., -0.7280, -0.0727,  0.3882],\n",
      "         [ 0.3199,  0.0375, -0.1971,  ..., -0.8087, -0.0075,  0.7024],\n",
      "         [ 0.0607,  0.1602, -0.1515,  ..., -0.7852,  0.1931,  0.4436]],\n",
      "\n",
      "        [[-0.0045,  0.0489, -0.3378,  ..., -1.1069,  0.4614,  0.4501],\n",
      "         [ 0.2345,  0.0223, -0.4852,  ..., -0.8132,  0.8829,  0.2983],\n",
      "         [ 0.2834,  0.3108, -0.1814,  ..., -0.9158,  0.4742, -0.0801],\n",
      "         ...,\n",
      "         [ 0.0232,  0.1933, -0.6287,  ..., -1.0427,  0.6040,  0.6637],\n",
      "         [ 0.0483,  0.1937, -0.5094,  ..., -1.0116,  0.6524,  0.7225],\n",
      "         [ 0.1236,  0.4045, -0.4873,  ..., -1.0093,  0.6961,  0.5821]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2936,  0.3954, -0.2854,  ..., -0.0753,  0.2848, -0.2921],\n",
      "         [-0.1829,  0.3711, -0.5092,  ...,  0.0341,  0.4321, -0.3088],\n",
      "         [-0.2557,  0.4289, -0.0961,  ...,  0.3654,  0.4852, -0.3322],\n",
      "         ...,\n",
      "         [-0.0492,  0.4241, -0.2829,  ..., -0.1166,  0.1767, -0.2178],\n",
      "         [-0.4221,  0.5420, -0.2569,  ...,  0.2545,  0.4055, -0.3835],\n",
      "         [-0.2266,  0.3322, -0.5640,  ...,  0.2704,  0.1195, -0.2721]],\n",
      "\n",
      "        [[-0.2708,  0.7177, -0.3284,  ..., -0.2518,  0.1518,  0.0838],\n",
      "         [-0.3022,  0.4369, -0.3387,  ..., -0.1110,  0.2146, -0.0084],\n",
      "         [-0.5320,  0.6724, -0.5441,  ..., -0.1693,  0.5220,  0.0702],\n",
      "         ...,\n",
      "         [-0.3319,  0.2416, -0.4814,  ..., -0.1714,  0.2441,  0.3804],\n",
      "         [-0.0883,  0.6422, -0.3195,  ..., -0.0858,  0.1662,  0.0362],\n",
      "         [-0.5131,  0.3831, -0.5117,  ..., -0.4097,  0.4295,  0.2906]],\n",
      "\n",
      "        [[-0.3209,  0.4047, -0.5028,  ..., -0.0920,  0.1724, -0.2868],\n",
      "         [-0.3036,  0.4918, -0.2490,  ...,  0.1092,  0.2198, -0.3623],\n",
      "         [-0.5695,  0.5631, -0.0815,  ...,  0.2219,  0.2501, -0.2498],\n",
      "         ...,\n",
      "         [-0.2609,  0.4178, -0.1478,  ...,  0.1134,  0.4170, -0.3186],\n",
      "         [-0.1515,  0.1446,  0.0037,  ..., -0.1617,  0.2936, -0.3503],\n",
      "         [-0.2769,  0.5430, -0.2366,  ...,  0.1808,  0.2551, -0.0801]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3797343969345093\n",
      "Model outputs:  tensor([[[ 0.1560,  0.0980, -0.4803,  ...,  0.1475, -0.1417,  0.0032],\n",
      "         [-0.0552, -0.0433, -0.7954,  ...,  0.2218,  0.0819,  0.3900],\n",
      "         [-0.0025,  0.4436, -0.7894,  ...,  0.3079,  0.1383,  0.4221],\n",
      "         ...,\n",
      "         [-0.3352,  0.3799, -0.7114,  ...,  0.4637, -0.0201,  0.3945],\n",
      "         [ 0.0641,  0.5936, -0.9962,  ...,  0.1395, -0.0094,  0.1853],\n",
      "         [-0.3691,  0.6906, -0.6895,  ..., -0.0526, -0.2864,  0.1065]],\n",
      "\n",
      "        [[ 0.1114,  0.0534, -0.4175,  ..., -0.0435, -0.1355,  0.1354],\n",
      "         [-0.2769,  0.4752, -0.8985,  ...,  0.2352, -0.1080,  0.2443],\n",
      "         [-0.0338,  0.2585, -0.8346,  ..., -0.0157,  0.1239,  0.2030],\n",
      "         ...,\n",
      "         [-0.3783,  0.4037, -0.7005,  ..., -0.1717,  0.1377,  0.1969],\n",
      "         [-0.2574,  0.5374, -0.7261,  ...,  0.1497, -0.2147, -0.0592],\n",
      "         [-0.3593,  0.3701, -0.6409,  ..., -0.2821, -0.1402,  0.2857]],\n",
      "\n",
      "        [[-0.2149,  0.3786, -0.2557,  ...,  0.2946,  0.3440, -0.1455],\n",
      "         [-0.6950,  0.5272, -0.7250,  ...,  0.4735,  0.0978,  0.4384],\n",
      "         [-0.4304,  0.6090, -0.5827,  ...,  0.4744,  0.2654,  0.1826],\n",
      "         ...,\n",
      "         [-0.4433,  0.3332, -0.2764,  ...,  0.4169,  0.3772,  0.0185],\n",
      "         [-0.6343,  0.6059, -0.4472,  ...,  0.5209,  0.0395, -0.0589],\n",
      "         [-0.7893,  0.6257, -0.1188,  ...,  0.2870,  0.1660,  0.2053]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.4074, -0.1888,  0.1281,  ...,  0.4409, -0.1690, -0.6936],\n",
      "         [-0.1668,  0.1158, -0.0665,  ...,  0.2965,  0.1099, -0.7257],\n",
      "         [ 0.1245, -0.1762, -0.2776,  ...,  0.3274, -0.3343, -0.3158],\n",
      "         ...,\n",
      "         [-0.1284, -0.0990, -0.0924,  ...,  0.1653, -0.1866, -0.3259],\n",
      "         [ 0.4263, -0.0112,  0.1090,  ...,  0.4277,  0.0135, -0.4878],\n",
      "         [-0.0510,  0.1014,  0.0085,  ...,  0.4153, -0.3281, -0.5566]],\n",
      "\n",
      "        [[ 0.3677, -0.2930, -0.0563,  ..., -0.2883, -0.1279,  0.2475],\n",
      "         [ 0.2994, -0.0049, -0.4435,  ..., -0.4552, -0.1960,  0.7121],\n",
      "         [ 0.1447,  0.0442, -0.2056,  ..., -0.1824, -0.1377,  0.6573],\n",
      "         ...,\n",
      "         [-0.0036,  0.2072, -0.1115,  ..., -0.2119, -0.4479,  0.6188],\n",
      "         [ 0.1343,  0.0858, -0.4025,  ..., -0.2960, -0.1383,  0.6011],\n",
      "         [-0.0212,  0.1229, -0.2253,  ..., -0.3720, -0.3597,  0.5763]],\n",
      "\n",
      "        [[ 0.3064,  0.0182, -0.5805,  ...,  0.5729,  0.1112,  0.2085],\n",
      "         [-0.0395,  0.4016, -0.9325,  ...,  0.6196,  0.1161,  0.3129],\n",
      "         [-0.2272,  0.4622, -1.0472,  ...,  0.2964,  0.2101,  0.2251],\n",
      "         ...,\n",
      "         [-0.2618,  0.4665, -0.7223,  ...,  0.7688,  0.2290,  0.3205],\n",
      "         [-0.4605,  0.5807, -0.6774,  ...,  0.7029,  0.0121, -0.1001],\n",
      "         [-0.4887,  0.7016, -0.6965,  ...,  0.6241, -0.0099,  0.3663]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9111332297325134\n",
      "Model outputs:  tensor([[[-0.5280,  0.5416, -0.2667,  ...,  0.4569,  0.4133,  0.0202],\n",
      "         [-0.4473,  0.7086, -0.4649,  ...,  0.3650,  0.2543, -0.5414],\n",
      "         [-0.6360,  0.5352, -0.2839,  ...,  0.2586,  0.3007, -0.2300],\n",
      "         ...,\n",
      "         [-0.4264,  0.6636, -0.0843,  ...,  0.1957,  0.3976, -0.2734],\n",
      "         [-0.4404,  0.8142, -0.4408,  ...,  0.1517,  0.3371, -0.3364],\n",
      "         [-0.4753,  0.6614, -0.1388,  ...,  0.3447,  0.2540, -0.4016]],\n",
      "\n",
      "        [[ 0.0983,  0.3683, -0.3063,  ..., -0.2848,  0.0058,  0.5479],\n",
      "         [ 0.0497,  0.1339, -0.1862,  ..., -0.3585, -0.1506,  0.1326],\n",
      "         [-0.1475,  0.3710, -0.6035,  ..., -0.4515, -0.0394,  0.3998],\n",
      "         ...,\n",
      "         [-0.1320,  0.5146, -0.5683,  ..., -0.4837,  0.2038,  0.2232],\n",
      "         [ 0.2039,  0.1829, -0.1313,  ..., -0.4869,  0.2171,  0.0442],\n",
      "         [ 0.1083,  0.4174, -0.1988,  ..., -0.4387, -0.0980,  0.1456]],\n",
      "\n",
      "        [[-0.5024,  0.5738, -0.3452,  ...,  0.3085,  0.4310,  0.0164],\n",
      "         [-0.4880,  0.7637, -0.4796,  ...,  0.0789,  0.4407, -0.0175],\n",
      "         [-0.5685,  0.5479, -0.4326,  ...,  0.0372,  0.0602, -0.0152],\n",
      "         ...,\n",
      "         [-0.3425,  0.5391, -0.1963,  ...,  0.1248,  0.3945, -0.0832],\n",
      "         [-0.4035,  0.7521, -0.1684,  ..., -0.0952,  0.0603, -0.1893],\n",
      "         [-0.2534,  0.7132, -0.2721,  ...,  0.3425,  0.1608, -0.0661]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3186,  0.4819, -0.8052,  ...,  0.3616,  0.1647,  0.0099],\n",
      "         [-0.5099,  0.6748, -0.7687,  ...,  0.2585,  0.3387, -0.1168],\n",
      "         [-0.5873,  0.7150, -0.7478,  ...,  0.2642,  0.3036,  0.1497],\n",
      "         ...,\n",
      "         [-0.3713,  0.5359, -0.7688,  ...,  0.2422,  0.3706, -0.2085],\n",
      "         [-0.4248,  0.9029, -0.5738,  ...,  0.2180,  0.2200, -0.6705],\n",
      "         [-0.3495,  0.7866, -0.8013,  ...,  0.3758,  0.2253, -0.1409]],\n",
      "\n",
      "        [[-0.2069,  0.5165, -0.7916,  ..., -0.0702,  0.2133,  0.2280],\n",
      "         [-0.1510,  0.1285, -0.4186,  ..., -0.3530, -0.0844,  0.1555],\n",
      "         [-0.0444,  0.3880, -0.3540,  ..., -0.5567, -0.0684,  0.2078],\n",
      "         ...,\n",
      "         [-0.0432,  0.3849, -0.4596,  ..., -0.5151, -0.0158,  0.1929],\n",
      "         [-0.0607,  0.2370, -0.1863,  ..., -0.4028, -0.1344,  0.0688],\n",
      "         [ 0.0188,  0.2310, -0.5424,  ..., -0.5765, -0.1188,  0.1029]],\n",
      "\n",
      "        [[-0.2335,  0.5634, -0.9127,  ...,  0.1341,  0.1716, -0.0240],\n",
      "         [-0.2797,  0.6652, -0.7622,  ...,  0.0155, -0.1938,  0.1233],\n",
      "         [-0.2776,  0.4776, -0.8300,  ..., -0.2050, -0.2541,  0.0722],\n",
      "         ...,\n",
      "         [-0.0582,  0.5747, -0.5331,  ..., -0.0743,  0.2634, -0.1671],\n",
      "         [-0.1470,  0.6989, -0.7608,  ...,  0.0504,  0.1686, -0.3652],\n",
      "         [-0.0456,  0.6786, -0.6297,  ..., -0.0254,  0.3417, -0.0618]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1986877918243408\n",
      "Model outputs:  tensor([[[ 0.2746,  0.2482, -0.4510,  ..., -0.3839,  0.1549,  0.4614],\n",
      "         [ 0.2544,  0.1592, -0.4423,  ...,  0.0359,  0.1829,  0.5354],\n",
      "         [ 0.1850,  0.1314, -0.6519,  ..., -0.3137,  0.0944,  0.6538],\n",
      "         ...,\n",
      "         [ 0.3388,  0.1009, -0.7284,  ..., -0.1229, -0.0960,  0.4212],\n",
      "         [ 0.3848,  0.1990, -0.4905,  ..., -0.3317, -0.0493,  0.1947],\n",
      "         [ 0.2006,  0.3373, -0.6777,  ..., -0.0495,  0.2761,  0.3370]],\n",
      "\n",
      "        [[-0.3284,  0.6067, -0.1579,  ...,  0.7998,  0.3623, -0.0314],\n",
      "         [-0.2666,  0.3114, -0.2119,  ...,  0.5700,  0.5338,  0.0538],\n",
      "         [-0.1252,  0.1081, -0.1837,  ...,  0.3452,  0.4442, -0.0024],\n",
      "         ...,\n",
      "         [-0.1682,  0.1334, -0.2889,  ...,  0.6505,  0.2591, -0.0561],\n",
      "         [-0.4283,  0.2747, -0.0600,  ...,  0.4373,  0.3670, -0.0736],\n",
      "         [-0.1806,  0.5282, -0.3918,  ...,  0.7473,  0.4948,  0.0885]],\n",
      "\n",
      "        [[ 0.1149,  0.2145, -0.6711,  ...,  0.0436, -0.1441,  0.2798],\n",
      "         [ 0.1665,  0.0264, -0.9267,  ...,  0.3304,  0.3692,  0.2328],\n",
      "         [-0.0827,  0.2309, -0.9877,  ...,  0.4141,  0.3147,  0.2195],\n",
      "         ...,\n",
      "         [ 0.1565,  0.1949, -1.0427,  ...,  0.3223, -0.0305,  0.0397],\n",
      "         [ 0.1190,  0.1303, -0.6580,  ...,  0.0876, -0.1583,  0.2056],\n",
      "         [ 0.2045,  0.3357, -0.7727,  ...,  0.1622,  0.3081,  0.2972]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1947, -0.0962,  0.0682,  ...,  0.4082,  0.1094, -0.3755],\n",
      "         [ 0.6133, -0.0528, -0.0156,  ...,  0.4480,  0.0778, -0.3632],\n",
      "         [ 0.5422, -0.1115, -0.1121,  ...,  0.4099,  0.0500, -0.1797],\n",
      "         ...,\n",
      "         [ 0.5766, -0.3761, -0.0628,  ...,  0.5857,  0.0560, -0.4759],\n",
      "         [ 0.3752,  0.0382,  0.0898,  ...,  0.4514,  0.1940, -0.7970],\n",
      "         [ 0.2756, -0.1655,  0.0658,  ...,  0.5154, -0.1073, -0.5105]],\n",
      "\n",
      "        [[-0.5170,  0.4197, -0.4492,  ...,  0.2233,  0.1299,  0.4529],\n",
      "         [-0.2691,  0.3754, -0.5643,  ...,  0.6699,  0.2303,  0.2095],\n",
      "         [-0.3830,  0.3621, -0.7027,  ...,  0.0515,  0.2429,  0.4057],\n",
      "         ...,\n",
      "         [-0.3219,  0.4144, -0.7654,  ...,  0.2730,  0.3560, -0.0465],\n",
      "         [-0.4005,  0.3637, -0.4948,  ...,  0.4102,  0.2616,  0.2244],\n",
      "         [-0.2904,  0.4829, -0.5596,  ...,  0.4214,  0.3909,  0.4808]],\n",
      "\n",
      "        [[ 0.4231, -0.0597, -0.0248,  ...,  0.2106,  0.0057, -0.4548],\n",
      "         [ 0.4107, -0.0429, -0.1525,  ...,  0.2638,  0.2262, -0.2893],\n",
      "         [ 0.7301,  0.0123, -0.1154,  ...,  0.3584,  0.3092, -0.2162],\n",
      "         ...,\n",
      "         [ 0.5032, -0.3918, -0.0951,  ...,  0.0220,  0.2675, -0.3430],\n",
      "         [ 0.4243, -0.1565,  0.0197,  ...,  0.0164,  0.0951, -0.5758],\n",
      "         [ 0.3493,  0.0178, -0.4642,  ..., -0.0114,  0.0165, -0.1739]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.4761313199996948\n",
      "Model outputs:  tensor([[[-0.4642,  0.5342, -0.2378,  ...,  0.2691,  0.2884,  0.0975],\n",
      "         [-0.4095,  0.4378, -0.2299,  ...,  0.3740,  0.3753, -0.0726],\n",
      "         [-0.1531,  0.7983,  0.0367,  ...,  0.4799,  0.7018, -0.2466],\n",
      "         ...,\n",
      "         [-0.2736,  0.7335, -0.1184,  ...,  0.1142,  0.8844, -0.4239],\n",
      "         [-0.2755,  0.8467, -0.3739,  ...,  0.3570,  0.5222, -0.0897],\n",
      "         [-0.3207,  0.5930, -0.2069,  ...,  0.1349,  0.3196, -0.0554]],\n",
      "\n",
      "        [[-0.2412,  0.2933, -0.6007,  ..., -0.7397,  0.5157,  0.4594],\n",
      "         [ 0.0502,  0.3344, -0.6351,  ..., -0.9634,  0.3943,  1.0266],\n",
      "         [ 0.0152,  0.1421, -0.5508,  ..., -0.7545,  0.6551,  0.5984],\n",
      "         ...,\n",
      "         [ 0.1415,  0.2360, -0.5209,  ..., -0.9855,  0.6305,  0.2131],\n",
      "         [ 0.0551,  0.3787, -0.4742,  ..., -1.0822,  0.8845,  0.3221],\n",
      "         [-0.1182,  0.2279, -0.3289,  ..., -0.8045,  0.9645,  0.4658]],\n",
      "\n",
      "        [[-0.6976,  0.2036, -0.1729,  ...,  0.2806,  0.4321, -0.1829],\n",
      "         [-0.6458,  0.6121, -0.4382,  ...,  0.4439,  0.4406, -0.1117],\n",
      "         [-0.5110,  0.4000, -0.3356,  ...,  0.5115,  0.2663, -0.1441],\n",
      "         ...,\n",
      "         [-0.3282,  0.3580, -0.2064,  ...,  0.0610,  0.4727, -0.1202],\n",
      "         [-0.4426,  0.4791, -0.1219,  ...,  0.0151,  0.5494, -0.0780],\n",
      "         [-0.3966,  0.7068, -0.1578,  ...,  0.1544,  0.5128, -0.2613]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3697,  0.0827, -0.3932,  ..., -0.2383, -0.1999,  0.6509],\n",
      "         [ 0.0266,  0.2010, -0.2878,  ..., -0.6613, -0.1382,  0.7117],\n",
      "         [-0.0794,  0.2197, -0.2606,  ..., -0.2844, -0.2907,  0.3389],\n",
      "         ...,\n",
      "         [ 0.1096,  0.0315, -0.2923,  ..., -0.6534, -0.0476,  0.1412],\n",
      "         [ 0.2433,  0.3516, -0.2042,  ..., -0.6457, -0.1831,  0.3142],\n",
      "         [ 0.1740,  0.1418, -0.4142,  ..., -0.6146,  0.0916,  0.3085]],\n",
      "\n",
      "        [[-0.7514,  0.3721, -0.6022,  ...,  0.4967,  0.4411, -0.0538],\n",
      "         [-0.4847,  0.4942, -0.2060,  ...,  0.1546,  0.2788, -0.0843],\n",
      "         [-0.4191,  0.5047, -0.2913,  ...,  0.1362,  0.5066, -0.0064],\n",
      "         ...,\n",
      "         [-0.2284,  0.5729, -0.1141,  ..., -0.0868,  0.3418, -0.3357],\n",
      "         [-0.3053,  0.6110, -0.2313,  ...,  0.3072,  0.5254, -0.3258],\n",
      "         [-0.6599,  0.5630, -0.2704,  ...,  0.1445,  0.5542, -0.3699]],\n",
      "\n",
      "        [[-0.5772,  0.6216, -0.8987,  ...,  0.4910,  0.3804,  0.1992],\n",
      "         [-0.6910,  0.5854, -0.6591,  ...,  0.2962,  0.2415,  0.0483],\n",
      "         [-0.5036,  0.6820, -0.6257,  ...,  0.5773,  0.5111,  0.1981],\n",
      "         ...,\n",
      "         [-0.1408,  0.4394, -0.8321,  ..., -0.1154,  0.4336,  0.0598],\n",
      "         [-0.5045,  0.6491, -0.6551,  ...,  0.2106,  0.2959,  0.0017],\n",
      "         [-0.2055,  0.6737, -0.7565,  ...,  0.2344,  0.4938,  0.1296]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2958170175552368\n",
      "Model outputs:  tensor([[[ 0.3773,  0.1196, -0.5839,  ..., -0.4464,  0.6784,  0.2879],\n",
      "         [ 0.1494,  0.2785, -0.4698,  ..., -0.3353,  0.7253,  0.2344],\n",
      "         [ 0.3432,  0.4567, -0.4357,  ..., -0.7460,  0.6291,  0.3703],\n",
      "         ...,\n",
      "         [ 0.3752, -0.0181, -0.4707,  ..., -0.1852,  0.6636,  0.4246],\n",
      "         [ 0.3668,  0.3705, -0.2986,  ..., -0.5106,  0.3817,  0.3505],\n",
      "         [ 0.3623, -0.0039, -0.5359,  ..., -0.5178,  0.5597,  0.5346]],\n",
      "\n",
      "        [[-0.5258,  0.3610, -0.6525,  ...,  0.1355,  0.5156,  0.3099],\n",
      "         [-0.3774,  0.6059, -0.4085,  ...,  0.2395,  0.6029,  0.0549],\n",
      "         [-0.2739,  0.5574, -0.2592,  ..., -0.0395,  0.3490, -0.0347],\n",
      "         ...,\n",
      "         [-0.5679,  0.2915, -0.7173,  ...,  0.4332,  0.3894,  0.3620],\n",
      "         [-0.5634,  0.5367, -0.3915,  ...,  0.0879,  0.2675,  0.1033],\n",
      "         [-0.4849,  0.3516, -0.4801,  ...,  0.0285,  0.4336,  0.1420]],\n",
      "\n",
      "        [[ 0.2296,  0.3149, -0.8012,  ...,  0.0011,  0.3950,  0.5752],\n",
      "         [ 0.1136,  0.2201, -0.7334,  ..., -0.3415,  0.1421,  0.2000],\n",
      "         [ 0.0460,  0.4851, -0.5534,  ..., -0.3055,  0.2381,  0.0084],\n",
      "         ...,\n",
      "         [ 0.1266,  0.1547, -0.8414,  ..., -0.0120, -0.0156,  0.4408],\n",
      "         [-0.0402,  0.4353, -0.6838,  ..., -0.2098,  0.0313,  0.3380],\n",
      "         [ 0.2083,  0.3103, -0.8118,  ...,  0.0765,  0.3994,  0.1297]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2137,  0.0999, -0.5652,  ..., -0.3042,  0.1144,  0.4537],\n",
      "         [ 0.2605,  0.2747, -0.4005,  ..., -0.4621,  0.1220,  0.5052],\n",
      "         [ 0.3600,  0.2913, -0.3789,  ..., -0.5311, -0.0266,  0.3311],\n",
      "         ...,\n",
      "         [ 0.0512,  0.2049, -0.4772,  ..., -0.4187, -0.0947,  0.5396],\n",
      "         [ 0.4432,  0.1946, -0.4063,  ..., -0.3131, -0.0679,  0.4207],\n",
      "         [ 0.0453,  0.1244, -0.2013,  ..., -0.3459, -0.2207,  0.3741]],\n",
      "\n",
      "        [[ 0.2735,  0.1447, -0.3516,  ..., -0.3703,  0.2170,  0.3241],\n",
      "         [ 0.0742,  0.2509, -0.5229,  ..., -0.2568,  0.2537,  0.1510],\n",
      "         [-0.0882,  0.5306, -0.3821,  ..., -0.4267,  0.1328,  0.1835],\n",
      "         ...,\n",
      "         [ 0.3142,  0.2242, -0.6868,  ..., -0.1677,  0.3028,  0.5432],\n",
      "         [-0.0489,  0.3719, -0.6585,  ..., -0.2530,  0.1318,  0.3522],\n",
      "         [-0.1215,  0.1970, -0.8307,  ..., -0.3868,  0.0518,  0.2032]],\n",
      "\n",
      "        [[-0.3877,  0.3766, -0.7753,  ...,  0.2381,  0.4901,  0.0474],\n",
      "         [-0.2498,  0.6276, -0.4900,  ...,  0.2381,  0.5019, -0.0923],\n",
      "         [-0.7114,  0.6771, -0.6421,  ..., -0.1183,  0.3756,  0.0319],\n",
      "         ...,\n",
      "         [-0.2896,  0.3414, -0.6890,  ...,  0.3080,  0.4016,  0.1764],\n",
      "         [-0.5184,  0.5509, -0.5108,  ...,  0.4176,  0.4269,  0.0969],\n",
      "         [-0.6699,  0.6027, -0.4710,  ...,  0.3631,  0.2873,  0.1707]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3132147789001465\n",
      "Model outputs:  tensor([[[ 4.6310e-02, -1.5980e-03, -2.0781e-01,  ..., -4.8954e-01,\n",
      "          -3.2749e-01,  7.9766e-01],\n",
      "         [ 2.5340e-01,  9.0676e-02, -3.2691e-02,  ..., -3.7528e-01,\n",
      "          -2.6393e-01,  5.4728e-01],\n",
      "         [-3.4294e-02,  5.1673e-01,  4.2364e-04,  ..., -5.4023e-01,\n",
      "          -1.8349e-01,  3.2201e-01],\n",
      "         ...,\n",
      "         [ 5.1178e-01, -2.3354e-01,  1.0598e-01,  ..., -4.2581e-01,\n",
      "          -2.4604e-01,  3.7028e-01],\n",
      "         [ 1.7162e-01,  1.0991e-01, -4.2741e-01,  ..., -4.2909e-01,\n",
      "           1.5374e-02,  2.3084e-01],\n",
      "         [-7.9236e-02,  1.9853e-01, -4.5598e-01,  ..., -9.1940e-01,\n",
      "          -1.0118e-01,  4.5331e-01]],\n",
      "\n",
      "        [[-5.8424e-01,  6.2477e-01, -3.3738e-01,  ...,  9.2463e-01,\n",
      "           5.1119e-01, -1.2969e-01],\n",
      "         [ 5.5577e-02,  2.2826e-01, -1.0306e-01,  ...,  2.7755e-01,\n",
      "           2.0699e-01, -2.4398e-01],\n",
      "         [-6.2824e-01,  6.1928e-01, -1.1385e-01,  ...,  4.0708e-01,\n",
      "           2.3887e-01, -1.7462e-01],\n",
      "         ...,\n",
      "         [ 5.3077e-02,  3.2685e-01, -6.7307e-03,  ...,  1.3913e-01,\n",
      "           1.2585e-01, -2.6944e-01],\n",
      "         [-3.6378e-01,  5.9399e-01, -4.0408e-01,  ...,  2.8904e-01,\n",
      "           3.9177e-01, -1.0276e-03],\n",
      "         [-6.7441e-01,  8.6757e-01,  3.1409e-02,  ...,  2.5930e-01,\n",
      "           4.0912e-01, -2.0713e-01]],\n",
      "\n",
      "        [[ 9.0751e-02,  2.5449e-01, -5.4588e-01,  ..., -5.0477e-01,\n",
      "          -1.7712e-02,  5.2809e-01],\n",
      "         [ 6.1537e-01, -8.2510e-02, -3.0752e-02,  ..., -3.8104e-01,\n",
      "           8.6599e-02,  2.7966e-01],\n",
      "         [-4.1592e-02,  1.4804e-01, -3.2386e-01,  ..., -5.2382e-01,\n",
      "          -2.5156e-01,  3.4067e-01],\n",
      "         ...,\n",
      "         [ 4.0788e-01, -1.0178e-01, -1.3929e-02,  ..., -7.0532e-01,\n",
      "          -2.0347e-01,  2.8969e-01],\n",
      "         [ 1.1981e-01,  3.8605e-01, -3.3586e-01,  ..., -4.3839e-01,\n",
      "           1.1814e-01,  3.4408e-01],\n",
      "         [-4.3474e-03,  2.5985e-01, -2.8161e-01,  ..., -6.2274e-01,\n",
      "          -2.1757e-01,  2.1811e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.0718e-01,  4.5655e-01, -6.8855e-01,  ...,  6.6110e-01,\n",
      "           2.1697e-01,  2.3149e-01],\n",
      "         [-2.2805e-02,  1.5421e-01, -6.3222e-01,  ...,  4.2061e-01,\n",
      "           4.7336e-01, -4.2558e-01],\n",
      "         [-3.6328e-01,  7.8224e-01, -6.8442e-01,  ...,  1.2996e-01,\n",
      "           1.7347e-01,  1.0815e-01],\n",
      "         ...,\n",
      "         [ 3.3667e-01,  2.2929e-01, -7.8324e-01,  ...,  3.5453e-01,\n",
      "           4.3047e-02, -5.5602e-02],\n",
      "         [-1.5192e-01,  6.5026e-01, -5.2898e-01,  ...,  2.0632e-01,\n",
      "           5.0878e-01, -4.3317e-02],\n",
      "         [-4.9875e-01,  9.0783e-01, -8.2797e-01,  ...,  3.6383e-01,\n",
      "           2.2998e-01, -5.7866e-02]],\n",
      "\n",
      "        [[-5.6219e-01,  4.7287e-01, -4.9557e-01,  ...,  3.6947e-01,\n",
      "           1.4671e-01,  5.7110e-01],\n",
      "         [-1.5628e-01,  2.8300e-01, -3.6554e-01,  ..., -6.9304e-02,\n",
      "           3.7569e-01,  2.8969e-01],\n",
      "         [-5.4286e-01,  6.5453e-01, -3.8399e-01,  ...,  2.5131e-01,\n",
      "           2.9874e-01,  1.0756e-01],\n",
      "         ...,\n",
      "         [-2.5753e-01,  2.8365e-01, -3.5662e-01,  ..., -1.0653e-01,\n",
      "           1.3762e-01,  3.9865e-02],\n",
      "         [-3.5075e-01,  6.3743e-01, -6.5796e-01,  ...,  5.7568e-02,\n",
      "           2.3824e-01, -5.9813e-04],\n",
      "         [-4.4683e-01,  7.1345e-01, -6.0831e-01,  ..., -5.6280e-02,\n",
      "           2.8201e-01,  1.9866e-01]],\n",
      "\n",
      "        [[-6.9331e-01,  4.7847e-01,  1.0087e-01,  ...,  6.9824e-01,\n",
      "           3.6233e-01,  1.2574e-01],\n",
      "         [ 1.7319e-01,  4.3254e-01, -1.6430e-01,  ...,  2.5316e-01,\n",
      "           6.5452e-01, -2.1686e-01],\n",
      "         [-5.5787e-01,  8.9217e-01,  8.5122e-02,  ...,  2.7281e-01,\n",
      "           2.0252e-01, -1.9349e-03],\n",
      "         ...,\n",
      "         [ 4.3304e-02,  5.0941e-01, -2.4263e-01,  ...,  1.9166e-02,\n",
      "           3.2775e-01, -2.2927e-01],\n",
      "         [-2.5472e-01,  5.7353e-01, -2.9026e-01,  ...,  1.4978e-01,\n",
      "           4.6508e-01, -2.2757e-01],\n",
      "         [-2.5356e-01,  8.0135e-01,  1.6189e-01,  ...,  2.8637e-01,\n",
      "           3.0136e-01, -1.3882e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.057590365409851\n",
      "Model outputs:  tensor([[[ 0.4914,  0.1633, -0.1250,  ..., -0.6789, -0.1290,  0.0343],\n",
      "         [ 0.0676,  0.5478, -0.4184,  ..., -0.2960,  0.3112,  0.0476],\n",
      "         [ 0.0622,  0.3967, -0.3900,  ..., -0.3485,  0.3372,  0.3873],\n",
      "         ...,\n",
      "         [ 0.1720, -0.0161,  0.5076,  ..., -0.5048, -0.1757,  0.2851],\n",
      "         [ 0.0241,  0.4803,  0.3332,  ..., -0.4399, -0.3056,  0.1454],\n",
      "         [ 0.1897,  0.3534, -0.4956,  ..., -0.4917,  0.0311,  0.4225]],\n",
      "\n",
      "        [[ 0.4882, -0.0477,  0.1348,  ..., -0.1704,  0.2503, -0.7141],\n",
      "         [ 0.3649,  0.2455, -0.0183,  ...,  0.0391,  0.4231, -0.6778],\n",
      "         [ 0.3259,  0.0613,  0.1590,  ..., -0.0164,  0.1099, -0.4743],\n",
      "         ...,\n",
      "         [ 0.5806,  0.0755,  0.4643,  ..., -0.4979, -0.0543, -0.3216],\n",
      "         [ 0.2539, -0.1542,  0.5109,  ..., -0.3344, -0.0597, -0.3296],\n",
      "         [ 0.1655,  0.2740, -0.1299,  ..., -0.3007,  0.3360, -0.5350]],\n",
      "\n",
      "        [[ 0.3542,  0.1374, -0.1274,  ..., -0.7095,  0.6192, -0.1622],\n",
      "         [ 0.2640,  0.3411, -0.4067,  ..., -0.3593,  0.7111,  0.0660],\n",
      "         [ 0.2794, -0.0551, -0.4289,  ..., -0.6689,  0.6823,  0.2338],\n",
      "         ...,\n",
      "         [ 0.0318,  0.2350, -0.1251,  ...,  0.1960,  0.0292,  0.0139],\n",
      "         [-0.1732,  0.5111, -0.0318,  ...,  0.2162, -0.1543, -0.0171],\n",
      "         [-0.0887,  0.6410, -0.7921,  ...,  0.2399,  0.3605,  0.0636]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2165,  0.4441, -0.0558,  ...,  0.1636,  0.3735, -0.3158],\n",
      "         [-0.2644,  0.7809, -0.4989,  ...,  0.3187,  0.6453, -0.2833],\n",
      "         [-0.2143,  0.8949, -0.1870,  ...,  0.4996,  0.3801,  0.0541],\n",
      "         ...,\n",
      "         [-0.0774, -0.0070,  0.4382,  ...,  0.0937,  0.0644,  0.0066],\n",
      "         [-0.0735,  0.4793,  0.6111,  ...,  0.3700,  0.2819, -0.0141],\n",
      "         [-0.3238,  0.5907, -0.3473,  ...,  0.7193,  0.3672, -0.2128]],\n",
      "\n",
      "        [[ 0.4317,  0.0407, -0.3208,  ..., -0.4411,  0.0302,  0.2281],\n",
      "         [ 0.2489,  0.3802, -0.2457,  ..., -0.5848,  0.1161,  0.4615],\n",
      "         [ 0.3321,  0.3596, -0.4535,  ..., -0.5398,  0.1939,  0.5325],\n",
      "         ...,\n",
      "         [ 0.0647,  0.1056,  0.1024,  ..., -0.5121, -0.4738,  0.4249],\n",
      "         [ 0.0611,  0.3700,  0.6498,  ..., -0.5906, -0.3376,  0.1488],\n",
      "         [ 0.2709,  0.1584, -0.2228,  ..., -0.4586, -0.0731,  0.3986]],\n",
      "\n",
      "        [[ 0.2154,  0.0542, -0.1597,  ..., -0.7381, -0.0571,  0.2054],\n",
      "         [ 0.2703,  0.3310, -0.3000,  ..., -0.5742,  0.2449,  0.1529],\n",
      "         [ 0.1238,  0.3119, -0.2607,  ..., -0.5423,  0.1045,  0.1894],\n",
      "         ...,\n",
      "         [ 0.1349,  0.2169,  0.6344,  ..., -0.4682, -0.5290,  0.2882],\n",
      "         [-0.0573,  0.2773,  0.5303,  ..., -0.4386, -0.5889,  0.1965],\n",
      "         [ 0.3062,  0.4846, -0.4645,  ..., -0.4273, -0.0554,  0.5748]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2073434591293335\n",
      "Model outputs:  tensor([[[ 0.1819,  0.1636, -0.5571,  ..., -0.3084,  0.0584,  0.3760],\n",
      "         [ 0.0223,  0.1235, -0.5617,  ..., -0.3510,  0.3353,  0.7214],\n",
      "         [ 0.0666,  0.0944, -0.1676,  ..., -0.4375,  0.0426,  0.3460],\n",
      "         ...,\n",
      "         [ 0.0701, -0.2146, -0.7057,  ..., -0.1785, -0.0362,  0.4961],\n",
      "         [-0.0311, -0.0222, -0.5659,  ..., -0.4793, -0.3410,  0.6797],\n",
      "         [-0.0938,  0.2368, -0.3977,  ..., -0.3547, -0.2443,  0.4144]],\n",
      "\n",
      "        [[ 0.1942,  0.1121, -0.5615,  ..., -0.7008,  0.5416,  0.6645],\n",
      "         [ 0.0386,  0.0115, -0.2185,  ..., -0.4158,  0.5603,  0.2881],\n",
      "         [ 0.1704,  0.1285, -0.0672,  ..., -0.5883,  0.3401,  0.4525],\n",
      "         ...,\n",
      "         [-0.0983,  0.1479, -0.2556,  ..., -0.3335,  0.4675,  0.7916],\n",
      "         [-0.1692,  0.2161, -0.5640,  ..., -0.6465,  0.8413,  0.9821],\n",
      "         [ 0.1679,  0.1731, -0.2559,  ..., -0.5096,  0.6797,  0.6635]],\n",
      "\n",
      "        [[-0.4304,  0.2307, -0.6977,  ..., -0.1214,  0.0852,  0.3033],\n",
      "         [ 0.0079,  0.1510, -0.7444,  ...,  0.0597, -0.0264,  0.2782],\n",
      "         [-0.1482,  0.2622, -0.5519,  ..., -0.2864, -0.3969,  0.2012],\n",
      "         ...,\n",
      "         [-0.2676, -0.1454, -0.7034,  ...,  0.1060, -0.0766,  0.6492],\n",
      "         [-0.5832,  0.2830, -0.7249,  ..., -0.4809, -0.3675,  0.2442],\n",
      "         [-0.5663,  0.0406, -0.7518,  ..., -0.0320, -0.2377,  0.2269]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3772,  0.4493, -0.1913,  ...,  0.3962,  0.4445, -0.0843],\n",
      "         [-0.2728,  0.4990, -0.2993,  ...,  0.5066,  0.4984,  0.0497],\n",
      "         [-0.0756,  0.0982,  0.3457,  ...,  0.3228,  0.3369,  0.0120],\n",
      "         ...,\n",
      "         [-0.0989,  0.0227,  0.1872,  ...,  0.6044,  0.2491,  0.0482],\n",
      "         [-0.5804,  0.7188, -0.3630,  ...,  0.4968,  0.3685,  0.1046],\n",
      "         [-0.4123,  0.3874,  0.1322,  ...,  0.4008,  0.1496, -0.1095]],\n",
      "\n",
      "        [[ 0.2137, -0.0962, -0.3124,  ..., -0.6165, -0.1978,  0.5866],\n",
      "         [ 0.1938,  0.0169, -0.2157,  ..., -0.2994,  0.1837,  0.4144],\n",
      "         [ 0.3811,  0.0426, -0.1087,  ..., -0.4596, -0.5453,  0.7331],\n",
      "         ...,\n",
      "         [ 0.1835, -0.2291, -0.1620,  ..., -0.1151, -0.5572,  0.7200],\n",
      "         [ 0.1104,  0.0725, -0.2162,  ..., -0.6176, -0.6324,  0.7493],\n",
      "         [ 0.0965,  0.2043, -0.2948,  ..., -0.4129, -0.5268,  0.6943]],\n",
      "\n",
      "        [[-0.5414,  0.5520, -0.2786,  ...,  0.1285,  0.0240,  0.4078],\n",
      "         [-0.4268,  0.3740, -0.3290,  ...,  0.2512,  0.4619,  0.1916],\n",
      "         [-0.3554, -0.0349,  0.0100,  ...,  0.0260,  0.1014,  0.2541],\n",
      "         ...,\n",
      "         [-0.4354,  0.0895, -0.2942,  ...,  0.4288,  0.2398,  0.3294],\n",
      "         [-0.6807,  0.6507, -0.3552,  ...,  0.2137,  0.2673,  0.4773],\n",
      "         [-0.7168,  0.6734, -0.4278,  ...,  0.4385,  0.3211,  0.2733]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8888195753097534\n",
      "Model outputs:  tensor([[[-0.5574,  0.4325, -0.3427,  ...,  0.1787,  0.3085, -0.0944],\n",
      "         [-0.5436,  0.5375, -0.3349,  ...,  0.2660,  0.4789,  0.1867],\n",
      "         [-0.4335,  0.4770, -0.3662,  ...,  0.7377,  0.5049,  0.1522],\n",
      "         ...,\n",
      "         [-0.6024,  0.4318, -0.5420,  ...,  0.4659,  0.4555,  0.3860],\n",
      "         [-0.5941,  0.4586, -0.1980,  ...,  0.6895,  0.2701,  0.0976],\n",
      "         [-0.5039,  0.3074, -0.1379,  ...,  0.6241,  0.1410,  0.3250]],\n",
      "\n",
      "        [[-0.4006,  0.5185,  0.0192,  ...,  0.5945,  0.5201, -0.0930],\n",
      "         [-0.1059,  0.5156, -0.1090,  ...,  0.7150,  0.3110, -0.0643],\n",
      "         [ 0.0477,  0.2996, -0.2527,  ...,  0.8624,  0.4905, -0.1803],\n",
      "         ...,\n",
      "         [-0.2238,  0.5641, -0.3240,  ...,  0.8281,  0.2234,  0.2336],\n",
      "         [-0.4854,  0.7029, -0.5163,  ...,  0.4937,  0.4252, -0.1196],\n",
      "         [-0.3060,  0.3944, -0.3216,  ...,  0.6724,  0.2050,  0.1137]],\n",
      "\n",
      "        [[ 0.1598,  0.1193, -0.1658,  ..., -0.6863,  0.3017, -0.0811],\n",
      "         [ 0.1489,  0.1520, -0.1352,  ..., -0.5553,  0.4573,  0.1195],\n",
      "         [ 0.1651, -0.1299, -0.3801,  ..., -0.4720,  0.4210,  0.1224],\n",
      "         ...,\n",
      "         [-0.1066, -0.0441, -0.5042,  ..., -0.4531,  0.1179,  0.4250],\n",
      "         [ 0.1978,  0.0744, -0.2349,  ..., -0.6773,  0.1289,  0.1348],\n",
      "         [-0.1696,  0.0337, -0.3669,  ..., -0.5433,  0.1468,  0.4410]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0408,  0.4782, -0.6975,  ...,  0.1688,  0.2868, -0.0347],\n",
      "         [ 0.1126,  0.3679, -0.8091,  ...,  0.2452,  0.2902,  0.1378],\n",
      "         [-0.0628,  0.5063, -0.9928,  ...,  0.3189,  0.1356,  0.1824],\n",
      "         ...,\n",
      "         [ 0.2082,  0.2619, -0.7866,  ...,  0.4534,  0.3555,  0.3592],\n",
      "         [-0.0928,  0.6136, -0.9463,  ...,  0.2841,  0.1054, -0.0430],\n",
      "         [-0.1670,  0.4261, -1.0118,  ...,  0.4559,  0.0367,  0.2421]],\n",
      "\n",
      "        [[-0.3092,  0.5377, -0.4155,  ...,  0.4902,  0.4776, -0.1129],\n",
      "         [-0.4020,  0.3461, -0.1379,  ...,  0.8211,  0.2279, -0.1316],\n",
      "         [-0.4814,  0.3316, -0.2356,  ...,  0.8536,  0.1920, -0.0896],\n",
      "         ...,\n",
      "         [-0.0792,  0.7332, -0.5308,  ...,  0.9286,  0.4598,  0.2852],\n",
      "         [-0.2266,  0.4852, -0.1112,  ...,  0.6723,  0.3831, -0.1320],\n",
      "         [-0.4677,  0.4545, -0.3500,  ...,  0.7688,  0.2280, -0.0684]],\n",
      "\n",
      "        [[ 0.0798,  0.1773, -0.3546,  ..., -0.2418, -0.0137,  0.1813],\n",
      "         [-0.0268,  0.0899, -0.5481,  ..., -0.1042, -0.2552,  0.6560],\n",
      "         [ 0.0799, -0.0265, -0.4340,  ..., -0.0628, -0.0985,  0.4859],\n",
      "         ...,\n",
      "         [ 0.0909,  0.0676, -0.8511,  ..., -0.0739, -0.0269,  0.5402],\n",
      "         [-0.0435,  0.2559, -0.3961,  ..., -0.1306, -0.1801,  0.3486],\n",
      "         [-0.0220,  0.0286, -0.6931,  ..., -0.1386, -0.2553,  0.6192]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1205766201019287\n",
      "Model outputs:  tensor([[[-0.3344,  0.5238, -0.8465,  ...,  0.6283, -0.3141,  0.2861],\n",
      "         [-0.3079,  0.4768, -0.5179,  ...,  0.2601,  0.0214, -0.1157],\n",
      "         [-0.3421, -0.0814, -0.7872,  ...,  0.3387, -0.5426,  0.2538],\n",
      "         ...,\n",
      "         [-0.3615,  0.3245, -0.8623,  ...,  0.2278, -0.1787,  0.1990],\n",
      "         [-0.2049,  0.6553, -0.5210,  ...,  0.0668, -0.0669, -0.0700],\n",
      "         [-0.0058,  0.0509, -0.4210,  ...,  0.0818,  0.0884, -0.1269]],\n",
      "\n",
      "        [[-0.0139,  0.4556, -0.6206,  ...,  0.2998, -0.3409,  0.2560],\n",
      "         [-0.1081,  0.4560, -0.6231,  ...,  0.3076,  0.1521,  0.0938],\n",
      "         [-0.1188, -0.0051, -0.5242,  ...,  0.2244, -0.2142,  0.6372],\n",
      "         ...,\n",
      "         [-0.0887,  0.1112, -0.9032,  ...,  0.0311, -0.2805,  0.1444],\n",
      "         [-0.2205,  0.5724, -0.5446,  ...,  0.1016, -0.0922, -0.2431],\n",
      "         [-0.1461,  0.2047, -0.1134,  ..., -0.5214, -0.0903,  0.0492]],\n",
      "\n",
      "        [[ 0.0187,  0.0247, -0.2221,  ..., -0.3501, -0.3729,  0.6550],\n",
      "         [-0.0917,  0.3474, -0.0681,  ..., -0.4258, -0.2268,  0.4064],\n",
      "         [-0.0133, -0.2621, -0.2609,  ..., -0.3314, -0.5210,  1.0624],\n",
      "         ...,\n",
      "         [ 0.2332, -0.0104, -0.3493,  ..., -0.4269, -0.2933,  0.4867],\n",
      "         [ 0.1392, -0.0685, -0.2066,  ..., -0.6268, -0.3375,  0.4169],\n",
      "         [ 0.1043, -0.0800, -0.0587,  ..., -0.7912, -0.2582,  0.2464]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0286,  0.0720, -0.3934,  ..., -0.0615, -0.3229,  0.6341],\n",
      "         [ 0.0691,  0.2042, -0.3948,  ..., -0.1311, -0.0188,  0.2812],\n",
      "         [-0.0584, -0.4173, -0.4084,  ..., -0.1038, -0.3873,  0.5603],\n",
      "         ...,\n",
      "         [-0.1394, -0.1532, -0.7062,  ..., -0.1729, -0.1329,  0.4406],\n",
      "         [ 0.2250, -0.1131, -0.4122,  ..., -0.6984, -0.3615,  0.0511],\n",
      "         [ 0.1270,  0.1271, -0.1996,  ..., -0.6325, -0.2809,  0.1838]],\n",
      "\n",
      "        [[-0.6028,  0.7435, -0.3699,  ...,  0.4517,  0.1566,  0.5670],\n",
      "         [-0.3741,  0.4443, -0.1362,  ..., -0.0059,  0.1525,  0.1488],\n",
      "         [-0.6397,  0.0030, -0.2129,  ...,  0.3045, -0.1228,  0.6788],\n",
      "         ...,\n",
      "         [-0.1985,  0.3578, -0.5337,  ...,  0.2416,  0.2794,  0.1836],\n",
      "         [-0.4369,  0.7062, -0.2994,  ...,  0.0828,  0.1497,  0.1192],\n",
      "         [-0.5050,  0.0397, -0.1668,  ..., -0.3062, -0.0074,  0.0302]],\n",
      "\n",
      "        [[ 0.0534, -0.0035, -0.5121,  ..., -0.3638, -0.3985,  0.7012],\n",
      "         [-0.0816, -0.0317, -0.3397,  ..., -0.4184, -0.1237,  0.4542],\n",
      "         [ 0.0450, -0.3773, -0.1900,  ..., -0.3103, -0.1191,  0.8008],\n",
      "         ...,\n",
      "         [ 0.0618,  0.0610, -0.3089,  ..., -0.2847, -0.3359,  0.5893],\n",
      "         [ 0.0363,  0.1385, -0.3244,  ..., -0.8163, -0.2854,  0.2070],\n",
      "         [ 0.1637,  0.2326, -0.0104,  ..., -0.7244, -0.1327,  0.2817]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9343922734260559\n",
      "Model outputs:  tensor([[[-0.2125,  0.3528, -0.9233,  ...,  0.1464,  0.1935, -0.1780],\n",
      "         [-0.3062,  0.7461, -0.9283,  ...,  0.1823,  0.5356, -0.0218],\n",
      "         [-0.2949,  0.6726, -0.7358,  ..., -0.0470,  0.4012, -0.3665],\n",
      "         ...,\n",
      "         [ 0.0519, -0.0986, -1.0299,  ...,  0.4641,  0.1985,  0.1633],\n",
      "         [-0.4665,  0.7836, -0.7473,  ..., -0.1908,  0.1745,  0.1172],\n",
      "         [-0.3229,  0.3804, -1.0596,  ...,  0.2333,  0.1980,  0.0942]],\n",
      "\n",
      "        [[-0.0078,  0.4043, -0.8218,  ..., -0.1428,  0.1022,  0.3276],\n",
      "         [ 0.0522,  0.4326, -0.8018,  ..., -0.0323,  0.2788,  0.3125],\n",
      "         [-0.1612,  0.5617, -0.6500,  ..., -0.0552,  0.2898, -0.1285],\n",
      "         ...,\n",
      "         [ 0.2851, -0.0104, -0.8748,  ..., -0.1391,  0.0840,  0.4637],\n",
      "         [-0.0532,  0.5889, -0.4922,  ..., -0.6718,  0.2319,  0.2580],\n",
      "         [ 0.1700,  0.4008, -1.0047,  ..., -0.1530,  0.1004,  0.6591]],\n",
      "\n",
      "        [[-0.3834,  0.5337, -0.7431,  ..., -0.2188,  0.4328,  0.0852],\n",
      "         [-0.1041,  0.6104, -0.7848,  ..., -0.0250,  0.3915,  0.1049],\n",
      "         [-0.4502,  0.5146, -0.6954,  ..., -0.2916,  0.1518, -0.1180],\n",
      "         ...,\n",
      "         [-0.3224, -0.0159, -0.9852,  ..., -0.0449,  0.3331,  0.6189],\n",
      "         [-0.4817,  0.3753, -0.9438,  ..., -0.5121,  0.2072,  0.2931],\n",
      "         [-0.3701,  0.6045, -0.8653,  ...,  0.0957,  0.0597,  0.2867]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2742,  0.4885, -0.8840,  ...,  0.5150,  0.2247,  0.3117],\n",
      "         [-0.2613,  0.5621, -0.6911,  ...,  0.4481,  0.6616, -0.2982],\n",
      "         [-0.3319,  0.4098, -0.7929,  ...,  0.1154,  0.5986, -0.4426],\n",
      "         ...,\n",
      "         [ 0.0621,  0.1115, -0.8445,  ...,  0.7619,  0.6306, -0.1082],\n",
      "         [-0.0449,  0.7705, -0.7589,  ...,  0.3329,  0.1600,  0.0541],\n",
      "         [-0.1454,  0.4202, -1.0959,  ...,  0.7829,  0.4237,  0.1106]],\n",
      "\n",
      "        [[-0.2494,  0.6595, -0.9164,  ...,  0.0931,  0.4524,  0.3656],\n",
      "         [-0.3729,  0.6710, -1.0763,  ..., -0.2271,  0.5836, -0.0986],\n",
      "         [-0.2961,  0.3428, -0.8473,  ..., -0.2411,  0.3019, -0.1935],\n",
      "         ...,\n",
      "         [-0.2411, -0.1206, -0.9027,  ...,  0.1543,  0.1117,  0.2419],\n",
      "         [-0.3804,  0.5250, -0.8276,  ..., -0.5173,  0.0961,  0.1326],\n",
      "         [-0.3990,  0.4262, -0.8056,  ...,  0.1890,  0.2468, -0.0154]],\n",
      "\n",
      "        [[-0.1003,  0.4063, -1.1723,  ...,  0.1350,  0.2444,  0.1334],\n",
      "         [-0.1279,  0.5812, -0.6733,  ...,  0.1408,  0.5369, -0.0639],\n",
      "         [-0.2472,  0.5181, -1.0290,  ...,  0.0696,  0.3312, -0.0827],\n",
      "         ...,\n",
      "         [ 0.0108, -0.2324, -0.8514,  ...,  0.3260,  0.2221,  0.4292],\n",
      "         [-0.3271,  0.6200, -0.9496,  ...,  0.0694,  0.2977,  0.0747],\n",
      "         [-0.4630,  0.4968, -1.1322,  ...,  0.1884, -0.0214,  0.2503]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.6095118522644043\n",
      "Model outputs:  tensor([[[-0.3428,  0.6015, -0.9106,  ..., -0.0230,  0.1037,  0.1874],\n",
      "         [-0.4326,  0.5748, -0.8974,  ...,  0.3362,  0.1141,  0.1390],\n",
      "         [ 0.0287,  0.1969, -0.7077,  ...,  0.3027,  0.4786,  0.3456],\n",
      "         ...,\n",
      "         [ 0.1178,  0.0178, -0.5893,  ...,  0.0964,  0.1825,  0.1776],\n",
      "         [-0.2164,  0.4251, -0.6689,  ...,  0.0053, -0.1135,  0.3135],\n",
      "         [-0.1543,  0.3611, -0.5450,  ...,  0.0524, -0.0377,  0.2982]],\n",
      "\n",
      "        [[ 0.2189,  0.1195,  0.0316,  ...,  0.2768,  0.1078, -0.3716],\n",
      "         [ 0.0868, -0.0369, -0.0446,  ...,  0.3838, -0.1425, -0.6220],\n",
      "         [ 0.3416, -0.2884, -0.1171,  ...,  0.3006,  0.2751, -0.5879],\n",
      "         ...,\n",
      "         [ 0.6455, -0.2266, -0.0213,  ...,  0.2375,  0.1354, -0.1173],\n",
      "         [ 0.2751,  0.2290, -0.0245,  ...,  0.5405, -0.1247, -0.6410],\n",
      "         [ 0.3015, -0.2354,  0.0341,  ...,  0.1245, -0.2259, -0.3992]],\n",
      "\n",
      "        [[-0.6622,  0.7120, -0.7921,  ...,  0.4214,  0.2211, -0.1533],\n",
      "         [-0.1915,  0.6660, -0.8905,  ...,  0.5535,  0.0321,  0.3132],\n",
      "         [-0.0460,  0.2373, -0.9764,  ...,  0.5482,  0.5926,  0.1023],\n",
      "         ...,\n",
      "         [ 0.1138,  0.1650, -0.4940,  ...,  0.3502,  0.1947,  0.0571],\n",
      "         [-0.4911,  0.5168, -0.8558,  ...,  0.7628,  0.3200,  0.0776],\n",
      "         [-0.4191,  0.4202, -0.8138,  ...,  0.3724,  0.1426,  0.2071]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5860,  0.8038, -0.9198,  ...,  0.9265,  0.2744, -0.2768],\n",
      "         [-0.5612,  0.6325, -1.1874,  ...,  0.9602,  0.1731,  0.1455],\n",
      "         [-0.0176,  0.3074, -0.9739,  ...,  0.7075,  0.5543,  0.1284],\n",
      "         ...,\n",
      "         [-0.1186,  0.3238, -0.6794,  ...,  0.5755,  0.1582, -0.0275],\n",
      "         [-0.3713,  0.7997, -0.6808,  ...,  0.8015,  0.2027,  0.1324],\n",
      "         [-0.4733,  0.4615, -0.9840,  ...,  0.6614,  0.2263,  0.1526]],\n",
      "\n",
      "        [[-0.7285,  0.8520, -1.1129,  ...,  0.5430,  0.3086, -0.2865],\n",
      "         [-0.2299,  0.3832, -0.8897,  ...,  0.8693,  0.1991,  0.1326],\n",
      "         [-0.0889,  0.3430, -1.1045,  ...,  0.9735,  0.3296,  0.3668],\n",
      "         ...,\n",
      "         [-0.1292,  0.1463, -0.7396,  ...,  0.6762, -0.0760,  0.2373],\n",
      "         [-0.5304,  0.8661, -0.8170,  ...,  1.0356,  0.3350,  0.1863],\n",
      "         [-0.2756,  0.7044, -0.8499,  ...,  0.7798,  0.4628,  0.0493]],\n",
      "\n",
      "        [[-0.5475,  0.7361, -0.0277,  ...,  0.6458,  0.5307, -0.2330],\n",
      "         [-0.2797,  0.3435, -0.2257,  ...,  0.6955,  0.3791, -0.0525],\n",
      "         [-0.2933,  0.1065, -0.4191,  ...,  0.6531,  0.6140,  0.2784],\n",
      "         ...,\n",
      "         [-0.1503, -0.0036, -0.0325,  ...,  0.3249,  0.2645,  0.2755],\n",
      "         [-0.6563,  0.5467, -0.1715,  ...,  0.9232,  0.3151, -0.1009],\n",
      "         [-0.3641,  0.1632, -0.2193,  ...,  0.4900,  0.4588,  0.0664]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1381568908691406\n",
      "Model outputs:  tensor([[[ 0.1434,  0.3386, -0.1142,  ..., -0.4574, -0.1153,  0.1611],\n",
      "         [ 0.2024,  0.0949, -0.2842,  ..., -0.6079, -0.1047,  0.4735],\n",
      "         [ 0.0160,  0.3117, -0.4183,  ..., -0.4645, -0.0515,  0.6864],\n",
      "         ...,\n",
      "         [ 0.2865,  0.3029, -0.4466,  ..., -0.5798,  0.1102,  0.2936],\n",
      "         [ 0.1920,  0.2145, -0.2385,  ..., -0.4185, -0.0066,  0.1203],\n",
      "         [ 0.2161,  0.3423, -0.2121,  ..., -0.4615,  0.0314,  0.2587]],\n",
      "\n",
      "        [[ 0.0442,  0.4184, -0.6718,  ..., -0.9381,  0.6439,  0.5078],\n",
      "         [ 0.1640,  0.1069, -0.2380,  ..., -0.7875,  0.4168,  0.4829],\n",
      "         [ 0.0730, -0.0302, -0.5930,  ..., -1.1621,  0.5741,  0.3966],\n",
      "         ...,\n",
      "         [ 0.1949,  0.2285, -0.1986,  ..., -0.8095,  0.7003,  0.4193],\n",
      "         [ 0.1439,  0.1512, -0.4528,  ..., -0.8000,  0.7893,  0.4454],\n",
      "         [ 0.2543,  0.4461, -0.0299,  ..., -0.7788,  0.8055,  0.3131]],\n",
      "\n",
      "        [[-0.3667,  0.7577, -0.7079,  ...,  0.0396,  0.4999,  0.1019],\n",
      "         [-0.4222,  0.5775, -0.5137,  ..., -0.1291,  0.5083, -0.1086],\n",
      "         [-0.4013,  0.5965, -0.7386,  ...,  0.1694,  0.2672,  0.4452],\n",
      "         ...,\n",
      "         [-0.4571,  0.4362, -0.4198,  ...,  0.0696,  0.4365,  0.1010],\n",
      "         [-0.4989,  0.5701, -0.4590,  ...,  0.1499,  0.4443, -0.2388],\n",
      "         [-0.0705,  0.7464, -0.2941,  ..., -0.0794,  0.5663, -0.0518]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1057,  0.2317, -0.1280,  ..., -0.3878, -0.0728,  0.3967],\n",
      "         [ 0.3438,  0.0661, -0.2585,  ..., -0.7276, -0.1141,  0.2668],\n",
      "         [ 0.0957, -0.0019, -0.1333,  ..., -0.5718, -0.2354,  0.5245],\n",
      "         ...,\n",
      "         [ 0.2086,  0.2259, -0.1371,  ..., -0.5216, -0.0070,  0.3386],\n",
      "         [ 0.2110,  0.1463, -0.2144,  ..., -0.4340,  0.2448,  0.0879],\n",
      "         [ 0.3552,  0.3879, -0.0464,  ..., -0.7742, -0.1746,  0.2061]],\n",
      "\n",
      "        [[ 0.4122,  0.3511, -0.4667,  ..., -0.8442,  0.6389,  0.7420],\n",
      "         [ 0.1462,  0.1359, -0.4076,  ..., -0.7050,  0.9006,  0.5431],\n",
      "         [ 0.2547, -0.1289, -0.3953,  ..., -0.9594,  0.7310,  0.9182],\n",
      "         ...,\n",
      "         [ 0.0731,  0.2218, -0.3512,  ..., -0.8086,  0.7805,  0.2837],\n",
      "         [ 0.1549,  0.3025, -0.2646,  ..., -0.5626,  0.8542,  0.2414],\n",
      "         [ 0.2505, -0.0866, -0.1699,  ..., -0.7347,  0.5646,  0.4830]],\n",
      "\n",
      "        [[-0.4182,  0.5708, -0.0271,  ...,  0.0575,  0.4159, -0.2128],\n",
      "         [-0.2291,  0.5655, -0.5627,  ...,  0.1152,  0.5875, -0.0637],\n",
      "         [-0.5490,  0.5278, -0.5482,  ...,  0.2772,  0.3717, -0.0337],\n",
      "         ...,\n",
      "         [-0.2583,  0.3474, -0.0098,  ..., -0.0241,  0.5112, -0.1690],\n",
      "         [-0.1398,  0.3932, -0.1083,  ..., -0.0017,  0.6066, -0.3322],\n",
      "         [-0.4413,  0.2051, -0.2974,  ..., -0.0281,  0.2349, -0.0907]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1015833616256714\n",
      "Model outputs:  tensor([[[-0.3077,  0.8911, -0.6144,  ...,  0.2705,  0.1045,  0.2145],\n",
      "         [-0.1929,  0.4842, -0.5653,  ...,  0.6441,  0.1935,  0.0208],\n",
      "         [-0.3417,  0.5992, -0.7099,  ...,  0.5507,  0.0841,  0.0640],\n",
      "         ...,\n",
      "         [-0.0251,  0.4165, -0.8563,  ...,  0.6149,  0.3193, -0.0114],\n",
      "         [-0.2506,  0.0707, -0.8749,  ...,  0.6462, -0.0705,  0.4473],\n",
      "         [-0.2055,  0.6057, -0.6579,  ...,  0.5438,  0.3689, -0.0201]],\n",
      "\n",
      "        [[-0.4803,  0.5853, -0.3315,  ...,  0.1055,  0.2972,  0.0216],\n",
      "         [-0.2615,  0.5640, -0.4105,  ...,  0.3161,  0.2492,  0.1916],\n",
      "         [-0.5656,  0.5479, -0.2694,  ...,  0.3360,  0.0571,  0.2988],\n",
      "         ...,\n",
      "         [-0.0722, -0.0413, -0.2987,  ...,  0.3135,  0.2132,  0.1728],\n",
      "         [-0.3743,  0.0484, -0.5218,  ...,  0.1204,  0.3376,  0.1878],\n",
      "         [-0.9361,  0.6239, -0.4190,  ...,  0.5282,  0.2433,  0.0996]],\n",
      "\n",
      "        [[-0.7979,  0.4512, -0.3326,  ..., -0.1777,  0.1596,  0.1788],\n",
      "         [-0.6020,  0.7474, -0.6111,  ..., -0.0250, -0.0764,  0.1122],\n",
      "         [-0.6970,  0.3131,  0.0691,  ...,  0.1804,  0.0490,  0.4322],\n",
      "         ...,\n",
      "         [-0.2078, -0.0447, -0.3418,  ...,  0.1409,  0.1379,  0.1436],\n",
      "         [-0.3721,  0.1412, -0.3732,  ...,  0.2490,  0.4078,  0.4224],\n",
      "         [-0.4980,  0.3553, -0.1297,  ...,  0.3014,  0.3518,  0.2392]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2703,  0.5710, -0.4269,  ...,  0.3667,  0.3067, -0.0543],\n",
      "         [-0.3309,  0.4248, -0.7705,  ...,  0.5443, -0.2459,  0.0583],\n",
      "         [-0.1645,  0.1753, -0.6703,  ...,  0.6443,  0.1095,  0.0381],\n",
      "         ...,\n",
      "         [ 0.1058,  0.4053, -0.9069,  ...,  0.6751,  0.2230,  0.0609],\n",
      "         [ 0.0975, -0.1201, -0.6830,  ...,  0.5793,  0.0736,  0.2878],\n",
      "         [ 0.0287,  0.4662, -0.6510,  ...,  0.5958,  0.1182, -0.0822]],\n",
      "\n",
      "        [[-0.0083,  0.2377, -0.0993,  ..., -0.5653, -0.1988,  0.2473],\n",
      "         [ 0.0557,  0.0991, -0.2095,  ..., -0.1508,  0.1138,  0.3282],\n",
      "         [ 0.3914, -0.1681, -0.3610,  ..., -0.2245, -0.1392,  0.6405],\n",
      "         ...,\n",
      "         [ 0.2553,  0.0078, -0.3747,  ..., -0.2398, -0.2580,  0.5748],\n",
      "         [ 0.1127, -0.1995, -0.2101,  ..., -0.3371, -0.0945,  0.6442],\n",
      "         [ 0.0468,  0.0767, -0.2955,  ..., -0.5714, -0.4282,  0.7016]],\n",
      "\n",
      "        [[-0.4021,  0.7048, -0.5655,  ...,  0.2342,  0.4124, -0.0174],\n",
      "         [-0.2530,  0.3336, -0.5275,  ...,  0.4550, -0.0108, -0.1925],\n",
      "         [-0.2704,  0.6015, -0.5136,  ...,  0.5978,  0.4402,  0.0031],\n",
      "         ...,\n",
      "         [ 0.0478,  0.2883, -0.9693,  ...,  0.6635,  0.0982,  0.1037],\n",
      "         [-0.0467,  0.1630, -0.7345,  ...,  0.5831,  0.3614,  0.0366],\n",
      "         [-0.3122,  0.3463, -0.6440,  ...,  0.5348,  0.0345,  0.1134]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2848381996154785\n",
      "Model outputs:  tensor([[[-0.1448,  0.3191, -0.9744,  ...,  0.6097,  0.1335,  0.4412],\n",
      "         [-0.4655,  0.5319, -0.7152,  ...,  0.3085, -0.2120,  0.1106],\n",
      "         [ 0.2674,  0.4017, -0.8981,  ...,  0.3029,  0.0486,  0.3060],\n",
      "         ...,\n",
      "         [ 0.1097,  0.1611, -0.9066,  ...,  0.5321,  0.1172,  0.4671],\n",
      "         [-0.2140,  0.7561, -1.1973,  ...,  0.2674,  0.3715,  0.2004],\n",
      "         [-0.1028,  0.6322, -0.5670,  ...,  0.1143,  0.3600,  0.2864]],\n",
      "\n",
      "        [[-0.4620,  0.1253, -0.9062,  ...,  0.0655,  0.1500,  0.5437],\n",
      "         [-0.7237,  0.4385, -0.9165,  ...,  0.0725, -0.0389,  0.1257],\n",
      "         [-0.4339,  0.2697, -1.0369,  ...,  0.2996,  0.7113,  0.4822],\n",
      "         ...,\n",
      "         [-0.4840,  0.6704, -1.0529,  ...,  0.1523,  0.4430,  0.2278],\n",
      "         [-0.4369,  0.4253, -0.8913,  ...,  0.0592,  0.3928,  0.4804],\n",
      "         [-0.6677,  0.6591, -0.9027,  ..., -0.0606,  0.5257,  0.4205]],\n",
      "\n",
      "        [[-0.4169,  0.1689, -0.4535,  ...,  0.8242,  0.4671,  0.1546],\n",
      "         [-0.6127,  0.5924, -0.2554,  ...,  0.7813,  0.2698,  0.0110],\n",
      "         [-0.4425,  0.4287, -0.5231,  ...,  0.9468,  0.7123,  0.1426],\n",
      "         ...,\n",
      "         [-0.4028,  0.5534, -0.4039,  ...,  0.5892,  0.5162,  0.1111],\n",
      "         [-0.7078,  0.7192, -0.3367,  ...,  0.6359,  0.1568,  0.0232],\n",
      "         [-0.5782,  0.7613, -0.1923,  ...,  0.3193,  0.5484,  0.0180]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4744,  0.2681, -0.5847,  ...,  0.7605,  0.4990,  0.2158],\n",
      "         [-0.5721,  0.5926, -0.5230,  ...,  0.7642,  0.2898,  0.2799],\n",
      "         [-0.3830,  0.3137, -0.5387,  ...,  0.8050,  0.3420,  0.1171],\n",
      "         ...,\n",
      "         [-0.3066,  0.5481, -0.5188,  ...,  0.7497,  0.3387,  0.1411],\n",
      "         [-0.6060,  0.6779, -0.3544,  ...,  0.7004,  0.3436, -0.3086],\n",
      "         [-0.5129,  0.7450, -0.4321,  ...,  0.4870,  0.4312, -0.1011]],\n",
      "\n",
      "        [[-0.4474,  0.4482, -1.3372,  ...,  0.8310,  0.1277,  0.4531],\n",
      "         [-0.5296,  0.5480, -1.0843,  ...,  0.4132, -0.0787,  0.1670],\n",
      "         [-0.1209,  0.5168, -1.1995,  ...,  0.6048, -0.0140,  0.3019],\n",
      "         ...,\n",
      "         [-0.1635,  0.4937, -1.1346,  ...,  0.4056,  0.2784,  0.1786],\n",
      "         [-0.3824,  0.6508, -0.8971,  ...,  0.3724,  0.1959,  0.1168],\n",
      "         [-0.3271,  0.5460, -0.8936,  ...,  0.3298,  0.2845,  0.1728]],\n",
      "\n",
      "        [[-0.1877,  0.1837, -0.7322,  ..., -0.1691, -0.3804,  0.6895],\n",
      "         [-0.0063,  0.2841, -0.6545,  ..., -0.1954, -0.4790,  0.7203],\n",
      "         [ 0.1411,  0.2313, -0.8893,  ..., -0.1924,  0.0050,  0.9823],\n",
      "         ...,\n",
      "         [ 0.2023,  0.0733, -0.5748,  ..., -0.1281,  0.0307,  0.8123],\n",
      "         [-0.1168,  0.4161, -0.6968,  ..., -0.1744, -0.1309,  0.6253],\n",
      "         [-0.0158,  0.2060, -0.3141,  ..., -0.4037, -0.1043,  0.3303]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.4992096424102783\n",
      "Model outputs:  tensor([[[-0.3316,  0.1189, -0.2859,  ...,  0.7844,  0.7045,  0.1348],\n",
      "         [-0.2057,  0.3589, -0.3117,  ...,  0.6803,  0.3738, -0.1827],\n",
      "         [-0.7763,  0.1535, -0.3091,  ...,  0.7412, -0.2609,  0.3099],\n",
      "         ...,\n",
      "         [-0.5655,  0.4347, -0.5712,  ...,  0.9455,  0.2349,  0.2425],\n",
      "         [-0.2059,  0.2713, -0.1499,  ...,  0.6297,  0.3445, -0.0489],\n",
      "         [-0.3717,  0.2393, -0.0517,  ...,  0.7821,  0.6291, -0.0905]],\n",
      "\n",
      "        [[-0.2761,  0.2740, -0.9866,  ...,  0.6887,  0.3217,  0.1815],\n",
      "         [-0.1845,  0.2548, -0.7927,  ...,  0.6161,  0.2073, -0.1344],\n",
      "         [-0.4022,  0.2273, -0.9678,  ...,  0.5577, -0.2182, -0.0678],\n",
      "         ...,\n",
      "         [-0.4473,  0.7519, -1.0915,  ...,  1.1194,  0.2083, -0.0258],\n",
      "         [-0.5089,  0.1590, -0.9922,  ...,  0.8071,  0.2396, -0.0097],\n",
      "         [-0.3842,  0.4302, -0.6919,  ...,  0.6716,  0.5409,  0.0018]],\n",
      "\n",
      "        [[ 0.1815, -0.0165, -0.1911,  ...,  0.3354,  0.3309, -0.4759],\n",
      "         [ 0.3013,  0.0177, -0.2484,  ...,  0.5563,  0.3402, -0.6430],\n",
      "         [-0.0188, -0.2504, -0.2371,  ...,  0.4636, -0.4836, -0.1909],\n",
      "         ...,\n",
      "         [ 0.2616,  0.0230, -0.3285,  ...,  0.1217,  0.1326, -0.0654],\n",
      "         [ 0.1848, -0.2591, -0.3156,  ...,  0.0623,  0.0380, -0.4486],\n",
      "         [ 0.2361, -0.1075, -0.2693,  ...,  0.2620,  0.1397, -0.2381]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3330,  0.2113, -0.3919,  ...,  0.5477,  0.4795, -0.0102],\n",
      "         [-0.2303,  0.2230, -0.1235,  ...,  0.4474,  0.9099, -0.1751],\n",
      "         [-0.3821,  0.1628, -0.7076,  ...,  0.7659,  0.1368, -0.0753],\n",
      "         ...,\n",
      "         [-0.5200,  0.2893, -0.1812,  ...,  0.8212,  0.2957, -0.0743],\n",
      "         [-0.2719,  0.0830, -0.0580,  ...,  0.6837,  0.3659,  0.0642],\n",
      "         [-0.3714,  0.0352, -0.4365,  ...,  0.4315,  0.5851,  0.0917]],\n",
      "\n",
      "        [[-0.4630,  0.1916, -0.3206,  ...,  0.5589,  0.5171,  0.0297],\n",
      "         [-0.5150,  0.2907, -0.4479,  ...,  0.5878,  0.5818, -0.0237],\n",
      "         [-0.7938,  0.2268, -0.4402,  ...,  0.7399, -0.1699,  0.1428],\n",
      "         ...,\n",
      "         [-0.5153,  0.2135, -0.5257,  ...,  0.7756,  0.5074,  0.1087],\n",
      "         [-0.4162,  0.3343, -0.2801,  ...,  0.6468,  0.5646, -0.0974],\n",
      "         [-0.6836,  0.3697, -0.1479,  ...,  0.8243,  0.5196,  0.0136]],\n",
      "\n",
      "        [[-0.2119, -0.0517, -0.6595,  ...,  0.0221, -0.0542,  0.6275],\n",
      "         [ 0.1442,  0.1802, -0.6756,  ..., -0.1909,  0.2959, -0.0541],\n",
      "         [-0.3091, -0.0714, -0.6355,  ..., -0.1841, -0.5026,  0.3148],\n",
      "         ...,\n",
      "         [ 0.0026,  0.2708, -0.7229,  ..., -0.0016, -0.0812,  0.7887],\n",
      "         [-0.0533, -0.0132, -0.9302,  ...,  0.1010,  0.0354,  0.6764],\n",
      "         [ 0.1183,  0.0466, -0.5772,  ..., -0.1221,  0.2014,  0.5295]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.4158096313476562\n",
      "Model outputs:  tensor([[[-4.3682e-01,  1.3060e-01, -9.1194e-02,  ...,  6.5734e-01,\n",
      "           3.0988e-01,  1.6916e-03],\n",
      "         [-4.1229e-01,  3.1341e-01, -2.1152e-01,  ...,  8.6205e-01,\n",
      "           3.1530e-01, -5.7640e-02],\n",
      "         [-3.1748e-01,  3.9448e-01, -2.4752e-01,  ...,  5.3625e-01,\n",
      "           3.0019e-01,  3.3899e-02],\n",
      "         ...,\n",
      "         [-8.2862e-01,  7.9852e-02, -4.8973e-02,  ...,  4.6301e-01,\n",
      "           2.0541e-01,  1.6331e-01],\n",
      "         [-5.4468e-01,  4.3365e-01, -4.4418e-01,  ...,  6.0800e-01,\n",
      "           5.5155e-01,  3.9922e-01],\n",
      "         [-5.2957e-01,  3.8614e-01, -3.3044e-01,  ...,  7.0164e-01,\n",
      "           2.5176e-01,  2.8238e-01]],\n",
      "\n",
      "        [[-1.0087e-01,  1.0804e-01, -3.5418e-01,  ..., -2.4258e-01,\n",
      "          -4.3476e-02,  5.5450e-01],\n",
      "         [ 5.9779e-02, -3.4918e-01, -2.2575e-01,  ..., -1.6354e-01,\n",
      "          -1.3597e-01,  7.5442e-01],\n",
      "         [ 2.1311e-01,  1.2978e-01, -4.2099e-01,  ..., -5.5308e-01,\n",
      "          -1.7915e-01,  5.5966e-01],\n",
      "         ...,\n",
      "         [-2.7498e-01, -3.4641e-02, -2.3631e-01,  ..., -2.6473e-01,\n",
      "          -3.8248e-01,  8.9425e-01],\n",
      "         [ 3.6797e-02, -2.6829e-01, -4.3294e-01,  ..., -3.0566e-01,\n",
      "          -1.6584e-01,  6.7084e-01],\n",
      "         [-1.9659e-01, -1.9650e-01, -5.2793e-01,  ..., -1.7997e-01,\n",
      "          -1.4956e-01,  8.2176e-01]],\n",
      "\n",
      "        [[ 3.8745e-04, -2.0210e-01, -3.7711e-01,  ..., -4.1578e-01,\n",
      "          -1.0324e-01,  6.2178e-01],\n",
      "         [-1.0363e-01,  1.3301e-01, -2.0606e-01,  ..., -2.6205e-01,\n",
      "           1.2519e-02,  6.5405e-01],\n",
      "         [ 2.8458e-01,  1.3878e-01, -2.8700e-01,  ..., -3.9066e-01,\n",
      "           7.5719e-02,  3.9591e-01],\n",
      "         ...,\n",
      "         [-2.5842e-01,  6.6869e-03, -2.1570e-01,  ..., -1.0031e-01,\n",
      "          -3.0374e-01,  7.9841e-01],\n",
      "         [-6.2175e-02, -4.4473e-02, -4.6193e-01,  ..., -3.4311e-01,\n",
      "          -2.5298e-01,  8.9488e-01],\n",
      "         [-3.4660e-01, -2.2300e-01, -4.1935e-01,  ..., -5.1302e-01,\n",
      "          -5.2191e-01,  7.8437e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5971e-01,  1.4325e-01, -3.9692e-01,  ..., -2.3381e-01,\n",
      "          -2.1115e-01,  5.0791e-01],\n",
      "         [-1.5767e-01,  1.1200e-01, -4.6343e-01,  ...,  1.1542e-01,\n",
      "           1.2624e-01,  3.5283e-01],\n",
      "         [ 1.0171e-01,  1.5526e-01, -2.8027e-01,  ..., -6.4353e-02,\n",
      "           2.7962e-01,  5.2456e-01],\n",
      "         ...,\n",
      "         [-2.7825e-01, -1.0890e-01, -6.2229e-01,  ..., -1.2306e-01,\n",
      "          -5.1853e-01,  8.5840e-01],\n",
      "         [-8.6583e-02, -2.8352e-02, -4.5574e-01,  ..., -2.5838e-01,\n",
      "           9.5716e-02,  7.2268e-01],\n",
      "         [-1.3618e-01,  6.2725e-02, -3.8123e-01,  ..., -4.2414e-01,\n",
      "          -3.1511e-01,  7.0416e-01]],\n",
      "\n",
      "        [[-2.0074e-01,  5.6504e-01, -8.6346e-01,  ...,  7.2946e-01,\n",
      "           3.6007e-01,  2.2678e-02],\n",
      "         [-4.1626e-02,  2.6532e-01, -8.6404e-01,  ...,  8.3668e-01,\n",
      "           4.7208e-01,  2.0667e-01],\n",
      "         [-2.3273e-01,  5.4745e-01, -8.9323e-01,  ...,  5.1508e-01,\n",
      "           5.7461e-01,  3.5367e-02],\n",
      "         ...,\n",
      "         [-2.9503e-01,  6.8784e-01, -1.0862e+00,  ...,  8.6759e-01,\n",
      "           1.1509e-01,  7.0973e-01],\n",
      "         [-4.0563e-01,  5.4027e-01, -5.9438e-01,  ...,  6.3005e-01,\n",
      "           1.0799e-01,  3.8342e-01],\n",
      "         [-5.0361e-01,  4.1248e-01, -1.0225e+00,  ...,  7.8179e-01,\n",
      "           1.1569e-01,  5.0584e-01]],\n",
      "\n",
      "        [[ 6.0201e-02,  1.9292e-01, -6.8296e-01,  ...,  8.3191e-02,\n",
      "           1.4630e-01,  3.5703e-01],\n",
      "         [-8.6765e-02,  3.4381e-01, -4.9229e-01,  ..., -1.4669e-02,\n",
      "          -5.0173e-02,  5.9201e-01],\n",
      "         [-1.4315e-01,  2.9825e-01, -5.2887e-01,  ..., -2.1209e-01,\n",
      "           8.3950e-02,  2.6549e-01],\n",
      "         ...,\n",
      "         [-1.2281e-01,  1.8495e-01, -3.3488e-01,  ...,  1.2053e-01,\n",
      "           1.0020e-01,  6.5696e-01],\n",
      "         [-9.6208e-02,  1.0458e-02, -5.3097e-01,  ..., -2.3325e-01,\n",
      "           4.4300e-02,  8.4734e-01],\n",
      "         [-3.9276e-01, -1.5016e-02, -5.2478e-01,  ...,  8.8565e-02,\n",
      "          -1.5329e-01,  5.6829e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8977930545806885\n",
      "Model outputs:  tensor([[[ 0.0122, -0.0497, -0.3799,  ...,  0.1340, -0.2299,  0.7633],\n",
      "         [-0.1848,  0.0772, -0.7011,  ...,  0.0168,  0.1720,  0.5938],\n",
      "         [ 0.1724,  0.0609, -0.3892,  ..., -0.1164,  0.1503,  0.4414],\n",
      "         ...,\n",
      "         [ 0.0582,  0.0892, -0.2574,  ..., -0.2214,  0.0076,  0.3653],\n",
      "         [-0.0878, -0.2063, -0.4049,  ..., -0.1751, -0.0065,  0.2000],\n",
      "         [ 0.1929, -0.0790, -0.5570,  ...,  0.1363, -0.0030,  0.5494]],\n",
      "\n",
      "        [[-0.0931,  0.0549, -0.5427,  ...,  0.8587,  0.6810,  0.4535],\n",
      "         [-0.3445,  0.1637, -0.4450,  ...,  0.7690,  0.3171, -0.0025],\n",
      "         [-0.4789,  0.3674, -0.6272,  ...,  0.3420,  0.6113,  0.0326],\n",
      "         ...,\n",
      "         [-0.3351,  0.2032, -0.6881,  ...,  0.5126,  0.3041,  0.4482],\n",
      "         [-0.2238,  0.0454, -0.3521,  ...,  0.6659,  0.6050,  0.1949],\n",
      "         [-0.4949, -0.0590, -0.7015,  ...,  0.6808,  0.1842,  0.2290]],\n",
      "\n",
      "        [[-0.0403,  0.1084, -0.1008,  ...,  0.1608,  0.1090,  0.7002],\n",
      "         [ 0.0534, -0.1061, -0.5634,  ..., -0.1726, -0.0684,  0.6777],\n",
      "         [-0.0126,  0.0334, -0.3732,  ..., -0.3037, -0.1977,  0.4040],\n",
      "         ...,\n",
      "         [-0.0711,  0.0775, -0.4775,  ..., -0.4216, -0.3847,  0.7520],\n",
      "         [ 0.1886, -0.1360, -0.3129,  ..., -0.2751, -0.1242,  0.7074],\n",
      "         [ 0.0788, -0.1552, -0.5845,  ..., -0.0538, -0.3127,  0.5920]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1806, -0.1739,  0.2157,  ...,  0.5948,  0.0208, -0.2927],\n",
      "         [ 0.1349, -0.1390,  0.0522,  ...,  0.4143, -0.2501, -0.1862],\n",
      "         [ 0.3122, -0.0354, -0.1138,  ...,  0.4686,  0.1110, -0.5786],\n",
      "         ...,\n",
      "         [ 0.3656, -0.5474,  0.1544,  ...,  0.3133,  0.0143, -0.3777],\n",
      "         [ 0.3438, -0.2753,  0.0644,  ...,  0.3862, -0.1622, -0.4283],\n",
      "         [ 0.4661, -0.1626, -0.0778,  ...,  0.6823, -0.2303, -0.5717]],\n",
      "\n",
      "        [[-0.1650,  0.0433, -0.1786,  ...,  0.8472,  0.6465, -0.0492],\n",
      "         [-0.1315,  0.1746, -0.2936,  ...,  0.8893,  0.2546, -0.0605],\n",
      "         [-0.2504,  0.4343, -0.1984,  ...,  0.6465,  0.1279, -0.1079],\n",
      "         ...,\n",
      "         [-0.3987,  0.1806, -0.2403,  ...,  0.8011,  0.6406, -0.0339],\n",
      "         [-0.2349, -0.0916,  0.0291,  ...,  0.7304,  0.4700,  0.1012],\n",
      "         [ 0.0767, -0.0045, -0.2442,  ...,  0.9226,  0.5331, -0.0625]],\n",
      "\n",
      "        [[ 0.0614, -0.2177, -0.3296,  ..., -0.0294, -0.3380,  1.0110],\n",
      "         [ 0.1659, -0.1515, -0.1832,  ..., -0.2480, -0.2102,  0.6557],\n",
      "         [ 0.0497, -0.0404, -0.1807,  ..., -0.1126, -0.0877,  0.7628],\n",
      "         ...,\n",
      "         [ 0.1267, -0.1913, -0.4169,  ..., -0.2126, -0.0997,  0.6686],\n",
      "         [ 0.1763, -0.1831, -0.1812,  ..., -0.3967, -0.0576,  0.8575],\n",
      "         [ 0.2144, -0.1325, -0.4498,  ..., -0.1299, -0.3229,  0.8883]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2989126443862915\n",
      "Model outputs:  tensor([[[ 0.1404, -0.4330, -0.3344,  ..., -0.3624, -0.4328,  0.7858],\n",
      "         [ 0.0831, -0.3834, -0.5164,  ..., -0.3345, -0.3936,  0.7186],\n",
      "         [ 0.0918, -0.3016, -0.3485,  ..., -0.7531, -0.3615,  0.8787],\n",
      "         ...,\n",
      "         [ 0.2319, -0.3492, -0.5406,  ..., -0.8519, -0.4241,  0.6760],\n",
      "         [ 0.0205, -0.3909, -0.5578,  ..., -0.7051, -0.5209,  0.8939],\n",
      "         [-0.1459, -0.5943, -0.4926,  ..., -0.6442, -0.4518,  0.8300]],\n",
      "\n",
      "        [[-0.0982, -0.0020, -1.0651,  ...,  0.7658, -0.0366,  0.3924],\n",
      "         [-0.1631,  0.2487, -1.2638,  ...,  0.8248,  0.4460,  0.1881],\n",
      "         [ 0.0100,  0.2018, -1.0167,  ...,  0.5391,  0.1655,  0.4213],\n",
      "         ...,\n",
      "         [-0.2747,  0.2350, -0.8097,  ...,  0.3775, -0.1306, -0.2269],\n",
      "         [-0.5005,  0.3386, -0.8976,  ...,  0.3441, -0.5014,  0.3799],\n",
      "         [-0.3531,  0.0409, -0.9678,  ...,  0.3368,  0.0543,  0.3423]],\n",
      "\n",
      "        [[ 0.0167, -0.1877, -0.7614,  ...,  0.0790,  0.1701,  0.3961],\n",
      "         [ 0.1374, -0.1536, -1.0368,  ..., -0.3898, -0.2218,  0.4633],\n",
      "         [-0.1299, -0.2715, -0.6959,  ..., -0.4463,  0.0648,  0.6075],\n",
      "         ...,\n",
      "         [-0.1248, -0.3767, -0.8909,  ..., -0.2837, -0.1557,  0.5792],\n",
      "         [-0.5036, -0.3295, -1.0679,  ..., -0.5393, -0.3330,  0.5857],\n",
      "         [-0.6387, -0.2471, -0.9030,  ..., -0.0413, -0.1799,  0.2441]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.8224,  0.0083, -0.1990,  ...,  0.5310,  0.0028, -0.3220],\n",
      "         [ 0.4264,  0.1260, -0.2134,  ...,  0.2349, -0.3770, -0.3971],\n",
      "         [ 0.2575, -0.0178,  0.0372,  ...,  0.3182, -0.0133, -0.3249],\n",
      "         ...,\n",
      "         [ 0.2989, -0.0401, -0.2525,  ...,  0.1483, -0.1133, -0.2980],\n",
      "         [ 0.4854,  0.0598, -0.1075,  ...,  0.2467, -0.1477, -0.2824],\n",
      "         [ 0.4451, -0.2483, -0.0612,  ...,  0.1797,  0.0191, -0.3077]],\n",
      "\n",
      "        [[-0.3165,  0.1544, -0.8810,  ...,  0.8822,  0.2243,  0.1569],\n",
      "         [-0.4114,  0.2461, -1.0786,  ...,  0.4972,  0.2882,  0.3497],\n",
      "         [-0.3234,  0.4710, -0.9965,  ...,  0.1893,  0.1926,  0.0953],\n",
      "         ...,\n",
      "         [-0.3738,  0.2860, -0.8654,  ...,  0.0441, -0.0141,  0.1830],\n",
      "         [-0.6233,  0.1244, -0.8663,  ...,  0.3649, -0.2008,  0.3516],\n",
      "         [-0.3145, -0.0302, -0.5730,  ...,  0.4682,  0.0928,  0.3875]],\n",
      "\n",
      "        [[ 0.0660,  0.0428, -0.7498,  ...,  0.7396,  0.2069,  0.4527],\n",
      "         [-0.4497,  0.3284, -1.0466,  ...,  0.3096,  0.3230,  0.4319],\n",
      "         [-0.3838,  0.4649, -1.1152,  ...,  0.3776,  0.3938,  0.2408],\n",
      "         ...,\n",
      "         [-0.3746,  0.0171, -0.8461,  ...,  0.2431, -0.1211,  0.0546],\n",
      "         [-0.6347,  0.5526, -0.9513,  ...,  0.2661,  0.0213,  0.4367],\n",
      "         [-0.2948,  0.0962, -0.7781,  ...,  0.4255,  0.1028,  0.1738]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3881464004516602\n",
      "Model outputs:  tensor([[[-0.2859,  0.5259, -0.7681,  ...,  0.6849,  0.2279,  0.2266],\n",
      "         [ 0.0105,  0.6392, -0.9725,  ...,  0.7719,  0.2586,  0.3080],\n",
      "         [-0.0970,  0.5010, -0.5475,  ...,  0.6643,  0.0826,  0.2472],\n",
      "         ...,\n",
      "         [ 0.1351,  0.4465, -0.8688,  ...,  0.7412,  0.4704, -0.0905],\n",
      "         [ 0.1032,  0.0298, -0.7056,  ...,  0.7441,  0.2297,  0.0511],\n",
      "         [-0.4169,  0.7343, -0.6910,  ...,  0.6767,  0.4490,  0.0068]],\n",
      "\n",
      "        [[ 0.2368,  0.0485, -0.4319,  ..., -0.3504,  0.0717,  0.1769],\n",
      "         [ 0.1752, -0.0424, -0.5874,  ..., -0.0182,  0.1774, -0.0021],\n",
      "         [ 0.4609, -0.0203, -0.0487,  ..., -0.3956,  0.2286,  0.0386],\n",
      "         ...,\n",
      "         [ 0.0842,  0.1465, -0.5549,  ..., -0.0223, -0.0893, -0.1211],\n",
      "         [ 0.3581, -0.2752, -0.7188,  ..., -0.0421, -0.2748,  0.0549],\n",
      "         [ 0.2381,  0.3229, -0.4202,  ..., -0.2917, -0.1944, -0.0173]],\n",
      "\n",
      "        [[ 0.3764,  0.4139, -0.4701,  ..., -0.3284,  0.1191,  0.5937],\n",
      "         [-0.1044,  0.1971, -0.6241,  ..., -0.2345, -0.1755,  0.4971],\n",
      "         [-0.0393,  0.2630, -0.2182,  ..., -0.3165, -0.2465,  0.5726],\n",
      "         ...,\n",
      "         [ 0.2480,  0.1707, -0.5642,  ..., -0.1891, -0.0729,  0.6841],\n",
      "         [ 0.3830,  0.3137, -0.3797,  ..., -0.2249, -0.0606,  0.6103],\n",
      "         [ 0.3073,  0.1208, -0.3281,  ..., -0.3174, -0.1866,  0.4281]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1202,  0.1981, -0.5937,  ..., -0.1593,  0.0294,  0.4554],\n",
      "         [ 0.2244,  0.2183, -0.7612,  ..., -0.1164,  0.0416,  0.5105],\n",
      "         [-0.1017,  0.0572, -0.3174,  ..., -0.4485, -0.1934,  0.5092],\n",
      "         ...,\n",
      "         [ 0.0284,  0.2640, -0.3550,  ..., -0.1691,  0.0242,  0.5877],\n",
      "         [ 0.1615,  0.1440, -0.4120,  ..., -0.1527, -0.1473,  0.3725],\n",
      "         [-0.0344,  0.3898, -0.2247,  ..., -0.5238, -0.2583,  0.5558]],\n",
      "\n",
      "        [[-0.3516,  0.4894, -0.0394,  ...,  0.9478,  0.1943, -0.0486],\n",
      "         [-0.3443,  0.5381, -0.5448,  ...,  0.6241,  0.1659, -0.0194],\n",
      "         [-0.1735,  0.1791, -0.1506,  ...,  0.7449,  0.6060, -0.0749],\n",
      "         ...,\n",
      "         [-0.3365,  0.2664, -0.1510,  ...,  0.6726,  0.4347, -0.0806],\n",
      "         [ 0.0582, -0.2563, -0.3184,  ...,  0.7119,  0.5917, -0.2682],\n",
      "         [-0.2541,  0.2803, -0.3184,  ...,  0.5554,  0.3662, -0.2758]],\n",
      "\n",
      "        [[ 0.5069,  0.0864,  0.1529,  ...,  0.2766, -0.0588, -0.4000],\n",
      "         [ 0.3495, -0.0734, -0.2280,  ...,  0.7705, -0.1560, -0.4474],\n",
      "         [ 0.4631,  0.0236,  0.1017,  ...,  0.4874, -0.0097, -0.3088],\n",
      "         ...,\n",
      "         [ 0.4484,  0.0188, -0.3099,  ...,  0.5015,  0.2326, -0.1845],\n",
      "         [ 0.4361, -0.1797,  0.1761,  ...,  0.6067, -0.0175, -0.3029],\n",
      "         [ 0.4119,  0.1679, -0.1434,  ...,  0.3440,  0.0410, -0.4264]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3910585641860962\n",
      "Model outputs:  tensor([[[-6.7740e-02,  2.0321e-01, -3.7575e-01,  ..., -3.2310e-01,\n",
      "          -6.4243e-02,  4.2336e-01],\n",
      "         [ 1.2036e-01, -2.8422e-02, -3.8046e-01,  ..., -1.7479e-01,\n",
      "           1.0097e-01,  6.7539e-01],\n",
      "         [ 9.9312e-02,  2.1242e-01, -5.4456e-01,  ..., -2.4607e-01,\n",
      "           1.1450e-01,  4.9769e-01],\n",
      "         ...,\n",
      "         [-2.0753e-01, -7.2425e-02, -5.5321e-01,  ..., -1.4224e-01,\n",
      "          -2.6581e-01,  5.1846e-01],\n",
      "         [-1.7571e-01, -1.4654e-01, -4.5903e-01,  ..., -8.5377e-02,\n",
      "          -2.5176e-01,  6.7449e-01],\n",
      "         [ 1.2611e-01,  3.0827e-01, -6.8445e-01,  ..., -2.3485e-01,\n",
      "          -2.0401e-01,  4.9987e-01]],\n",
      "\n",
      "        [[-4.7856e-01,  5.3386e-01, -8.9422e-01,  ..., -2.0198e-01,\n",
      "           8.2450e-02,  3.9559e-01],\n",
      "         [-4.4992e-01,  2.3043e-01, -8.0886e-01,  ...,  3.3128e-01,\n",
      "           1.5354e-02,  2.9482e-01],\n",
      "         [-4.4362e-01,  3.3728e-01, -9.2665e-01,  ...,  3.3195e-01,\n",
      "           9.6654e-02,  3.6412e-02],\n",
      "         ...,\n",
      "         [-5.9237e-01, -8.6064e-02, -9.9604e-01,  ...,  1.9895e-01,\n",
      "          -1.1026e-03,  2.8365e-01],\n",
      "         [-4.0122e-01, -1.8240e-01, -7.7267e-01,  ...,  2.6237e-01,\n",
      "           2.9192e-01,  3.7128e-01],\n",
      "         [-5.9573e-01,  3.3465e-01, -8.0254e-01,  ...,  9.3523e-02,\n",
      "          -1.5474e-02,  1.9047e-01]],\n",
      "\n",
      "        [[ 1.0796e-01,  1.6962e-01, -4.4322e-01,  ..., -6.1582e-01,\n",
      "           8.4969e-01,  4.9714e-01],\n",
      "         [ 1.8686e-02,  1.1215e-01, -5.8396e-01,  ..., -6.6240e-01,\n",
      "           6.3353e-01,  7.6295e-01],\n",
      "         [-1.1689e-01,  2.4491e-01, -5.8194e-01,  ..., -5.4919e-01,\n",
      "           4.9559e-01,  9.0915e-01],\n",
      "         ...,\n",
      "         [-1.2302e-01,  1.7232e-01, -5.5083e-01,  ..., -3.3674e-01,\n",
      "           4.4267e-01,  3.0249e-01],\n",
      "         [-2.6720e-02, -2.4479e-01, -3.9881e-01,  ..., -4.2774e-01,\n",
      "           3.6594e-01,  7.3604e-01],\n",
      "         [-1.5635e-02,  3.0829e-01, -4.7752e-01,  ..., -5.3604e-01,\n",
      "           5.3100e-01,  8.1579e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.1224e-01,  4.8954e-01, -8.0383e-01,  ..., -1.7386e-02,\n",
      "           3.1328e-01,  9.3210e-02],\n",
      "         [-5.4621e-01,  2.1265e-01, -7.4180e-01,  ...,  4.2166e-01,\n",
      "           2.2146e-01,  2.5703e-01],\n",
      "         [-4.3889e-01,  2.6390e-01, -8.4275e-01,  ...,  9.4307e-02,\n",
      "           1.3376e-01,  1.9077e-01],\n",
      "         ...,\n",
      "         [-5.3699e-01, -1.1960e-03, -1.0309e+00,  ...,  1.0055e-01,\n",
      "           2.4474e-01,  5.4412e-01],\n",
      "         [-7.5256e-01, -1.5131e-01, -7.7476e-01,  ...,  7.1408e-02,\n",
      "          -1.7992e-01,  4.4772e-01],\n",
      "         [-5.9561e-01,  3.8164e-01, -1.1245e+00,  ...,  1.0109e-01,\n",
      "          -1.9348e-01,  3.1131e-01]],\n",
      "\n",
      "        [[-5.2805e-02,  5.3672e-01, -6.8865e-01,  ...,  2.4434e-01,\n",
      "           2.2185e-03,  3.8414e-01],\n",
      "         [ 4.8136e-02,  2.4241e-01, -7.7827e-01,  ...,  5.3583e-01,\n",
      "           3.2285e-01,  2.5609e-01],\n",
      "         [-1.1628e-01,  4.0840e-01, -8.3398e-01,  ...,  2.1779e-01,\n",
      "          -1.0228e-01,  1.1411e-01],\n",
      "         ...,\n",
      "         [-1.5088e-01,  1.6194e-01, -6.8531e-01,  ...,  4.0662e-01,\n",
      "          -1.8735e-01,  4.2249e-02],\n",
      "         [ 9.4015e-03, -3.4789e-01, -7.3192e-01,  ...,  2.4527e-01,\n",
      "           4.4586e-02,  2.5075e-01],\n",
      "         [-3.2366e-02,  5.2761e-01, -9.6700e-01,  ...,  4.7086e-01,\n",
      "          -1.1733e-01,  6.2302e-02]],\n",
      "\n",
      "        [[-1.2916e-01,  3.2902e-01, -7.9210e-01,  ...,  1.7755e-01,\n",
      "           2.8172e-01,  2.9551e-01],\n",
      "         [-1.6655e-02,  3.0699e-02, -5.4601e-01,  ...,  3.3296e-01,\n",
      "           2.9680e-01,  1.6857e-01],\n",
      "         [-1.3543e-01,  4.9677e-01, -7.4306e-01,  ...,  1.4498e-01,\n",
      "           1.3272e-01,  2.8523e-01],\n",
      "         ...,\n",
      "         [-3.6195e-02,  3.6944e-02, -8.4933e-01,  ...,  2.5346e-01,\n",
      "          -3.3825e-02,  3.8644e-01],\n",
      "         [-2.5822e-01,  1.0013e-01, -6.0250e-01,  ...,  2.7806e-01,\n",
      "           1.4956e-01,  4.8430e-01],\n",
      "         [ 1.6633e-01,  3.3130e-01, -7.7590e-01,  ...,  2.9047e-01,\n",
      "           9.2876e-02,  3.3888e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2497329711914062\n",
      "Model outputs:  tensor([[[-0.4639,  0.4365, -0.6413,  ..., -0.4557,  0.1521,  0.2022],\n",
      "         [-0.0016,  0.4324, -0.4479,  ..., -0.4143,  0.1867,  0.0962],\n",
      "         [-0.1880,  0.5570, -0.3542,  ..., -0.1722,  0.3536, -0.1085],\n",
      "         ...,\n",
      "         [-0.2620, -0.0572, -0.6879,  ...,  0.2311,  0.2725,  0.4437],\n",
      "         [-0.0904,  0.2948, -0.6915,  ..., -0.1329,  0.1325,  0.4501],\n",
      "         [-0.1778,  0.2691, -0.3477,  ..., -0.4763,  0.1542,  0.1105]],\n",
      "\n",
      "        [[-0.4355,  0.5508, -0.2034,  ...,  0.2451,  0.4004, -0.1620],\n",
      "         [-0.2874,  0.5553, -0.4007,  ...,  0.2254,  0.3435, -0.1264],\n",
      "         [-0.2161,  0.5741, -0.5195,  ...,  0.0810,  0.3224, -0.2617],\n",
      "         ...,\n",
      "         [-0.5563,  0.0786, -0.2542,  ...,  0.9762,  0.3304,  0.3601],\n",
      "         [-0.3533,  0.3890, -0.4915,  ...,  0.5602,  0.3418,  0.1784],\n",
      "         [-0.3884,  0.5949, -0.4798,  ...,  0.4365,  0.7527,  0.1417]],\n",
      "\n",
      "        [[-0.1323,  0.1843, -0.3339,  ..., -0.6585,  0.0560,  0.2893],\n",
      "         [ 0.2388,  0.0501, -0.3954,  ..., -0.4338,  0.0310,  0.1959],\n",
      "         [ 0.0851,  0.0528, -0.4565,  ..., -0.6330,  0.0114,  0.1686],\n",
      "         ...,\n",
      "         [-0.1383, -0.2495, -0.4718,  ...,  0.0323, -0.0808,  0.5432],\n",
      "         [-0.2444,  0.0072, -0.4878,  ..., -0.3450, -0.4832,  0.3881],\n",
      "         [-0.1361,  0.3043, -0.6608,  ..., -0.4827,  0.0595,  0.5317]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4566,  0.7804, -0.7091,  ...,  0.1711,  0.4353,  0.0435],\n",
      "         [-0.3994,  0.3615, -0.5913,  ...,  0.1396,  0.6593, -0.4424],\n",
      "         [-0.1486,  0.6135, -0.7913,  ...,  0.1083,  0.4323, -0.2304],\n",
      "         ...,\n",
      "         [-0.1490,  0.3292, -1.0869,  ...,  0.9960,  0.4734,  0.1495],\n",
      "         [-0.3864,  0.7224, -0.6237,  ...,  0.5970,  0.6364,  0.0302],\n",
      "         [-0.5664,  0.5036, -0.7897,  ...,  0.3565,  0.5472, -0.2321]],\n",
      "\n",
      "        [[-0.5074,  0.6503, -0.4898,  ...,  0.2711,  0.4683, -0.3046],\n",
      "         [-0.4996,  0.7037, -0.3001,  ...,  0.0224,  0.3020, -0.1141],\n",
      "         [-0.2105,  0.6555, -0.0558,  ...,  0.0197,  0.3966, -0.5157],\n",
      "         ...,\n",
      "         [-0.1531,  0.2606, -0.3065,  ...,  0.8319,  0.6771,  0.0258],\n",
      "         [-0.5369,  0.7878, -0.2283,  ...,  0.6043,  0.5783,  0.1972],\n",
      "         [-0.4298,  0.6332, -0.5458,  ...,  0.2735,  0.7850,  0.1002]],\n",
      "\n",
      "        [[-0.0557, -0.0592, -0.5828,  ..., -0.8686,  0.9068,  0.4252],\n",
      "         [-0.0608,  0.2986, -0.2643,  ..., -0.9416,  0.7150,  0.3676],\n",
      "         [ 0.0568,  0.0856, -0.4333,  ..., -0.8521,  0.6073,  0.3214],\n",
      "         ...,\n",
      "         [-0.0948,  0.1535, -0.1223,  ..., -0.1684,  0.8275,  0.4860],\n",
      "         [-0.0797,  0.0357, -0.6238,  ..., -0.8758,  0.8318,  0.2949],\n",
      "         [ 0.0948, -0.1960, -0.4718,  ..., -0.9004,  0.9023,  0.3711]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0626957416534424\n",
      "Model outputs:  tensor([[[-2.2294e-01,  1.4052e-01, -2.7262e-01,  ...,  8.3782e-01,\n",
      "           2.9265e-01,  1.0643e-01],\n",
      "         [-1.9125e-02, -2.3137e-01, -3.7807e-01,  ...,  3.6772e-01,\n",
      "          -2.2420e-02,  7.0442e-02],\n",
      "         [-2.0613e-01,  8.1218e-02, -3.8994e-01,  ...,  8.9456e-01,\n",
      "           4.3259e-02,  1.3478e-02],\n",
      "         ...,\n",
      "         [-9.5790e-02,  6.9485e-02, -2.1391e-01,  ...,  6.3429e-01,\n",
      "           3.3281e-01,  3.8483e-02],\n",
      "         [ 1.3148e-01,  1.0644e-01, -1.5506e-01,  ...,  7.2325e-01,\n",
      "           2.5708e-01,  2.0367e-01],\n",
      "         [ 2.2509e-02,  1.7668e-01, -3.6082e-01,  ...,  6.6214e-01,\n",
      "           2.0648e-01,  7.2566e-02]],\n",
      "\n",
      "        [[-3.0512e-01,  1.2087e-01, -2.7676e-01,  ...,  6.5882e-01,\n",
      "           2.8847e-01, -1.8483e-01],\n",
      "         [-3.2752e-01, -3.1074e-01, -3.6604e-01,  ...,  2.4800e-01,\n",
      "          -3.7567e-02, -1.0091e-01],\n",
      "         [-3.8392e-01, -5.6102e-02, -2.7109e-01,  ...,  5.9831e-01,\n",
      "           2.3319e-01,  2.7680e-01],\n",
      "         ...,\n",
      "         [-2.6895e-01,  2.6274e-01, -2.8917e-01,  ...,  4.6323e-01,\n",
      "           4.3443e-01,  7.8864e-02],\n",
      "         [ 4.3100e-01, -1.5497e-01, -8.1251e-02,  ...,  5.1217e-01,\n",
      "           1.3832e-01,  2.8559e-02],\n",
      "         [-4.1936e-01, -4.9111e-02, -1.7412e-01,  ...,  6.4660e-01,\n",
      "           1.7698e-01, -4.6462e-02]],\n",
      "\n",
      "        [[-4.4035e-01, -6.3598e-02, -4.7842e-01,  ...,  3.5072e-01,\n",
      "          -5.5855e-03,  2.7728e-01],\n",
      "         [ 2.0670e-01, -1.5946e-01, -5.2090e-01,  ...,  2.7948e-01,\n",
      "           6.5192e-02,  3.9699e-01],\n",
      "         [-3.6190e-01,  1.0677e-02, -7.5769e-01,  ...,  4.7406e-01,\n",
      "           1.5044e-01,  3.1194e-01],\n",
      "         ...,\n",
      "         [-5.0569e-01,  5.5815e-01, -4.6157e-01,  ...,  3.3683e-01,\n",
      "           2.0884e-01,  2.2788e-01],\n",
      "         [-7.2956e-02, -1.4800e-01, -2.5550e-01,  ...,  3.3925e-01,\n",
      "           1.9142e-01,  1.8040e-01],\n",
      "         [-6.1242e-01,  2.3993e-01, -2.9967e-01,  ...,  2.5230e-01,\n",
      "           2.7922e-01,  3.5008e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4.1252e-03,  1.2077e-01, -3.7251e-01,  ..., -2.8325e-01,\n",
      "           4.1290e-01,  6.7421e-01],\n",
      "         [ 3.5472e-01, -3.6856e-01, -4.9397e-01,  ..., -6.3943e-01,\n",
      "           1.9152e-01,  5.2669e-01],\n",
      "         [ 1.4906e-01, -1.6975e-02, -1.9858e-01,  ..., -3.6520e-04,\n",
      "           2.2813e-01,  5.1690e-01],\n",
      "         ...,\n",
      "         [-5.9774e-02,  6.8681e-01, -4.7295e-01,  ..., -4.3425e-01,\n",
      "           6.0693e-01,  5.3659e-01],\n",
      "         [ 4.3388e-01, -3.3431e-01, -3.1411e-01,  ..., -5.4782e-01,\n",
      "           2.3934e-01,  6.6468e-01],\n",
      "         [ 1.6908e-01,  4.3637e-02, -5.2300e-01,  ..., -6.1029e-01,\n",
      "           5.7903e-01,  6.6184e-01]],\n",
      "\n",
      "        [[-1.9812e-01, -1.6332e-01, -1.3521e-01,  ...,  9.0735e-01,\n",
      "           4.2413e-01, -1.2423e-02],\n",
      "         [-2.4432e-02, -1.8660e-01, -1.7926e-01,  ...,  5.0300e-01,\n",
      "           2.2197e-01, -2.3631e-01],\n",
      "         [-3.2308e-01,  1.6052e-01, -2.6956e-01,  ...,  7.9903e-01,\n",
      "           3.8962e-01,  3.5476e-01],\n",
      "         ...,\n",
      "         [-1.1549e-01,  7.6389e-02, -4.5419e-01,  ...,  2.5410e-01,\n",
      "           2.4191e-01,  1.2095e-01],\n",
      "         [-6.9507e-03,  3.8570e-02,  1.3626e-03,  ...,  6.1559e-01,\n",
      "           2.2975e-01, -1.0598e-02],\n",
      "         [-3.0024e-01,  2.2918e-01, -3.3440e-01,  ...,  6.5383e-01,\n",
      "           3.9719e-01,  1.2238e-01]],\n",
      "\n",
      "        [[ 6.0127e-02, -2.3046e-01, -9.0242e-01,  ...,  5.7987e-01,\n",
      "           2.2936e-01,  3.1895e-01],\n",
      "         [ 2.6355e-01, -4.1609e-02, -7.0624e-01,  ...,  2.6174e-01,\n",
      "          -5.1772e-01,  1.8951e-01],\n",
      "         [ 1.9769e-01, -2.4335e-01, -8.5342e-01,  ...,  4.6335e-01,\n",
      "           1.0066e-01,  3.5024e-01],\n",
      "         ...,\n",
      "         [-1.3893e-01,  9.7066e-02, -7.7974e-01,  ...,  1.2824e-01,\n",
      "          -5.6498e-03,  6.3944e-02],\n",
      "         [ 3.3979e-01, -2.0022e-01, -4.9042e-01,  ...,  2.6512e-01,\n",
      "          -5.7751e-03,  2.4114e-01],\n",
      "         [ 4.7885e-02,  7.9435e-02, -8.9919e-01,  ...,  4.1281e-01,\n",
      "          -2.0367e-01,  3.7224e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1531180143356323\n",
      "Model outputs:  tensor([[[-0.2171, -0.0332, -0.7676,  ..., -0.0700,  0.2079,  0.8224],\n",
      "         [-0.1074,  0.1492, -0.3719,  ..., -0.1428,  0.7455,  0.6773],\n",
      "         [-0.0491,  0.3726, -0.5854,  ..., -0.1206,  0.7225,  0.6981],\n",
      "         ...,\n",
      "         [-0.1796,  0.0975, -0.7815,  ..., -0.3877,  0.6740,  0.9872],\n",
      "         [ 0.0513, -0.0046, -0.1208,  ..., -0.2384,  0.4748,  0.7196],\n",
      "         [-0.1341,  0.4600, -1.1066,  ..., -0.1114,  0.8404,  0.7116]],\n",
      "\n",
      "        [[-0.0877, -0.1624, -0.6937,  ...,  0.0934, -0.5387,  0.8087],\n",
      "         [-0.1253,  0.1976, -0.7958,  ..., -0.0545, -0.2263,  0.5966],\n",
      "         [ 0.1046,  0.1372, -0.4334,  ...,  0.0773,  0.0505,  0.6630],\n",
      "         ...,\n",
      "         [-0.0233,  0.0065, -0.4734,  ..., -0.3471, -0.1874,  0.3950],\n",
      "         [ 0.2372, -0.0282, -0.3458,  ..., -0.0259, -0.0200,  0.7597],\n",
      "         [ 0.2313,  0.1740, -0.5253,  ..., -0.1617, -0.3301,  0.5966]],\n",
      "\n",
      "        [[ 0.1051, -0.1723, -0.3136,  ...,  0.5767, -0.1000, -0.2813],\n",
      "         [ 0.1686, -0.1397, -0.2104,  ...,  0.7492, -0.0152, -0.3893],\n",
      "         [ 0.4808,  0.0096, -0.1944,  ...,  0.5287,  0.1123, -0.1121],\n",
      "         ...,\n",
      "         [ 0.5313, -0.0812, -0.4039,  ...,  0.6402,  0.1563, -0.4663],\n",
      "         [ 0.3259, -0.3216,  0.0634,  ...,  0.6077, -0.0438, -0.4040],\n",
      "         [ 0.4423, -0.0135, -0.3047,  ...,  0.5465,  0.0024, -0.2972]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2422,  0.3718, -0.9815,  ...,  1.0617, -0.3605,  0.3799],\n",
      "         [-0.2812,  0.6481, -0.9866,  ...,  1.0004,  0.1570,  0.1379],\n",
      "         [-0.6007,  0.3634, -0.8516,  ...,  0.8198,  0.1589,  0.1379],\n",
      "         ...,\n",
      "         [-0.1231,  0.4274, -0.8547,  ...,  1.0523,  0.1061, -0.0982],\n",
      "         [-0.1398,  0.3313, -0.8380,  ...,  0.8527,  0.1993,  0.3461],\n",
      "         [-0.1105,  0.4360, -1.0903,  ...,  0.8430,  0.0382,  0.1122]],\n",
      "\n",
      "        [[-0.0744,  0.0635, -0.5767,  ..., -0.1118, -0.6574,  0.6633],\n",
      "         [-0.1257,  0.0749, -0.3377,  ..., -0.0794, -0.1344,  0.6846],\n",
      "         [ 0.2340, -0.0292, -0.6348,  ..., -0.1257, -0.1792,  0.8096],\n",
      "         ...,\n",
      "         [-0.0514,  0.0247, -0.5536,  ..., -0.1453, -0.0829,  0.5961],\n",
      "         [ 0.0199, -0.0107, -0.2747,  ...,  0.0869,  0.0320,  0.6380],\n",
      "         [-0.0509,  0.1717, -0.6650,  ..., -0.3881, -0.3053,  0.5698]],\n",
      "\n",
      "        [[-0.8451,  0.2520, -0.8304,  ...,  0.5297, -0.2475,  0.5158],\n",
      "         [-0.5551,  0.4150, -0.7417,  ...,  0.9301,  0.2624,  0.2047],\n",
      "         [-0.2156,  0.4487, -0.7044,  ...,  0.8212,  0.3061,  0.2449],\n",
      "         ...,\n",
      "         [-0.2666,  0.2830, -0.6926,  ...,  0.8990,  0.6426,  0.2352],\n",
      "         [-0.5339,  0.1337, -0.6884,  ...,  0.7137,  0.5035,  0.1030],\n",
      "         [-0.5688,  0.5606, -0.9278,  ...,  0.6661,  0.3200,  0.4346]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.210291862487793\n",
      "Model outputs:  tensor([[[-5.7917e-01,  3.5253e-01, -2.2318e-01,  ...,  1.3775e-01,\n",
      "           3.0530e-02,  1.3844e-01],\n",
      "         [-4.0620e-01,  3.3328e-01, -4.0627e-01,  ...,  1.4507e-01,\n",
      "           3.1438e-02,  3.9563e-01],\n",
      "         [-4.2111e-01,  2.0831e-01, -4.5912e-01,  ...,  3.3515e-01,\n",
      "           3.2038e-01,  5.3953e-01],\n",
      "         ...,\n",
      "         [-7.9951e-01,  4.6283e-01, -6.3848e-01,  ...,  2.0962e-01,\n",
      "           2.8299e-01,  2.1225e-01],\n",
      "         [-2.7660e-01,  2.4807e-01, -4.3454e-01,  ...,  3.9319e-01,\n",
      "           3.7268e-01,  4.2492e-01],\n",
      "         [-5.5447e-01,  1.0999e-02, -3.1880e-01,  ...,  4.1860e-01,\n",
      "           2.7384e-01,  2.0030e-01]],\n",
      "\n",
      "        [[-3.4526e-01,  4.7926e-01,  3.4257e-01,  ...,  5.2485e-01,\n",
      "           5.1843e-04, -1.3393e-01],\n",
      "         [-2.6882e-01,  4.2863e-01, -1.2214e-01,  ...,  2.3778e-01,\n",
      "           3.2601e-01, -1.0347e-01],\n",
      "         [-4.4644e-01,  1.9893e-01, -2.4527e-01,  ...,  6.5626e-01,\n",
      "           5.0155e-01,  1.6951e-01],\n",
      "         ...,\n",
      "         [-5.4421e-01,  5.8716e-01,  1.4224e-02,  ...,  7.2641e-01,\n",
      "           5.0910e-01, -3.6124e-03],\n",
      "         [-3.9445e-01,  8.7487e-02, -3.3923e-01,  ...,  5.7694e-01,\n",
      "           2.4646e-01,  3.0932e-01],\n",
      "         [-2.7931e-01, -1.1460e-02, -3.7149e-01,  ...,  7.6018e-01,\n",
      "           3.9987e-01,  7.0822e-02]],\n",
      "\n",
      "        [[ 5.7584e-01, -3.9486e-02,  1.9447e-01,  ...,  8.7656e-03,\n",
      "           1.9075e-02, -5.4605e-01],\n",
      "         [ 1.9494e-01, -5.6498e-02, -8.7961e-02,  ...,  5.4044e-03,\n",
      "           8.1958e-03, -5.9920e-01],\n",
      "         [ 5.4570e-01, -3.8453e-01, -8.8080e-02,  ...,  2.8076e-01,\n",
      "           1.6953e-01, -2.9357e-01],\n",
      "         ...,\n",
      "         [ 3.2405e-01, -1.2824e-01, -3.3224e-01,  ..., -4.2341e-01,\n",
      "           1.8236e-01,  2.2711e-02],\n",
      "         [ 7.5340e-02, -2.0827e-01, -2.4938e-01,  ..., -5.1143e-01,\n",
      "           3.6064e-01, -3.4599e-02],\n",
      "         [ 1.3526e-01, -4.0253e-01, -1.4379e-01,  ..., -2.0340e-01,\n",
      "           3.8652e-01, -4.2913e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0677e-01,  1.0288e-01, -2.6290e-01,  ...,  2.4633e-01,\n",
      "          -2.4285e-01, -5.9671e-02],\n",
      "         [ 1.3795e-01,  3.5440e-01, -6.9636e-01,  ...,  2.7729e-01,\n",
      "           1.0148e-01,  3.0634e-01],\n",
      "         [ 2.4841e-01,  1.4826e-01, -7.6996e-01,  ...,  4.6229e-01,\n",
      "           2.9801e-01,  2.7612e-01],\n",
      "         ...,\n",
      "         [-3.5771e-01,  2.2683e-01, -8.5854e-01,  ...,  2.7791e-01,\n",
      "          -1.3149e-01,  2.8778e-01],\n",
      "         [ 5.1670e-02,  1.0899e-01, -5.9898e-01,  ...,  6.6540e-01,\n",
      "           1.9657e-01,  1.8040e-02],\n",
      "         [ 1.2063e-01,  6.0563e-02, -7.0084e-01,  ...,  3.6665e-01,\n",
      "          -3.4945e-02,  2.6089e-01]],\n",
      "\n",
      "        [[ 4.9986e-02,  4.1442e-01, -3.6050e-01,  ...,  3.4862e-01,\n",
      "          -8.0584e-02, -1.7317e-01],\n",
      "         [ 7.3077e-02,  3.3490e-01, -9.1375e-01,  ...,  2.8414e-01,\n",
      "           8.9837e-03, -3.0133e-02],\n",
      "         [ 2.6776e-01,  1.1325e-02, -8.2210e-01,  ...,  3.1709e-01,\n",
      "           6.7993e-02,  1.9360e-01],\n",
      "         ...,\n",
      "         [-1.5805e-01,  2.0835e-01, -8.5536e-01,  ...,  3.7860e-01,\n",
      "          -7.1893e-02,  2.5077e-01],\n",
      "         [ 8.6985e-02,  1.9705e-01, -7.9290e-01,  ...,  1.8258e-01,\n",
      "           9.7620e-03,  1.9271e-01],\n",
      "         [-1.2197e-01,  5.4948e-02, -5.9036e-01,  ...,  4.8856e-01,\n",
      "           2.5833e-01,  1.9480e-01]],\n",
      "\n",
      "        [[-3.9667e-01,  5.7935e-01, -2.8806e-01,  ...,  5.2177e-01,\n",
      "           7.6441e-02,  1.2144e-01],\n",
      "         [-5.2557e-01,  3.5252e-01, -6.5494e-01,  ...,  4.5874e-01,\n",
      "           3.7843e-01,  8.4819e-02],\n",
      "         [-5.5215e-01,  2.4067e-01, -5.9607e-01,  ...,  7.6445e-01,\n",
      "           5.7233e-01,  1.4148e-01],\n",
      "         ...,\n",
      "         [-5.8294e-01,  4.0466e-01, -8.3144e-01,  ...,  6.3688e-01,\n",
      "           3.5389e-01,  2.6921e-01],\n",
      "         [-3.0493e-01,  3.2083e-01, -6.7594e-01,  ...,  4.4000e-01,\n",
      "           2.6209e-01,  1.6584e-01],\n",
      "         [-1.1762e-01, -3.2512e-02, -5.8014e-01,  ...,  7.5080e-01,\n",
      "           5.7107e-01,  2.0513e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.261683464050293\n",
      "Model outputs:  tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.2174,  0.2656, -0.2968,  ...,  0.4627,  0.4591, -0.0159],\n",
      "         [-0.0954,  0.1801, -0.5514,  ...,  0.5925,  0.4474,  0.3382],\n",
      "         ...,\n",
      "         [-0.4074,  0.0095, -0.5265,  ...,  0.2382,  0.2308,  0.4256],\n",
      "         [-0.6701,  0.5127, -0.6577,  ...,  0.4239,  0.0137, -0.0470],\n",
      "         [-0.6307,  0.3164, -0.2002,  ...,  0.4382,  0.1469,  0.0313]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.3451,  0.2766, -0.3591,  ...,  0.8256,  0.2929, -0.1082],\n",
      "         [-0.0962, -0.0910, -0.1335,  ...,  0.7503,  0.3742, -0.2194],\n",
      "         ...,\n",
      "         [-0.3306, -0.0689, -0.1013,  ...,  0.8757,  0.1017, -0.1142],\n",
      "         [-0.6324,  0.2616, -0.4105,  ...,  0.7439,  0.4813, -0.2752],\n",
      "         [-0.4985,  0.3530, -0.2470,  ...,  0.6968,  0.4217, -0.0061]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.1709,  0.2498, -0.2333,  ..., -0.0087,  0.1153,  0.3779],\n",
      "         [ 0.1361,  0.0592, -0.3424,  ...,  0.0067, -0.1535,  0.5966],\n",
      "         ...,\n",
      "         [-0.0139, -0.1845, -0.5708,  ..., -0.1449, -0.0797,  0.4573],\n",
      "         [ 0.2369,  0.1212, -0.9473,  ..., -0.1082,  0.1948,  0.2426],\n",
      "         [ 0.0968, -0.0181, -0.2938,  ..., -0.0345,  0.0495,  0.2643]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.2994,  0.4476, -0.3727,  ...,  0.7352,  0.1824,  0.0239],\n",
      "         [-0.2111, -0.0623, -0.4785,  ...,  0.5354,  0.1707,  0.3467],\n",
      "         ...,\n",
      "         [-0.3570,  0.1700, -0.5955,  ...,  0.7375,  0.2298,  0.2428],\n",
      "         [-0.6587,  0.5876, -0.7632,  ...,  0.3580,  0.3155,  0.0897],\n",
      "         [-0.6070,  0.5183, -0.1872,  ...,  0.1684,  0.2085,  0.1759]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.4064,  0.3753, -0.6709,  ...,  0.3278, -0.2010,  0.3312],\n",
      "         [ 0.2982,  0.0279, -0.7903,  ...,  0.4207,  0.0798,  0.4494],\n",
      "         ...,\n",
      "         [ 0.1097, -0.0535, -0.4574,  ...,  0.7599, -0.0125,  0.3159],\n",
      "         [ 0.0174,  0.2737, -0.9480,  ...,  0.4253, -0.0147,  0.1922],\n",
      "         [ 0.2044,  0.2708, -0.6536,  ...,  0.3195, -0.1360,  0.1987]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.1736,  0.3257, -0.3549,  ...,  0.4734,  0.4303, -0.1042],\n",
      "         [-0.3164,  0.1027, -0.3413,  ...,  0.3892,  0.2880,  0.4723],\n",
      "         ...,\n",
      "         [-0.2807,  0.1523, -0.2670,  ...,  0.6207,  0.3536,  0.3850],\n",
      "         [-0.4258,  0.5235, -0.6827,  ...,  0.4737,  0.2753,  0.2249],\n",
      "         [-0.4568,  0.3768, -0.3908,  ...,  0.1995,  0.2166,  0.3098]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [2/10], Loss: 1.2617\n",
      "Model outputs:  tensor([[[-0.0655,  0.1052, -0.5749,  ...,  0.2991, -0.1801,  0.6010],\n",
      "         [ 0.0145,  0.2572, -0.8583,  ..., -0.0013, -0.1498,  0.4256],\n",
      "         [ 0.0023,  0.1266, -0.6943,  ...,  0.3604, -0.0945,  0.3135],\n",
      "         ...,\n",
      "         [-0.2780,  0.3296, -0.8728,  ..., -0.2599, -0.3867,  0.3434],\n",
      "         [ 0.1880,  0.0894, -0.9386,  ...,  0.4468,  0.1201,  0.3330],\n",
      "         [ 0.1816,  0.1305, -0.6795,  ...,  0.0068, -0.0071,  0.0868]],\n",
      "\n",
      "        [[-0.0182, -0.1492, -0.5236,  ..., -0.3005, -0.1958,  0.6800],\n",
      "         [ 0.1146, -0.1348, -0.4939,  ..., -0.4903, -0.1992,  0.6019],\n",
      "         [ 0.2236, -0.1252, -0.4029,  ...,  0.0444, -0.1852,  0.5799],\n",
      "         ...,\n",
      "         [-0.0753, -0.2365, -0.5885,  ..., -0.3897, -0.4785,  0.6815],\n",
      "         [ 0.1476,  0.1863, -0.6549,  ..., -0.1653, -0.3278,  0.7608],\n",
      "         [ 0.0346,  0.0236, -0.5253,  ..., -0.3874, -0.2935,  0.7170]],\n",
      "\n",
      "        [[-0.3289, -0.0597, -0.4536,  ..., -0.0127, -0.2365,  0.4646],\n",
      "         [ 0.0876, -0.1369, -0.6382,  ..., -0.0778, -0.2521,  0.4931],\n",
      "         [-0.1441, -0.1205, -0.3646,  ...,  0.0082, -0.0413,  0.6092],\n",
      "         ...,\n",
      "         [-0.3800, -0.3109, -0.5331,  ..., -0.3755, -0.6596,  0.5242],\n",
      "         [ 0.1378, -0.1525, -0.6711,  ..., -0.0350,  0.0135,  0.5302],\n",
      "         [ 0.2618,  0.0656, -0.5658,  ..., -0.4006, -0.2137,  0.8115]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3197, -0.1514, -0.6933,  ...,  0.1850, -0.0582,  0.4830],\n",
      "         [-0.2939,  0.0308, -0.9212,  ...,  0.0254,  0.0437,  0.5134],\n",
      "         [-0.0240, -0.0998, -0.4986,  ...,  0.3506,  0.1083,  0.3578],\n",
      "         ...,\n",
      "         [-0.4977, -0.0082, -0.7289,  ...,  0.0035, -0.2303,  0.3496],\n",
      "         [-0.1213,  0.1409, -0.7719,  ...,  0.0713,  0.1116,  0.3640],\n",
      "         [-0.4064,  0.4297, -0.6002,  ..., -0.2505, -0.1496,  0.2322]],\n",
      "\n",
      "        [[ 0.1363, -0.1467, -0.2306,  ..., -0.2393, -0.1715,  0.5276],\n",
      "         [ 0.0695, -0.0509, -0.2899,  ..., -0.0729, -0.0344,  0.5910],\n",
      "         [ 0.0448, -0.0507, -0.4817,  ...,  0.2246,  0.0254,  0.7000],\n",
      "         ...,\n",
      "         [-0.3675, -0.2027, -0.4977,  ..., -0.4929, -0.2111,  0.4175],\n",
      "         [ 0.0922,  0.0208, -0.6743,  ...,  0.0760, -0.0821,  0.5212],\n",
      "         [ 0.4449,  0.2298, -0.5758,  ..., -0.4879, -0.3649,  0.4043]],\n",
      "\n",
      "        [[-0.2759,  0.0760, -0.7897,  ...,  0.5304,  0.1356,  0.3320],\n",
      "         [-0.3097,  0.3088, -0.6998,  ...,  0.7656,  0.0659,  0.2290],\n",
      "         [-0.0602, -0.0016, -0.5002,  ...,  0.7880,  0.2760,  0.0180],\n",
      "         ...,\n",
      "         [-0.5798,  0.5382, -0.9609,  ...,  0.7663,  0.0721,  0.3585],\n",
      "         [-0.3967,  0.1542, -0.9442,  ...,  0.6768,  0.4049,  0.2360],\n",
      "         [-0.2591,  0.2544, -0.7091,  ...,  0.5548,  0.2740,  0.1580]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0952296257019043\n",
      "Model outputs:  tensor([[[-0.3246,  0.5058, -0.7501,  ...,  0.6466,  0.1584, -0.1112],\n",
      "         [-0.5988,  0.7804, -0.4052,  ...,  0.5453, -0.0829, -0.1876],\n",
      "         [-0.4879,  0.4124, -0.7014,  ...,  0.3582,  0.2159,  0.0510],\n",
      "         ...,\n",
      "         [-0.1626,  0.4033, -0.7906,  ...,  0.7188,  0.1472,  0.3080],\n",
      "         [-0.3868,  0.5202, -0.3794,  ...,  0.2279,  0.2365,  0.0341],\n",
      "         [-0.3897,  0.4712, -0.5918,  ...,  0.5782,  0.3648, -0.0925]],\n",
      "\n",
      "        [[-0.2852,  0.5775, -0.5705,  ...,  0.2037,  0.0409, -0.0752],\n",
      "         [-0.3353,  0.6072, -0.8187,  ...,  0.4149, -0.0083, -0.2546],\n",
      "         [-0.1652,  0.2917, -0.8020,  ...,  0.0616,  0.0488, -0.0153],\n",
      "         ...,\n",
      "         [-0.4856,  0.4001, -0.9404,  ...,  0.6039,  0.3113,  0.0486],\n",
      "         [-0.2183,  0.7673, -0.7322,  ...,  0.4379,  0.2044, -0.2176],\n",
      "         [-0.1387,  0.0352, -0.8144,  ...,  0.3413,  0.0390, -0.1353]],\n",
      "\n",
      "        [[-0.0067,  0.1728, -0.4000,  ..., -0.6569, -0.1095,  0.5092],\n",
      "         [ 0.1322,  0.1764, -0.4777,  ..., -0.2017, -0.1693,  0.4507],\n",
      "         [ 0.2277,  0.3690, -0.4712,  ..., -0.6823, -0.2376,  0.2721],\n",
      "         ...,\n",
      "         [ 0.1816,  0.0838, -0.4464,  ..., -0.2583, -0.1684,  0.8649],\n",
      "         [-0.0101,  0.4458, -0.3193,  ..., -0.4826, -0.4242,  0.4137],\n",
      "         [ 0.1358, -0.0914, -0.4589,  ..., -0.6753, -0.2714,  0.6553]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1703,  0.1263, -0.2788,  ..., -0.4372, -0.0652,  0.1341],\n",
      "         [-0.1134,  0.3329, -0.5750,  ..., -0.3999, -0.1900,  0.4190],\n",
      "         [-0.0199,  0.3120, -0.5162,  ..., -0.4739, -0.2912,  0.3445],\n",
      "         ...,\n",
      "         [ 0.2085, -0.0101, -0.5787,  ..., -0.3082, -0.1255,  0.4313],\n",
      "         [ 0.1148,  0.2090, -0.5042,  ..., -0.7016, -0.1371,  0.0917],\n",
      "         [ 0.1657,  0.1590, -0.6328,  ..., -0.5326, -0.3801,  0.5980]],\n",
      "\n",
      "        [[-0.3578,  0.2991, -0.0491,  ...,  0.6495,  0.3831, -0.0993],\n",
      "         [-0.8524,  0.6252, -0.0912,  ...,  0.4475,  0.3428, -0.2507],\n",
      "         [-0.4138,  0.4574, -0.1525,  ...,  0.5513,  0.2686, -0.2823],\n",
      "         ...,\n",
      "         [-0.3406,  0.3848, -0.2720,  ...,  0.7194,  0.3105, -0.2388],\n",
      "         [-0.5395,  0.5641, -0.0298,  ...,  0.5660,  0.2705, -0.5151],\n",
      "         [-0.2843,  0.2375,  0.0170,  ...,  0.5022,  0.2037, -0.1919]],\n",
      "\n",
      "        [[-0.1894,  0.7202, -0.8499,  ...,  0.3769,  0.1673, -0.0420],\n",
      "         [-0.4656,  0.8617, -0.5784,  ...,  0.6972,  0.1596, -0.2040],\n",
      "         [-0.2615,  0.6115, -0.7687,  ...,  0.3282,  0.4214, -0.0152],\n",
      "         ...,\n",
      "         [-0.1520,  0.3271, -0.7019,  ...,  0.6129,  0.1610,  0.1442],\n",
      "         [-0.4179,  0.7709, -0.5176,  ...,  0.2151,  0.0352, -0.1014],\n",
      "         [-0.4793,  0.5602, -0.7699,  ...,  0.3404,  0.4716, -0.0587]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2090610265731812\n",
      "Model outputs:  tensor([[[-0.5476,  0.7360,  0.1778,  ...,  0.1976,  0.0198, -0.1145],\n",
      "         [-0.5260,  0.5640, -0.2239,  ...,  0.6325,  0.4710, -0.0672],\n",
      "         [-0.6731,  0.8579, -0.0516,  ...,  0.3267, -0.0594, -0.3396],\n",
      "         ...,\n",
      "         [-0.4975,  0.5058, -0.0706,  ...,  0.1543,  0.0507, -0.0620],\n",
      "         [-0.4309,  0.7485, -0.2831,  ...,  0.1758,  0.3432, -0.0464],\n",
      "         [-0.3491,  0.5077, -0.0960,  ...,  0.3706,  0.1425, -0.0736]],\n",
      "\n",
      "        [[-0.7501,  0.4015, -0.1297,  ..., -0.0329,  0.2419, -0.0551],\n",
      "         [-0.5034,  0.4932, -0.4214,  ...,  0.3254,  0.4723,  0.1834],\n",
      "         [-0.6185,  0.5262, -0.2153,  ...,  0.1087,  0.2212,  0.2772],\n",
      "         ...,\n",
      "         [-0.4433,  0.6695, -0.2972,  ..., -0.0747,  0.0151,  0.4239],\n",
      "         [-0.4630,  0.6680, -0.1549,  ...,  0.1508,  0.1286,  0.1312],\n",
      "         [-0.5867,  0.4400, -0.3416,  ...,  0.1163,  0.3661,  0.0041]],\n",
      "\n",
      "        [[-0.4980,  0.4659, -0.1863,  ..., -0.2212, -0.1087,  0.0175],\n",
      "         [-0.5548,  0.3781, -0.5531,  ...,  0.3766,  0.2502,  0.2135],\n",
      "         [-0.7405,  0.4288, -0.4993,  ...,  0.0608,  0.2221,  0.1248],\n",
      "         ...,\n",
      "         [-0.5215,  0.6290, -0.1052,  ...,  0.1447,  0.2182,  0.2084],\n",
      "         [-0.5098,  0.4530, -0.3485,  ...,  0.1041,  0.0565,  0.2559],\n",
      "         [-0.3132,  0.5042, -0.1271,  ...,  0.0068,  0.2942, -0.0353]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4843,  0.3315, -0.4104,  ...,  0.1086, -0.1454, -0.2264],\n",
      "         [-0.3917,  0.2880, -0.7572,  ...,  0.2574,  0.1840,  0.1190],\n",
      "         [-0.4670,  0.4863, -0.6765,  ...,  0.1238,  0.0379,  0.0457],\n",
      "         ...,\n",
      "         [-0.5572,  0.6693, -0.7210,  ...,  0.0735, -0.0924,  0.0663],\n",
      "         [-0.1428,  0.4393, -0.5580,  ...,  0.2320, -0.0396, -0.0405],\n",
      "         [-0.3863,  0.4560, -0.6469,  ...,  0.2408,  0.1438, -0.0203]],\n",
      "\n",
      "        [[-0.7752,  0.6190, -0.5473,  ...,  0.1614,  0.1422, -0.1836],\n",
      "         [-0.7407,  0.6984, -0.7159,  ...,  0.4800,  0.3384,  0.0727],\n",
      "         [-0.7927,  0.5608, -0.6545,  ...,  0.1519,  0.1329,  0.0174],\n",
      "         ...,\n",
      "         [-0.6029,  0.4612, -0.6040,  ..., -0.0029, -0.0890,  0.2944],\n",
      "         [-0.4186,  0.8238, -0.5966,  ...,  0.2488,  0.2720,  0.0482],\n",
      "         [-0.7464,  0.6412, -0.1329,  ...,  0.1777,  0.4000,  0.0127]],\n",
      "\n",
      "        [[-0.3216,  0.4072, -0.2998,  ...,  0.1452, -0.1313, -0.1928],\n",
      "         [-0.4385,  0.4743, -0.5864,  ...,  0.4668,  0.1884, -0.0433],\n",
      "         [-0.3859,  0.5219, -0.6256,  ...,  0.4694,  0.2373, -0.1172],\n",
      "         ...,\n",
      "         [-0.4239,  0.6970, -0.6190,  ...,  0.2313, -0.1311, -0.1243],\n",
      "         [-0.2908,  0.6956, -0.7949,  ...,  0.3085, -0.0675, -0.1649],\n",
      "         [-0.2653,  0.5657, -0.5078,  ...,  0.5838,  0.2006, -0.3047]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9203191995620728\n",
      "Model outputs:  tensor([[[ 9.0310e-02,  2.6462e-01, -2.6038e-01,  ..., -2.0839e-01,\n",
      "           1.4602e-01, -4.9778e-01],\n",
      "         [-7.4859e-02,  2.7836e-01, -3.1343e-01,  ..., -2.0509e-01,\n",
      "           2.5689e-01, -3.1520e-01],\n",
      "         [ 5.8138e-02,  2.0043e-01, -4.7071e-01,  ..., -5.3394e-02,\n",
      "           2.3788e-01, -1.9388e-01],\n",
      "         ...,\n",
      "         [ 3.7529e-01, -1.7353e-01, -4.2553e-01,  ...,  2.1133e-01,\n",
      "          -2.4621e-01, -2.0153e-02],\n",
      "         [-4.9447e-02,  4.2598e-01, -5.8568e-01,  ..., -8.9242e-02,\n",
      "           9.2892e-02, -1.7364e-01],\n",
      "         [-4.6214e-02,  9.7092e-02, -5.7164e-01,  ..., -2.8210e-01,\n",
      "          -4.0582e-02, -1.9144e-02]],\n",
      "\n",
      "        [[ 1.2726e-01,  2.9702e-01, -2.2618e-01,  ..., -6.5059e-01,\n",
      "           8.0710e-02,  1.7783e-01],\n",
      "         [ 1.7099e-01,  4.2729e-01, -4.8859e-01,  ..., -5.6663e-01,\n",
      "          -1.3655e-01,  1.6389e-01],\n",
      "         [ 1.9196e-01,  3.2877e-01, -5.8679e-01,  ..., -6.8064e-02,\n",
      "          -1.1719e-01,  5.6323e-01],\n",
      "         ...,\n",
      "         [ 1.4338e-01,  1.4781e-01, -5.1054e-01,  ..., -1.6789e-01,\n",
      "          -1.4990e-01,  4.5654e-01],\n",
      "         [-9.2162e-02,  3.4750e-01, -6.0244e-01,  ...,  5.0264e-02,\n",
      "           1.6547e-01,  3.4027e-01],\n",
      "         [ 1.2387e-01,  2.7592e-01, -4.0495e-01,  ..., -3.4830e-01,\n",
      "           7.6679e-03,  3.7903e-01]],\n",
      "\n",
      "        [[-8.1705e-02,  3.2212e-01, -2.0555e-01,  ..., -5.9128e-01,\n",
      "           9.1930e-02,  4.3534e-02],\n",
      "         [-3.2705e-02,  4.0121e-01, -2.8867e-01,  ..., -3.3689e-01,\n",
      "           7.6613e-02,  3.7892e-01],\n",
      "         [-1.2334e-01,  3.8761e-01, -9.4758e-01,  ..., -1.5248e-01,\n",
      "           2.4952e-01,  4.7814e-01],\n",
      "         ...,\n",
      "         [ 2.5008e-01,  7.0489e-02, -6.2811e-01,  ..., -1.1351e-01,\n",
      "           1.1936e-02,  3.3325e-01],\n",
      "         [ 1.5837e-01,  2.7116e-01, -5.5009e-01,  ..., -1.2726e-01,\n",
      "           3.0202e-01,  5.1940e-01],\n",
      "         [ 1.0246e-02,  4.3036e-01, -6.3642e-01,  ..., -3.2399e-01,\n",
      "           8.0334e-02,  2.4731e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.0193e-02,  5.5442e-01, -4.1814e-01,  ..., -4.3474e-01,\n",
      "           9.0592e-02,  3.3277e-01],\n",
      "         [ 1.2475e-01,  1.4857e-01, -5.2489e-01,  ..., -4.2987e-01,\n",
      "           3.9251e-01,  2.6127e-01],\n",
      "         [ 2.5523e-02,  4.2176e-01, -3.4743e-01,  ..., -4.0117e-01,\n",
      "          -7.6187e-02,  5.6595e-01],\n",
      "         ...,\n",
      "         [ 1.4081e-01,  3.9753e-02, -5.7333e-01,  ..., -3.8976e-02,\n",
      "          -7.4483e-05,  3.9833e-01],\n",
      "         [ 1.6128e-01,  3.1873e-01, -8.3758e-01,  ..., -2.0118e-01,\n",
      "           1.1336e-01,  2.2178e-01],\n",
      "         [-4.7991e-02,  3.3105e-01, -4.2220e-01,  ..., -3.6993e-01,\n",
      "           1.4122e-01,  4.3548e-01]],\n",
      "\n",
      "        [[-4.0636e-01,  8.5021e-01, -5.0230e-01,  ...,  3.8569e-01,\n",
      "           5.3690e-01, -1.0115e-01],\n",
      "         [-3.4395e-01,  7.2303e-01, -6.1275e-01,  ...,  3.9897e-01,\n",
      "           4.5506e-01,  2.4987e-02],\n",
      "         [-4.3469e-01,  8.0756e-01, -9.4973e-01,  ...,  6.7292e-01,\n",
      "           5.5321e-01,  2.8025e-01],\n",
      "         ...,\n",
      "         [ 2.4088e-02,  1.5329e-01, -6.9355e-01,  ...,  5.5660e-01,\n",
      "           3.3820e-01,  6.5149e-02],\n",
      "         [-4.6486e-01,  5.0357e-01, -7.3407e-01,  ...,  6.7671e-01,\n",
      "           6.3072e-01,  2.0190e-01],\n",
      "         [-4.5224e-01,  7.1516e-01, -6.2832e-01,  ...,  5.5108e-01,\n",
      "           3.6572e-01,  1.3333e-01]],\n",
      "\n",
      "        [[-4.5868e-02,  3.2866e-01, -3.3482e-01,  ..., -7.8944e-01,\n",
      "           8.2513e-01,  1.6138e-01],\n",
      "         [-5.1591e-02,  3.7403e-01, -4.5583e-01,  ..., -5.5076e-01,\n",
      "           1.0526e+00,  4.1272e-01],\n",
      "         [ 1.7478e-01,  1.7965e-01, -8.5386e-01,  ..., -6.0088e-01,\n",
      "           6.9380e-01,  6.4846e-01],\n",
      "         ...,\n",
      "         [ 2.0907e-01, -1.4404e-01, -3.2965e-01,  ..., -2.0595e-01,\n",
      "           6.6354e-01,  7.1997e-01],\n",
      "         [ 7.6103e-02,  2.9925e-01, -4.4405e-01,  ..., -1.9437e-01,\n",
      "           8.5282e-01,  6.2914e-01],\n",
      "         [ 6.3368e-02, -2.5942e-02, -4.5295e-01,  ..., -4.1235e-01,\n",
      "           6.0886e-01,  7.8718e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3779692649841309\n",
      "Model outputs:  tensor([[[-3.5151e-02,  1.5217e-01, -9.1887e-01,  ...,  6.5117e-02,\n",
      "          -6.3320e-02,  5.7784e-01],\n",
      "         [-2.1459e-01,  2.4612e-01, -8.9152e-01,  ...,  6.5088e-02,\n",
      "           2.5140e-01,  3.6316e-01],\n",
      "         [-4.1987e-02,  1.3671e-01, -7.5121e-01,  ..., -9.3685e-02,\n",
      "          -2.4257e-01,  6.0653e-01],\n",
      "         ...,\n",
      "         [ 3.7632e-03,  1.3480e-01, -8.4322e-01,  ..., -5.1776e-03,\n",
      "          -1.1669e-01,  5.7754e-01],\n",
      "         [ 3.2834e-01,  1.0189e-01, -5.9157e-01,  ...,  1.5753e-01,\n",
      "          -1.3277e-02,  3.7873e-01],\n",
      "         [-1.1850e-02,  1.7466e-01, -4.3926e-01,  ..., -4.9688e-02,\n",
      "           2.3851e-01,  6.2284e-01]],\n",
      "\n",
      "        [[-6.0449e-01,  5.2505e-01, -6.6052e-01,  ...,  4.4913e-01,\n",
      "           3.9696e-01,  1.6559e-01],\n",
      "         [-6.8748e-01,  2.5542e-01, -4.8554e-01,  ...,  3.3851e-01,\n",
      "           4.6500e-01,  3.4341e-01],\n",
      "         [-5.5265e-01,  2.2390e-01, -6.6539e-01,  ...,  3.4486e-01,\n",
      "           4.1055e-01,  5.4799e-01],\n",
      "         ...,\n",
      "         [-5.1006e-01,  3.5101e-01, -5.5556e-01,  ...,  4.5350e-01,\n",
      "           4.7626e-01,  3.8174e-01],\n",
      "         [-2.8487e-01,  2.9105e-01, -2.0318e-01,  ...,  3.5877e-01,\n",
      "           2.3216e-01,  3.6872e-01],\n",
      "         [-4.8354e-01,  3.5007e-01, -3.9205e-01,  ...,  6.1713e-01,\n",
      "           5.4169e-01,  2.8650e-01]],\n",
      "\n",
      "        [[ 3.9938e-02, -4.2091e-02, -5.3536e-01,  ..., -1.4678e-01,\n",
      "          -1.4186e-01,  5.1210e-01],\n",
      "         [ 5.4679e-02,  2.8204e-01, -7.5189e-01,  ..., -5.8336e-02,\n",
      "          -2.4732e-01,  6.0196e-01],\n",
      "         [ 3.1940e-01, -1.3486e-01, -5.4014e-01,  ..., -2.5889e-01,\n",
      "           5.8811e-03,  7.7832e-01],\n",
      "         ...,\n",
      "         [ 1.1523e-01,  1.5093e-01, -5.1030e-01,  ..., -8.7330e-03,\n",
      "          -1.7370e-01,  2.2201e-01],\n",
      "         [ 2.6631e-01, -2.0107e-01, -4.9509e-01,  ...,  5.3434e-02,\n",
      "           1.1182e-01,  6.3787e-01],\n",
      "         [ 4.9170e-02, -1.6989e-01, -5.0785e-01,  ..., -1.5291e-01,\n",
      "          -4.2106e-02,  4.8731e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.3641e-01,  5.1141e-01, -1.1173e+00,  ...,  8.9903e-01,\n",
      "           1.4046e-01,  4.3069e-01],\n",
      "         [-4.9500e-01,  4.2941e-01, -8.6763e-01,  ...,  8.6532e-01,\n",
      "           3.0662e-01,  4.5188e-01],\n",
      "         [-4.0208e-01,  3.6113e-01, -7.4370e-01,  ...,  9.3044e-01,\n",
      "           1.4388e-01,  1.6669e-01],\n",
      "         ...,\n",
      "         [-1.9602e-01,  1.8006e-01, -1.0090e+00,  ...,  7.6237e-01,\n",
      "           2.3951e-01,  3.4898e-01],\n",
      "         [-9.2544e-02,  3.5393e-01, -6.4052e-01,  ...,  6.9159e-01,\n",
      "           5.0401e-01, -5.5236e-02],\n",
      "         [-2.2611e-01,  3.5340e-02, -7.2648e-01,  ...,  1.0020e+00,\n",
      "           2.9421e-01,  3.1261e-01]],\n",
      "\n",
      "        [[ 3.1525e-01,  2.4975e-01, -7.3056e-01,  ..., -1.6975e-01,\n",
      "          -3.8212e-01,  6.5308e-01],\n",
      "         [ 1.9950e-01, -2.2790e-02, -7.0650e-01,  ..., -3.6892e-01,\n",
      "          -1.8128e-01,  7.3819e-01],\n",
      "         [ 3.4548e-01, -2.1829e-02, -6.8131e-01,  ..., -1.5304e-01,\n",
      "          -1.9057e-01,  7.9379e-01],\n",
      "         ...,\n",
      "         [-2.8310e-02,  1.0769e-01, -5.7884e-01,  ..., -1.7184e-01,\n",
      "          -2.1385e-01,  5.5777e-01],\n",
      "         [ 2.4827e-01, -7.5418e-02,  1.0923e-03,  ...,  3.8092e-02,\n",
      "          -3.3516e-02,  7.1067e-01],\n",
      "         [ 1.9685e-01, -3.8272e-04, -2.5087e-01,  ..., -3.0255e-01,\n",
      "          -3.2012e-01,  7.7064e-01]],\n",
      "\n",
      "        [[-2.3724e-01,  4.3789e-01, -3.0875e-01,  ...,  1.0471e+00,\n",
      "           4.9736e-01, -3.7328e-02],\n",
      "         [-5.4795e-01,  4.6503e-01, -7.6250e-01,  ...,  8.7713e-01,\n",
      "           3.7243e-01,  1.1393e-01],\n",
      "         [-2.8889e-01,  3.3762e-01, -5.0442e-01,  ...,  9.3432e-01,\n",
      "           3.0526e-01,  5.1529e-02],\n",
      "         ...,\n",
      "         [-1.3545e-01,  1.5363e-01, -3.0357e-01,  ...,  7.8944e-01,\n",
      "           3.9275e-01, -2.7520e-02],\n",
      "         [-8.0859e-02,  3.0438e-01, -1.7771e-01,  ...,  7.6999e-01,\n",
      "           5.2824e-01, -4.3873e-02],\n",
      "         [-2.9817e-01, -8.2315e-02, -1.0478e-01,  ...,  6.5968e-01,\n",
      "           3.2135e-01,  1.5549e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.080260157585144\n",
      "Model outputs:  tensor([[[ 2.4780e-01, -5.6887e-02, -6.0255e-01,  ..., -5.2208e-01,\n",
      "          -4.7133e-01,  5.6377e-01],\n",
      "         [ 1.3074e-01, -3.1886e-01, -4.4264e-01,  ..., -4.1938e-01,\n",
      "          -2.7475e-01,  6.0399e-01],\n",
      "         [ 1.4772e-01, -2.9793e-01, -5.0041e-01,  ..., -5.3360e-01,\n",
      "          -4.2400e-01,  5.9152e-01],\n",
      "         ...,\n",
      "         [ 4.7728e-01, -9.6529e-02, -6.1344e-01,  ..., -5.1732e-01,\n",
      "          -1.7738e-01,  7.2885e-01],\n",
      "         [ 1.9753e-02, -1.4504e-01, -2.1502e-01,  ..., -7.8388e-01,\n",
      "          -3.9567e-01,  4.1432e-01],\n",
      "         [-8.7822e-02, -6.1852e-02, -6.2100e-01,  ..., -6.3644e-01,\n",
      "          -2.1530e-01,  7.6305e-01]],\n",
      "\n",
      "        [[ 1.0416e-01,  5.6834e-02, -6.0239e-01,  ..., -6.8274e-01,\n",
      "           4.1702e-01,  3.3803e-01],\n",
      "         [ 3.5229e-01, -1.6252e-01, -8.2642e-01,  ..., -7.0045e-01,\n",
      "           1.4286e-01,  4.2998e-01],\n",
      "         [ 5.3839e-02, -1.3770e-01, -6.5213e-01,  ..., -7.5309e-01,\n",
      "           3.7038e-01,  6.4116e-01],\n",
      "         ...,\n",
      "         [ 1.2029e-01, -7.2680e-02, -3.3401e-01,  ..., -4.3785e-01,\n",
      "           4.6013e-01, -1.3915e-01],\n",
      "         [ 1.7129e-01, -1.4594e-01, -4.0197e-01,  ..., -9.8198e-01,\n",
      "           3.1760e-01,  1.1496e-01],\n",
      "         [ 2.2966e-01, -1.8042e-01, -8.1733e-01,  ..., -8.8551e-01,\n",
      "           2.5101e-01,  2.3629e-01]],\n",
      "\n",
      "        [[ 6.7969e-02,  5.6600e-02, -8.2288e-01,  ..., -3.2572e-04,\n",
      "           2.9424e-04,  2.6578e-01],\n",
      "         [-1.1969e-03,  1.1356e-01, -1.0240e+00,  ...,  3.7501e-02,\n",
      "          -2.3162e-01,  2.9067e-01],\n",
      "         [-5.7145e-02, -5.5025e-02, -1.1448e+00,  ...,  6.1806e-02,\n",
      "           6.2643e-02,  2.6233e-01],\n",
      "         ...,\n",
      "         [ 1.6639e-01,  3.8050e-02, -8.4030e-01,  ...,  1.1867e-01,\n",
      "          -4.6534e-02,  6.8541e-02],\n",
      "         [-4.0283e-01,  2.4894e-01, -8.0613e-01,  ..., -1.0749e-01,\n",
      "          -1.0577e-01, -1.0844e-01],\n",
      "         [-2.1810e-01,  3.2583e-01, -6.8485e-01,  ...,  3.3625e-02,\n",
      "          -2.0765e-01,  1.6224e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.8342e-01,  7.3172e-02, -5.4789e-01,  ...,  1.5025e-01,\n",
      "           1.5328e-01, -1.2506e-02],\n",
      "         [-2.7667e-01,  2.0925e-01, -6.9817e-01,  ...,  3.1304e-01,\n",
      "           4.4101e-01,  1.2398e-01],\n",
      "         [-2.8797e-01,  4.7934e-02, -7.2410e-01,  ...,  6.8616e-02,\n",
      "           4.1876e-01,  1.0800e-01],\n",
      "         ...,\n",
      "         [-4.0701e-01,  2.9537e-01, -3.5364e-01,  ...,  5.3726e-02,\n",
      "           2.9159e-01, -9.0044e-02],\n",
      "         [-4.3514e-01,  2.4169e-01, -2.3238e-01,  ...,  4.0024e-01,\n",
      "           2.7158e-01, -2.5890e-01],\n",
      "         [-3.8725e-01,  2.7696e-01, -4.4542e-01,  ...,  2.0095e-01,\n",
      "           3.2885e-01, -1.5590e-01]],\n",
      "\n",
      "        [[-1.0785e-01, -1.5329e-01, -7.1403e-01,  ..., -7.4613e-01,\n",
      "           4.7172e-01,  5.1433e-01],\n",
      "         [ 1.8667e-01, -1.5602e-01, -8.2680e-01,  ..., -8.0752e-01,\n",
      "           5.9919e-01,  4.9283e-01],\n",
      "         [ 7.0916e-02, -2.8519e-01, -7.9442e-01,  ..., -6.6815e-01,\n",
      "           5.5364e-01,  6.1376e-01],\n",
      "         ...,\n",
      "         [ 1.4763e-01, -9.3021e-02, -3.9531e-01,  ..., -8.0087e-01,\n",
      "          -2.3360e-02,  1.5267e-01],\n",
      "         [ 9.1293e-02,  3.9875e-02, -4.2099e-01,  ..., -8.7506e-01,\n",
      "          -4.2617e-02,  2.7612e-01],\n",
      "         [ 1.4026e-01,  9.1722e-03, -5.3248e-01,  ..., -6.9542e-01,\n",
      "           5.9286e-01,  3.3137e-01]],\n",
      "\n",
      "        [[-1.7369e-01,  4.6928e-02, -3.8129e-01,  ...,  3.2371e-01,\n",
      "           3.5874e-01,  5.6452e-04],\n",
      "         [-1.1761e-01,  4.3111e-01, -6.0849e-01,  ...,  7.3275e-01,\n",
      "           2.8985e-01,  1.4154e-01],\n",
      "         [-4.1677e-01,  5.3036e-01, -5.0331e-01,  ...,  3.5633e-01,\n",
      "           2.7057e-01,  1.7052e-01],\n",
      "         ...,\n",
      "         [-3.2540e-01,  2.3502e-01, -1.4281e-01,  ...,  3.1790e-01,\n",
      "           1.9847e-01, -2.6133e-01],\n",
      "         [-5.2355e-01,  3.9514e-01, -3.1029e-01,  ...,  1.8920e-01,\n",
      "           2.3996e-01, -2.3894e-01],\n",
      "         [-2.5581e-01,  3.7245e-01, -5.2077e-01,  ...,  2.2534e-01,\n",
      "           2.3167e-01, -1.5841e-02]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1877025365829468\n",
      "Model outputs:  tensor([[[-3.2169e-01,  3.8296e-01, -4.3129e-01,  ..., -8.6428e-02,\n",
      "          -6.9250e-02,  3.3306e-01],\n",
      "         [-5.3904e-01,  4.0479e-01, -8.3712e-01,  ...,  1.3561e-01,\n",
      "           1.3969e-01,  4.1170e-01],\n",
      "         [-2.6841e-01,  5.5407e-02, -8.4329e-01,  ...,  1.4333e-01,\n",
      "          -3.2554e-03,  5.3386e-01],\n",
      "         ...,\n",
      "         [-1.8019e-01,  2.9544e-01, -8.0028e-01,  ...,  1.4492e-01,\n",
      "          -2.3509e-02,  3.1969e-01],\n",
      "         [-5.0545e-01,  6.7771e-01, -7.4957e-01,  ...,  5.9439e-02,\n",
      "          -1.5998e-02,  2.3636e-02],\n",
      "         [-2.1572e-01,  1.2966e-02, -6.8408e-01,  ...,  1.7911e-01,\n",
      "           2.1854e-01,  1.3683e-01]],\n",
      "\n",
      "        [[-3.1370e-01,  5.0562e-01, -4.4346e-01,  ...,  4.9327e-01,\n",
      "           2.0664e-01,  1.7664e-01],\n",
      "         [-2.8159e-01,  4.2235e-01, -6.4990e-01,  ...,  8.8539e-01,\n",
      "           2.6803e-01,  3.9265e-01],\n",
      "         [-5.1087e-01,  1.9579e-01, -2.5444e-01,  ...,  8.3599e-01,\n",
      "           4.3281e-01,  2.4210e-02],\n",
      "         ...,\n",
      "         [-4.7679e-01,  6.1838e-01, -6.7184e-01,  ...,  6.1075e-01,\n",
      "           3.1832e-01,  2.7561e-01],\n",
      "         [-4.5812e-01,  7.8704e-01, -5.7105e-01,  ...,  7.7073e-01,\n",
      "           7.5218e-02,  2.2101e-01],\n",
      "         [-5.1896e-01, -1.6503e-02, -2.9423e-01,  ...,  7.1262e-01,\n",
      "           2.5988e-01,  3.7105e-01]],\n",
      "\n",
      "        [[-5.8608e-01,  3.9152e-01,  2.7668e-02,  ...,  5.2864e-01,\n",
      "          -1.6537e-02,  2.3820e-01],\n",
      "         [-5.7367e-01,  5.7663e-01, -4.1395e-01,  ...,  6.9640e-01,\n",
      "           1.7424e-01,  5.3932e-02],\n",
      "         [-2.8198e-01,  1.3252e-01, -3.2621e-01,  ...,  8.3860e-01,\n",
      "           4.7785e-01,  3.3992e-01],\n",
      "         ...,\n",
      "         [-4.2667e-01,  3.5654e-01, -2.6460e-01,  ...,  6.3533e-01,\n",
      "           4.5570e-01,  2.2914e-01],\n",
      "         [-6.8232e-01,  5.1838e-01, -3.7668e-01,  ...,  3.0766e-01,\n",
      "           3.4584e-01,  1.2060e-02],\n",
      "         [-4.6662e-01,  2.5017e-01, -1.6236e-01,  ...,  6.5156e-01,\n",
      "           5.6591e-01,  3.4840e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.1227e-01,  5.0253e-01, -3.1740e-01,  ...,  5.1012e-01,\n",
      "           1.7609e-01, -3.7472e-03],\n",
      "         [-2.7979e-01,  6.5672e-01, -1.3532e-01,  ...,  5.9635e-01,\n",
      "           6.5243e-01,  1.6535e-01],\n",
      "         [-2.2309e-01,  7.0342e-02, -4.2337e-01,  ...,  8.2361e-01,\n",
      "           2.5657e-01,  2.8063e-01],\n",
      "         ...,\n",
      "         [-3.7504e-01,  2.3970e-01, -3.1774e-01,  ...,  7.2656e-01,\n",
      "           2.3716e-01,  1.4305e-04],\n",
      "         [-3.8571e-01,  6.0137e-01,  2.2832e-02,  ...,  5.0165e-01,\n",
      "           3.1510e-01, -5.6712e-03],\n",
      "         [-3.4604e-01, -1.3415e-02,  1.8179e-01,  ...,  7.2930e-01,\n",
      "           3.3251e-01,  1.2617e-01]],\n",
      "\n",
      "        [[-3.2954e-02,  1.6680e-01, -4.5608e-01,  ...,  5.2925e-03,\n",
      "          -9.3493e-03,  3.9082e-01],\n",
      "         [ 1.5581e-01,  4.5396e-01, -8.0520e-01,  ..., -6.3499e-02,\n",
      "          -7.4370e-02,  2.5662e-01],\n",
      "         [ 8.0371e-02, -7.5840e-02, -5.0662e-01,  ...,  2.2557e-01,\n",
      "           1.1964e-01,  5.6386e-01],\n",
      "         ...,\n",
      "         [ 1.4594e-01, -6.8264e-03, -8.5109e-01,  ...,  1.1420e-01,\n",
      "          -2.7522e-04,  3.3029e-01],\n",
      "         [ 3.5041e-01,  2.6521e-01, -6.2734e-01,  ...,  2.8651e-01,\n",
      "          -9.2222e-03,  2.7247e-01],\n",
      "         [ 1.2537e-01,  7.0228e-02, -5.2029e-01,  ...,  1.5641e-01,\n",
      "           1.3616e-01,  5.5813e-01]],\n",
      "\n",
      "        [[ 1.0448e-01, -1.7308e-01, -1.8213e-01,  ..., -6.2507e-01,\n",
      "          -2.7673e-01,  4.4822e-01],\n",
      "         [ 6.2936e-02,  3.3604e-02, -2.2939e-01,  ..., -2.3434e-01,\n",
      "          -2.5435e-01,  5.8945e-01],\n",
      "         [-5.7963e-03, -2.1363e-01, -3.1584e-01,  ..., -1.5057e-01,\n",
      "          -2.2720e-01,  4.3449e-01],\n",
      "         ...,\n",
      "         [ 2.6503e-01, -1.4942e-02, -6.2541e-01,  ..., -3.0072e-01,\n",
      "          -1.8950e-01,  7.0554e-01],\n",
      "         [ 3.0353e-01, -5.2803e-02, -2.7938e-01,  ..., -4.1841e-01,\n",
      "          -4.1321e-01,  7.7214e-01],\n",
      "         [ 8.0060e-02, -2.7303e-01, -3.4829e-01,  ..., -5.5606e-02,\n",
      "          -3.5073e-01,  8.3454e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0794081687927246\n",
      "Model outputs:  tensor([[[-0.5503,  0.1466, -0.6987,  ...,  0.2969, -0.8414,  0.3393],\n",
      "         [-0.4910,  0.3843, -0.8150,  ...,  0.3874, -0.5782,  0.2074],\n",
      "         [-0.4403,  0.3003, -0.2760,  ...,  0.2904, -0.1662,  0.1193],\n",
      "         ...,\n",
      "         [-0.0699,  0.1792, -0.8169,  ...,  0.5185, -0.1008,  0.0043],\n",
      "         [-0.1576,  0.1160, -0.7781,  ...,  0.5948, -0.2267,  0.2362],\n",
      "         [-0.1366, -0.2233, -0.5097,  ...,  0.4216, -0.1864,  0.2057]],\n",
      "\n",
      "        [[-0.5001,  0.2295, -0.6427,  ...,  0.9609, -0.5801,  0.2095],\n",
      "         [-0.8114,  0.4387, -0.7512,  ...,  0.9581, -0.3782,  0.1862],\n",
      "         [-0.5085,  0.6900, -0.5425,  ...,  0.8715,  0.1607,  0.1566],\n",
      "         ...,\n",
      "         [ 0.0196,  0.4644, -0.7135,  ...,  1.1804,  0.1762,  0.2617],\n",
      "         [-0.0473,  0.2731, -0.8006,  ...,  1.0520,  0.2955,  0.2995],\n",
      "         [-0.3910, -0.0028, -0.5154,  ...,  0.7778,  0.0536,  0.1603]],\n",
      "\n",
      "        [[-0.8115,  0.4329, -0.5863,  ...,  0.3735, -0.3328,  0.6718],\n",
      "         [-1.0268,  0.5719, -0.6748,  ...,  0.4774, -0.2037,  0.3361],\n",
      "         [-0.5874,  0.5193, -0.2856,  ...,  0.4147,  0.1878,  0.2106],\n",
      "         ...,\n",
      "         [-0.4470,  0.5397, -0.4281,  ...,  0.8469,  0.3697,  0.3830],\n",
      "         [-0.2713,  0.4247, -0.3154,  ...,  0.6221,  0.3437,  0.4186],\n",
      "         [-0.6645, -0.0251, -0.1989,  ...,  0.5892,  0.2084,  0.3228]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.8627,  0.0954, -0.8375,  ...,  0.0311, -0.9567,  0.4502],\n",
      "         [-1.1036,  0.4683, -0.8651,  ...,  0.0768, -0.3632,  0.4083],\n",
      "         [-0.7839,  0.0745, -0.4646,  ..., -0.2185, -0.0403,  0.2813],\n",
      "         ...,\n",
      "         [-0.5196,  0.2788, -0.7432,  ...,  0.5227, -0.1245,  0.2998],\n",
      "         [-0.2658, -0.0241, -0.7996,  ...,  0.3001,  0.0118,  0.6196],\n",
      "         [-0.2590, -0.2991, -0.7103,  ...,  0.2925,  0.0235,  0.4022]],\n",
      "\n",
      "        [[-0.4618,  0.0828, -0.6445,  ..., -0.3170, -0.8420,  0.8007],\n",
      "         [-0.5186, -0.0842, -0.5768,  ..., -0.3265, -0.6614,  0.8733],\n",
      "         [-0.0497, -0.1119, -0.3356,  ..., -0.5017, -0.3932,  0.7492],\n",
      "         ...,\n",
      "         [-0.1777,  0.0122, -0.4712,  ...,  0.1144, -0.6609,  0.4729],\n",
      "         [-0.2164, -0.1696, -0.2684,  ..., -0.1593, -0.0569,  0.6148],\n",
      "         [-0.0018, -0.3605, -0.2449,  ..., -0.0655, -0.2451,  0.5377]],\n",
      "\n",
      "        [[-0.5029,  0.0916, -0.4893,  ..., -0.1614, -0.9013,  0.7561],\n",
      "         [-0.8124,  0.0371, -0.3612,  ..., -0.3636, -0.7427,  0.6683],\n",
      "         [-0.1660,  0.0555, -0.1723,  ..., -0.3704, -0.6059,  0.5325],\n",
      "         ...,\n",
      "         [ 0.2444,  0.2094, -0.5189,  ..., -0.2410, -0.2434,  0.7865],\n",
      "         [ 0.0410, -0.2167, -0.1733,  ..., -0.2762, -0.2470,  0.6655],\n",
      "         [-0.1840, -0.2712, -0.0789,  ...,  0.0526, -0.4374,  0.6422]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.7251361608505249\n",
      "Model outputs:  tensor([[[-0.2983,  0.2539, -0.8220,  ...,  0.5166,  0.1384,  0.1965],\n",
      "         [-0.3682,  0.6467, -1.1792,  ...,  0.8733,  0.3179, -0.0748],\n",
      "         [-0.2189,  0.4313, -1.0408,  ...,  0.9289,  0.4150,  0.2346],\n",
      "         ...,\n",
      "         [-0.1237,  0.4231, -1.0427,  ...,  0.9724,  0.1284,  0.0713],\n",
      "         [-0.1467,  0.5578, -0.9567,  ...,  1.0615,  0.1467,  0.0876],\n",
      "         [ 0.2586,  0.0866, -0.2833,  ...,  0.7351,  0.2893, -0.1101]],\n",
      "\n",
      "        [[-0.0996,  0.3647, -0.9456,  ...,  0.4465,  0.0230,  0.1443],\n",
      "         [-0.3867,  0.6762, -1.2284,  ...,  0.8750,  0.2489, -0.1350],\n",
      "         [-0.3001,  0.6046, -1.0871,  ...,  0.8255,  0.2288,  0.1359],\n",
      "         ...,\n",
      "         [-0.2980,  0.1314, -0.9138,  ...,  0.8088,  0.0178,  0.2227],\n",
      "         [-0.1927,  0.4621, -0.8929,  ...,  0.8434,  0.0220,  0.0634],\n",
      "         [ 0.2667,  0.2478, -0.5173,  ...,  0.8323,  0.0622,  0.0457]],\n",
      "\n",
      "        [[-0.5873,  0.1538, -0.5773,  ...,  0.0013, -0.0698,  0.1557],\n",
      "         [-0.6694,  0.5389, -0.4286,  ...,  0.3014,  0.4057,  0.1010],\n",
      "         [-0.5445,  0.2429, -0.8194,  ...,  0.3083,  0.2303,  0.3086],\n",
      "         ...,\n",
      "         [-0.4608,  0.2388, -0.4978,  ...,  0.3002,  0.3904,  0.3274],\n",
      "         [-0.6616,  0.2520, -0.6718,  ...,  0.4752,  0.3336,  0.2934],\n",
      "         [-0.0878,  0.2431, -0.1332,  ...,  0.1625,  0.2239,  0.2707]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5205, -0.0944, -0.3218,  ..., -0.4110,  0.0460, -0.2452],\n",
      "         [ 0.1071, -0.0460, -0.5008,  ...,  0.0139,  0.1038, -0.3096],\n",
      "         [ 0.2716, -0.2057, -0.3520,  ...,  0.2457, -0.0828, -0.1672],\n",
      "         ...,\n",
      "         [ 0.4892,  0.0995, -0.5386,  ..., -0.2408,  0.7512,  0.3131],\n",
      "         [ 0.1527, -0.1421, -0.4305,  ..., -0.4004,  0.5174,  0.1191],\n",
      "         [ 0.6259, -0.0446,  0.0730,  ..., -0.4096,  0.3435, -0.2906]],\n",
      "\n",
      "        [[ 0.2127, -0.3087, -0.6629,  ..., -0.4671, -0.4296,  0.6327],\n",
      "         [-0.2440,  0.1837, -0.6584,  ..., -0.0265, -0.2255,  0.5898],\n",
      "         [-0.0250,  0.2576, -0.4548,  ..., -0.0968, -0.0108,  0.6313],\n",
      "         ...,\n",
      "         [-0.0143,  0.0877, -0.5453,  ..., -0.1376, -0.1689,  0.7850],\n",
      "         [ 0.0652, -0.0158, -0.4710,  ..., -0.1539, -0.4731,  0.4133],\n",
      "         [ 0.3019,  0.0136, -0.1128,  ..., -0.2718, -0.4125,  0.5135]],\n",
      "\n",
      "        [[ 0.4242, -0.1687, -0.3226,  ..., -0.3393,  0.1209, -0.2313],\n",
      "         [ 0.0332, -0.2236, -0.4214,  ...,  0.3259,  0.0837, -0.5748],\n",
      "         [ 0.2104, -0.0772, -0.3997,  ...,  0.3732,  0.2214, -0.3889],\n",
      "         ...,\n",
      "         [ 0.3185,  0.0308, -0.6652,  ..., -0.1215,  0.5751,  0.2252],\n",
      "         [ 0.3518, -0.2189, -0.3793,  ..., -0.4276,  0.5090,  0.1588],\n",
      "         [ 0.6231, -0.4374,  0.1857,  ..., -0.1292,  0.1098, -0.2763]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2325340509414673\n",
      "Model outputs:  tensor([[[-0.4285,  0.2529, -0.8377,  ...,  0.1741, -0.0639,  0.4195],\n",
      "         [-0.4100,  0.2701, -0.6809,  ..., -0.0951, -0.0566,  0.4685],\n",
      "         [-0.1886,  0.1565, -0.5369,  ..., -0.0667, -0.0346,  0.3780],\n",
      "         ...,\n",
      "         [-0.3056,  0.2638, -0.5648,  ..., -0.1391,  0.3014,  0.3832],\n",
      "         [-0.5089,  0.3027, -0.6666,  ...,  0.1070, -0.0147,  0.2416],\n",
      "         [-0.5611,  0.2884, -0.6368,  ..., -0.1156,  0.1634,  0.5927]],\n",
      "\n",
      "        [[ 0.2761,  0.0939, -0.7009,  ...,  0.5595,  0.0623,  0.1571],\n",
      "         [ 0.0216,  0.3910, -0.4409,  ...,  0.1137, -0.0647,  0.4110],\n",
      "         [ 0.2044,  0.4478, -0.6376,  ...,  0.2913, -0.0129,  0.3994],\n",
      "         ...,\n",
      "         [ 0.0185,  0.2457, -0.5874,  ...,  0.0180,  0.2163,  0.1896],\n",
      "         [ 0.0594,  0.4231, -0.9975,  ...,  0.2184,  0.1953,  0.4024],\n",
      "         [ 0.0217,  0.3792, -0.7114,  ...,  0.3950,  0.4051,  0.4471]],\n",
      "\n",
      "        [[ 0.2371,  0.2254, -0.2641,  ..., -0.1516,  0.1598,  0.4789],\n",
      "         [ 0.0702,  0.4771, -0.4677,  ..., -0.0254,  0.4193,  0.5514],\n",
      "         [ 0.3162,  0.2145, -0.5188,  ..., -0.1914,  0.4902,  0.5997],\n",
      "         ...,\n",
      "         [ 0.3441,  0.0931, -0.5034,  ..., -0.3979,  0.6029,  0.4385],\n",
      "         [ 0.1334,  0.4230, -0.4214,  ..., -0.2942,  0.5654,  0.3660],\n",
      "         [ 0.3645,  0.1419, -0.4108,  ..., -0.2711,  0.2873,  0.5440]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0702,  0.2869, -0.5430,  ...,  0.1917, -0.0239,  0.2056],\n",
      "         [-0.1498,  0.0348, -0.4627,  ..., -0.1969,  0.1149,  0.5070],\n",
      "         [ 0.1911,  0.3732, -0.5224,  ...,  0.0458, -0.0884,  0.4461],\n",
      "         ...,\n",
      "         [-0.0400,  0.2335, -0.6135,  ...,  0.0038,  0.2718,  0.3799],\n",
      "         [-0.1021,  0.3463, -0.6664,  ..., -0.0078,  0.0957,  0.4180],\n",
      "         [-0.2549,  0.1700, -0.4249,  ..., -0.1280,  0.0019,  0.5087]],\n",
      "\n",
      "        [[-0.3308,  0.3698, -0.6643,  ...,  0.9624,  0.3733,  0.2028],\n",
      "         [-0.3981,  0.4431, -0.6248,  ...,  0.6595,  0.1964,  0.4204],\n",
      "         [-0.2601,  0.2679, -0.6828,  ...,  0.9549,  0.4612,  0.3284],\n",
      "         ...,\n",
      "         [-0.2229,  0.5996, -0.7928,  ...,  0.6882,  0.6259,  0.2628],\n",
      "         [-0.4698,  0.4328, -0.4354,  ...,  0.9469,  0.6188,  0.1405],\n",
      "         [-0.5369,  0.2268, -0.6653,  ...,  0.8412,  0.5149,  0.3223]],\n",
      "\n",
      "        [[-0.0589,  0.2243, -0.3540,  ..., -0.1164, -0.3058,  0.4763],\n",
      "         [ 0.0328, -0.0500, -0.3459,  ..., -0.1680, -0.1106,  0.4274],\n",
      "         [ 0.1308,  0.0180, -0.2669,  ...,  0.0044,  0.0526,  0.4587],\n",
      "         ...,\n",
      "         [ 0.0590,  0.1800, -0.2267,  ..., -0.0555, -0.1996,  0.7233],\n",
      "         [ 0.1776,  0.0199, -0.2679,  ..., -0.0206, -0.0258,  0.2702],\n",
      "         [ 0.0327,  0.1127, -0.3426,  ..., -0.1146, -0.1831,  0.6661]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9884569644927979\n",
      "Model outputs:  tensor([[[-1.6214e-01,  4.7720e-01, -4.8425e-01,  ...,  3.5437e-02,\n",
      "          -8.3739e-03, -1.0303e-01],\n",
      "         [-2.0393e-01,  7.9006e-01, -6.8314e-01,  ...,  1.9627e-01,\n",
      "           2.3523e-01, -5.6026e-02],\n",
      "         [-3.0836e-01,  1.7723e-01, -7.9040e-01,  ...,  2.7087e-01,\n",
      "           8.2755e-02,  1.3884e-01],\n",
      "         ...,\n",
      "         [-2.0648e-01,  2.8186e-01, -8.5448e-01,  ...,  3.8805e-01,\n",
      "           1.9101e-01, -3.2069e-01],\n",
      "         [-9.5103e-02,  5.3521e-01, -7.1682e-01,  ...,  3.2696e-01,\n",
      "           2.1896e-01,  1.1774e-01],\n",
      "         [-2.6340e-01,  3.6366e-01, -8.8600e-01,  ...,  3.7157e-01,\n",
      "           1.6892e-01, -2.5007e-02]],\n",
      "\n",
      "        [[ 7.8997e-04,  3.5550e-01,  4.9280e-02,  ..., -1.3722e-01,\n",
      "          -1.3713e-01,  1.5818e-01],\n",
      "         [ 4.1885e-02,  3.2823e-01, -4.8385e-01,  ..., -2.8145e-01,\n",
      "          -1.0050e-01,  2.6844e-01],\n",
      "         [-8.3105e-03,  1.9079e-01, -5.3312e-01,  ..., -6.4164e-02,\n",
      "          -2.8403e-01,  1.3485e-01],\n",
      "         ...,\n",
      "         [-6.3549e-02,  2.7475e-01, -7.0453e-01,  ..., -1.8881e-01,\n",
      "           9.5818e-02,  1.1737e-01],\n",
      "         [-2.3686e-01,  3.6190e-01, -3.5662e-01,  ..., -1.3290e-01,\n",
      "           2.2797e-02,  1.7380e-01],\n",
      "         [-2.5935e-01,  1.6921e-01, -7.4822e-01,  ..., -4.4419e-01,\n",
      "          -1.6023e-02,  1.6646e-01]],\n",
      "\n",
      "        [[ 1.4701e-02,  4.5986e-01, -2.1848e-01,  ..., -1.0438e-01,\n",
      "          -1.5397e-01,  5.2112e-03],\n",
      "         [ 1.1942e-01,  3.1987e-01, -1.0262e+00,  ..., -2.6577e-01,\n",
      "           1.1204e-01,  2.5588e-01],\n",
      "         [-1.3316e-01,  2.9886e-01, -7.5899e-01,  ..., -6.1391e-02,\n",
      "           1.9904e-01,  2.1331e-01],\n",
      "         ...,\n",
      "         [ 1.3338e-01,  4.2567e-01, -4.4594e-01,  ...,  2.5608e-02,\n",
      "           2.2528e-01,  4.1999e-03],\n",
      "         [-2.5807e-02,  1.6337e-01, -5.7008e-01,  ..., -5.3990e-02,\n",
      "           2.9264e-02,  3.3751e-01],\n",
      "         [ 7.9048e-02,  2.3569e-01, -5.9791e-01,  ...,  5.3206e-03,\n",
      "           2.5923e-01,  2.8054e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.6092e-02, -4.3516e-02, -2.1955e-01,  ...,  1.6054e-02,\n",
      "          -5.6666e-02, -4.0850e-01],\n",
      "         [-9.3660e-02,  2.9275e-01, -6.2543e-01,  ..., -5.4177e-02,\n",
      "           1.0576e-01, -2.8716e-01],\n",
      "         [ 1.5587e-02, -9.7926e-02, -8.3286e-01,  ..., -3.6255e-01,\n",
      "          -1.3203e-01, -1.6569e-01],\n",
      "         ...,\n",
      "         [-1.0337e-01,  1.9142e-01, -8.0734e-01,  ..., -1.6946e-01,\n",
      "          -5.6099e-02, -4.3649e-01],\n",
      "         [-1.5523e-02, -1.0262e-01, -8.8673e-01,  ..., -2.9024e-01,\n",
      "          -6.0524e-02, -1.4075e-01],\n",
      "         [-1.5908e-01,  1.3972e-01, -3.7033e-01,  ..., -1.8146e-01,\n",
      "          -2.2458e-01, -3.3919e-01]],\n",
      "\n",
      "        [[-2.9954e-03,  2.0329e-01, -8.5477e-02,  ..., -2.5919e-01,\n",
      "          -3.4963e-02, -8.3549e-02],\n",
      "         [-4.5881e-02,  9.0990e-02, -5.2514e-01,  ..., -4.3221e-01,\n",
      "           4.3480e-02,  1.1104e-01],\n",
      "         [-2.0341e-01,  1.1630e-01, -5.3160e-01,  ..., -1.9866e-01,\n",
      "          -3.6810e-02,  4.8002e-01],\n",
      "         ...,\n",
      "         [-2.6650e-01,  3.9090e-01, -5.8077e-01,  ...,  7.2741e-02,\n",
      "          -6.0188e-02,  1.1654e-01],\n",
      "         [-1.6573e-01, -4.0678e-02, -3.6300e-01,  ..., -3.4767e-01,\n",
      "           2.1909e-02,  3.2342e-01],\n",
      "         [-2.5607e-01,  2.4542e-01, -1.9651e-01,  ..., -3.2470e-01,\n",
      "           9.3243e-02,  2.9047e-01]],\n",
      "\n",
      "        [[ 2.8076e-01, -3.9170e-02, -1.7613e-01,  ..., -1.5259e-01,\n",
      "          -1.8692e-02, -3.1188e-01],\n",
      "         [ 3.3866e-01,  1.7373e-01, -3.2817e-01,  ..., -4.8059e-01,\n",
      "           8.9845e-02, -2.7748e-01],\n",
      "         [ 9.9819e-02,  2.0629e-02, -5.9439e-01,  ..., -5.5248e-01,\n",
      "           2.4599e-01, -1.1101e-01],\n",
      "         ...,\n",
      "         [ 1.8364e-02,  2.7767e-01, -5.0674e-01,  ..., -5.9568e-03,\n",
      "          -8.9352e-02, -6.4319e-01],\n",
      "         [-5.8223e-02,  8.3457e-02, -5.2119e-01,  ..., -2.5811e-01,\n",
      "          -5.5436e-02, -6.8817e-02],\n",
      "         [-2.8420e-02,  2.7226e-02, -2.0008e-01,  ..., -3.1796e-01,\n",
      "           2.7453e-01, -4.3716e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1640872955322266\n",
      "Model outputs:  tensor([[[-0.2057, -0.1485, -0.8653,  ...,  0.6845, -0.0582,  0.3637],\n",
      "         [ 0.0010,  0.2337, -0.7660,  ...,  0.6054,  0.1526,  0.1529],\n",
      "         [-0.2665,  0.6170, -0.8584,  ...,  0.8784,  0.0539, -0.0157],\n",
      "         ...,\n",
      "         [-0.0956,  0.1012, -0.3455,  ...,  0.9825,  0.1231,  0.2234],\n",
      "         [-0.2232,  0.0567, -0.9536,  ...,  0.8144,  0.0569,  0.2927],\n",
      "         [-0.0791,  0.2296, -0.7597,  ...,  1.0127,  0.0928,  0.0654]],\n",
      "\n",
      "        [[-0.3721, -0.1237, -0.6721,  ...,  0.6047,  0.3426,  0.2789],\n",
      "         [-0.6205,  0.4057, -0.7063,  ...,  0.4819,  0.3007,  0.0880],\n",
      "         [-0.6933,  0.4908, -0.5659,  ...,  0.7188,  0.2227,  0.2311],\n",
      "         ...,\n",
      "         [-0.1408,  0.1321, -0.5707,  ...,  0.9331,  0.3647,  0.0589],\n",
      "         [-0.2210,  0.2624, -0.3954,  ...,  0.6193,  0.3007,  0.1562],\n",
      "         [-0.1842,  0.2532, -0.8395,  ...,  0.7502,  0.4588,  0.3133]],\n",
      "\n",
      "        [[ 0.0065, -0.3152, -0.6147,  ..., -0.1024, -0.4139,  0.4827],\n",
      "         [ 0.2992, -0.1926, -0.4903,  ..., -0.3195, -0.2457,  0.4181],\n",
      "         [ 0.0091,  0.0530, -0.3499,  ..., -0.2803, -0.3180,  0.5831],\n",
      "         ...,\n",
      "         [ 0.2457, -0.0686, -0.2867,  ...,  0.1781, -0.2768,  0.7882],\n",
      "         [-0.0896, -0.1235, -0.5754,  ..., -0.1677, -0.2570,  0.5910],\n",
      "         [ 0.2924, -0.3068, -0.4348,  ..., -0.1377, -0.5931,  0.5447]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1113, -0.0514, -0.7155,  ...,  0.6103, -0.1708,  0.4091],\n",
      "         [-0.0143,  0.2572, -0.4605,  ...,  0.3306, -0.2055,  0.1822],\n",
      "         [ 0.0021,  0.2473, -0.6647,  ...,  0.1563, -0.1016,  0.0536],\n",
      "         ...,\n",
      "         [ 0.1052,  0.0512, -0.5913,  ...,  0.4121, -0.0167,  0.3030],\n",
      "         [-0.1325,  0.1269, -0.8356,  ...,  0.2893, -0.0119,  0.2112],\n",
      "         [ 0.2679, -0.0233, -0.8754,  ...,  0.3647, -0.2142,  0.0917]],\n",
      "\n",
      "        [[-0.0127, -0.1835, -0.5769,  ...,  0.2037, -0.2394,  0.6288],\n",
      "         [ 0.0959,  0.0153, -0.3635,  ..., -0.1020, -0.2069,  0.4555],\n",
      "         [ 0.2615,  0.1984, -0.4226,  ..., -0.1238, -0.1457,  0.4063],\n",
      "         ...,\n",
      "         [ 0.0203, -0.0885, -0.1764,  ..., -0.0767, -0.1243,  0.9521],\n",
      "         [ 0.2112, -0.0632, -0.4368,  ..., -0.0067, -0.4945,  0.4554],\n",
      "         [-0.0220, -0.2158, -0.7177,  ..., -0.1734, -0.3382,  0.4532]],\n",
      "\n",
      "        [[ 0.0831, -0.1918, -0.4185,  ...,  0.0841, -0.2247,  0.8160],\n",
      "         [ 0.1940, -0.0549, -0.3224,  ..., -0.2053, -0.0500,  0.3890],\n",
      "         [ 0.1748,  0.0430, -0.3848,  ..., -0.1909, -0.2820,  0.6575],\n",
      "         ...,\n",
      "         [ 0.1933, -0.2738, -0.2878,  ...,  0.0935, -0.1331,  0.5167],\n",
      "         [ 0.0903, -0.2257, -0.3772,  ..., -0.1634, -0.3343,  0.6561],\n",
      "         [ 0.2823, -0.1527, -0.2298,  ..., -0.1674, -0.3394,  0.3933]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9508122801780701\n",
      "Model outputs:  tensor([[[-1.2282e-01,  1.5651e-01, -4.6854e-01,  ...,  7.9302e-02,\n",
      "          -3.5574e-01, -9.1143e-02],\n",
      "         [-3.0943e-02,  2.0148e-01, -6.6618e-01,  ..., -5.4292e-02,\n",
      "          -2.8598e-04, -8.8964e-02],\n",
      "         [-6.9007e-02,  3.0947e-01, -5.0296e-01,  ..., -1.0150e-01,\n",
      "          -4.2495e-01,  4.0788e-02],\n",
      "         ...,\n",
      "         [-4.2424e-01,  6.9071e-02, -7.8598e-01,  ..., -1.3932e-01,\n",
      "           1.4605e-01, -1.8591e-01],\n",
      "         [-3.7080e-01,  2.5540e-01, -7.1853e-01,  ..., -5.8404e-02,\n",
      "           4.0785e-02,  6.7758e-02],\n",
      "         [-3.8429e-01,  2.5633e-01, -4.5753e-01,  ..., -2.2381e-01,\n",
      "           7.9023e-02,  2.9618e-01]],\n",
      "\n",
      "        [[-1.5954e-01,  1.4032e-01, -6.7557e-01,  ..., -2.1231e-01,\n",
      "           8.5628e-02, -7.0989e-02],\n",
      "         [ 1.8839e-02,  1.1477e-01, -8.8050e-01,  ...,  1.3587e-01,\n",
      "          -1.1386e-03,  1.0293e-01],\n",
      "         [-4.2890e-01,  5.0088e-01, -6.9915e-01,  ..., -1.7698e-01,\n",
      "          -1.6239e-01,  1.1442e-02],\n",
      "         ...,\n",
      "         [-1.7257e-01,  5.4454e-01, -7.6406e-01,  ..., -9.0084e-02,\n",
      "          -1.2632e-02,  7.3418e-02],\n",
      "         [-2.2249e-01,  2.4990e-01, -5.8892e-01,  ...,  2.2451e-02,\n",
      "           8.8242e-02,  9.5922e-03],\n",
      "         [-3.6873e-01,  3.0534e-01, -5.4969e-01,  ..., -2.8764e-01,\n",
      "           4.3991e-02,  2.3389e-01]],\n",
      "\n",
      "        [[ 1.8965e-01,  3.1163e-02, -1.3314e-01,  ..., -5.6756e-01,\n",
      "          -2.8588e-01,  2.5201e-01],\n",
      "         [ 5.4054e-02, -9.8334e-02,  1.1050e-01,  ..., -4.9997e-01,\n",
      "          -3.3522e-01,  3.8571e-01],\n",
      "         [ 3.1083e-01, -1.6514e-01, -1.2423e-01,  ..., -6.7553e-01,\n",
      "          -2.3468e-01,  3.5171e-01],\n",
      "         ...,\n",
      "         [ 1.4859e-01, -3.3513e-02, -1.4373e-01,  ..., -6.2082e-01,\n",
      "          -4.2516e-01,  3.0030e-01],\n",
      "         [-1.1948e-01,  2.2684e-01, -2.7736e-01,  ..., -5.3271e-01,\n",
      "          -6.7205e-02,  2.3370e-01],\n",
      "         [ 1.2035e-01, -1.8799e-01, -2.6241e-01,  ..., -6.8563e-01,\n",
      "          -2.5351e-01,  4.4202e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.9163e-01,  1.1020e-01, -2.6293e-01,  ..., -6.0838e-01,\n",
      "          -2.2836e-01,  4.2922e-01],\n",
      "         [ 2.8889e-01, -2.2979e-01, -3.8572e-02,  ..., -7.6303e-01,\n",
      "          -1.9172e-01,  4.6283e-01],\n",
      "         [ 1.8796e-01, -2.3629e-01, -1.8898e-01,  ..., -5.0709e-01,\n",
      "          -3.1903e-01,  3.8168e-01],\n",
      "         ...,\n",
      "         [ 1.4376e-01, -3.6442e-02, -4.1227e-01,  ..., -5.4962e-01,\n",
      "          -6.0590e-02,  2.5439e-01],\n",
      "         [-3.0905e-03, -2.2797e-01, -2.7945e-01,  ..., -2.4071e-01,\n",
      "          -2.1986e-01,  4.1315e-01],\n",
      "         [ 5.9541e-03, -2.6005e-02, -5.9043e-02,  ..., -5.2677e-01,\n",
      "          -2.3375e-01,  3.5734e-01]],\n",
      "\n",
      "        [[-4.2930e-02,  1.8764e-01, -5.1180e-01,  ...,  4.5998e-02,\n",
      "           2.9366e-02,  1.4931e-02],\n",
      "         [-1.0609e-01,  5.2560e-02, -3.5705e-01,  ..., -4.7809e-02,\n",
      "          -1.5147e-01,  1.6233e-01],\n",
      "         [-1.8299e-01,  1.9498e-01, -2.5842e-01,  ..., -2.2107e-01,\n",
      "          -2.3250e-02, -9.7083e-02],\n",
      "         ...,\n",
      "         [-3.2935e-01,  3.5721e-01, -6.5509e-01,  ..., -2.8627e-02,\n",
      "           7.0821e-02, -1.4010e-01],\n",
      "         [-2.8474e-01,  4.6312e-01, -8.7608e-01,  ..., -9.4544e-02,\n",
      "           5.5972e-03,  2.9086e-01],\n",
      "         [-7.7995e-02,  2.9370e-01, -4.1912e-01,  ..., -7.7503e-02,\n",
      "           6.8117e-02, -2.7483e-01]],\n",
      "\n",
      "        [[ 1.8343e-01, -1.2879e-02, -3.8080e-01,  ..., -3.5923e-01,\n",
      "          -2.9620e-01,  5.7858e-01],\n",
      "         [ 1.9857e-02,  2.0021e-01, -3.3408e-01,  ..., -7.3860e-01,\n",
      "           3.3976e-02,  4.9305e-01],\n",
      "         [ 2.9279e-01, -9.8890e-02, -4.7001e-01,  ..., -5.7804e-01,\n",
      "          -1.4654e-01,  2.7151e-01],\n",
      "         ...,\n",
      "         [-4.7181e-03,  1.1709e-01, -3.2214e-01,  ..., -5.4842e-01,\n",
      "           4.0603e-02,  4.1186e-02],\n",
      "         [ 3.0375e-02,  4.6534e-03, -2.9485e-01,  ..., -5.9346e-01,\n",
      "          -1.7471e-01,  4.7788e-01],\n",
      "         [ 2.4265e-01,  3.4273e-02, -4.2900e-01,  ..., -5.4570e-01,\n",
      "          -2.4777e-01,  1.4766e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1198687553405762\n",
      "Model outputs:  tensor([[[-0.3628,  0.3296, -0.4509,  ..., -0.2002,  0.0569,  0.1085],\n",
      "         [-0.2788,  0.3029, -0.7567,  ..., -0.1157,  0.2847,  0.1558],\n",
      "         [-0.4830, -0.0170, -0.5985,  ...,  0.0296,  0.0878,  0.2024],\n",
      "         ...,\n",
      "         [-0.1419,  0.3050, -0.5065,  ..., -0.2021,  0.0474, -0.2080],\n",
      "         [-0.0965,  0.3823, -0.3473,  ..., -0.5044, -0.1164, -0.1445],\n",
      "         [-0.2737,  0.2168, -0.5279,  ..., -0.4335, -0.1176,  0.1337]],\n",
      "\n",
      "        [[ 0.0015,  0.1378, -0.1216,  ..., -0.5183, -0.2631,  0.2589],\n",
      "         [-0.0555,  0.0150, -0.2771,  ..., -0.5541, -0.0552,  0.3921],\n",
      "         [ 0.3039, -0.0757, -0.4393,  ..., -0.3780, -0.0097,  0.4786],\n",
      "         ...,\n",
      "         [ 0.2801, -0.0923, -0.0101,  ..., -0.7928,  0.0300, -0.1024],\n",
      "         [ 0.3710, -0.1744, -0.1158,  ..., -0.5451, -0.1448,  0.1859],\n",
      "         [ 0.3650,  0.1893, -0.0571,  ..., -0.4624, -0.1877,  0.1869]],\n",
      "\n",
      "        [[-0.6926,  0.6036, -0.6940,  ...,  0.0732,  0.4330,  0.2139],\n",
      "         [-0.5167,  0.3549, -0.4782,  ...,  0.1708,  0.5745, -0.0061],\n",
      "         [-0.3783,  0.3329, -0.4301,  ...,  0.2687,  0.3495,  0.1419],\n",
      "         ...,\n",
      "         [-0.1482,  0.4091, -0.3288,  ..., -0.3740,  0.1467, -0.1976],\n",
      "         [-0.2799,  0.4010, -0.5179,  ..., -0.1931,  0.3923,  0.0613],\n",
      "         [-0.4607,  0.4420, -0.1760,  ..., -0.1508,  0.2334, -0.0745]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7842,  0.3988, -0.0780,  ..., -0.1582,  0.2508, -0.1177],\n",
      "         [-0.5276,  0.1626, -0.4203,  ...,  0.0897,  0.6433,  0.1442],\n",
      "         [-0.3724,  0.2353, -0.2064,  ...,  0.2157,  0.6051,  0.1803],\n",
      "         ...,\n",
      "         [-0.2649,  0.3688, -0.2253,  ..., -0.3230,  0.1738, -0.2661],\n",
      "         [-0.4824,  0.2963, -0.2770,  ..., -0.3565,  0.2394, -0.1274],\n",
      "         [-0.1166,  0.2284, -0.2370,  ..., -0.1192,  0.5144,  0.0247]],\n",
      "\n",
      "        [[-0.3791,  0.4795, -0.1411,  ...,  0.1480,  0.0969,  0.0190],\n",
      "         [-0.0070,  0.2063, -0.5498,  ...,  0.6741,  0.6445,  0.1553],\n",
      "         [-0.2540,  0.2137, -0.1953,  ...,  0.4316,  0.5575,  0.1220],\n",
      "         ...,\n",
      "         [-0.2785,  0.2983, -0.1364,  ..., -0.0478,  0.1683, -0.3841],\n",
      "         [-0.2513,  0.6013, -0.0376,  ...,  0.2727,  0.1540, -0.4872],\n",
      "         [-0.3098,  0.0913, -0.2326,  ...,  0.0446,  0.2117, -0.2251]],\n",
      "\n",
      "        [[ 0.3346,  0.0476, -0.3124,  ..., -0.5070,  0.3895, -0.1456],\n",
      "         [ 0.4923, -0.2022, -0.3371,  ..., -0.6293,  0.3817, -0.0303],\n",
      "         [ 0.4394, -0.4767, -0.3050,  ..., -0.2900,  0.0727, -0.7725],\n",
      "         ...,\n",
      "         [ 0.1546,  0.0994, -0.1308,  ..., -0.8520,  0.5936,  0.4105],\n",
      "         [ 0.1252,  0.1930, -0.1820,  ..., -0.7965,  0.6687,  0.2277],\n",
      "         [ 0.2793,  0.2111, -0.3160,  ..., -0.7523,  0.6007,  0.3054]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2448012828826904\n",
      "Model outputs:  tensor([[[ 0.4932,  0.0999, -0.2482,  ..., -0.6090, -0.2535,  0.0642],\n",
      "         [ 0.3364, -0.0244, -0.1130,  ..., -0.6574, -0.0688,  0.2796],\n",
      "         [ 0.1228, -0.1788, -0.0824,  ..., -0.7727, -0.3288,  0.1188],\n",
      "         ...,\n",
      "         [ 0.2783,  0.2343, -0.1325,  ..., -0.5925,  0.0286, -0.0017],\n",
      "         [ 0.3380, -0.2549, -0.1075,  ..., -0.8778, -0.3135,  0.1878],\n",
      "         [ 0.2603,  0.0567,  0.0522,  ..., -0.7921, -0.2825,  0.4017]],\n",
      "\n",
      "        [[-0.4034,  0.5790, -0.4614,  ...,  0.0571,  0.1127, -0.1438],\n",
      "         [-0.2921,  0.2975, -0.2702,  ..., -0.2486, -0.0844, -0.2480],\n",
      "         [-0.0934,  0.2546, -0.5600,  ..., -0.0422,  0.1657, -0.0636],\n",
      "         ...,\n",
      "         [-0.2104,  0.4974, -0.6737,  ...,  0.1980,  0.1663, -0.3179],\n",
      "         [-0.1339,  0.1043, -0.6411,  ...,  0.1178,  0.1777, -0.0458],\n",
      "         [-0.3554,  0.4816, -0.5238,  ..., -0.2028,  0.0146, -0.1244]],\n",
      "\n",
      "        [[-0.2955,  0.5070, -0.3351,  ...,  0.0558,  0.4769, -0.0702],\n",
      "         [-0.1019,  0.2796, -0.1904,  ..., -0.2549,  0.4038, -0.0514],\n",
      "         [-0.2106,  0.2507, -0.2262,  ...,  0.0040,  0.2469, -0.3230],\n",
      "         ...,\n",
      "         [-0.1358,  0.3220, -0.2023,  ..., -0.2781,  0.2217, -0.4482],\n",
      "         [-0.2478,  0.3624, -0.3253,  ..., -0.0341,  0.3648, -0.5469],\n",
      "         [-0.4311,  0.2867, -0.0423,  ..., -0.2034,  0.2959, -0.1685]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0672,  0.0836, -0.3188,  ..., -0.8666, -0.1400,  0.2567],\n",
      "         [ 0.1460,  0.0935, -0.1414,  ..., -0.6917, -0.1912, -0.2003],\n",
      "         [ 0.3150,  0.2665, -0.4324,  ..., -0.7672, -0.1796,  0.3100],\n",
      "         ...,\n",
      "         [ 0.1475,  0.2728, -0.3216,  ..., -0.4722, -0.0809, -0.0606],\n",
      "         [ 0.1807,  0.1895, -0.2251,  ..., -0.5847, -0.0647,  0.1755],\n",
      "         [ 0.0036,  0.0798, -0.2744,  ..., -0.5531, -0.1144, -0.0325]],\n",
      "\n",
      "        [[-0.4131,  0.2004, -0.6627,  ..., -0.1277,  0.1109,  0.0015],\n",
      "         [-0.5854,  0.7097, -0.6270,  ..., -0.1320,  0.3781,  0.0812],\n",
      "         [-0.4528,  0.3198, -0.6219,  ..., -0.1772,  0.2123,  0.1068],\n",
      "         ...,\n",
      "         [-0.3127,  0.3863, -0.4722,  ..., -0.2541,  0.2278, -0.2071],\n",
      "         [-0.0455,  0.2637, -0.5473,  ..., -0.3144,  0.1045, -0.2510],\n",
      "         [-0.3167,  0.3752, -0.5520,  ..., -0.3725,  0.1421, -0.0175]],\n",
      "\n",
      "        [[ 0.3173,  0.2623, -0.3408,  ..., -0.6564, -0.2315,  0.2101],\n",
      "         [-0.0313,  0.1958, -0.3301,  ..., -0.6669,  0.0378,  0.1125],\n",
      "         [ 0.1062,  0.0560, -0.2583,  ..., -0.7713, -0.1003,  0.3595],\n",
      "         ...,\n",
      "         [ 0.2055,  0.2370, -0.2958,  ..., -0.3445, -0.0852, -0.0069],\n",
      "         [ 0.2830,  0.3464, -0.3525,  ..., -0.3914,  0.0914,  0.2573],\n",
      "         [ 0.0825,  0.1619, -0.2680,  ..., -0.4726, -0.2115,  0.0395]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0244495868682861\n",
      "Model outputs:  tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.0468, -0.1229, -0.2523,  ...,  0.8069,  0.2175, -0.1797],\n",
      "         [-0.2710, -0.1784, -0.3089,  ...,  0.7495,  0.2188,  0.3361],\n",
      "         ...,\n",
      "         [-0.4272, -0.1005, -0.1443,  ...,  0.6859,  0.3594,  0.1531],\n",
      "         [-0.3695, -0.0571, -0.1956,  ...,  0.5345,  0.1399, -0.0998],\n",
      "         [-0.1932,  0.1923, -0.1630,  ...,  0.5241,  0.1391,  0.0557]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.0903,  0.2749, -0.5562,  ...,  1.2177,  0.0582,  0.2508],\n",
      "         [ 0.0126,  0.1017, -0.7566,  ...,  1.0990, -0.0426,  0.1737],\n",
      "         ...,\n",
      "         [ 0.0713, -0.1938, -0.7098,  ...,  0.8205,  0.1100,  0.5902],\n",
      "         [-0.0814,  0.2062, -0.7480,  ...,  0.6587,  0.0116, -0.0302],\n",
      "         [-0.1109,  0.3574, -0.8708,  ...,  0.5921,  0.3305,  0.1548]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.0034,  0.1143, -0.2452,  ...,  0.6275,  0.1564,  0.2154],\n",
      "         [-0.0117, -0.2663, -0.3426,  ...,  0.8084,  0.3381,  0.0685],\n",
      "         ...,\n",
      "         [-0.3720,  0.0736, -0.1651,  ...,  0.6619,  0.1737,  0.2805],\n",
      "         [-0.5264,  0.1127, -0.3065,  ...,  0.5655,  0.1835,  0.1672],\n",
      "         [-0.1418, -0.1066, -0.2130,  ...,  0.6502,  0.2822, -0.0542]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.1850, -0.0954, -0.3121,  ..., -0.0820,  0.4766,  0.7904],\n",
      "         [ 0.0249, -0.2290, -0.3959,  ...,  0.1337,  0.3196,  0.6257],\n",
      "         ...,\n",
      "         [-0.0306,  0.0035, -0.4769,  ..., -0.2783,  0.4074,  0.8676],\n",
      "         [ 0.1665,  0.1350, -0.5587,  ..., -0.2612,  0.5912,  0.8111],\n",
      "         [ 0.1132,  0.0490, -0.3069,  ..., -0.3250,  0.5379,  0.6130]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.3986,  0.1348, -0.5815,  ...,  0.4307, -0.4234,  0.2227],\n",
      "         [ 0.2380, -0.1012, -0.5911,  ...,  0.3729, -0.1654,  0.4925],\n",
      "         ...,\n",
      "         [ 0.1010, -0.2352, -0.6083,  ...,  0.4896, -0.0650,  0.7079],\n",
      "         [-0.1311,  0.1825, -0.8379,  ...,  0.2557, -0.4298,  0.4143],\n",
      "         [ 0.2612,  0.2369, -0.5086,  ...,  0.1276, -0.1414,  0.1696]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.3887,  0.2862, -0.3132,  ...,  0.3274, -0.2580,  0.1393],\n",
      "         [ 0.1383,  0.1913, -0.7067,  ...,  0.6619, -0.2629,  0.4152],\n",
      "         ...,\n",
      "         [ 0.0458, -0.0739, -0.6388,  ...,  0.3618, -0.0392,  0.3454],\n",
      "         [ 0.1376,  0.3462, -0.7057,  ...,  0.4114,  0.0554,  0.2051],\n",
      "         [-0.1030,  0.0795, -0.7089,  ...,  0.5061, -0.0158, -0.0435]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [3/10], Loss: 1.0244\n",
      "Model outputs:  tensor([[[-1.9410e-01,  3.3985e-01, -6.0767e-01,  ...,  1.7596e-01,\n",
      "           5.8606e-02,  4.9838e-04],\n",
      "         [-3.2240e-01,  1.3137e-01, -8.4555e-01,  ...,  6.7799e-02,\n",
      "           5.4919e-01,  1.1701e-01],\n",
      "         [-4.4175e-01,  2.6304e-01, -6.4942e-01,  ...,  9.1369e-02,\n",
      "           9.2833e-02,  5.0108e-02],\n",
      "         ...,\n",
      "         [-5.9953e-01,  5.2548e-02, -9.4927e-01,  ...,  1.5176e-01,\n",
      "           1.2610e-01,  3.1208e-01],\n",
      "         [-3.0781e-01,  3.3637e-01, -8.7634e-01,  ...,  1.1959e-01,\n",
      "           1.3193e-01,  1.2328e-01],\n",
      "         [-5.6617e-01,  2.6841e-01, -6.7454e-01,  ..., -1.7974e-01,\n",
      "           1.8835e-01, -2.7405e-01]],\n",
      "\n",
      "        [[-6.6186e-01,  3.8841e-01, -2.6797e-01,  ...,  4.1109e-01,\n",
      "           1.3832e-01, -5.7735e-02],\n",
      "         [-5.5400e-01,  1.0638e-01, -5.2216e-01,  ...,  5.9527e-01,\n",
      "           5.2734e-01,  1.8404e-01],\n",
      "         [-4.5936e-01,  2.9400e-01, -7.3847e-01,  ...,  7.9202e-01,\n",
      "           2.4297e-01,  1.5786e-01],\n",
      "         ...,\n",
      "         [-3.4748e-01,  5.8947e-01, -9.4721e-01,  ...,  7.4125e-01,\n",
      "           2.7213e-01,  1.1745e-01],\n",
      "         [-6.7030e-01,  5.1901e-01, -3.1846e-01,  ...,  5.8039e-01,\n",
      "           3.9727e-01, -1.6537e-01],\n",
      "         [-4.9212e-01,  4.4195e-01, -5.9229e-01,  ...,  1.7645e-01,\n",
      "           5.6260e-01, -2.6939e-01]],\n",
      "\n",
      "        [[-1.6625e-01,  2.9896e-01, -6.9273e-01,  ...,  1.9627e-01,\n",
      "           4.8482e-03, -1.7415e-01],\n",
      "         [ 1.5644e-01, -4.0246e-02, -5.6524e-01,  ...,  5.7821e-02,\n",
      "          -2.1327e-01,  3.2264e-01],\n",
      "         [ 1.3875e-01,  2.6527e-01, -7.4596e-01,  ...,  2.6069e-01,\n",
      "           1.8973e-02,  1.0137e-01],\n",
      "         ...,\n",
      "         [-6.2765e-02,  1.8885e-01, -1.1139e+00,  ...,  4.8510e-01,\n",
      "           1.1134e-01,  4.2939e-01],\n",
      "         [ 6.1402e-02,  2.2500e-01, -8.6913e-01,  ...,  1.4585e-01,\n",
      "           1.5762e-01, -4.0061e-02],\n",
      "         [-2.2015e-01,  3.2270e-01, -4.8984e-01,  ...,  1.0639e-02,\n",
      "           1.5927e-01, -3.1936e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.3114e-01,  2.0061e-01, -3.6985e-01,  ...,  1.7348e-02,\n",
      "           5.8619e-02, -8.7676e-03],\n",
      "         [-5.2073e-02,  6.2443e-02, -6.7151e-01,  ...,  2.6179e-01,\n",
      "          -8.9754e-02,  2.4631e-01],\n",
      "         [ 1.4134e-01,  4.6076e-01, -8.2080e-01,  ...,  2.8076e-01,\n",
      "          -8.4513e-02,  3.0808e-02],\n",
      "         ...,\n",
      "         [-4.4464e-03,  4.5051e-02, -7.2935e-01,  ...,  2.2479e-01,\n",
      "          -3.5052e-02,  1.8809e-01],\n",
      "         [ 2.2213e-01,  3.6241e-01, -8.5857e-01,  ...,  3.8607e-01,\n",
      "          -1.7152e-01,  4.2928e-02],\n",
      "         [-2.5766e-01,  6.5299e-01, -6.9520e-01,  ..., -9.4928e-03,\n",
      "           4.2909e-02, -3.4811e-01]],\n",
      "\n",
      "        [[ 6.6212e-03,  3.7056e-01, -3.8235e-01,  ..., -2.0429e-01,\n",
      "           1.4804e-03, -7.3506e-02],\n",
      "         [-5.7384e-03,  4.9156e-02, -4.4109e-01,  ..., -2.6721e-01,\n",
      "          -4.7051e-02,  5.1384e-01],\n",
      "         [ 1.2689e-01,  1.5701e-01, -5.9788e-01,  ..., -2.2458e-01,\n",
      "           8.0528e-02,  4.1519e-01],\n",
      "         ...,\n",
      "         [-5.2929e-02,  1.9678e-01, -8.6897e-01,  ...,  2.4917e-01,\n",
      "           3.8082e-02,  6.0710e-01],\n",
      "         [-2.3612e-01,  1.8046e-01, -7.9742e-01,  ...,  4.7000e-02,\n",
      "           3.8032e-03,  2.1155e-01],\n",
      "         [-8.3364e-02,  2.2987e-01, -5.1490e-01,  ...,  6.2594e-02,\n",
      "          -8.4232e-02,  2.2737e-02]],\n",
      "\n",
      "        [[ 3.7260e-01, -1.4783e-01,  2.2879e-01,  ...,  4.0049e-02,\n",
      "           3.4656e-01, -7.1240e-01],\n",
      "         [ 2.1738e-01, -2.3518e-01,  5.0505e-02,  ...,  1.1701e-01,\n",
      "           2.7235e-01, -5.6849e-01],\n",
      "         [ 2.5809e-01, -2.8011e-01, -2.4561e-01,  ...,  2.6186e-02,\n",
      "           1.0160e-01, -4.9160e-01],\n",
      "         ...,\n",
      "         [ 9.0824e-02, -4.3197e-01, -7.9379e-02,  ...,  4.1832e-01,\n",
      "           2.7411e-01, -5.0444e-01],\n",
      "         [ 2.1267e-01,  5.7097e-02,  5.6854e-02,  ...,  7.3880e-02,\n",
      "          -3.0569e-02, -4.5775e-01],\n",
      "         [ 2.5240e-01,  2.0516e-01,  2.1946e-01,  ..., -2.5063e-02,\n",
      "           2.6692e-02, -8.8243e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3133524656295776\n",
      "Model outputs:  tensor([[[-5.2971e-01,  2.2013e-01, -4.9981e-01,  ...,  2.8860e-02,\n",
      "           2.6856e-01, -2.7014e-01],\n",
      "         [-4.0752e-01,  1.7291e-01, -6.8052e-01,  ..., -4.6652e-02,\n",
      "           4.3002e-01,  7.8736e-02],\n",
      "         [-2.6218e-01,  1.8670e-01, -5.1218e-01,  ..., -1.0549e-01,\n",
      "           3.3020e-01,  1.1741e-01],\n",
      "         ...,\n",
      "         [-3.0173e-01,  3.4814e-01, -5.1975e-01,  ..., -2.8132e-01,\n",
      "          -7.5341e-03,  4.9372e-03],\n",
      "         [-5.2204e-01,  1.2042e-01, -3.6336e-01,  ..., -1.9692e-01,\n",
      "           4.6019e-02, -1.9830e-01],\n",
      "         [-2.7104e-01,  1.7627e-01, -4.9417e-01,  ..., -3.5762e-01,\n",
      "           4.2749e-01, -1.6121e-01]],\n",
      "\n",
      "        [[-1.4714e-01,  3.7104e-01, -5.8596e-01,  ..., -2.4554e-01,\n",
      "           2.4991e-01, -9.1660e-02],\n",
      "         [-9.6969e-02,  5.0629e-01, -5.4954e-01,  ...,  1.3823e-01,\n",
      "           2.2813e-01,  1.9341e-03],\n",
      "         [-4.4723e-02,  4.8989e-01, -6.8493e-01,  ...,  2.6481e-01,\n",
      "           3.3030e-01,  8.6517e-02],\n",
      "         ...,\n",
      "         [-1.8632e-01,  1.7512e-01, -7.1958e-01,  ..., -1.5565e-01,\n",
      "           5.0266e-02, -2.2098e-01],\n",
      "         [-2.4635e-01,  6.0321e-01, -4.6652e-01,  ..., -5.7817e-02,\n",
      "           2.7719e-01,  5.6088e-04],\n",
      "         [-2.5434e-01,  2.3734e-01, -6.5790e-01,  ..., -2.1900e-03,\n",
      "           2.6518e-02, -5.7227e-02]],\n",
      "\n",
      "        [[-2.4250e-01,  3.7755e-02, -4.7634e-01,  ..., -3.3251e-01,\n",
      "           2.0438e-01,  5.6804e-02],\n",
      "         [-1.7126e-01,  2.4707e-01, -3.9359e-01,  ..., -3.2719e-01,\n",
      "           1.4083e-01,  2.3235e-01],\n",
      "         [ 3.6623e-01,  1.7329e-01, -6.1263e-01,  ..., -2.4495e-01,\n",
      "           4.1636e-01,  1.2653e-01],\n",
      "         ...,\n",
      "         [ 1.4663e-01,  2.2315e-01, -3.9246e-01,  ..., -2.8272e-01,\n",
      "           1.7870e-01, -1.3558e-02],\n",
      "         [ 1.0507e-01,  3.9689e-01, -4.3582e-01,  ..., -4.0873e-01,\n",
      "           9.9409e-02,  2.1733e-01],\n",
      "         [-3.3915e-02,  2.1723e-02, -5.1480e-01,  ..., -5.3677e-01,\n",
      "           3.1816e-01,  4.1418e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.7864e-01,  6.8697e-01, -2.0310e-01,  ...,  2.4077e-01,\n",
      "           3.7502e-01, -4.5790e-01],\n",
      "         [-4.0437e-01,  4.2151e-01,  6.9937e-02,  ...,  3.2467e-01,\n",
      "           3.2955e-01, -3.1918e-01],\n",
      "         [-2.5350e-01,  6.8579e-01, -4.5275e-04,  ...,  5.0671e-01,\n",
      "           4.4989e-01, -2.7569e-01],\n",
      "         ...,\n",
      "         [-1.4573e-01,  3.2882e-01,  3.0467e-02,  ...,  1.1978e-01,\n",
      "           3.7288e-01, -3.7476e-01],\n",
      "         [-1.7377e-01,  6.7813e-01, -1.3971e-01,  ...,  2.6409e-01,\n",
      "           2.5529e-01, -2.9991e-01],\n",
      "         [-2.8835e-01,  3.5694e-01, -3.3562e-01,  ...,  2.9842e-01,\n",
      "           3.2655e-01, -5.2157e-01]],\n",
      "\n",
      "        [[ 2.0495e-01, -8.0572e-03, -2.6988e-01,  ..., -4.0994e-01,\n",
      "          -2.2106e-01,  2.3510e-01],\n",
      "         [-2.4432e-01,  7.8364e-02, -2.6196e-01,  ..., -1.9229e-01,\n",
      "          -1.4021e-01,  2.2432e-01],\n",
      "         [-5.8926e-02, -4.1439e-02, -4.1837e-01,  ..., -6.0969e-01,\n",
      "           1.4270e-01,  2.6319e-01],\n",
      "         ...,\n",
      "         [ 9.2169e-02,  5.3441e-03, -3.2027e-01,  ..., -8.0289e-01,\n",
      "           1.9452e-02,  1.1569e-01],\n",
      "         [ 1.7280e-01,  1.4023e-01, -3.3082e-01,  ..., -6.2374e-01,\n",
      "           8.4190e-02,  2.7216e-01],\n",
      "         [-8.0799e-02,  1.8739e-01, -1.3640e-02,  ..., -5.4203e-01,\n",
      "          -2.5696e-01,  1.7265e-01]],\n",
      "\n",
      "        [[-2.9205e-01,  4.0535e-01, -1.5913e-01,  ...,  1.8230e-01,\n",
      "           2.0233e-01, -2.6125e-01],\n",
      "         [-5.1798e-01,  5.6508e-01,  1.1246e-01,  ...,  5.2957e-01,\n",
      "           3.0197e-01, -2.1456e-01],\n",
      "         [-2.8050e-01,  4.5834e-01,  3.3653e-02,  ...,  5.5564e-01,\n",
      "           7.2502e-01, -3.6702e-01],\n",
      "         ...,\n",
      "         [-2.5935e-01,  5.6936e-01, -3.3825e-01,  ...,  1.8076e-01,\n",
      "           6.7136e-01, -5.1416e-01],\n",
      "         [-2.7551e-01,  3.6525e-01, -3.0192e-01,  ...,  4.8620e-01,\n",
      "           1.9826e-01, -2.9539e-01],\n",
      "         [-3.6111e-01,  3.8638e-01, -3.6783e-01,  ...,  3.1340e-01,\n",
      "           4.5787e-01, -2.5339e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3986856937408447\n",
      "Model outputs:  tensor([[[-0.1198, -0.1458, -0.5911,  ..., -0.2289, -0.2794,  0.3455],\n",
      "         [ 0.1053,  0.0983, -0.2920,  ..., -0.2267, -0.2023,  0.3244],\n",
      "         [-0.3873, -0.2044, -0.5132,  ..., -0.3403, -0.7774,  0.5255],\n",
      "         ...,\n",
      "         [-0.0616,  0.1055, -0.6634,  ...,  0.0149, -0.1599,  0.4423],\n",
      "         [-0.1480, -0.1855, -0.5411,  ..., -0.1863, -0.2133,  0.5248],\n",
      "         [-0.1397,  0.1075, -0.2240,  ..., -0.5363, -0.3340,  0.3938]],\n",
      "\n",
      "        [[-0.5448,  0.0573, -0.4982,  ...,  0.7537,  0.3610,  0.1316],\n",
      "         [-0.7406,  0.6738, -0.6545,  ...,  0.7243,  0.2922, -0.0784],\n",
      "         [-0.6397,  0.3614, -0.8431,  ...,  0.6019, -0.4939,  0.3033],\n",
      "         ...,\n",
      "         [-0.3964,  0.3316, -0.8701,  ...,  0.8858, -0.2339,  0.4380],\n",
      "         [-0.5252,  0.3096, -0.6424,  ...,  0.6105,  0.0810,  0.1924],\n",
      "         [-0.5755,  0.2360, -0.3935,  ...,  0.2838, -0.0452,  0.0073]],\n",
      "\n",
      "        [[ 0.2408, -0.1130, -0.3556,  ..., -0.1435,  0.2666,  0.0640],\n",
      "         [ 0.0993,  0.1207, -0.4765,  ..., -0.6459,  0.2094, -0.0594],\n",
      "         [ 0.0735, -0.0175, -0.5559,  ..., -0.4070, -0.2807, -0.0862],\n",
      "         ...,\n",
      "         [ 0.0677, -0.1699, -0.2879,  ..., -0.1850,  0.2543,  0.2753],\n",
      "         [ 0.0718, -0.1321, -0.3988,  ..., -0.5023,  0.2201,  0.3820],\n",
      "         [ 0.1593, -0.2030, -0.3219,  ..., -0.6562,  0.1973,  0.2089]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1538,  0.3129, -0.4327,  ...,  0.1748, -0.0568,  0.2273],\n",
      "         [-0.4576,  0.3288, -0.4485,  ..., -0.0996,  0.0022,  0.1326],\n",
      "         [-0.3114,  0.2907, -0.8710,  ..., -0.1132, -0.6986,  0.2115],\n",
      "         ...,\n",
      "         [-0.2543,  0.2732, -0.7381,  ...,  0.0675, -0.1503,  0.4493],\n",
      "         [-0.1934, -0.0513, -0.2510,  ..., -0.1350, -0.5057,  0.4816],\n",
      "         [-0.3138,  0.1687, -0.3647,  ..., -0.3465, -0.3609,  0.1302]],\n",
      "\n",
      "        [[-0.1410, -0.0088, -0.4020,  ..., -0.0097, -0.2849,  0.4784],\n",
      "         [-0.2033,  0.3546, -0.4778,  ..., -0.1267, -0.1244,  0.1514],\n",
      "         [-0.4600,  0.3065, -0.6047,  ..., -0.2038, -0.4129,  0.4980],\n",
      "         ...,\n",
      "         [-0.2303,  0.2421, -0.8628,  ...,  0.1702, -0.1123,  0.6848],\n",
      "         [-0.1216, -0.1087, -0.1959,  ..., -0.2163, -0.1350,  0.6491],\n",
      "         [-0.1782,  0.0142, -0.5794,  ..., -0.3520, -0.1363,  0.4709]],\n",
      "\n",
      "        [[-0.6961,  0.3553, -0.3923,  ...,  0.4213,  0.1508,  0.1537],\n",
      "         [-0.6743,  0.5253, -0.4759,  ...,  0.2216,  0.1858,  0.2638],\n",
      "         [-0.9394,  0.3237, -0.4678,  ...,  0.3457, -0.2991,  0.5838],\n",
      "         ...,\n",
      "         [-0.7066,  0.1750, -0.4301,  ...,  0.3852,  0.2338,  0.2366],\n",
      "         [-0.7977,  0.4236, -0.3361,  ...,  0.2399,  0.2096,  0.3656],\n",
      "         [-0.5592,  0.0428, -0.5885,  ..., -0.2520,  0.0077,  0.2368]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8202686309814453\n",
      "Model outputs:  tensor([[[-9.2308e-02,  1.4513e-01, -5.8460e-01,  ..., -2.7792e-01,\n",
      "          -6.8246e-02,  1.6037e-01],\n",
      "         [-3.3661e-01,  6.8996e-02, -1.6635e-01,  ..., -1.8449e-01,\n",
      "           8.9330e-02,  3.3472e-01],\n",
      "         [ 5.6533e-02,  1.6249e-01, -3.9189e-01,  ..., -2.8625e-01,\n",
      "          -1.0433e-01,  4.1868e-01],\n",
      "         ...,\n",
      "         [-1.3823e-01,  7.2097e-02, -6.7842e-01,  ...,  5.3480e-03,\n",
      "          -2.0010e-01,  4.5355e-01],\n",
      "         [ 1.2076e-01,  4.4809e-02, -2.5717e-01,  ...,  2.9586e-02,\n",
      "          -1.4136e-01,  3.2342e-01],\n",
      "         [-1.6396e-01,  5.0698e-02, -5.7704e-01,  ...,  1.3212e-01,\n",
      "          -3.6375e-01,  5.1338e-01]],\n",
      "\n",
      "        [[-4.9477e-01,  4.5038e-01, -3.3683e-01,  ...,  5.4217e-01,\n",
      "           2.5191e-01, -1.4096e-01],\n",
      "         [-3.4845e-01,  2.5009e-01, -5.6335e-02,  ...,  6.4721e-01,\n",
      "           5.7237e-01, -1.5043e-01],\n",
      "         [-6.7976e-01,  4.2300e-01,  1.9201e-02,  ...,  4.8219e-01,\n",
      "           3.8311e-01, -3.4114e-01],\n",
      "         ...,\n",
      "         [-5.1850e-01,  9.5432e-02, -5.2986e-01,  ...,  8.5896e-01,\n",
      "          -4.8499e-02,  8.1126e-02],\n",
      "         [-2.5263e-01,  1.8879e-01, -2.7144e-01,  ...,  8.1961e-01,\n",
      "           3.6491e-01, -7.2449e-02],\n",
      "         [-5.9679e-01,  2.0605e-01, -4.1562e-01,  ...,  6.3732e-01,\n",
      "           2.5273e-01, -2.1537e-01]],\n",
      "\n",
      "        [[-6.2006e-02, -4.8648e-02, -5.1095e-01,  ..., -3.1721e-01,\n",
      "          -2.9838e-01,  5.8359e-01],\n",
      "         [-1.5307e-01, -2.1083e-01,  2.3783e-04,  ..., -8.0034e-02,\n",
      "          -2.5121e-01,  6.6600e-01],\n",
      "         [-1.5115e-03, -5.1040e-02, -2.5628e-01,  ..., -2.7292e-01,\n",
      "          -2.1917e-02,  4.7428e-01],\n",
      "         ...,\n",
      "         [-2.2151e-01, -4.3729e-02, -3.0056e-01,  ..., -2.5618e-01,\n",
      "          -3.7955e-01,  6.6205e-01],\n",
      "         [-1.4563e-01, -2.6913e-01, -2.4438e-01,  ..., -2.9413e-01,\n",
      "          -4.2923e-01,  9.9494e-01],\n",
      "         [ 6.7478e-02, -1.0412e-01, -4.2456e-01,  ..., -7.2612e-02,\n",
      "          -5.4636e-01,  7.7560e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.8840e-01,  5.4305e-02, -5.5330e-01,  ..., -5.2793e-01,\n",
      "          -1.0539e-01,  4.1771e-01],\n",
      "         [ 5.3695e-02, -1.9353e-01, -1.1208e-01,  ...,  1.2341e-02,\n",
      "          -1.6334e-01,  3.6131e-01],\n",
      "         [-6.0324e-03,  3.6367e-02, -2.3693e-01,  ..., -5.3420e-01,\n",
      "          -1.5496e-01,  3.4467e-01],\n",
      "         ...,\n",
      "         [-1.9521e-01, -2.2877e-02, -4.3420e-01,  ..., -2.5483e-02,\n",
      "          -5.0846e-01,  6.7733e-01],\n",
      "         [ 2.2021e-02, -8.8716e-02, -2.5484e-02,  ..., -2.1496e-01,\n",
      "          -2.9695e-01,  6.2203e-01],\n",
      "         [-1.0982e-01, -1.1119e-01, -2.9770e-01,  ..., -2.0505e-02,\n",
      "          -7.8514e-01,  4.2661e-01]],\n",
      "\n",
      "        [[-4.3801e-01,  3.0463e-01, -3.4171e-01,  ...,  5.7642e-01,\n",
      "           2.0640e-01, -1.6277e-01],\n",
      "         [-5.6708e-01,  1.0895e-01, -6.9330e-02,  ...,  5.9455e-01,\n",
      "           2.0068e-01, -2.7473e-01],\n",
      "         [-5.2375e-01,  5.2393e-01, -4.8473e-02,  ...,  3.1878e-01,\n",
      "           4.6798e-01, -1.8714e-01],\n",
      "         ...,\n",
      "         [-4.3055e-01,  9.8993e-02, -3.1890e-01,  ...,  7.4901e-01,\n",
      "           1.2037e-01,  1.8681e-02],\n",
      "         [-3.8259e-01,  1.9445e-01, -1.8681e-01,  ...,  8.3489e-01,\n",
      "           1.4233e-01, -1.1490e-02],\n",
      "         [-7.2949e-01,  1.3255e-02, -3.0971e-01,  ...,  8.1733e-01,\n",
      "          -1.0671e-01, -5.6851e-02]],\n",
      "\n",
      "        [[-9.3162e-02, -1.5131e-02, -6.1995e-01,  ..., -3.0662e-01,\n",
      "          -2.6585e-01,  2.6442e-01],\n",
      "         [-6.9929e-02, -1.9087e-01, -1.8201e-01,  ..., -1.3231e-01,\n",
      "          -9.4758e-02,  3.2546e-01],\n",
      "         [-2.6294e-01,  1.0031e-01, -3.4380e-01,  ..., -4.1299e-01,\n",
      "          -5.9073e-02,  1.3550e-01],\n",
      "         ...,\n",
      "         [-1.4675e-01, -2.6213e-02, -5.4543e-01,  ..., -5.4459e-02,\n",
      "          -3.8672e-01,  6.5174e-01],\n",
      "         [ 2.5040e-02, -2.2087e-01, -3.5884e-01,  ..., -9.9098e-02,\n",
      "          -2.1293e-01,  5.6729e-01],\n",
      "         [-1.2036e-01, -2.9402e-01, -3.4964e-01,  ..., -3.8485e-01,\n",
      "          -3.3295e-01,  7.6711e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8308743834495544\n",
      "Model outputs:  tensor([[[-3.0362e-01,  3.5239e-01, -4.4247e-01,  ...,  1.0780e-01,\n",
      "           3.8940e-01,  1.5496e-01],\n",
      "         [-4.2854e-01,  5.0925e-01, -2.8178e-01,  ..., -1.3965e-01,\n",
      "           4.3655e-01,  1.4957e-01],\n",
      "         [-3.7154e-01,  4.7285e-01, -2.8188e-01,  ..., -1.5465e-01,\n",
      "           2.2047e-01,  1.0478e-01],\n",
      "         ...,\n",
      "         [-3.8715e-01,  2.4447e-01, -2.4167e-01,  ...,  2.2492e-01,\n",
      "           1.2896e-01,  2.0866e-01],\n",
      "         [-2.9864e-01,  1.4967e-01, -4.6389e-01,  ...,  2.2789e-01,\n",
      "           5.4775e-01,  1.8136e-01],\n",
      "         [-2.4011e-01,  1.8243e-01, -6.3043e-01,  ..., -8.5554e-04,\n",
      "           2.7718e-01,  2.9828e-01]],\n",
      "\n",
      "        [[ 2.0714e-01, -3.5961e-02, -6.0253e-01,  ..., -3.6225e-01,\n",
      "          -8.7705e-02,  4.1006e-01],\n",
      "         [ 1.4668e-01,  9.7372e-02, -3.8468e-01,  ..., -3.1909e-01,\n",
      "          -1.9226e-02,  1.8290e-02],\n",
      "         [ 2.4888e-01, -1.1376e-01, -4.1856e-01,  ..., -3.8630e-01,\n",
      "          -8.0826e-02,  1.0879e-01],\n",
      "         ...,\n",
      "         [ 5.7891e-03,  2.9074e-01, -3.5210e-01,  ..., -2.3079e-01,\n",
      "           4.7418e-02,  2.4897e-01],\n",
      "         [ 2.7399e-01,  2.4031e-01, -4.4962e-01,  ..., -2.7844e-01,\n",
      "           1.6289e-01,  2.7833e-01],\n",
      "         [ 1.9973e-01, -1.0324e-02, -4.8077e-01,  ..., -3.5422e-01,\n",
      "           9.3586e-02,  3.6661e-01]],\n",
      "\n",
      "        [[ 7.6793e-02,  1.2693e-01, -9.2744e-01,  ...,  7.1780e-01,\n",
      "           1.5689e-01, -8.2703e-02],\n",
      "         [-1.1683e-01,  5.3399e-01, -7.2040e-01,  ...,  5.7437e-01,\n",
      "           2.3671e-01, -2.5357e-01],\n",
      "         [ 1.4052e-01,  3.5427e-01, -5.4235e-01,  ...,  4.5067e-01,\n",
      "           1.4551e-02, -5.6474e-02],\n",
      "         ...,\n",
      "         [-2.4886e-03,  2.7308e-01, -4.0018e-01,  ...,  7.3259e-01,\n",
      "           1.0738e-01,  7.4931e-02],\n",
      "         [-1.2147e-01,  1.9806e-01, -7.1536e-01,  ...,  5.1911e-01,\n",
      "           3.6294e-01, -1.5090e-02],\n",
      "         [ 1.0874e-01,  3.0311e-01, -7.1925e-01,  ...,  1.9437e-01,\n",
      "          -6.6564e-02, -3.8023e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.7277e-01,  1.6830e-01, -9.7320e-01,  ...,  2.7835e-01,\n",
      "           1.8388e-01, -2.1402e-01],\n",
      "         [-1.1982e-01,  3.1378e-01, -6.6635e-01,  ...,  2.0750e-02,\n",
      "           1.0423e-01, -3.4383e-01],\n",
      "         [-3.2991e-01,  4.7189e-01, -8.3195e-01,  ...,  2.3173e-01,\n",
      "          -1.1161e-01, -1.4308e-01],\n",
      "         ...,\n",
      "         [-1.2054e-01,  1.8742e-01, -7.0387e-01,  ...,  6.8500e-01,\n",
      "          -3.0307e-01,  2.3607e-01],\n",
      "         [-1.4751e-01,  2.6608e-01, -6.5331e-01,  ...,  3.2211e-01,\n",
      "          -1.7419e-01, -1.3244e-01],\n",
      "         [-6.5738e-02,  3.0241e-01, -6.8046e-01,  ...,  1.9238e-01,\n",
      "          -2.7615e-02, -5.2637e-01]],\n",
      "\n",
      "        [[-4.3628e-01,  2.4709e-01, -5.0244e-01,  ...,  5.8124e-02,\n",
      "           2.8885e-02,  1.4634e-01],\n",
      "         [-4.1910e-01,  4.2275e-01, -2.7561e-01,  ...,  1.5450e-01,\n",
      "           1.8487e-01,  1.0213e-01],\n",
      "         [-4.4100e-01,  6.0449e-01, -2.2510e-01,  ...,  6.9910e-02,\n",
      "           3.2694e-01,  3.2268e-02],\n",
      "         ...,\n",
      "         [-5.0328e-01,  4.2924e-01, -2.9356e-01,  ...,  3.7872e-01,\n",
      "           6.0746e-02,  1.3382e-01],\n",
      "         [-4.4193e-01,  3.3967e-01, -6.7651e-01,  ..., -1.2927e-02,\n",
      "           3.6562e-01,  2.6932e-01],\n",
      "         [-2.1355e-01,  3.6978e-01, -4.1789e-01,  ...,  2.1066e-01,\n",
      "           3.5179e-01,  1.5595e-01]],\n",
      "\n",
      "        [[-5.1814e-01,  5.0320e-01, -4.3555e-01,  ...,  4.3135e-01,\n",
      "           2.8943e-01, -4.1162e-02],\n",
      "         [-3.1358e-02,  5.4407e-01, -2.4258e-01,  ...,  4.9794e-01,\n",
      "           2.6874e-01, -2.1462e-01],\n",
      "         [-3.4678e-01,  5.1424e-01,  2.0865e-02,  ...,  4.8423e-01,\n",
      "           2.9760e-01, -3.5765e-01],\n",
      "         ...,\n",
      "         [-4.1812e-01,  2.4533e-01,  1.6542e-01,  ...,  6.0900e-01,\n",
      "           4.3555e-01, -7.7030e-02],\n",
      "         [-3.6629e-01,  6.1661e-01, -3.9839e-01,  ...,  6.1947e-01,\n",
      "           2.9653e-01, -2.7230e-02],\n",
      "         [-1.6484e-01,  4.4063e-01,  2.2150e-02,  ...,  4.1175e-01,\n",
      "           1.6294e-01, -3.7361e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0728919506072998\n",
      "Model outputs:  tensor([[[ 2.0176e-02, -5.9892e-02, -3.5037e-01,  ..., -6.2622e-01,\n",
      "           4.0117e-03,  1.4238e-01],\n",
      "         [ 2.1623e-01, -3.3993e-02, -2.4398e-01,  ..., -4.7309e-01,\n",
      "           2.9834e-03,  1.6224e-02],\n",
      "         [ 9.8166e-02,  8.3954e-02, -1.2998e-01,  ..., -5.9403e-01,\n",
      "          -2.7555e-01,  1.1434e-01],\n",
      "         ...,\n",
      "         [ 3.4026e-01,  3.8246e-02, -3.7676e-01,  ..., -5.0313e-01,\n",
      "          -8.6732e-02,  2.9163e-01],\n",
      "         [ 1.4284e-01,  3.3454e-01, -4.4660e-01,  ..., -4.3072e-01,\n",
      "          -1.0262e-01,  3.3991e-01],\n",
      "         [ 2.1656e-01,  7.7161e-02, -2.3604e-01,  ..., -5.7337e-01,\n",
      "           8.0485e-02,  1.6689e-01]],\n",
      "\n",
      "        [[ 1.0608e-01,  4.1798e-01, -8.2283e-01,  ..., -2.1999e-01,\n",
      "           1.8263e-01,  2.4245e-01],\n",
      "         [-1.2204e-01,  1.6524e-01, -4.0228e-01,  ..., -2.7736e-01,\n",
      "           3.3416e-01, -5.3258e-02],\n",
      "         [-1.4194e-01,  5.2542e-01, -5.7565e-01,  ..., -1.1323e-01,\n",
      "           3.6777e-01, -4.6383e-02],\n",
      "         ...,\n",
      "         [-2.4181e-01,  4.4972e-01, -7.5771e-01,  ..., -1.9870e-01,\n",
      "           2.1410e-01, -2.4195e-01],\n",
      "         [-2.4230e-01,  6.9459e-01, -5.8156e-01,  ...,  1.0921e-01,\n",
      "           3.2108e-01, -2.2029e-01],\n",
      "         [ 6.2225e-02,  3.8751e-01, -5.8302e-01,  ..., -2.0289e-02,\n",
      "           5.1236e-02, -1.3308e-01]],\n",
      "\n",
      "        [[-4.7043e-02,  1.6956e-01, -3.8027e-01,  ..., -6.6766e-02,\n",
      "           1.3484e-01, -4.7374e-01],\n",
      "         [-1.8662e-02,  7.1561e-02, -2.7250e-01,  ..., -1.9132e-01,\n",
      "           1.7269e-01, -5.7158e-01],\n",
      "         [ 2.8022e-01,  1.7936e-02, -5.7853e-01,  ..., -4.8589e-01,\n",
      "           3.8855e-01, -3.7995e-01],\n",
      "         ...,\n",
      "         [ 1.5598e-01,  5.1621e-01, -3.4069e-01,  ..., -2.4457e-01,\n",
      "          -8.8249e-02, -4.2993e-01],\n",
      "         [-1.7066e-01,  3.3221e-01, -4.1557e-01,  ..., -2.3117e-01,\n",
      "           3.4831e-01, -3.2094e-01],\n",
      "         [-8.7788e-02,  3.1466e-02, -4.5545e-01,  ..., -3.3343e-01,\n",
      "          -3.6686e-02, -3.5183e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2795e-01,  2.0000e-01,  2.2012e-02,  ...,  3.1805e-01,\n",
      "           1.0055e-01, -6.1788e-01],\n",
      "         [ 1.1418e-01,  1.4291e-02,  3.0312e-01,  ..., -5.3398e-02,\n",
      "          -2.1110e-02, -8.2287e-01],\n",
      "         [ 2.4722e-01,  8.6532e-02,  1.1786e-01,  ...,  9.2292e-02,\n",
      "           3.8811e-02, -6.4809e-01],\n",
      "         ...,\n",
      "         [ 2.3786e-02,  2.1050e-01,  2.6145e-01,  ...,  3.3731e-01,\n",
      "          -1.4537e-01, -8.5623e-01],\n",
      "         [ 3.5352e-02,  1.7056e-01,  9.7320e-02,  ..., -7.5122e-03,\n",
      "           1.5076e-01, -5.8811e-01],\n",
      "         [ 1.0389e-01,  2.6691e-01,  1.0790e-01,  ...,  1.7467e-01,\n",
      "          -2.7058e-04, -8.5585e-01]],\n",
      "\n",
      "        [[ 9.5887e-02,  3.4221e-01, -2.9434e-01,  ..., -1.9602e-01,\n",
      "           9.6630e-02,  2.5122e-01],\n",
      "         [ 1.3578e-02, -8.6415e-02, -5.3366e-02,  ..., -7.3847e-01,\n",
      "           2.4839e-01,  7.7802e-02],\n",
      "         [ 2.0521e-01,  3.7403e-01, -2.1164e-01,  ..., -4.6709e-01,\n",
      "           1.1608e-01, -1.5736e-01],\n",
      "         ...,\n",
      "         [ 2.7015e-01, -1.6207e-01, -2.7607e-01,  ..., -5.1848e-01,\n",
      "           3.9207e-02,  1.9054e-01],\n",
      "         [ 4.8502e-02,  2.9937e-01, -2.7302e-01,  ..., -5.0484e-01,\n",
      "          -9.7008e-02,  3.2405e-02],\n",
      "         [-2.8373e-02,  1.6363e-01, -3.3139e-01,  ..., -2.8463e-01,\n",
      "          -9.5559e-02,  1.9682e-01]],\n",
      "\n",
      "        [[-3.7843e-01,  1.9886e-01,  1.6263e-01,  ...,  4.2169e-01,\n",
      "           4.1115e-01, -2.4015e-01],\n",
      "         [-1.4215e-01,  1.1762e-02, -2.2493e-01,  ...,  7.2727e-02,\n",
      "           3.9888e-01, -4.2428e-01],\n",
      "         [-3.3475e-01,  6.5227e-01, -3.4675e-01,  ...,  3.1425e-01,\n",
      "           2.3356e-01, -3.0159e-01],\n",
      "         ...,\n",
      "         [-5.3139e-01,  4.8777e-01, -2.4883e-01,  ...,  2.9225e-01,\n",
      "           3.8586e-01, -4.3064e-01],\n",
      "         [-4.5996e-01,  4.9831e-01, -4.5381e-01,  ...,  4.8218e-01,\n",
      "           2.7029e-01, -4.5840e-01],\n",
      "         [-1.3656e-01,  4.0218e-01, -1.2782e-01,  ...,  2.9575e-01,\n",
      "           4.7796e-01, -4.7029e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.4529999494552612\n",
      "Model outputs:  tensor([[[ 0.2627, -0.0415,  0.1323,  ...,  0.3933,  0.0282, -0.3492],\n",
      "         [ 0.2332,  0.0863, -0.0797,  ...,  0.7525,  0.0074, -0.4178],\n",
      "         [ 0.1547, -0.0366, -0.0285,  ...,  0.4859,  0.3820, -0.3167],\n",
      "         ...,\n",
      "         [ 0.1352, -0.3326,  0.1061,  ...,  0.3606, -0.3123, -0.3741],\n",
      "         [ 0.1377,  0.0277, -0.2634,  ...,  0.5504, -0.0946, -0.1928],\n",
      "         [ 0.5078, -0.0038, -0.0818,  ...,  0.3612,  0.1943, -0.4382]],\n",
      "\n",
      "        [[ 0.0359,  0.2850, -0.7228,  ...,  0.0674,  0.2677,  0.0370],\n",
      "         [-0.2873,  0.2339, -0.8862,  ...,  0.5557, -0.1803,  0.0487],\n",
      "         [-0.0163,  0.1136, -0.9516,  ...,  0.4072,  0.2995,  0.3383],\n",
      "         ...,\n",
      "         [-0.1530,  0.2561, -0.8365,  ...,  0.4315,  0.2517,  0.0889],\n",
      "         [-0.1914,  0.1199, -0.9153,  ...,  0.1798,  0.1405,  0.1506],\n",
      "         [-0.2739,  0.2608, -0.7585,  ...,  0.0339,  0.4291, -0.0729]],\n",
      "\n",
      "        [[-0.2550, -0.0730, -0.3783,  ..., -0.2969,  0.5020,  0.6290],\n",
      "         [ 0.2525, -0.0929, -0.4066,  ..., -0.0809,  0.5168,  0.5601],\n",
      "         [ 0.1606,  0.0851, -0.5074,  ..., -0.3975,  0.6032,  0.7803],\n",
      "         ...,\n",
      "         [ 0.0753, -0.0650, -0.6121,  ..., -0.1002,  0.7692,  0.4124],\n",
      "         [-0.0110,  0.2556, -0.9116,  ..., -0.4681,  0.4145,  0.5031],\n",
      "         [ 0.0368, -0.2233, -0.3942,  ..., -0.3288,  0.5368,  0.4477]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3123,  0.4531, -0.9172,  ...,  0.7649,  0.0266,  0.1490],\n",
      "         [ 0.0278,  0.3893, -0.9011,  ...,  0.9997,  0.3611, -0.0515],\n",
      "         [ 0.0074,  0.1674, -0.8162,  ...,  0.9017,  0.3646,  0.2113],\n",
      "         ...,\n",
      "         [-0.1146,  0.1925, -0.8891,  ...,  0.8910,  0.1432,  0.1312],\n",
      "         [ 0.0027,  0.2672, -1.0129,  ...,  0.7886,  0.4055,  0.0371],\n",
      "         [-0.1938,  0.2886, -0.9435,  ...,  0.7911,  0.1933,  0.0694]],\n",
      "\n",
      "        [[-0.4927,  0.3612, -0.3847,  ...,  0.7199,  0.2613, -0.1800],\n",
      "         [-0.2094,  0.1472, -0.3884,  ...,  0.7723,  0.4339,  0.0634],\n",
      "         [-0.3534,  0.3204, -0.5012,  ...,  0.7348,  0.3529,  0.2219],\n",
      "         ...,\n",
      "         [-0.6730,  0.3464, -0.4859,  ...,  0.8303,  0.2715,  0.1469],\n",
      "         [-0.5017,  0.3645, -0.4314,  ...,  0.6632,  0.2316,  0.1843],\n",
      "         [-0.3647,  0.2482, -0.1867,  ...,  0.7914,  0.2691, -0.0240]],\n",
      "\n",
      "        [[ 0.0409,  0.2174, -0.5157,  ..., -0.1216, -0.0577,  0.6524],\n",
      "         [-0.1194, -0.0494, -0.4592,  ...,  0.1914, -0.1585,  0.3173],\n",
      "         [ 0.1090,  0.1874, -0.3633,  ...,  0.0614,  0.1463,  0.6635],\n",
      "         ...,\n",
      "         [-0.1234, -0.0853, -0.5161,  ..., -0.2628, -0.0030,  0.3963],\n",
      "         [ 0.0621,  0.2839, -0.5461,  ..., -0.0731, -0.0915,  0.5178],\n",
      "         [ 0.2644,  0.1283, -0.3276,  ..., -0.2916, -0.2542,  0.5315]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.308645486831665\n",
      "Model outputs:  tensor([[[-4.4424e-01,  2.1190e-02, -1.3352e-01,  ...,  6.8920e-03,\n",
      "           3.5704e-02,  2.9143e-02],\n",
      "         [-3.2544e-01,  2.2607e-01, -1.5590e-01,  ...,  3.6624e-01,\n",
      "           1.1198e-01, -8.0593e-03],\n",
      "         [-3.5372e-01,  1.9496e-01, -4.8188e-01,  ...,  3.2328e-01,\n",
      "           1.1483e-01,  2.8992e-01],\n",
      "         ...,\n",
      "         [-2.0880e-01,  1.2599e-01, -4.8106e-01,  ...,  2.6893e-01,\n",
      "           2.5286e-01, -8.0925e-02],\n",
      "         [-1.4226e-02,  2.6100e-01, -2.3262e-01,  ...,  4.0308e-05,\n",
      "           1.4136e-01, -3.1776e-01],\n",
      "         [-4.6659e-01,  2.6495e-01, -1.1695e-01,  ..., -7.3856e-02,\n",
      "           1.6927e-01,  6.7884e-02]],\n",
      "\n",
      "        [[ 1.1987e-01, -1.6950e-01, -3.5742e-01,  ..., -5.9114e-01,\n",
      "          -4.1135e-01,  5.2959e-01],\n",
      "         [ 3.1074e-01, -2.6304e-01, -4.6773e-01,  ..., -2.1973e-01,\n",
      "          -4.4774e-01,  5.8105e-01],\n",
      "         [ 2.7756e-01, -6.4534e-02, -4.3197e-01,  ..., -1.5893e-01,\n",
      "          -4.4468e-01,  7.6445e-01],\n",
      "         ...,\n",
      "         [ 2.6368e-01, -1.4108e-01, -5.2030e-01,  ..., -5.0345e-01,\n",
      "          -2.5820e-01,  6.0681e-01],\n",
      "         [ 4.8996e-01, -2.1084e-01,  1.7920e-02,  ..., -8.6156e-01,\n",
      "          -3.6093e-01,  2.0116e-01],\n",
      "         [ 3.4454e-01, -8.2526e-02, -3.0344e-01,  ..., -3.2330e-01,\n",
      "          -4.4044e-02,  6.4911e-01]],\n",
      "\n",
      "        [[ 2.5006e-01, -5.2026e-01, -2.6783e-01,  ..., -2.6049e-01,\n",
      "           6.5999e-02, -2.6163e-01],\n",
      "         [ 1.6626e-01, -3.5792e-01, -3.8765e-01,  ..., -2.5601e-01,\n",
      "          -1.1294e-01,  2.9341e-02],\n",
      "         [ 2.4874e-01, -2.5386e-01, -3.9454e-01,  ..., -3.6955e-01,\n",
      "          -8.2615e-02,  4.7558e-01],\n",
      "         ...,\n",
      "         [ 5.9310e-02,  9.9155e-02, -3.0179e-01,  ..., -2.6479e-01,\n",
      "           2.1900e-01,  2.3559e-02],\n",
      "         [ 2.2413e-01, -1.6798e-01, -7.5588e-02,  ..., -4.4789e-01,\n",
      "           9.9733e-02, -6.8298e-01],\n",
      "         [ 3.4857e-01, -2.0858e-01, -4.0401e-01,  ..., -2.4860e-01,\n",
      "          -1.0601e-01, -5.0796e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.5431e-01,  2.9932e-01, -5.4944e-01,  ..., -2.0258e-01,\n",
      "          -6.8942e-02,  7.5062e-02],\n",
      "         [ 1.6624e-01, -2.6279e-02, -8.1358e-01,  ...,  2.0324e-01,\n",
      "           2.2374e-02,  2.6786e-01],\n",
      "         [ 2.8278e-02, -4.3621e-02, -6.5857e-01,  ..., -2.2652e-01,\n",
      "          -6.9561e-02,  3.6314e-01],\n",
      "         ...,\n",
      "         [ 1.2741e-01,  1.7337e-01, -6.8001e-01,  ...,  1.2101e-01,\n",
      "           1.7708e-01,  1.0894e-01],\n",
      "         [ 4.8608e-01,  3.0755e-01, -3.1283e-01,  ..., -2.1512e-02,\n",
      "          -3.3271e-02, -2.3147e-01],\n",
      "         [-1.6026e-01, -1.4779e-01, -4.6688e-01,  ..., -1.1714e-01,\n",
      "           7.3908e-02,  1.0689e-02]],\n",
      "\n",
      "        [[-3.9608e-01, -3.3484e-02, -3.9698e-01,  ..., -2.0209e-01,\n",
      "          -1.3188e-01,  2.4177e-02],\n",
      "         [-2.8493e-01,  1.0564e-01, -3.3352e-01,  ...,  1.1431e-03,\n",
      "          -2.8640e-01,  3.4828e-01],\n",
      "         [-3.3357e-01, -1.4665e-01, -5.3141e-01,  ..., -2.3774e-01,\n",
      "          -1.4652e-01,  5.5361e-01],\n",
      "         ...,\n",
      "         [-3.2161e-01, -6.3342e-02, -5.0780e-01,  ..., -3.0737e-01,\n",
      "           1.3682e-02,  2.5358e-01],\n",
      "         [-6.2626e-02, -2.6247e-01,  8.8292e-02,  ..., -4.7040e-01,\n",
      "          -1.6960e-01, -1.8456e-01],\n",
      "         [-1.8958e-01, -1.5173e-01, -5.0714e-01,  ..., -1.7836e-01,\n",
      "           6.8725e-02,  6.1522e-02]],\n",
      "\n",
      "        [[-1.6440e-01,  9.9821e-02, -3.8336e-01,  ...,  8.8571e-02,\n",
      "           1.3539e-01,  1.1840e-01],\n",
      "         [-4.1750e-01,  5.5196e-02, -6.6298e-01,  ...,  2.9183e-01,\n",
      "          -7.1560e-02,  7.0281e-02],\n",
      "         [-1.7731e-01, -1.3165e-01, -6.7627e-01,  ...,  2.6953e-02,\n",
      "          -2.1849e-01,  3.4138e-01],\n",
      "         ...,\n",
      "         [-1.7649e-01,  5.4967e-02, -6.4721e-01,  ..., -4.1780e-02,\n",
      "          -2.3849e-01,  2.3040e-02],\n",
      "         [ 6.2297e-02, -1.1316e-01, -3.1644e-01,  ..., -3.4458e-01,\n",
      "           9.9417e-02, -3.9781e-01],\n",
      "         [-2.8370e-01, -1.3480e-01, -6.6273e-01,  ..., -1.8766e-01,\n",
      "           7.5417e-03,  3.3441e-02]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1715493202209473\n",
      "Model outputs:  tensor([[[-4.2685e-02,  5.5442e-01, -6.9236e-01,  ...,  8.7918e-01,\n",
      "           6.3216e-01,  4.2858e-02],\n",
      "         [-1.3616e-01,  4.4529e-01, -5.2185e-01,  ...,  9.3890e-01,\n",
      "           3.0715e-01, -1.6010e-01],\n",
      "         [-2.9972e-01,  2.9328e-01, -6.0770e-01,  ...,  7.3677e-01,\n",
      "           2.7470e-01, -8.2817e-02],\n",
      "         ...,\n",
      "         [-1.5126e-01,  9.2846e-02, -8.4323e-01,  ...,  8.5446e-01,\n",
      "           1.2111e-01,  4.4279e-03],\n",
      "         [-5.0292e-01,  6.8598e-01, -6.6261e-01,  ...,  2.9816e-01,\n",
      "           4.4368e-02, -2.9720e-01],\n",
      "         [-2.4193e-01,  3.0949e-01, -9.2111e-01,  ...,  8.8403e-01,\n",
      "           2.9238e-01, -1.4995e-01]],\n",
      "\n",
      "        [[ 2.2095e-01,  3.2033e-01, -6.6434e-01,  ...,  1.4088e-01,\n",
      "           3.7921e-01,  3.0399e-02],\n",
      "         [ 1.4082e-02,  1.5657e-01, -6.8383e-01,  ...,  3.8618e-01,\n",
      "          -1.1831e-01,  1.7271e-01],\n",
      "         [ 1.7148e-01,  3.8680e-01, -8.6937e-01,  ...,  1.6651e-01,\n",
      "          -2.9778e-02,  3.0208e-01],\n",
      "         ...,\n",
      "         [ 6.7106e-02,  2.0081e-01, -6.3817e-01,  ...,  3.2729e-01,\n",
      "          -1.1950e-01,  1.5956e-02],\n",
      "         [-1.0797e-01,  5.0003e-01, -4.9932e-01,  ...,  1.5427e-01,\n",
      "           2.6427e-01, -1.3983e-01],\n",
      "         [-1.1196e-01,  1.5278e-01, -8.0117e-01,  ...,  4.3068e-01,\n",
      "           2.6248e-01,  3.1018e-01]],\n",
      "\n",
      "        [[-2.9331e-01,  6.6069e-01, -3.7365e-01,  ...,  7.3296e-01,\n",
      "           2.0416e-01, -3.2421e-01],\n",
      "         [-1.3376e-01,  5.1842e-01, -3.1857e-01,  ...,  5.8197e-01,\n",
      "           2.8401e-01,  6.5261e-02],\n",
      "         [-4.5377e-01,  4.6998e-01, -7.6006e-02,  ...,  5.2001e-01,\n",
      "           2.4775e-01, -4.3999e-02],\n",
      "         ...,\n",
      "         [-1.5508e-01,  1.4293e-01, -1.7056e-01,  ...,  5.0573e-01,\n",
      "           8.1037e-02, -6.1980e-02],\n",
      "         [-4.9226e-01,  8.9160e-01,  1.0070e-01,  ...,  3.2669e-01,\n",
      "           6.4638e-02, -2.2901e-01],\n",
      "         [-3.3918e-01,  4.5838e-01, -1.4171e-01,  ...,  7.4860e-01,\n",
      "           2.0316e-01,  6.2942e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.5120e-01,  6.2446e-01, -6.8547e-01,  ...,  6.2794e-01,\n",
      "           3.4026e-01, -9.4547e-02],\n",
      "         [-1.6979e-01,  5.3590e-01, -5.6215e-01,  ...,  7.9394e-01,\n",
      "           5.2601e-01,  2.7894e-01],\n",
      "         [-7.1661e-01,  3.5905e-01, -7.8047e-01,  ...,  7.9437e-01,\n",
      "           1.6799e-01,  2.5307e-01],\n",
      "         ...,\n",
      "         [-3.5600e-01,  3.0088e-01, -7.5782e-01,  ...,  7.5484e-01,\n",
      "           2.2164e-01,  2.1728e-01],\n",
      "         [-5.3825e-01,  6.3297e-01, -2.9787e-01,  ...,  3.4502e-01,\n",
      "           1.5922e-01,  1.5146e-02],\n",
      "         [-3.8393e-01,  5.0806e-01, -7.7901e-01,  ...,  9.1077e-01,\n",
      "           6.8009e-04,  1.9373e-01]],\n",
      "\n",
      "        [[-5.4416e-01,  5.0540e-01, -5.1867e-01,  ...,  8.5827e-01,\n",
      "           2.2474e-01,  5.8622e-02],\n",
      "         [-3.5895e-01,  5.7830e-01, -7.3661e-01,  ...,  8.9648e-01,\n",
      "           3.9935e-01,  7.1130e-02],\n",
      "         [-5.8680e-01,  5.8129e-01, -5.7471e-01,  ...,  7.8571e-01,\n",
      "           3.3743e-01,  3.4036e-02],\n",
      "         ...,\n",
      "         [-4.8051e-01,  2.2738e-01, -6.3251e-01,  ...,  7.0468e-01,\n",
      "           2.3802e-01,  2.4659e-01],\n",
      "         [-5.3333e-01,  9.0039e-01, -4.8748e-01,  ...,  7.0638e-01,\n",
      "           9.4752e-02, -8.2854e-02],\n",
      "         [-3.4842e-01,  5.2118e-01, -5.9181e-01,  ...,  9.4883e-01,\n",
      "           2.1592e-01,  1.5834e-01]],\n",
      "\n",
      "        [[-5.2945e-01,  4.0392e-01, -2.7713e-02,  ...,  6.5465e-01,\n",
      "           1.7411e-01,  1.1311e-01],\n",
      "         [-1.1949e-01,  7.1982e-01, -1.2425e-01,  ...,  6.7389e-01,\n",
      "           4.4295e-01,  1.5982e-01],\n",
      "         [-4.0826e-01,  5.6016e-01, -2.3344e-03,  ...,  6.1485e-01,\n",
      "           3.3581e-01, -2.2481e-01],\n",
      "         ...,\n",
      "         [-2.9519e-02,  2.5329e-01, -2.1686e-01,  ...,  7.1036e-01,\n",
      "           1.4485e-01, -1.4467e-02],\n",
      "         [-1.9684e-01,  5.7923e-01, -2.3377e-01,  ...,  5.4917e-01,\n",
      "           2.0008e-01, -4.0363e-01],\n",
      "         [-4.3482e-01,  6.3841e-01, -1.5690e-01,  ...,  5.9279e-01,\n",
      "           1.3425e-01, -1.2246e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.014971375465393\n",
      "Model outputs:  tensor([[[-0.0774,  0.1605, -0.4766,  ..., -0.3910,  0.7520,  0.5305],\n",
      "         [-0.1105,  0.1902, -0.5332,  ..., -0.6736,  0.5653,  0.7136],\n",
      "         [ 0.2949, -0.2349, -0.4051,  ..., -0.0775,  0.5528,  0.6268],\n",
      "         ...,\n",
      "         [-0.0425,  0.3013, -0.4859,  ..., -0.4941,  0.6691,  0.5974],\n",
      "         [-0.0864,  0.1941, -0.4660,  ..., -0.4369,  0.5581,  0.9901],\n",
      "         [-0.1624, -0.1414, -0.3373,  ..., -0.3617,  0.4296,  0.4610]],\n",
      "\n",
      "        [[-0.2650,  0.3907, -0.2145,  ...,  0.5228,  0.1302,  0.0250],\n",
      "         [-0.4707,  0.5272, -0.1810,  ...,  0.5024,  0.2862, -0.1140],\n",
      "         [-0.0249, -0.1605, -0.4151,  ...,  0.5946,  0.2448, -0.0727],\n",
      "         ...,\n",
      "         [-0.6569,  0.4319,  0.0364,  ...,  0.4510,  0.3155,  0.0459],\n",
      "         [-0.2914,  0.5187, -0.1902,  ...,  0.4398,  0.1854,  0.0074],\n",
      "         [-0.4060,  0.0013,  0.0243,  ...,  0.5690,  0.2103,  0.1348]],\n",
      "\n",
      "        [[-0.1767,  0.3306, -0.9328,  ...,  0.8795,  0.0451,  0.1948],\n",
      "         [-0.0158,  0.1077, -0.7696,  ...,  0.4973,  0.1921, -0.0150],\n",
      "         [ 0.1602, -0.2229, -0.7393,  ...,  0.8195,  0.0245, -0.1152],\n",
      "         ...,\n",
      "         [-0.3652,  0.1795, -0.6889,  ...,  0.3773, -0.0143, -0.0221],\n",
      "         [-0.1107,  0.2703, -0.9615,  ...,  0.3657,  0.0857, -0.1194],\n",
      "         [-0.1500, -0.1207, -0.6835,  ...,  0.5224, -0.0926, -0.0836]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5770,  0.3703, -0.8967,  ...,  0.5545, -0.0592,  0.0941],\n",
      "         [-0.3093,  0.2640, -0.6600,  ...,  0.4663,  0.1690, -0.1200],\n",
      "         [ 0.0286, -0.0739, -0.5715,  ...,  0.6026,  0.1981,  0.0351],\n",
      "         ...,\n",
      "         [-0.3723,  0.3734, -0.7548,  ...,  0.1558,  0.1911,  0.0697],\n",
      "         [-0.1749,  0.3318, -0.8820,  ...,  0.3485, -0.2338,  0.2028],\n",
      "         [-0.4460,  0.0647, -0.9090,  ...,  0.3114, -0.0063,  0.2445]],\n",
      "\n",
      "        [[-0.2355, -0.1112, -0.4136,  ...,  0.0471, -0.1602,  0.2512],\n",
      "         [ 0.0629, -0.0205, -0.6065,  ..., -0.3124, -0.1372,  0.2971],\n",
      "         [ 0.3785, -0.1113, -0.2772,  ...,  0.2977,  0.0574,  0.2005],\n",
      "         ...,\n",
      "         [ 0.0757, -0.0279, -0.5404,  ..., -0.1647,  0.0276,  0.4541],\n",
      "         [-0.0541,  0.0108, -0.6488,  ..., -0.1021,  0.1193,  0.4662],\n",
      "         [-0.5088, -0.1809, -0.6109,  ..., -0.1628, -0.2212,  0.3068]],\n",
      "\n",
      "        [[-0.1209,  0.4735, -0.7516,  ...,  0.1954,  0.3465,  0.2204],\n",
      "         [-0.0220,  0.1202, -0.7716,  ...,  0.0145, -0.2125,  0.1037],\n",
      "         [-0.0156, -0.0684, -0.5716,  ...,  0.5071, -0.0790,  0.0719],\n",
      "         ...,\n",
      "         [-0.1636,  0.1107, -0.6896,  ...,  0.2773, -0.1492,  0.0534],\n",
      "         [-0.0540,  0.2627, -0.7001,  ..., -0.1238, -0.0110,  0.1386],\n",
      "         [-0.1001, -0.2009, -0.4416,  ...,  0.4629, -0.3367,  0.1022]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0190560817718506\n",
      "Model outputs:  tensor([[[ 0.3548,  0.1041, -0.4049,  ...,  0.1409,  0.2689, -0.0322],\n",
      "         [ 0.3202, -0.2078, -0.5379,  ..., -0.4329,  0.0258,  0.1440],\n",
      "         [ 0.1221, -0.0891, -0.5056,  ..., -0.2077, -0.1020,  0.2601],\n",
      "         ...,\n",
      "         [ 0.0295, -0.0347, -0.2258,  ..., -0.2531,  0.0267,  0.1819],\n",
      "         [-0.0418, -0.1410, -0.1103,  ..., -0.5707, -0.1026,  0.4333],\n",
      "         [ 0.2454, -0.1805, -0.5673,  ..., -0.1588,  0.0058,  0.1237]],\n",
      "\n",
      "        [[ 0.0813,  0.2883, -0.2551,  ..., -0.0704,  0.0297,  0.4442],\n",
      "         [ 0.2084, -0.2283, -0.4239,  ..., -0.6771, -0.2313,  0.1354],\n",
      "         [ 0.3619, -0.1667, -0.5667,  ..., -0.1336, -0.1171,  0.5974],\n",
      "         ...,\n",
      "         [ 0.1405,  0.0123, -0.3445,  ..., -0.3071, -0.0958,  0.1499],\n",
      "         [ 0.2133,  0.0147, -0.2438,  ..., -0.6272, -0.1368,  0.1881],\n",
      "         [ 0.3416, -0.2655, -0.5129,  ..., -0.3552, -0.0359,  0.3698]],\n",
      "\n",
      "        [[ 0.3016,  0.2492, -0.6385,  ...,  0.2828,  0.2092,  0.2644],\n",
      "         [ 0.2362,  0.1734, -0.7581,  ..., -0.3618,  0.2637,  0.2070],\n",
      "         [ 0.0312, -0.0211, -0.7648,  ...,  0.0364,  0.1659,  0.1404],\n",
      "         ...,\n",
      "         [ 0.0422,  0.4442, -0.5365,  ...,  0.0178,  0.1895,  0.0862],\n",
      "         [ 0.0417,  0.2209, -0.2947,  ..., -0.3746,  0.1875,  0.0499],\n",
      "         [ 0.2057,  0.0440, -0.5654,  ..., -0.3219,  0.3179,  0.1313]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2524,  0.0495, -0.6461,  ..., -0.1012,  0.1702,  0.3302],\n",
      "         [ 0.2097, -0.1647, -0.3010,  ..., -0.6381,  0.1388,  0.3926],\n",
      "         [ 0.1156, -0.0732, -0.5639,  ..., -0.3737,  0.2742,  0.3395],\n",
      "         ...,\n",
      "         [ 0.0536,  0.0899, -0.3633,  ..., -0.3702,  0.1579,  0.1624],\n",
      "         [ 0.0111,  0.0447, -0.4922,  ..., -0.7256,  0.2161,  0.2709],\n",
      "         [ 0.0540, -0.2188, -0.5298,  ..., -0.4316, -0.0074,  0.4008]],\n",
      "\n",
      "        [[ 0.3368, -0.0215, -0.2231,  ...,  0.3800,  0.2695, -0.3006],\n",
      "         [ 0.1564,  0.0792, -0.1339,  ...,  0.2260,  0.1210, -0.6227],\n",
      "         [ 0.0514, -0.0693,  0.1182,  ...,  0.5356, -0.1440, -0.5984],\n",
      "         ...,\n",
      "         [ 0.1409,  0.0439, -0.0040,  ...,  0.1553, -0.0891, -0.7262],\n",
      "         [-0.0377,  0.2867,  0.0532,  ...,  0.2152,  0.0764, -0.6807],\n",
      "         [ 0.3113, -0.0504, -0.2969,  ...,  0.4654,  0.1430, -0.5075]],\n",
      "\n",
      "        [[-0.2909,  0.5066, -0.4097,  ...,  0.4834,  0.4512, -0.2765],\n",
      "         [-0.2207,  0.4868, -0.4451,  ...,  0.2159,  0.7833, -0.4779],\n",
      "         [-0.2474,  0.5037, -0.1881,  ...,  0.3878,  0.1552, -0.1274],\n",
      "         ...,\n",
      "         [-0.1869,  0.3200, -0.1610,  ...,  0.3824,  0.3200, -0.5041],\n",
      "         [-0.4276,  0.4550, -0.5082,  ...,  0.4124,  0.1240, -0.1962],\n",
      "         [-0.1565,  0.0610, -0.4692,  ...,  0.5723,  0.3323, -0.0536]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.5479528903961182\n",
      "Model outputs:  tensor([[[-1.6280e-02,  1.6666e-01, -3.9041e-01,  ...,  1.0004e-01,\n",
      "          -9.6812e-02,  5.8722e-01],\n",
      "         [ 7.4124e-02, -2.6296e-01, -4.7285e-02,  ..., -2.6968e-01,\n",
      "          -2.5145e-01,  6.5582e-01],\n",
      "         [ 2.1564e-01, -2.3718e-01, -1.4216e-01,  ..., -4.0880e-01,\n",
      "          -3.0999e-01,  3.9784e-01],\n",
      "         ...,\n",
      "         [ 2.0908e-01, -3.3023e-01, -2.0260e-01,  ..., -2.7826e-01,\n",
      "          -2.2359e-01,  5.8420e-01],\n",
      "         [ 4.4037e-02, -1.3071e-01, -7.1530e-02,  ..., -9.5420e-02,\n",
      "          -3.0957e-01,  6.5248e-01],\n",
      "         [ 5.3856e-02, -2.5828e-01, -1.0222e-01,  ..., -2.2903e-03,\n",
      "          -3.7969e-01,  6.1441e-01]],\n",
      "\n",
      "        [[-2.5587e-01,  1.9464e-01, -6.7214e-01,  ...,  1.3856e-01,\n",
      "           4.2911e-02,  2.0971e-01],\n",
      "         [-3.5132e-01, -5.5739e-02, -6.4675e-01,  ...,  3.1693e-01,\n",
      "           1.5156e-01,  1.7724e-01],\n",
      "         [-2.3047e-01, -6.9352e-02, -4.6280e-01,  ..., -5.2282e-02,\n",
      "           7.6417e-02,  9.5340e-02],\n",
      "         ...,\n",
      "         [ 7.1549e-02, -2.6332e-01, -4.3870e-01,  ..., -2.0245e-01,\n",
      "          -1.6758e-01,  2.9293e-01],\n",
      "         [-1.8265e-01, -3.2693e-01, -4.9179e-01,  ...,  1.9085e-01,\n",
      "          -5.1359e-02,  3.3378e-01],\n",
      "         [-2.0266e-01, -9.0119e-02, -6.8247e-01,  ...,  1.8036e-01,\n",
      "          -1.9180e-01,  7.3792e-02]],\n",
      "\n",
      "        [[ 1.5872e-01,  1.3741e-01, -8.5236e-01,  ...,  8.2172e-01,\n",
      "           3.3754e-01,  9.5941e-02],\n",
      "         [-4.3761e-01,  2.8650e-02, -9.9560e-01,  ...,  6.8673e-01,\n",
      "           9.8902e-03,  1.5276e-01],\n",
      "         [-6.7290e-02,  5.3691e-02, -4.6615e-01,  ...,  5.1386e-01,\n",
      "          -1.7626e-01, -1.4147e-01],\n",
      "         ...,\n",
      "         [ 1.5056e-01, -2.0337e-01, -4.8003e-01,  ...,  6.2686e-01,\n",
      "           1.9174e-02, -1.3819e-02],\n",
      "         [-1.4407e-01, -1.2054e-01, -5.2096e-01,  ...,  5.8653e-01,\n",
      "          -7.7956e-02, -1.2642e-01],\n",
      "         [-4.4751e-02,  7.2199e-02, -6.6322e-01,  ...,  7.5493e-01,\n",
      "           7.5579e-02,  2.3749e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.1268e-02, -8.5450e-02, -3.7846e-01,  ..., -7.1910e-03,\n",
      "           1.1232e-02,  4.3268e-01],\n",
      "         [ 4.4590e-02, -1.3970e-01, -6.2267e-01,  ...,  8.9553e-02,\n",
      "           3.7883e-02,  5.0077e-01],\n",
      "         [-8.6198e-02,  5.9251e-03, -1.6696e-01,  ..., -5.7195e-02,\n",
      "          -5.5618e-01,  1.7831e-01],\n",
      "         ...,\n",
      "         [ 2.7094e-01, -2.4437e-01, -3.1165e-01,  ...,  4.1848e-02,\n",
      "          -1.2634e-01,  5.1860e-01],\n",
      "         [-2.6370e-01, -2.4588e-01, -4.1583e-01,  ...,  1.0228e-01,\n",
      "          -1.4332e-01,  3.1531e-01],\n",
      "         [-8.2605e-03, -1.7057e-01, -6.5350e-01,  ..., -1.8107e-01,\n",
      "          -3.4747e-02,  4.9079e-01]],\n",
      "\n",
      "        [[-3.8633e-01,  3.8631e-01, -1.5484e-01,  ...,  6.6807e-01,\n",
      "           2.2129e-01, -4.5099e-02],\n",
      "         [-5.2528e-02, -1.1975e-01, -3.4329e-01,  ...,  7.2213e-01,\n",
      "           3.8697e-01,  5.6189e-02],\n",
      "         [-1.8935e-01,  2.0444e-01,  3.3411e-02,  ...,  4.9096e-01,\n",
      "           7.3874e-02, -1.9258e-01],\n",
      "         ...,\n",
      "         [-2.4325e-03, -2.3596e-01,  7.9560e-02,  ...,  6.6417e-01,\n",
      "           1.2369e-01,  9.5332e-04],\n",
      "         [-1.7797e-01, -1.4602e-01,  8.2396e-02,  ...,  7.2533e-01,\n",
      "           8.2970e-02,  1.2347e-01],\n",
      "         [-5.7758e-01, -6.3574e-03, -2.2935e-01,  ...,  6.9115e-01,\n",
      "           2.1406e-01,  6.3993e-02]],\n",
      "\n",
      "        [[-7.7294e-02,  2.5360e-01, -6.2916e-01,  ...,  7.4715e-01,\n",
      "           1.4794e-01,  7.9154e-02],\n",
      "         [-1.7735e-01,  2.4073e-01, -6.9596e-01,  ...,  6.4761e-01,\n",
      "           3.7388e-01, -1.6587e-01],\n",
      "         [-8.6627e-02,  4.9811e-01, -4.1489e-01,  ...,  7.5199e-01,\n",
      "          -2.9705e-02, -1.8144e-01],\n",
      "         ...,\n",
      "         [-9.6946e-03, -3.3274e-02, -4.2858e-01,  ...,  8.7691e-01,\n",
      "           3.1873e-01, -1.0134e-02],\n",
      "         [-1.0647e-01,  2.9037e-01, -4.7839e-01,  ...,  8.6976e-01,\n",
      "           2.7252e-01,  2.2623e-01],\n",
      "         [-1.5532e-01,  2.7884e-01, -7.5619e-01,  ...,  8.5707e-01,\n",
      "           1.0448e-01,  3.6094e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9504029154777527\n",
      "Model outputs:  tensor([[[-0.2729,  0.7392, -0.7293,  ...,  0.3067,  0.1780, -0.0050],\n",
      "         [-0.1963,  0.5482, -0.4188,  ...,  0.0560,  0.2904, -0.0161],\n",
      "         [-0.2442,  0.6513, -0.4614,  ..., -0.0682,  0.2472,  0.1271],\n",
      "         ...,\n",
      "         [-0.2911,  0.3131, -0.7112,  ...,  0.2439,  0.1949, -0.0298],\n",
      "         [ 0.0457,  0.1676, -0.7487,  ...,  0.3343,  0.0413,  0.3146],\n",
      "         [-0.5185,  0.6761, -0.6654,  ...,  0.2706, -0.0446, -0.1817]],\n",
      "\n",
      "        [[-0.4816,  0.4708, -0.3392,  ...,  0.2840,  0.2797, -0.2772],\n",
      "         [-0.1371,  0.5714, -0.1119,  ...,  0.5890,  0.5164, -0.0850],\n",
      "         [-0.5250,  0.5337, -0.0947,  ...,  0.1802,  0.4679, -0.3915],\n",
      "         ...,\n",
      "         [-0.0407,  0.1399, -0.0891,  ...,  0.6953,  0.5125,  0.2027],\n",
      "         [-0.1794,  0.2215, -0.2580,  ...,  0.3973,  0.3035,  0.1088],\n",
      "         [-0.6672,  0.5348,  0.0332,  ...,  0.6129,  0.2458, -0.0462]],\n",
      "\n",
      "        [[ 0.0782,  0.0054,  0.2429,  ...,  0.2302, -0.1035, -0.5654],\n",
      "         [ 0.3203,  0.2838, -0.0779,  ...,  0.3332,  0.0800, -0.6425],\n",
      "         [ 0.0654,  0.1750,  0.1361,  ...,  0.0865, -0.2043, -0.6464],\n",
      "         ...,\n",
      "         [ 0.1061, -0.2543,  0.2885,  ...,  0.3374,  0.0583, -0.4497],\n",
      "         [ 0.2882, -0.1844,  0.1054,  ...,  0.5385,  0.0451, -0.4263],\n",
      "         [ 0.0864,  0.2885,  0.2374,  ...,  0.5612, -0.1361, -0.7442]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2604,  0.0050, -0.1274,  ...,  0.2153, -0.0036, -0.5774],\n",
      "         [ 0.2155,  0.0234, -0.3853,  ...,  0.0826, -0.1006, -0.6286],\n",
      "         [ 0.2467,  0.1777, -0.0452,  ...,  0.0847, -0.0381, -0.6001],\n",
      "         ...,\n",
      "         [ 0.4622, -0.2925,  0.1111,  ..., -0.0119,  0.3602, -0.1823],\n",
      "         [ 0.2450, -0.2680, -0.2650,  ...,  0.0444, -0.0175, -0.5702],\n",
      "         [ 0.3990, -0.0458, -0.2172,  ...,  0.0303, -0.0473, -0.4341]],\n",
      "\n",
      "        [[-0.1243,  0.0748, -0.1942,  ..., -0.3158,  0.0311, -0.5347],\n",
      "         [ 0.2138,  0.1450, -0.3199,  ..., -0.0961,  0.2883, -0.4526],\n",
      "         [ 0.0546,  0.0538, -0.3315,  ..., -0.3030, -0.0829, -0.4452],\n",
      "         ...,\n",
      "         [-0.1179, -0.0092, -0.2037,  ..., -0.1001,  0.1642, -0.0849],\n",
      "         [-0.0446,  0.0484, -0.5389,  ...,  0.1842,  0.0689, -0.1972],\n",
      "         [-0.1661,  0.3891, -0.2014,  ..., -0.1442,  0.1431, -0.1658]],\n",
      "\n",
      "        [[-0.0908,  0.3615, -0.3424,  ..., -0.3256,  0.1032,  0.2905],\n",
      "         [-0.0323,  0.2273, -0.4277,  ..., -0.5397, -0.0380,  0.3710],\n",
      "         [ 0.0259,  0.0268, -0.3301,  ..., -0.1686,  0.0774,  0.2770],\n",
      "         ...,\n",
      "         [-0.1275,  0.1003, -0.1275,  ..., -0.1488,  0.2584,  0.2990],\n",
      "         [ 0.2185, -0.2335, -0.5262,  ..., -0.2331,  0.0350,  0.5252],\n",
      "         [-0.2147,  0.1619, -0.3296,  ..., -0.2517, -0.0883,  0.1747]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9677577614784241\n",
      "Model outputs:  tensor([[[ 0.1301, -0.0579,  0.1622,  ...,  0.2001, -0.2070, -0.8602],\n",
      "         [ 0.0544, -0.0651, -0.1284,  ...,  0.3596, -0.1417, -0.4415],\n",
      "         [ 0.1056, -0.5573,  0.1428,  ...,  0.6602, -0.0504, -0.4085],\n",
      "         ...,\n",
      "         [ 0.1404, -0.2748, -0.0261,  ...,  0.5486,  0.1394, -0.5523],\n",
      "         [ 0.0241, -0.2072, -0.0483,  ...,  0.4367,  0.0545, -0.5377],\n",
      "         [ 0.0822, -0.0506, -0.0473,  ...,  0.2077, -0.1413, -0.5292]],\n",
      "\n",
      "        [[-0.2784,  0.2797, -0.8308,  ...,  0.2003,  0.0103, -0.1044],\n",
      "         [-0.0202,  0.3595, -0.9154,  ...,  0.3442,  0.1467,  0.1231],\n",
      "         [-0.2513, -0.2998, -0.5800,  ...,  0.1700,  0.1326,  0.1905],\n",
      "         ...,\n",
      "         [-0.2172,  0.1692, -0.5413,  ...,  0.1782,  0.0105,  0.1517],\n",
      "         [ 0.0656,  0.0221, -0.9263,  ...,  0.4258,  0.1366, -0.2074],\n",
      "         [-0.1166,  0.3299, -0.9154,  ...,  0.2947, -0.1849, -0.1474]],\n",
      "\n",
      "        [[-0.5832,  0.5320, -0.7123,  ...,  0.3043,  0.2032,  0.0766],\n",
      "         [-0.4651,  0.4528, -0.7361,  ...,  0.6597,  0.3901,  0.2478],\n",
      "         [-0.7507, -0.2150, -0.5050,  ...,  0.4403,  0.1007,  0.2113],\n",
      "         ...,\n",
      "         [-0.4680,  0.4156, -0.7343,  ...,  0.5326,  0.3665,  0.2641],\n",
      "         [-0.5099,  0.4548, -0.6908,  ...,  0.5472,  0.3944,  0.0233],\n",
      "         [-0.7141,  0.6138, -0.7358,  ...,  0.2789,  0.3446, -0.0218]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0586, -0.0798, -0.4967,  ..., -0.3273, -0.3299,  0.4316],\n",
      "         [ 0.0887, -0.0691, -0.3875,  ..., -0.3621, -0.0998,  0.5977],\n",
      "         [-0.0376, -0.1408, -0.2481,  ..., -0.2504, -0.4743,  0.6330],\n",
      "         ...,\n",
      "         [-0.0680,  0.0085, -0.2904,  ..., -0.1575, -0.2603,  0.6063],\n",
      "         [-0.2114,  0.0834, -0.3389,  ..., -0.4047, -0.3230,  0.3712],\n",
      "         [ 0.1018, -0.1817, -0.5277,  ..., -0.2871, -0.3666,  0.4807]],\n",
      "\n",
      "        [[-0.5174,  0.5532, -0.9368,  ...,  0.4314,  0.2338, -0.1155],\n",
      "         [-0.2229,  0.4389, -1.0604,  ...,  0.8186,  0.1481, -0.3386],\n",
      "         [-0.1470, -0.3529, -0.5933,  ...,  0.5899,  0.2544,  0.1559],\n",
      "         ...,\n",
      "         [-0.1653, -0.1062, -0.6506,  ...,  0.7167,  0.1740,  0.1477],\n",
      "         [-0.1711,  0.3131, -1.1054,  ...,  0.7709,  0.3649, -0.0707],\n",
      "         [-0.4400,  0.5354, -0.7659,  ...,  0.5540,  0.1367, -0.2237]],\n",
      "\n",
      "        [[-0.3245,  0.5231, -0.1792,  ...,  0.2548,  0.3657, -0.0407],\n",
      "         [-0.5984,  0.2354, -0.2260,  ...,  0.8796,  0.2246, -0.0148],\n",
      "         [-0.3681, -0.3049, -0.3014,  ...,  0.4902,  0.4415,  0.0068],\n",
      "         ...,\n",
      "         [-0.3476, -0.2140, -0.2313,  ...,  0.4725,  0.5059, -0.0769],\n",
      "         [-0.3682,  0.1113, -0.3917,  ...,  0.6110,  0.1847,  0.1348],\n",
      "         [-0.3580,  0.1240, -0.3044,  ...,  0.3466,  0.1113, -0.0254]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2079342603683472\n",
      "Model outputs:  tensor([[[-0.0017, -0.0411, -0.4304,  ..., -0.3714, -0.1805,  0.3234],\n",
      "         [-0.2057, -0.0946, -0.4413,  ..., -0.3409, -0.1450,  0.2665],\n",
      "         [-0.1453,  0.0650, -0.2379,  ..., -0.3171, -0.3741,  0.4430],\n",
      "         ...,\n",
      "         [-0.1545, -0.0203, -0.5975,  ..., -0.3440, -0.3896,  0.5213],\n",
      "         [-0.2141, -0.0208, -0.2683,  ..., -0.4125, -0.6273,  0.3041],\n",
      "         [ 0.4007, -0.2301,  0.1408,  ..., -0.4748, -0.0425,  0.1671]],\n",
      "\n",
      "        [[-0.5206,  0.5122, -0.6118,  ...,  0.4882, -0.0168, -0.2475],\n",
      "         [-0.5458,  0.5466, -0.7859,  ...,  0.5711,  0.2612, -0.1874],\n",
      "         [-0.5585,  0.3009, -0.7865,  ...,  0.5736,  0.0084, -0.0571],\n",
      "         ...,\n",
      "         [-0.2825,  0.0195, -0.4790,  ...,  0.8932,  0.2858, -0.2701],\n",
      "         [-0.4194,  0.2326, -0.7417,  ...,  0.4709,  0.1648, -0.3173],\n",
      "         [ 0.1995,  0.2499, -0.3273,  ...,  0.3590,  0.0376, -0.2136]],\n",
      "\n",
      "        [[-0.5031,  0.5024, -0.4915,  ..., -0.0526,  0.0227, -0.3773],\n",
      "         [-0.8429,  0.4071, -0.6767,  ...,  0.1428, -0.1151,  0.2233],\n",
      "         [-0.5868,  0.2676, -0.8921,  ...,  0.0145, -0.2809, -0.0038],\n",
      "         ...,\n",
      "         [-0.2538,  0.2608, -0.5595,  ..., -0.0567, -0.0597, -0.0732],\n",
      "         [-0.6982,  0.6145, -0.5875,  ...,  0.0755,  0.0328,  0.0286],\n",
      "         [-0.1157,  0.0704, -0.0952,  ..., -0.0425,  0.0984, -0.0704]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5481,  0.2561, -0.2400,  ...,  0.2379,  0.2629, -0.2719],\n",
      "         [-0.4996,  0.1827, -0.2197,  ...,  0.6647,  0.1802, -0.1392],\n",
      "         [-0.7354,  0.3964, -0.3955,  ...,  0.6362,  0.5474, -0.0400],\n",
      "         ...,\n",
      "         [-0.3643,  0.3283, -0.1708,  ...,  0.4159,  0.2998, -0.2293],\n",
      "         [-0.6494,  0.5157,  0.0461,  ...,  0.4401,  0.3476, -0.3334],\n",
      "         [ 0.1172,  0.1796,  0.1207,  ...,  0.4325,  0.2105, -0.1290]],\n",
      "\n",
      "        [[-0.4297,  0.5446, -0.6122,  ...,  0.0417, -0.2823, -0.0149],\n",
      "         [-0.5590,  0.3807, -0.8253,  ...,  0.1227, -0.1166,  0.1257],\n",
      "         [-0.5233,  0.1988, -0.8992,  ...,  0.1861, -0.3136,  0.0800],\n",
      "         ...,\n",
      "         [-0.0856,  0.2904, -0.5268,  ...,  0.1932, -0.3729,  0.1995],\n",
      "         [-0.0265,  0.1875, -0.4672,  ...,  0.1772,  0.0952, -0.2448],\n",
      "         [ 0.2717,  0.1780, -0.0051,  ..., -0.1833, -0.0598, -0.1230]],\n",
      "\n",
      "        [[-0.2028,  0.3376, -0.5429,  ..., -0.1471, -0.0259, -0.1164],\n",
      "         [-0.2984,  0.1834, -0.9016,  ...,  0.2855, -0.0557,  0.3070],\n",
      "         [-0.1525,  0.0651, -0.7840,  ...,  0.1277, -0.2217,  0.2711],\n",
      "         ...,\n",
      "         [ 0.0597, -0.0232, -0.6210,  ...,  0.2094, -0.2891,  0.2436],\n",
      "         [-0.3378,  0.2617, -0.5744,  ...,  0.0744,  0.2015, -0.1111],\n",
      "         [ 0.3000, -0.2553, -0.2218,  ..., -0.1980,  0.2717,  0.1448]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.048163890838623\n",
      "Model outputs:  tensor([[[-0.0098,  0.0050, -0.6134,  ..., -0.2049,  0.3019,  0.6903],\n",
      "         [ 0.2070,  0.3573, -0.4139,  ..., -0.0055,  0.3161,  0.6385],\n",
      "         [ 0.5258, -0.0896, -0.4794,  ..., -0.4377,  0.3403,  0.6596],\n",
      "         ...,\n",
      "         [ 0.0341,  0.0061, -0.4709,  ..., -0.3797, -0.1299,  0.5053],\n",
      "         [ 0.1015, -0.1211, -0.5178,  ..., -0.3272,  0.3355,  0.4464],\n",
      "         [ 0.2020,  0.0470, -0.4865,  ...,  0.0067,  0.3298,  0.7904]],\n",
      "\n",
      "        [[-0.0370, -0.1859, -0.4900,  ..., -0.1597, -0.1935,  0.4254],\n",
      "         [ 0.0588,  0.2526, -0.8295,  ...,  0.0167,  0.0814,  0.4598],\n",
      "         [ 0.0895,  0.2442, -0.5405,  ..., -0.0142, -0.0237,  0.3042],\n",
      "         ...,\n",
      "         [-0.3603, -0.0158, -1.0932,  ...,  0.1480, -0.5521,  0.4877],\n",
      "         [-0.1067,  0.1749, -0.4623,  ..., -0.1816, -0.2392,  0.5209],\n",
      "         [-0.0353,  0.2261, -0.7208,  ...,  0.4257, -0.1457,  0.6144]],\n",
      "\n",
      "        [[ 0.1062,  0.2483, -0.2721,  ..., -0.2048,  0.6686,  0.5939],\n",
      "         [ 0.0246, -0.1281, -0.5448,  ..., -0.4028,  0.5156,  0.6331],\n",
      "         [ 0.2135, -0.1189, -0.5138,  ..., -0.4438,  0.5145,  0.4866],\n",
      "         ...,\n",
      "         [-0.5166,  0.0108, -0.6393,  ..., -0.2928, -0.0427,  0.8651],\n",
      "         [ 0.1129,  0.0694, -0.4393,  ..., -0.3101,  0.6853,  0.8140],\n",
      "         [-0.2311,  0.0992, -0.3955,  ..., -0.3732,  0.3055,  0.9362]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5400,  0.3035, -0.1277,  ...,  0.6245,  0.1737, -0.0677],\n",
      "         [-0.2226,  0.1196, -0.4677,  ...,  0.6658,  0.4749,  0.1618],\n",
      "         [-0.1795,  0.2402, -0.2503,  ...,  0.7121,  0.3917, -0.2793],\n",
      "         ...,\n",
      "         [-0.7616,  0.0219, -0.3730,  ...,  0.6455, -0.0931, -0.0154],\n",
      "         [-0.4659,  0.2325, -0.2664,  ...,  0.6715,  0.3723, -0.0070],\n",
      "         [-0.7591,  0.0266, -0.3792,  ...,  0.8677,  0.2611,  0.2123]],\n",
      "\n",
      "        [[-0.1779, -0.0120, -0.6117,  ...,  0.1420, -0.0019,  0.3146],\n",
      "         [-0.0065,  0.0764, -0.7502,  ...,  0.3727,  0.1150,  0.4591],\n",
      "         [ 0.0275,  0.1232, -0.5719,  ...,  0.2720,  0.0577,  0.2476],\n",
      "         ...,\n",
      "         [-0.4567, -0.1209, -0.9592,  ...,  0.1517, -0.8086,  0.2641],\n",
      "         [-0.2433,  0.3423, -0.6434,  ...,  0.2270, -0.0568,  0.3437],\n",
      "         [-0.2113,  0.0857, -0.6685,  ...,  0.3126, -0.2174,  0.4977]],\n",
      "\n",
      "        [[-0.1823,  0.3233, -0.5263,  ...,  0.2682, -0.2633,  0.2952],\n",
      "         [-0.1715,  0.4066, -0.9568,  ...,  0.4824, -0.0363,  0.0766],\n",
      "         [-0.2250,  0.0733, -0.6370,  ...,  0.3220,  0.0111, -0.0074],\n",
      "         ...,\n",
      "         [-0.1738,  0.1169, -1.0486,  ...,  0.4361, -0.7478,  0.3732],\n",
      "         [-0.2230,  0.2555, -0.9051,  ...,  0.2440, -0.3557,  0.1577],\n",
      "         [-0.3824,  0.1386, -0.5053,  ...,  0.5791, -0.1564,  0.3342]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1405055522918701\n",
      "Model outputs:  tensor([[[ 0.0426, -0.3968, -0.2761,  ..., -0.3880,  0.3462,  0.7500],\n",
      "         [ 0.0147,  0.0449, -0.2397,  ..., -0.3294,  0.7002,  0.7067],\n",
      "         [ 0.1777,  0.0063, -0.6970,  ..., -0.4221,  0.5896,  0.7361],\n",
      "         ...,\n",
      "         [ 0.1707, -0.2326, -0.5505,  ..., -0.3704,  0.6186,  0.6533],\n",
      "         [ 0.1415, -0.1553, -0.4080,  ..., -0.4507,  0.5081,  0.9586],\n",
      "         [ 0.1917, -0.0845, -0.3391,  ..., -0.4404,  0.3943,  0.9052]],\n",
      "\n",
      "        [[-0.0430, -0.3280, -0.3279,  ...,  0.1121,  0.0125,  0.6546],\n",
      "         [-0.1248, -0.1466, -0.5776,  ...,  0.1563, -0.0090,  0.1879],\n",
      "         [ 0.2070, -0.3676, -0.4081,  ..., -0.1543, -0.0516,  0.6046],\n",
      "         ...,\n",
      "         [ 0.0057, -0.0825, -0.3596,  ..., -0.1007,  0.1216,  0.3449],\n",
      "         [ 0.2148, -0.2911, -0.3241,  ...,  0.1312,  0.3759,  0.5794],\n",
      "         [ 0.2633, -0.2740, -0.4503,  ..., -0.2864,  0.2181,  0.3696]],\n",
      "\n",
      "        [[ 0.2072, -0.1976, -0.5319,  ..., -0.2907,  0.4720,  0.7965],\n",
      "         [-0.2191, -0.0691, -0.4206,  ..., -0.4815,  0.6861,  0.6771],\n",
      "         [ 0.1536,  0.1812, -0.3325,  ..., -0.3279,  0.5783,  0.5517],\n",
      "         ...,\n",
      "         [ 0.1172,  0.0863, -0.5690,  ..., -0.2268,  0.6160,  0.7305],\n",
      "         [ 0.1562, -0.1647, -0.3669,  ..., -0.0764,  0.5312,  0.5383],\n",
      "         [ 0.2437,  0.0697, -0.5009,  ..., -0.4169,  0.7516,  0.6436]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5422, -0.3857, -0.0755,  ...,  0.5281, -0.0892, -0.3912],\n",
      "         [ 0.1548, -0.2263,  0.0660,  ...,  0.3938,  0.1746, -0.6341],\n",
      "         [ 0.4096, -0.4414, -0.1487,  ...,  0.5464, -0.0280, -0.1102],\n",
      "         ...,\n",
      "         [ 0.5167, -0.1736, -0.2090,  ...,  0.3550, -0.0198, -0.2083],\n",
      "         [ 0.5719, -0.3311, -0.0634,  ...,  0.4845,  0.0553, -0.3321],\n",
      "         [ 0.2702, -0.1502, -0.0702,  ...,  0.5692,  0.1589, -0.4698]],\n",
      "\n",
      "        [[-0.1203, -0.3687, -0.9625,  ...,  0.1681, -0.0088,  0.4830],\n",
      "         [-0.2475, -0.2715, -0.6699,  ..., -0.1606,  0.0935,  0.3236],\n",
      "         [-0.2833,  0.0055, -0.7284,  ..., -0.0472, -0.1476,  0.3529],\n",
      "         ...,\n",
      "         [-0.2505,  0.2499, -0.7062,  ..., -0.0186,  0.1400,  0.4130],\n",
      "         [-0.1604,  0.0304, -0.6464,  ...,  0.0720,  0.1313,  0.5543],\n",
      "         [-0.2185,  0.1670, -0.5708,  ...,  0.0334,  0.1434,  0.1531]],\n",
      "\n",
      "        [[-0.2717, -0.1639, -0.4426,  ...,  0.5686,  0.3844,  0.1240],\n",
      "         [-0.2382,  0.1516, -0.2166,  ...,  0.5750,  0.1221, -0.2681],\n",
      "         [-0.1254, -0.0198, -0.4085,  ...,  0.6860,  0.2225,  0.1965],\n",
      "         ...,\n",
      "         [-0.2879,  0.0774, -0.2496,  ...,  0.7907,  0.3105,  0.1601],\n",
      "         [-0.2405, -0.0090, -0.1053,  ...,  0.7148,  0.3991,  0.1648],\n",
      "         [-0.1317,  0.2750, -0.2925,  ...,  0.7140,  0.2302,  0.0776]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3658883571624756\n",
      "Model outputs:  tensor([[[-0.2315,  0.3992, -0.3003,  ...,  0.3456,  0.6218, -0.1514],\n",
      "         [-0.5854,  0.4324, -0.0193,  ...,  0.3219,  0.2353, -0.2021],\n",
      "         [-0.2770,  0.3165, -0.1460,  ...,  0.3740,  0.3333,  0.2473],\n",
      "         ...,\n",
      "         [-0.2397,  0.0371, -0.1126,  ...,  0.6280,  0.3760, -0.0620],\n",
      "         [-0.4697,  0.4311, -0.3862,  ...,  0.6247,  0.3313,  0.2215],\n",
      "         [-0.3212,  0.3352, -0.2624,  ...,  0.8282,  0.3876,  0.0312]],\n",
      "\n",
      "        [[ 0.0973,  0.2769, -0.3172,  ..., -0.2243,  0.2288,  0.1238],\n",
      "         [-0.0198, -0.0764, -0.4597,  ..., -0.5821, -0.0900,  0.0160],\n",
      "         [ 0.1159,  0.1894, -0.2407,  ..., -0.3045,  0.0953,  0.3322],\n",
      "         ...,\n",
      "         [-0.0143, -0.3227, -0.4425,  ...,  0.3976, -0.1564,  0.4735],\n",
      "         [-0.1922,  0.0731, -0.4182,  ..., -0.4105, -0.3399,  0.3198],\n",
      "         [ 0.1146, -0.0269, -0.4625,  ..., -0.1910,  0.0130,  0.3110]],\n",
      "\n",
      "        [[-0.2835,  0.2433, -0.7735,  ...,  0.0059,  0.0937, -0.1089],\n",
      "         [-0.3691,  0.4562, -0.6113,  ...,  0.1660, -0.1509, -0.0141],\n",
      "         [-0.2891,  0.4284, -0.5454,  ..., -0.0102,  0.1828,  0.0859],\n",
      "         ...,\n",
      "         [ 0.0952, -0.0651, -0.5040,  ...,  0.4672, -0.1731,  0.1762],\n",
      "         [-0.1136,  0.3703, -0.4659,  ...,  0.6094,  0.2717,  0.1225],\n",
      "         [-0.3539,  0.3100, -0.8098,  ...,  0.3506,  0.4167,  0.1930]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0408,  0.1358, -0.5070,  ..., -0.3221, -0.0769,  0.2575],\n",
      "         [-0.0304,  0.1370, -0.4637,  ..., -0.5577, -0.0956,  0.2176],\n",
      "         [ 0.1336, -0.2872, -0.3249,  ..., -0.4153, -0.2168,  0.3300],\n",
      "         ...,\n",
      "         [ 0.2388, -0.3357, -0.4378,  ...,  0.0101, -0.1742,  0.5078],\n",
      "         [-0.0112,  0.0819, -0.2626,  ..., -0.1979, -0.3205,  0.3949],\n",
      "         [ 0.0784,  0.2283, -0.5687,  ..., -0.2333, -0.0391,  0.6691]],\n",
      "\n",
      "        [[-0.1392,  0.2224, -0.8622,  ..., -0.0629,  0.1403,  0.1847],\n",
      "         [ 0.1743,  0.3574, -0.6766,  ...,  0.1780,  0.2702,  0.1837],\n",
      "         [-0.1598,  0.2727, -0.4153,  ...,  0.0754,  0.1615, -0.2312],\n",
      "         ...,\n",
      "         [ 0.2633, -0.1431, -0.4241,  ...,  0.4612,  0.0683,  0.0134],\n",
      "         [-0.3485,  0.1902, -0.7524,  ...,  0.3494,  0.0483,  0.3328],\n",
      "         [ 0.0937,  0.2223, -0.7287,  ...,  0.3976, -0.0842,  0.2642]],\n",
      "\n",
      "        [[ 0.1417,  0.1073,  0.2640,  ...,  0.1709, -0.0420, -0.8589],\n",
      "         [ 0.4390, -0.1548,  0.2381,  ...,  0.1184, -0.2098, -0.5238],\n",
      "         [ 0.0924, -0.3408,  0.2797,  ...,  0.3530,  0.0185, -0.8041],\n",
      "         ...,\n",
      "         [ 0.2320, -0.6231,  0.2271,  ...,  0.5793, -0.1367, -0.2779],\n",
      "         [ 0.4231, -0.2134,  0.1960,  ...,  0.2835,  0.0494, -0.6207],\n",
      "         [ 0.4305, -0.1501, -0.1009,  ...,  0.5763, -0.0553, -0.4316]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1835910081863403\n",
      "Model outputs:  tensor([[[ 0.0321,  0.1312, -0.9242,  ...,  0.6293,  0.5586,  0.2172],\n",
      "         [-0.1873, -0.1024, -0.7310,  ...,  0.4778,  0.2404, -0.0719],\n",
      "         [-0.3935,  0.2733, -0.6579,  ...,  0.6497,  0.5221,  0.0069],\n",
      "         ...,\n",
      "         [-0.4921,  0.5034, -0.6564,  ...,  0.4153,  0.3879, -0.0035],\n",
      "         [ 0.0642,  0.0673, -0.7430,  ...,  0.6000,  0.3205,  0.2303],\n",
      "         [-0.2703,  0.3732, -0.6262,  ...,  0.7260,  0.2240, -0.0638]],\n",
      "\n",
      "        [[-0.0135, -0.0145, -0.5153,  ...,  0.7107,  0.3554, -0.0349],\n",
      "         [-0.3171, -0.2132, -0.3777,  ...,  0.2731,  0.0546, -0.1706],\n",
      "         [-0.2794,  0.0492, -0.4607,  ...,  0.3865,  0.1179,  0.1093],\n",
      "         ...,\n",
      "         [-0.4232,  0.3219, -0.2321,  ...,  0.0316,  0.2404, -0.1901],\n",
      "         [ 0.0744, -0.0573, -0.2027,  ...,  0.3420,  0.1154, -0.1724],\n",
      "         [-0.5245,  0.2395, -0.1371,  ...,  0.4789,  0.1231,  0.0610]],\n",
      "\n",
      "        [[ 0.2687, -0.1610, -0.5110,  ..., -0.6976,  0.5701,  0.2905],\n",
      "         [ 0.4317, -0.2146, -0.0792,  ..., -0.6390,  0.0833,  0.0305],\n",
      "         [ 0.2447, -0.1227, -0.3029,  ..., -0.6226,  0.2863,  0.2883],\n",
      "         ...,\n",
      "         [ 0.1372,  0.0313, -0.2953,  ..., -0.5874,  0.3052, -0.3104],\n",
      "         [ 0.6821, -0.3506, -0.2985,  ..., -0.5503,  0.2599, -0.0296],\n",
      "         [ 0.2321, -0.2020, -0.2759,  ..., -0.5182,  0.1274,  0.2996]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4350, -0.0699, -0.4725,  ...,  0.9449,  0.7009, -0.0376],\n",
      "         [ 0.1108, -0.2794, -0.3355,  ...,  0.7935,  0.1430, -0.2533],\n",
      "         [-0.0937,  0.2756, -0.3780,  ...,  0.8317,  0.4872,  0.0787],\n",
      "         ...,\n",
      "         [-0.3631,  0.3853, -0.0517,  ...,  0.3490,  0.2122, -0.2110],\n",
      "         [ 0.2739, -0.1511, -0.3754,  ...,  0.6655,  0.2968, -0.1587],\n",
      "         [-0.3847,  0.3138, -0.3215,  ...,  0.7733,  0.1621, -0.1328]],\n",
      "\n",
      "        [[-0.5375,  0.1908, -0.6091,  ...,  0.2155,  0.2327,  0.2299],\n",
      "         [-0.1862, -0.0539, -0.5874,  ...,  0.1053, -0.0156,  0.1330],\n",
      "         [-0.2817,  0.3925, -0.6550,  ...,  0.2942,  0.0392,  0.4027],\n",
      "         ...,\n",
      "         [-0.3478,  0.2404, -0.1768,  ..., -0.0209,  0.3361,  0.0299],\n",
      "         [-0.1017,  0.0529, -0.5181,  ...,  0.2569,  0.3097,  0.1085],\n",
      "         [-0.2796,  0.3016, -0.4778,  ...,  0.4128,  0.3148,  0.1812]],\n",
      "\n",
      "        [[-0.0368,  0.2316, -1.0543,  ...,  0.7754,  0.1301,  0.1150],\n",
      "         [ 0.0289, -0.0692, -0.7973,  ...,  0.4443, -0.2116, -0.0738],\n",
      "         [ 0.0413, -0.0244, -0.9092,  ...,  0.7117,  0.2535, -0.0589],\n",
      "         ...,\n",
      "         [-0.2945,  0.0796, -0.4454,  ...,  0.0300,  0.0428, -0.1122],\n",
      "         [ 0.1015, -0.2429, -0.6385,  ...,  0.3458,  0.0629, -0.1932],\n",
      "         [-0.1803, -0.0614, -0.7852,  ...,  0.4091, -0.3997,  0.2109]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2922836542129517\n",
      "Model outputs:  tensor([[[-0.3113,  0.1033, -0.2939,  ..., -0.2210,  0.2396,  0.1450],\n",
      "         [-0.5696,  0.2242, -0.3242,  ..., -0.1134,  0.2131,  0.1863],\n",
      "         [-0.1721,  0.1815, -0.4231,  ..., -0.2142,  0.4358,  0.1210],\n",
      "         ...,\n",
      "         [-0.4582,  0.4719, -0.2395,  ..., -0.0646,  0.2858, -0.2744],\n",
      "         [-0.3984,  0.2623, -0.3669,  ..., -0.2223,  0.2848, -0.2627],\n",
      "         [-0.3103,  0.4961, -0.3191,  ...,  0.1692,  0.1331,  0.2582]],\n",
      "\n",
      "        [[-0.3637,  0.2547, -0.1794,  ..., -0.0247,  0.1324,  0.1378],\n",
      "         [-0.5307,  0.2257, -0.5411,  ...,  0.0269,  0.3436,  0.2848],\n",
      "         [-0.5236,  0.4404, -0.4103,  ..., -0.1739,  0.4805, -0.0119],\n",
      "         ...,\n",
      "         [-0.3190,  0.3467, -0.3979,  ..., -0.2183,  0.1726, -0.1383],\n",
      "         [-0.3032,  0.1473, -0.2480,  ..., -0.0524,  0.0991, -0.1684],\n",
      "         [-0.3002,  0.6997, -0.3414,  ...,  0.1693,  0.2148,  0.3761]],\n",
      "\n",
      "        [[-0.0202, -0.1431, -0.3243,  ..., -0.6245, -0.2414,  0.3510],\n",
      "         [-0.1090, -0.1283, -0.2240,  ..., -0.5322, -0.3727,  0.2982],\n",
      "         [-0.0933,  0.1855, -0.3315,  ..., -0.6504, -0.0198,  0.3686],\n",
      "         ...,\n",
      "         [ 0.1332,  0.0620, -0.2258,  ..., -0.4800, -0.3756,  0.1498],\n",
      "         [ 0.1765, -0.1562, -0.2809,  ..., -0.5622, -0.0630,  0.2677],\n",
      "         [-0.0083, -0.1045, -0.1927,  ..., -0.5902, -0.1622,  0.2742]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2641,  0.2115, -1.0226,  ...,  0.2697,  0.3200,  0.0790],\n",
      "         [-0.2770,  0.3504, -0.6134,  ...,  0.3379,  0.1401, -0.0518],\n",
      "         [-0.3569,  0.2837, -0.5245,  ...,  0.4046,  0.4329, -0.1829],\n",
      "         ...,\n",
      "         [-0.5777,  0.5975, -0.5304,  ...,  0.3917,  0.2943, -0.2507],\n",
      "         [-0.4189,  0.5958, -0.6725,  ...,  0.2418,  0.2325, -0.2702],\n",
      "         [-0.2386,  0.4382, -0.4442,  ...,  0.1107,  0.2004, -0.1478]],\n",
      "\n",
      "        [[-0.4137,  0.3767, -0.8526,  ...,  0.2628,  0.4284,  0.2482],\n",
      "         [-0.6373,  0.4611, -0.6041,  ...,  0.6532,  0.3986, -0.2047],\n",
      "         [-0.4358,  0.3333, -0.5199,  ...,  0.3550,  0.3292,  0.0336],\n",
      "         ...,\n",
      "         [-0.3490,  0.4191, -0.5941,  ...,  0.4700,  0.4613, -0.1164],\n",
      "         [-0.2562,  0.4541, -0.5291,  ..., -0.0686,  0.0745, -0.1385],\n",
      "         [-0.4500,  0.5913, -0.5992,  ...,  0.4339,  0.2674, -0.0586]],\n",
      "\n",
      "        [[ 0.0992, -0.2619,  0.2816,  ...,  0.2314, -0.1832, -0.4303],\n",
      "         [-0.0410,  0.3097, -0.0274,  ...,  0.3865, -0.0908, -0.7451],\n",
      "         [ 0.1987,  0.1775, -0.1220,  ...,  0.2435,  0.0143, -0.7505],\n",
      "         ...,\n",
      "         [ 0.0707,  0.0525,  0.1145,  ...,  0.1793, -0.1218, -0.7036],\n",
      "         [ 0.3968,  0.0449,  0.0749,  ...,  0.4470,  0.3013, -0.6487],\n",
      "         [ 0.0409,  0.0384,  0.2113,  ...,  0.2124, -0.1814, -0.6008]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9362192749977112\n",
      "Model outputs:  tensor([[[-3.7666e-01, -3.3977e-02, -2.3728e-01,  ...,  6.5900e-01,\n",
      "           1.8256e-01,  1.3048e-01],\n",
      "         [-4.0895e-01,  2.0500e-01, -4.4904e-01,  ...,  6.6267e-01,\n",
      "           3.4671e-01,  7.0283e-02],\n",
      "         [-4.4020e-01, -9.6154e-04, -2.3883e-01,  ...,  3.6159e-01,\n",
      "           2.0217e-01,  1.3600e-01],\n",
      "         ...,\n",
      "         [-4.4296e-01,  2.3488e-01, -4.2146e-01,  ...,  4.1231e-01,\n",
      "           1.9487e-01,  1.0002e-01],\n",
      "         [-6.8440e-01,  3.8285e-01, -1.2906e-01,  ...,  2.7781e-01,\n",
      "           4.1434e-01, -2.5744e-01],\n",
      "         [-5.6106e-01,  3.9502e-01, -3.9960e-01,  ...,  6.8039e-01,\n",
      "           2.5955e-01,  8.6847e-03]],\n",
      "\n",
      "        [[-1.4192e-01, -1.1448e-01, -1.4249e-01,  ...,  7.4978e-01,\n",
      "           9.0125e-02, -9.2256e-02],\n",
      "         [-2.5761e-01,  1.7325e-02, -4.1968e-01,  ...,  9.7598e-01,\n",
      "           4.0932e-01, -2.7011e-01],\n",
      "         [-3.3217e-01,  1.2116e-01, -3.4333e-01,  ...,  7.8955e-01,\n",
      "           3.7391e-01, -1.7354e-01],\n",
      "         ...,\n",
      "         [-2.8156e-01, -5.3003e-02, -1.4347e-01,  ...,  7.1986e-01,\n",
      "           4.8960e-01, -1.0743e-01],\n",
      "         [-2.1695e-01,  3.8055e-01,  2.9385e-03,  ...,  7.2048e-01,\n",
      "           3.9090e-01, -6.7434e-01],\n",
      "         [-2.4279e-01,  2.9067e-01, -1.2395e-01,  ...,  7.5592e-01,\n",
      "           1.9924e-01, -1.5184e-01]],\n",
      "\n",
      "        [[ 3.1048e-01, -1.7817e-01,  1.5324e-01,  ...,  5.5097e-01,\n",
      "          -2.5331e-01, -3.7453e-01],\n",
      "         [ 1.8019e-01, -2.7267e-01, -5.2234e-02,  ...,  6.8152e-01,\n",
      "          -1.3113e-01, -4.6099e-01],\n",
      "         [ 3.0520e-01, -9.6166e-02, -4.0197e-02,  ...,  6.3060e-01,\n",
      "           1.7453e-01, -4.1173e-01],\n",
      "         ...,\n",
      "         [ 3.3720e-01, -3.5169e-01, -8.8168e-02,  ...,  3.9764e-01,\n",
      "           1.7023e-01, -1.9677e-01],\n",
      "         [ 1.8461e-01,  4.0147e-02, -9.4893e-02,  ...,  2.4819e-01,\n",
      "           1.0903e-01, -7.5892e-01],\n",
      "         [ 4.7962e-01, -1.4564e-01,  3.9677e-02,  ...,  5.7274e-01,\n",
      "          -5.2690e-02, -4.5647e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.3849e-01, -2.8463e-01, -9.5693e-02,  ...,  5.4599e-01,\n",
      "          -8.4905e-02, -2.5436e-01],\n",
      "         [ 2.4170e-01, -1.4804e-01, -1.7088e-01,  ...,  4.6711e-01,\n",
      "          -2.8282e-01, -3.4215e-01],\n",
      "         [ 2.0682e-01, -8.4972e-02, -2.0790e-02,  ...,  4.7333e-01,\n",
      "           1.4353e-01, -5.9815e-01],\n",
      "         ...,\n",
      "         [ 1.5563e-01, -4.3639e-02,  2.0670e-01,  ...,  4.9076e-01,\n",
      "          -3.0130e-02, -3.6513e-01],\n",
      "         [ 2.7968e-01, -3.5381e-01,  1.5819e-01,  ...,  2.5843e-01,\n",
      "          -1.2368e-01, -5.1082e-01],\n",
      "         [ 2.4363e-01, -1.4129e-01,  5.5618e-02,  ...,  4.4444e-01,\n",
      "          -1.4861e-01, -4.8051e-01]],\n",
      "\n",
      "        [[-1.4779e-01, -8.1182e-03, -2.5230e-01,  ...,  7.8105e-01,\n",
      "           2.5672e-01,  2.2325e-02],\n",
      "         [-4.1537e-01,  1.9369e-01, -3.2960e-01,  ...,  6.4634e-01,\n",
      "           3.2244e-01, -1.0244e-01],\n",
      "         [-1.4693e-01,  1.2717e-01, -2.6717e-01,  ...,  6.7466e-01,\n",
      "           3.7405e-01,  1.2345e-01],\n",
      "         ...,\n",
      "         [-9.8221e-02, -5.5262e-02,  2.0529e-01,  ...,  3.8613e-01,\n",
      "           3.1536e-01, -1.0605e-01],\n",
      "         [-2.8278e-01,  3.6244e-01,  1.6066e-02,  ...,  3.4036e-01,\n",
      "           4.0010e-01, -2.7065e-01],\n",
      "         [-3.2603e-01,  3.0940e-01, -2.2972e-01,  ...,  5.5421e-01,\n",
      "           2.2772e-01,  2.5889e-02]],\n",
      "\n",
      "        [[ 1.5924e-01, -5.2181e-02, -1.7950e-01,  ..., -6.6420e-02,\n",
      "          -2.7748e-01,  4.6208e-01],\n",
      "         [-1.5688e-02,  8.3090e-03, -5.3023e-01,  ..., -1.1162e-01,\n",
      "          -3.8108e-01,  4.7434e-01],\n",
      "         [ 1.9111e-01,  6.7198e-04, -3.9467e-01,  ...,  8.0548e-02,\n",
      "           3.9794e-02,  4.4051e-01],\n",
      "         ...,\n",
      "         [ 2.0728e-01, -1.4478e-01, -3.4268e-01,  ..., -3.7269e-01,\n",
      "          -1.4916e-01,  2.9980e-01],\n",
      "         [ 1.3630e-01,  1.8542e-01, -4.6352e-01,  ..., -3.5456e-01,\n",
      "          -1.9190e-01,  1.7133e-01],\n",
      "         [ 2.2368e-01, -1.9715e-01, -2.8113e-01,  ...,  2.1169e-02,\n",
      "          -8.6928e-02,  4.6413e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1968525648117065\n",
      "Model outputs:  tensor([[[-0.3358, -0.0796, -0.5210,  ...,  0.7524,  0.1714,  0.0256],\n",
      "         [-0.5433,  0.3607, -0.1530,  ...,  0.7989,  0.6291,  0.0451],\n",
      "         [-0.0789,  0.3893, -0.2436,  ...,  0.8782,  0.3537, -0.0136],\n",
      "         ...,\n",
      "         [-0.3699,  0.3348, -0.3770,  ...,  0.7886,  0.1195, -0.1313],\n",
      "         [-0.0111, -0.1643, -0.5882,  ...,  0.9268,  0.5642, -0.1011],\n",
      "         [-0.1403,  0.0176, -0.4037,  ...,  0.7720,  0.1123,  0.1320]],\n",
      "\n",
      "        [[ 0.0921, -0.0435, -0.5218,  ...,  0.2707, -0.0240,  0.5035],\n",
      "         [ 0.1914,  0.0810, -0.6497,  ...,  0.1726,  0.2263,  0.3120],\n",
      "         [ 0.0038,  0.0142, -0.5183,  ...,  0.1879,  0.1797,  0.2297],\n",
      "         ...,\n",
      "         [-0.1656,  0.2846, -0.5500,  ...,  0.0943, -0.0413,  0.2691],\n",
      "         [-0.0446, -0.0760, -0.6235,  ...,  0.1747,  0.1719,  0.3530],\n",
      "         [ 0.2545, -0.1493, -0.4086,  ...,  0.2387,  0.2076,  0.5082]],\n",
      "\n",
      "        [[ 0.2642, -0.2224, -0.2417,  ...,  0.4724, -0.2824, -0.0584],\n",
      "         [ 0.2047, -0.0414, -0.0943,  ...,  0.7354,  0.3747, -0.3525],\n",
      "         [ 0.0493, -0.3099,  0.1158,  ...,  0.4660,  0.1139, -0.7361],\n",
      "         ...,\n",
      "         [-0.1246, -0.2263, -0.0137,  ...,  0.4203, -0.1403, -0.2694],\n",
      "         [ 0.1878, -0.2441,  0.0275,  ...,  0.2906, -0.1661, -0.2482],\n",
      "         [ 0.4827, -0.3644, -0.3588,  ...,  0.5626, -0.0654, -0.3528]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5906,  0.0217, -0.8675,  ...,  0.9749,  0.3417,  0.1321],\n",
      "         [-0.3379,  0.1606, -0.5044,  ...,  0.6404,  0.4718,  0.2409],\n",
      "         [-0.4429,  0.0740, -0.4923,  ...,  0.6187,  0.4459,  0.1331],\n",
      "         ...,\n",
      "         [-0.6766,  0.4115, -0.3365,  ...,  0.5354,  0.3235,  0.0129],\n",
      "         [-0.2546,  0.1453, -0.7436,  ...,  0.6620,  0.3670,  0.3103],\n",
      "         [-0.1236,  0.1342, -0.6787,  ...,  0.6180,  0.3156,  0.2379]],\n",
      "\n",
      "        [[-0.2417,  0.0864, -0.8588,  ...,  0.4837,  0.0654,  0.4124],\n",
      "         [-0.0526,  0.3218, -0.8211,  ...,  0.5225,  0.1000,  0.2365],\n",
      "         [-0.2704,  0.1722, -0.6755,  ...,  0.5371,  0.1100,  0.0947],\n",
      "         ...,\n",
      "         [-0.0442,  0.3057, -0.7916,  ...,  0.4579, -0.0994,  0.1422],\n",
      "         [-0.1066,  0.1955, -0.6673,  ...,  0.2688, -0.0680,  0.2025],\n",
      "         [ 0.1734, -0.0504, -0.5356,  ...,  0.5990, -0.1597,  0.2760]],\n",
      "\n",
      "        [[ 0.1577, -0.2257, -0.4323,  ...,  0.0211,  0.4310, -0.0690],\n",
      "         [ 0.2431, -0.1975, -0.2707,  ..., -0.1198,  0.4655, -0.4244],\n",
      "         [ 0.2162, -0.2800, -0.1754,  ...,  0.1643,  0.3414, -0.6880],\n",
      "         ...,\n",
      "         [-0.1172, -0.0265, -0.1927,  ..., -0.3561,  0.5196,  0.2584],\n",
      "         [ 0.1646, -0.1187, -0.4006,  ..., -0.2891,  0.6019,  0.4654],\n",
      "         [ 0.1386, -0.2394, -0.2634,  ..., -0.1526,  0.7007,  0.4371]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0324476957321167\n",
      "Model outputs:  tensor([[[ 2.3304e-01,  2.8737e-01, -3.1616e-01,  ..., -4.8435e-01,\n",
      "           1.2919e-01,  1.7483e-01],\n",
      "         [ 1.7512e-01,  1.6121e-02, -1.8208e-01,  ..., -3.1177e-01,\n",
      "          -3.9418e-02,  1.0633e-01],\n",
      "         [ 1.1044e-01,  3.8165e-02, -2.5916e-01,  ..., -2.6218e-01,\n",
      "          -6.8182e-03,  2.1220e-01],\n",
      "         ...,\n",
      "         [ 1.6727e-01,  3.5089e-02,  3.1145e-01,  ..., -4.8820e-01,\n",
      "          -3.7655e-01,  2.2571e-01],\n",
      "         [-1.0350e-01,  1.0024e-01,  2.0444e-01,  ..., -1.4225e-01,\n",
      "          -4.1037e-03,  3.0014e-01],\n",
      "         [ 2.2961e-01,  5.2480e-02, -4.5505e-01,  ..., -4.2761e-01,\n",
      "          -1.8293e-01,  1.3187e-01]],\n",
      "\n",
      "        [[-2.1197e-01,  5.1080e-01,  5.8835e-02,  ...,  3.6032e-01,\n",
      "           3.0460e-01, -4.4982e-01],\n",
      "         [ 5.0620e-02,  4.4616e-01, -1.4351e-01,  ...,  3.4065e-01,\n",
      "           3.4854e-01,  7.7239e-04],\n",
      "         [-1.5856e-01,  3.1584e-01, -1.5446e-01,  ...,  6.4457e-01,\n",
      "           4.0840e-01, -3.3151e-01],\n",
      "         ...,\n",
      "         [-7.1243e-02, -7.5558e-02,  6.1569e-01,  ...,  2.8578e-01,\n",
      "           8.4267e-03, -3.5294e-02],\n",
      "         [ 1.4840e-02,  3.9182e-01,  3.4707e-01,  ...,  5.4087e-01,\n",
      "           3.1913e-01, -2.3286e-01],\n",
      "         [-1.7846e-01,  3.5519e-01, -3.8372e-01,  ...,  4.8796e-01,\n",
      "           2.4885e-01,  8.9065e-03]],\n",
      "\n",
      "        [[-1.8761e-01,  3.4050e-01,  1.4173e-01,  ...,  1.0396e-01,\n",
      "           3.2683e-01, -4.4856e-01],\n",
      "         [-2.0040e-01,  2.3874e-01, -2.4381e-01,  ...,  1.8537e-01,\n",
      "           5.1564e-01, -2.4762e-01],\n",
      "         [-9.2497e-02,  6.7514e-01,  8.0818e-03,  ...,  6.9229e-01,\n",
      "           5.7806e-01, -4.9087e-01],\n",
      "         ...,\n",
      "         [ 2.2013e-02, -7.8485e-03,  2.6091e-01,  ...,  3.4753e-01,\n",
      "           3.1649e-02, -1.2302e-01],\n",
      "         [-2.4008e-01,  3.7033e-01,  4.3455e-01,  ...,  4.6097e-01,\n",
      "           1.7987e-01, -5.5357e-01],\n",
      "         [-1.9282e-01,  6.3490e-01, -3.6201e-01,  ...,  4.3444e-01,\n",
      "           3.9404e-01, -3.2192e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.5934e-01,  3.6024e-01, -2.8967e-01,  ...,  2.5221e-01,\n",
      "           2.0595e-01, -3.9603e-01],\n",
      "         [-6.1818e-01,  3.4344e-01, -8.2638e-02,  ...,  4.8033e-01,\n",
      "           4.4828e-01, -2.1799e-01],\n",
      "         [-4.4435e-01,  2.6346e-01, -2.7324e-01,  ...,  3.9928e-01,\n",
      "           3.1260e-01, -3.4518e-01],\n",
      "         ...,\n",
      "         [ 7.8557e-02, -3.1875e-01,  5.4607e-01,  ...,  3.2396e-01,\n",
      "           3.1290e-02, -3.9022e-03],\n",
      "         [-3.6356e-01,  2.1735e-01,  9.2543e-02,  ...,  2.9245e-01,\n",
      "          -9.9938e-02, -7.5391e-02],\n",
      "         [-1.7350e-01,  5.3812e-01, -2.6952e-01,  ...,  2.7506e-01,\n",
      "           4.1782e-01, -2.2257e-01]],\n",
      "\n",
      "        [[-2.6936e-01,  2.6059e-01, -2.9471e-01,  ...,  7.5259e-02,\n",
      "           4.9956e-01, -1.4968e-01],\n",
      "         [-5.2407e-01,  3.1282e-01, -3.6454e-01,  ...,  1.7918e-01,\n",
      "           4.1532e-01, -1.0944e-01],\n",
      "         [-2.7173e-01,  1.4326e-01, -3.2161e-01,  ...,  3.0636e-01,\n",
      "           5.3152e-01, -1.4581e-01],\n",
      "         ...,\n",
      "         [-1.5627e-01,  1.2276e-01, -1.5207e-01,  ..., -1.5641e-01,\n",
      "          -2.0302e-01, -8.9049e-03],\n",
      "         [-4.9116e-01,  2.9802e-01,  5.1952e-02,  ..., -1.7743e-01,\n",
      "           3.0975e-01,  2.2488e-01],\n",
      "         [-5.3147e-01,  3.5713e-01, -2.5984e-01,  ...,  6.3474e-02,\n",
      "           1.3742e-01, -1.5732e-01]],\n",
      "\n",
      "        [[-3.9819e-02,  4.2267e-01, -9.4520e-01,  ...,  3.1790e-01,\n",
      "           3.3946e-01, -4.8480e-01],\n",
      "         [-2.5321e-01,  5.9202e-01, -8.1341e-01,  ..., -2.7449e-02,\n",
      "           3.3543e-01, -1.7118e-01],\n",
      "         [-3.0060e-01,  4.1347e-01, -7.3068e-01,  ...,  3.6342e-01,\n",
      "           5.0268e-01, -1.7176e-01],\n",
      "         ...,\n",
      "         [ 1.0366e-02,  1.2432e-01, -2.5186e-01,  ...,  1.3039e-01,\n",
      "           1.0599e-01, -1.3291e-01],\n",
      "         [-5.2368e-01,  1.7753e-01, -3.1234e-02,  ..., -5.3560e-02,\n",
      "          -1.4682e-01, -1.2358e-01],\n",
      "         [-2.6320e-01,  3.0260e-01, -7.6107e-01,  ...,  2.7741e-01,\n",
      "           2.5161e-01, -2.2829e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2827024459838867\n",
      "Model outputs:  tensor([[[-0.1829,  0.2518, -0.6465,  ...,  0.4515, -0.0404,  0.2448],\n",
      "         [-0.3281,  0.2640, -0.6856,  ...,  0.5377, -0.0333,  0.2198],\n",
      "         [ 0.0793, -0.0843, -0.4318,  ...,  0.3749, -0.1162,  0.3665],\n",
      "         ...,\n",
      "         [-0.2609,  0.3372, -0.5217,  ...,  0.2156,  0.1609,  0.0545],\n",
      "         [-0.0514,  0.1687, -0.7425,  ...,  0.5602,  0.0589,  0.2589],\n",
      "         [-0.2348,  0.3249, -0.5976,  ...,  0.1860, -0.0562,  0.1962]],\n",
      "\n",
      "        [[-0.7348,  0.3509, -0.3638,  ...,  0.4973,  0.3391,  0.0692],\n",
      "         [-0.6135,  0.4671, -0.4106,  ...,  0.5624,  0.6078,  0.0842],\n",
      "         [-0.3115,  0.1196, -0.2866,  ...,  0.4107,  0.4340,  0.1928],\n",
      "         ...,\n",
      "         [-0.4420,  0.3197, -0.5307,  ...,  0.5565,  0.3087,  0.3829],\n",
      "         [-0.4370,  0.2214, -0.6705,  ...,  0.5215,  0.3655,  0.2074],\n",
      "         [-0.9214,  0.4864, -0.6189,  ...,  0.2666,  0.3398,  0.2151]],\n",
      "\n",
      "        [[-0.7306,  0.3240, -0.1902,  ...,  0.3811,  0.2646, -0.0462],\n",
      "         [-0.7891,  0.4779, -0.5380,  ...,  0.4016,  0.3580,  0.0107],\n",
      "         [-0.4729, -0.1015, -0.2404,  ...,  0.3802,  0.2519, -0.0093],\n",
      "         ...,\n",
      "         [-0.4275,  0.2121, -0.0484,  ...,  0.2010,  0.5168,  0.0117],\n",
      "         [-0.5786,  0.2590, -0.4567,  ...,  0.5560,  0.1968,  0.4885],\n",
      "         [-0.6365,  0.5357, -0.1274,  ...,  0.3034,  0.3937,  0.1423]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3353,  0.0508, -0.7524,  ...,  0.4065,  0.1289,  0.0818],\n",
      "         [-0.4300,  0.2642, -0.5873,  ...,  0.5356,  0.0327,  0.0245],\n",
      "         [ 0.2565, -0.2042, -0.5190,  ...,  0.4752,  0.1492,  0.4312],\n",
      "         ...,\n",
      "         [-0.2397,  0.3537, -0.4854,  ..., -0.0075,  0.1374,  0.0531],\n",
      "         [-0.0896,  0.1138, -0.6439,  ...,  0.4276, -0.1899,  0.2341],\n",
      "         [-0.2309,  0.3824, -0.8301,  ...,  0.2389, -0.2425,  0.1663]],\n",
      "\n",
      "        [[-0.4517,  0.4280, -0.7412,  ...,  1.0328,  0.3902,  0.0483],\n",
      "         [-0.6025,  0.3554, -0.6709,  ...,  0.8259,  0.3041,  0.0288],\n",
      "         [-0.0633,  0.2277, -0.5766,  ...,  0.8260,  0.3662, -0.0019],\n",
      "         ...,\n",
      "         [-0.5415,  0.2220, -0.3669,  ...,  0.5427,  0.2392, -0.2155],\n",
      "         [-0.4003,  0.4059, -0.8544,  ...,  0.7048,  0.2999,  0.2721],\n",
      "         [-0.6793,  0.6532, -0.5637,  ...,  0.8127,  0.3678,  0.1149]],\n",
      "\n",
      "        [[-0.3794,  0.3162, -0.6638,  ...,  0.5397,  0.0602,  0.1666],\n",
      "         [-0.5805,  0.1030, -0.8302,  ...,  0.6608,  0.1509,  0.0602],\n",
      "         [-0.2732,  0.1410, -0.7867,  ...,  0.5392,  0.1561, -0.1781],\n",
      "         ...,\n",
      "         [-0.3180,  0.0971, -0.6441,  ...,  0.2602, -0.0519,  0.0499],\n",
      "         [-0.2086,  0.4876, -0.9527,  ...,  0.6658,  0.0019, -0.1561],\n",
      "         [-0.5093,  0.5106, -0.7845,  ...,  0.5692,  0.1628,  0.2899]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8718186020851135\n",
      "Model outputs:  tensor([[[-2.4707e-02, -1.2023e-01, -2.8817e-01,  ...,  1.4532e-01,\n",
      "          -3.4086e-01,  5.0566e-01],\n",
      "         [ 1.4554e-01, -1.2780e-01, -3.5127e-01,  ..., -9.5893e-03,\n",
      "          -3.5622e-02,  5.4147e-01],\n",
      "         [-1.1416e-01, -4.0355e-02, -2.9702e-01,  ..., -4.5863e-01,\n",
      "          -1.5068e-01,  4.3575e-01],\n",
      "         ...,\n",
      "         [-4.6565e-02,  1.5791e-01, -6.1788e-01,  ...,  9.9669e-03,\n",
      "          -1.6258e-01,  6.2228e-01],\n",
      "         [ 1.3710e-01, -1.6544e-01, -3.3214e-01,  ...,  5.8829e-02,\n",
      "          -3.5439e-01,  5.6270e-01],\n",
      "         [ 1.2193e-01, -9.7222e-02, -5.0754e-01,  ...,  7.7830e-02,\n",
      "          -2.0564e-01,  4.2689e-01]],\n",
      "\n",
      "        [[-8.7984e-02,  3.6687e-02, -5.2051e-01,  ...,  5.9733e-01,\n",
      "          -2.5744e-01,  1.3375e-01],\n",
      "         [-2.0927e-01,  2.7073e-01, -8.7212e-01,  ...,  4.2863e-01,\n",
      "          -1.1494e-01,  2.5735e-01],\n",
      "         [-1.6134e-02,  1.8507e-01, -6.4274e-01,  ...,  2.0046e-01,\n",
      "           1.2464e-01,  7.6178e-02],\n",
      "         ...,\n",
      "         [-6.1479e-02,  2.1093e-01, -7.3427e-01,  ...,  8.0067e-01,\n",
      "          -5.0893e-04,  6.1263e-02],\n",
      "         [ 1.0637e-01,  8.0317e-02, -7.3397e-01,  ...,  6.6719e-01,\n",
      "           1.0418e-01, -1.2366e-01],\n",
      "         [-1.5736e-01,  1.7273e-01, -7.6850e-01,  ...,  2.1049e-01,\n",
      "           1.0212e-01,  8.1172e-02]],\n",
      "\n",
      "        [[ 3.0254e-03, -1.1837e-01, -5.1035e-01,  ...,  2.3693e-01,\n",
      "           1.4396e-02,  4.2868e-01],\n",
      "         [-1.1661e-01,  2.1696e-01, -6.3393e-01,  ...,  1.6670e-01,\n",
      "          -4.0950e-01,  1.8837e-01],\n",
      "         [-3.9843e-02,  1.5805e-01, -5.7003e-01,  ..., -5.1285e-01,\n",
      "          -4.5465e-02,  4.7839e-01],\n",
      "         ...,\n",
      "         [-8.9116e-03,  1.8366e-01, -7.4702e-01,  ...,  3.8312e-01,\n",
      "          -8.9481e-02,  2.3336e-01],\n",
      "         [ 1.6551e-01,  6.2608e-02, -4.4698e-01,  ...,  2.0376e-01,\n",
      "           5.6444e-02,  3.9907e-01],\n",
      "         [-5.0163e-01, -3.1458e-02, -5.5745e-01,  ...,  1.2732e-01,\n",
      "          -9.6816e-02,  2.6948e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.5971e-01, -3.9241e-02, -7.3985e-01,  ...,  2.1941e-01,\n",
      "          -1.4472e-01,  2.9953e-01],\n",
      "         [-3.9952e-01, -1.1184e-01, -7.6821e-01,  ...,  1.3902e-01,\n",
      "          -8.5094e-02,  5.0881e-01],\n",
      "         [-2.9981e-01, -3.5747e-02, -4.4914e-01,  ..., -2.9079e-01,\n",
      "           1.2978e-01,  3.0804e-01],\n",
      "         ...,\n",
      "         [-4.4514e-01,  4.5528e-01, -9.7373e-01,  ...,  2.6415e-01,\n",
      "           1.5825e-01,  1.5201e-02],\n",
      "         [-2.5795e-01, -2.1122e-01, -8.7696e-01,  ...,  4.1413e-01,\n",
      "           1.9377e-01,  4.8392e-01],\n",
      "         [-6.9095e-01,  1.3706e-01, -9.4957e-01,  ...,  1.5158e-01,\n",
      "           1.1967e-01,  5.2429e-02]],\n",
      "\n",
      "        [[-1.9900e-02, -7.1336e-02, -4.4517e-01,  ...,  3.9605e-02,\n",
      "          -3.0810e-01,  2.6452e-01],\n",
      "         [-6.8744e-02,  6.3833e-02, -4.5403e-01,  ...,  9.2697e-02,\n",
      "          -8.2618e-02,  5.1746e-01],\n",
      "         [-6.6418e-03, -5.6445e-02, -2.4461e-01,  ..., -4.2110e-01,\n",
      "          -5.0184e-01, -5.5631e-02],\n",
      "         ...,\n",
      "         [ 1.7050e-01,  1.0132e-01, -6.2303e-01,  ...,  1.8544e-01,\n",
      "          -2.2334e-01,  4.7086e-01],\n",
      "         [ 7.8366e-03, -3.2140e-01, -6.3187e-01,  ...,  4.9487e-01,\n",
      "          -2.4713e-01,  5.6056e-01],\n",
      "         [-2.6807e-01, -7.2385e-02, -5.4902e-01,  ..., -3.2811e-01,\n",
      "          -1.8225e-01,  3.2043e-01]],\n",
      "\n",
      "        [[-4.0536e-01,  8.7005e-02, -4.5486e-01,  ...,  7.7067e-01,\n",
      "           2.3927e-01, -6.8265e-02],\n",
      "         [-4.8076e-01,  2.3416e-01, -3.8019e-01,  ...,  8.7648e-01,\n",
      "           3.6694e-01,  1.4890e-01],\n",
      "         [-2.3836e-01,  4.5766e-02,  7.2458e-03,  ...,  4.9791e-01,\n",
      "           1.8567e-01,  9.8210e-02],\n",
      "         ...,\n",
      "         [-4.8492e-01,  3.3358e-01, -4.6012e-01,  ...,  1.0445e+00,\n",
      "           1.2892e-01,  6.1249e-03],\n",
      "         [-1.9139e-01, -1.0025e-01, -2.1743e-01,  ...,  9.5802e-01,\n",
      "           2.9625e-01,  2.0075e-02],\n",
      "         [-3.1842e-01,  5.6858e-01, -1.5620e-01,  ...,  6.0209e-01,\n",
      "           4.6204e-01, -1.8035e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1051713228225708\n",
      "Model outputs:  tensor([[[-0.3308, -0.3141, -0.4424,  ...,  0.7801,  0.2336, -0.0284],\n",
      "         [-0.1823,  0.2617, -0.3413,  ...,  0.5021,  0.1927, -0.0319],\n",
      "         [-0.1988,  0.0950, -0.0078,  ...,  0.7223,  0.1370, -0.1377],\n",
      "         ...,\n",
      "         [-0.1147, -0.1033,  0.0369,  ...,  0.8513,  0.2966, -0.2155],\n",
      "         [-0.2522,  0.3684, -0.1612,  ...,  0.7541,  0.4091, -0.2630],\n",
      "         [-0.3012,  0.2187, -0.3105,  ...,  0.4352,  0.5137, -0.5152]],\n",
      "\n",
      "        [[ 0.3892, -0.2115, -0.3929,  ..., -0.0161,  0.4356,  0.6139],\n",
      "         [ 0.3632,  0.1702, -0.6284,  ..., -0.3311,  0.7173,  0.3908],\n",
      "         [ 0.1629, -0.0763, -0.3540,  ..., -0.3380,  0.4370,  0.2678],\n",
      "         ...,\n",
      "         [ 0.3751, -0.2912, -0.1011,  ...,  0.0403,  0.3371,  0.4548],\n",
      "         [ 0.2201,  0.1043, -0.3765,  ...,  0.0921,  0.3921,  0.3252],\n",
      "         [ 0.3438, -0.1351, -0.5222,  ..., -0.3227,  0.4378,  0.3437]],\n",
      "\n",
      "        [[-0.0291, -0.2265, -0.5956,  ...,  0.1216, -0.1936,  0.6123],\n",
      "         [ 0.1356,  0.2881, -0.5957,  ...,  0.1455,  0.0275,  0.0544],\n",
      "         [-0.1672,  0.1940, -0.5064,  ..., -0.1761,  0.1375,  0.1774],\n",
      "         ...,\n",
      "         [ 0.0188, -0.0636, -0.3157,  ...,  0.1028,  0.1997,  0.2211],\n",
      "         [ 0.0997, -0.0040, -0.5323,  ..., -0.0405,  0.1698,  0.4385],\n",
      "         [-0.0802,  0.0363, -0.6364,  ..., -0.2162,  0.0377,  0.4704]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0756, -0.1606, -0.8968,  ...,  0.8926,  0.0910,  0.0848],\n",
      "         [-0.5214,  0.1048, -0.8550,  ...,  0.7021,  0.3239,  0.1134],\n",
      "         [-0.2631,  0.3752, -0.6499,  ...,  0.6248,  0.3813, -0.0698],\n",
      "         ...,\n",
      "         [-0.0371,  0.0695, -0.3390,  ...,  0.8314,  0.2593,  0.0171],\n",
      "         [-0.3064,  0.1696, -0.5583,  ...,  0.7903,  0.2607,  0.2548],\n",
      "         [-0.4496,  0.3409, -0.7951,  ...,  0.3572,  0.2469, -0.0331]],\n",
      "\n",
      "        [[ 0.0985, -0.3052, -0.0084,  ...,  0.6921, -0.2252, -0.1738],\n",
      "         [ 0.2106, -0.3674,  0.2245,  ...,  0.3599, -0.1518, -0.6467],\n",
      "         [ 0.1172, -0.1124,  0.1718,  ...,  0.1374, -0.1833, -0.6232],\n",
      "         ...,\n",
      "         [ 0.5277, -0.4774,  0.2057,  ...,  0.8202,  0.0275, -0.4710],\n",
      "         [ 0.2646, -0.0534,  0.0043,  ...,  0.5144,  0.0453, -0.2380],\n",
      "         [ 0.2911,  0.0201,  0.1447,  ...,  0.1746, -0.0738, -0.4941]],\n",
      "\n",
      "        [[ 0.1475,  0.0409, -0.3278,  ...,  0.0256, -0.1602,  0.6833],\n",
      "         [-0.0327,  0.1514, -0.3800,  ..., -0.1503, -0.1236,  0.2712],\n",
      "         [ 0.3390, -0.1134, -0.2345,  ..., -0.2154, -0.2802,  0.1754],\n",
      "         ...,\n",
      "         [ 0.2011, -0.2211, -0.0601,  ..., -0.0692, -0.3862,  0.5693],\n",
      "         [ 0.0795,  0.0325, -0.4826,  ..., -0.0563, -0.2287,  0.2560],\n",
      "         [ 0.2519, -0.2556, -0.4253,  ..., -0.4358, -0.2584,  0.4228]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1750824451446533\n",
      "Model outputs:  tensor([[[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 2.0691e-01, -2.1245e-01, -1.4930e-01,  ...,  2.6023e-01,\n",
      "           2.5874e-01, -2.8146e-01],\n",
      "         [ 4.1543e-01, -2.3211e-01, -3.3293e-01,  ..., -6.2626e-02,\n",
      "           2.9712e-01, -1.4485e-01],\n",
      "         ...,\n",
      "         [-9.6937e-03,  5.5263e-02, -5.9441e-01,  ..., -6.1629e-02,\n",
      "           7.3879e-01,  4.6012e-01],\n",
      "         [-7.9649e-02,  1.5706e-01, -5.1684e-01,  ..., -1.4531e-01,\n",
      "           3.8016e-01,  4.5694e-01],\n",
      "         [ 1.4588e-01,  1.3308e-01, -5.7066e-01,  ..., -1.9887e-01,\n",
      "           4.9377e-01,  6.7629e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [-1.6315e-01,  9.2886e-02, -2.5964e-02,  ...,  4.7738e-01,\n",
      "           5.1089e-02, -1.3261e-01],\n",
      "         [-4.6793e-01,  9.7294e-02, -5.6864e-01,  ...,  5.1822e-01,\n",
      "           2.0560e-01,  1.8891e-01],\n",
      "         ...,\n",
      "         [-5.5949e-01,  2.2627e-01, -7.0027e-01,  ...,  5.5925e-01,\n",
      "           3.9187e-01,  2.5410e-01],\n",
      "         [-1.6472e-01,  3.7190e-01, -5.5991e-01,  ...,  3.6543e-01,\n",
      "           3.9784e-01,  1.4442e-01],\n",
      "         [-2.9518e-01,  1.2082e-01, -4.4410e-01,  ...,  3.5176e-01,\n",
      "           1.4460e-01,  3.3769e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 2.4454e-01, -1.8353e-01,  7.9036e-02,  ...,  6.2895e-01,\n",
      "          -1.2177e-01, -6.2058e-01],\n",
      "         [ 3.2163e-01, -3.2799e-01, -2.1498e-01,  ...,  4.9031e-01,\n",
      "          -3.9656e-01, -2.7667e-01],\n",
      "         ...,\n",
      "         [ 2.9342e-01, -9.5890e-02, -3.3499e-01,  ...,  6.9470e-01,\n",
      "           1.5234e-01, -2.2271e-01],\n",
      "         [ 2.8088e-01,  1.9840e-02, -6.5138e-02,  ...,  5.1234e-01,\n",
      "          -6.5491e-02, -4.2240e-01],\n",
      "         [ 2.8289e-01, -8.1670e-02, -2.5850e-01,  ...,  6.0364e-01,\n",
      "           8.5841e-02, -6.6494e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 1.2159e-01,  8.1441e-02, -1.4706e-01,  ...,  3.5177e-02,\n",
      "          -3.4853e-01,  1.3189e-01],\n",
      "         [ 1.3163e-02,  1.7659e-01, -4.4481e-01,  ...,  6.6824e-02,\n",
      "          -2.1685e-01,  2.0682e-01],\n",
      "         ...,\n",
      "         [-4.1433e-02, -2.2450e-01, -8.8043e-01,  ...,  4.9932e-02,\n",
      "          -2.0305e-01,  5.3175e-01],\n",
      "         [-1.2465e-01,  2.1273e-01, -8.2799e-01,  ...,  8.6184e-03,\n",
      "          -2.9546e-01,  2.6631e-01],\n",
      "         [-5.9561e-04,  9.5326e-02, -6.0534e-01,  ...,  2.5041e-02,\n",
      "          -1.5386e-01,  3.2656e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 4.1564e-01, -5.4764e-02, -3.0278e-01,  ...,  2.6747e-01,\n",
      "          -2.2050e-01,  2.2700e-01],\n",
      "         [-8.4703e-02, -2.2688e-01, -4.8932e-01,  ...,  2.1798e-01,\n",
      "           1.2565e-02,  4.0240e-01],\n",
      "         ...,\n",
      "         [ 1.5228e-03, -1.2954e-01, -7.6230e-01,  ...,  2.4885e-01,\n",
      "          -3.1751e-02,  2.1778e-01],\n",
      "         [-9.6932e-02,  2.2819e-01, -8.4491e-01,  ...,  3.2809e-02,\n",
      "          -2.6270e-01,  2.7615e-01],\n",
      "         [ 6.7012e-02,  1.5649e-01, -8.4322e-01,  ...,  2.0589e-01,\n",
      "           7.5984e-02,  4.7652e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 1.7358e-01,  2.5596e-01, -6.6770e-01,  ...,  3.8656e-01,\n",
      "          -4.3092e-01,  1.6316e-01],\n",
      "         [ 1.4556e-01, -2.0730e-02, -6.9211e-01,  ...,  4.0021e-01,\n",
      "           4.3019e-02,  7.5253e-02],\n",
      "         ...,\n",
      "         [-2.0501e-01, -6.6311e-02, -9.5749e-01,  ...,  3.8143e-01,\n",
      "          -6.3317e-02,  1.6276e-01],\n",
      "         [-1.3637e-02,  5.2543e-01, -1.0865e+00,  ...,  5.0868e-01,\n",
      "          -8.3017e-02, -1.3191e-01],\n",
      "         [-6.2343e-02,  2.4772e-01, -7.2255e-01,  ...,  3.5642e-01,\n",
      "          -2.6553e-01,  1.4514e-02]]], grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [4/10], Loss: 1.1751\n",
      "Model outputs:  tensor([[[-2.5992e-02,  8.8732e-02,  3.5960e-02,  ...,  2.4382e-01,\n",
      "          -2.7423e-01, -7.4038e-01],\n",
      "         [ 1.5567e-02, -3.2317e-01,  2.2206e-01,  ...,  3.9608e-01,\n",
      "          -2.8236e-01, -6.9078e-01],\n",
      "         [-1.3628e-01, -4.9957e-02,  1.1947e-01,  ...,  3.3304e-01,\n",
      "           1.4179e-01, -5.8391e-01],\n",
      "         ...,\n",
      "         [ 4.1204e-01, -8.4671e-02,  2.6380e-01,  ...,  1.5599e-01,\n",
      "          -1.3571e-01, -5.7487e-01],\n",
      "         [ 1.6830e-01, -1.1818e-01,  4.1725e-02,  ...,  6.0855e-01,\n",
      "           2.6603e-01, -4.8451e-01],\n",
      "         [ 3.5715e-01, -2.8837e-01,  7.7446e-02,  ...,  4.7744e-01,\n",
      "          -9.7736e-02, -7.0873e-01]],\n",
      "\n",
      "        [[-6.2738e-01,  5.4968e-01, -4.6515e-01,  ...,  7.3906e-01,\n",
      "           1.6436e-01, -6.8837e-02],\n",
      "         [-6.1545e-01,  1.2322e-01, -3.1411e-01,  ...,  4.2904e-01,\n",
      "           1.9570e-01, -9.6856e-02],\n",
      "         [-3.4791e-01,  2.1322e-01, -3.2380e-01,  ...,  3.7730e-01,\n",
      "           1.8271e-01, -1.9403e-01],\n",
      "         ...,\n",
      "         [-2.7960e-01,  1.2641e-01, -3.3311e-01,  ...,  2.3603e-01,\n",
      "           1.4181e-01,  3.3017e-02],\n",
      "         [-4.2640e-01,  4.1840e-02, -4.5877e-01,  ...,  5.4035e-01,\n",
      "           2.6066e-01, -7.3479e-02],\n",
      "         [-1.6329e-01, -7.4122e-02, -3.3659e-02,  ...,  3.0203e-01,\n",
      "           3.1930e-01, -1.9659e-01]],\n",
      "\n",
      "        [[-4.1354e-01,  2.0907e-02, -2.9613e-01,  ..., -1.5202e-01,\n",
      "          -2.9553e-01,  3.8011e-01],\n",
      "         [ 3.6013e-02, -4.7878e-02, -3.7492e-01,  ..., -4.0998e-01,\n",
      "           3.1446e-02,  2.7772e-01],\n",
      "         [ 8.8022e-02,  3.2465e-02, -5.5453e-01,  ..., -1.4455e-01,\n",
      "          -3.7716e-01,  3.0337e-01],\n",
      "         ...,\n",
      "         [ 1.7695e-01,  5.8254e-02, -2.6082e-01,  ..., -3.0638e-01,\n",
      "          -1.7697e-01,  2.9981e-01],\n",
      "         [-1.6645e-01, -1.7100e-01, -3.7905e-01,  ..., -2.0337e-01,\n",
      "          -1.3829e-01,  7.8185e-01],\n",
      "         [ 2.0772e-01, -1.9900e-01, -9.6867e-02,  ..., -4.1116e-01,\n",
      "           1.3268e-01,  3.9673e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.5806e-01,  1.6428e-01, -6.6805e-01,  ..., -4.5402e-02,\n",
      "          -3.7864e-01, -1.2503e-02],\n",
      "         [ 2.2183e-01, -6.5749e-02, -3.7786e-01,  ...,  3.1903e-02,\n",
      "          -1.5954e-01, -4.7006e-02],\n",
      "         [-1.7198e-01,  1.3810e-01, -4.5930e-01,  ..., -1.4026e-01,\n",
      "           6.0452e-02,  3.9242e-02],\n",
      "         ...,\n",
      "         [ 1.4162e-01,  1.8960e-01, -6.0253e-01,  ..., -8.0075e-02,\n",
      "           8.8547e-02, -3.9637e-05],\n",
      "         [-1.3734e-01,  4.1294e-02, -5.7307e-01,  ...,  1.2162e-01,\n",
      "          -1.9469e-01,  2.6646e-01],\n",
      "         [ 3.2582e-01, -6.9631e-02, -4.7122e-01,  ...,  1.1945e-01,\n",
      "          -1.3331e-01, -2.9835e-02]],\n",
      "\n",
      "        [[-5.3736e-01,  2.0488e-01, -7.8176e-01,  ..., -5.2272e-02,\n",
      "          -3.1212e-01,  1.2482e-01],\n",
      "         [-6.4464e-01,  9.0021e-02, -7.1527e-01,  ..., -6.9895e-02,\n",
      "          -1.6632e-02, -8.0679e-02],\n",
      "         [-2.4322e-01,  9.8467e-02, -6.8462e-01,  ..., -2.4876e-01,\n",
      "           2.0497e-02,  1.2687e-01],\n",
      "         ...,\n",
      "         [-1.0471e-01, -1.3986e-01, -5.7043e-01,  ...,  2.7657e-02,\n",
      "           1.1279e-01,  2.2958e-02],\n",
      "         [-5.9362e-01, -7.1838e-02, -6.1360e-01,  ...,  8.4782e-04,\n",
      "           2.6906e-01,  1.0108e-01],\n",
      "         [ 1.7810e-01, -2.8811e-02, -4.5395e-01,  ..., -2.9048e-01,\n",
      "           8.4073e-02,  1.3441e-01]],\n",
      "\n",
      "        [[-3.9028e-01,  4.9143e-01, -9.8041e-02,  ...,  8.0155e-01,\n",
      "          -1.7414e-03, -1.8809e-01],\n",
      "         [-4.8521e-01, -1.5140e-01, -1.0095e-01,  ...,  3.9874e-01,\n",
      "           1.8294e-01, -1.4325e-01],\n",
      "         [-2.7097e-01,  3.0631e-01, -1.8413e-01,  ...,  4.1539e-01,\n",
      "           2.4933e-01, -3.8113e-01],\n",
      "         ...,\n",
      "         [-2.8028e-01,  1.4514e-01, -3.3854e-01,  ...,  5.7869e-01,\n",
      "           1.2282e-01, -2.0470e-01],\n",
      "         [-3.2130e-01, -3.9529e-02, -2.6162e-01,  ...,  7.9059e-01,\n",
      "           2.3762e-01, -2.3566e-02],\n",
      "         [ 1.3327e-01,  1.5789e-01, -8.6988e-02,  ...,  5.9097e-01,\n",
      "           2.4814e-01,  7.5985e-02]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.272770643234253\n",
      "Model outputs:  tensor([[[ 2.7413e-02, -1.1596e-02,  2.4015e-01,  ...,  1.3749e-01,\n",
      "          -2.0937e-01, -5.2146e-01],\n",
      "         [-1.8732e-01,  6.7848e-04,  3.3067e-01,  ...,  2.8159e-01,\n",
      "          -4.0020e-01, -6.2621e-01],\n",
      "         [ 1.0754e-01,  1.8726e-02,  1.5569e-01,  ...,  1.6439e-01,\n",
      "           1.1214e-01, -5.0471e-01],\n",
      "         ...,\n",
      "         [ 1.6236e-01, -1.5226e-01,  1.1252e-01,  ...,  4.0668e-01,\n",
      "          -2.8516e-01, -3.5321e-01],\n",
      "         [ 4.5933e-01, -3.6357e-01,  1.6206e-01,  ...,  2.0076e-01,\n",
      "          -2.2628e-01, -6.7617e-01],\n",
      "         [ 2.7186e-01, -1.2026e-01,  7.4603e-02,  ...,  3.6830e-01,\n",
      "           1.4364e-01, -4.4664e-01]],\n",
      "\n",
      "        [[-1.7124e-01,  1.2604e-01, -2.3686e-01,  ..., -5.5139e-01,\n",
      "          -2.2984e-01,  3.4079e-01],\n",
      "         [-2.4546e-01,  1.5354e-01, -1.9623e-01,  ..., -3.2615e-01,\n",
      "          -2.6389e-02,  1.3680e-01],\n",
      "         [-1.5845e-01,  9.0700e-02, -1.9854e-01,  ..., -2.1248e-01,\n",
      "          -1.3747e-01, -3.4064e-02],\n",
      "         ...,\n",
      "         [-1.1005e-01, -4.0929e-02, -7.1631e-01,  ..., -2.2566e-01,\n",
      "          -1.6337e-02,  2.3224e-01],\n",
      "         [ 6.8905e-02, -1.9683e-01, -1.2259e-01,  ..., -1.4612e-01,\n",
      "           3.2258e-01,  3.9575e-01],\n",
      "         [-2.7712e-01,  1.5855e-01, -5.8277e-01,  ..., -2.7958e-01,\n",
      "          -6.2167e-02,  2.7667e-01]],\n",
      "\n",
      "        [[-2.0294e-01,  6.5397e-02, -4.5367e-01,  ..., -4.2079e-01,\n",
      "           1.1136e-01,  2.0504e-01],\n",
      "         [-3.9956e-01,  9.4486e-02, -3.5413e-01,  ..., -3.4521e-01,\n",
      "          -3.2348e-01,  3.1417e-03],\n",
      "         [-7.8466e-02,  6.3786e-02, -3.5594e-01,  ..., -6.2289e-02,\n",
      "           3.4104e-02,  2.5710e-01],\n",
      "         ...,\n",
      "         [-6.6042e-02,  8.0797e-02, -5.9545e-01,  ..., -1.5351e-02,\n",
      "          -2.9196e-01,  2.1427e-01],\n",
      "         [ 6.5093e-02, -2.0545e-02, -4.3353e-01,  ..., -2.2946e-01,\n",
      "           1.8645e-01,  1.4142e-01],\n",
      "         [-2.4787e-01,  4.4550e-02, -6.7693e-01,  ..., -1.6402e-01,\n",
      "          -2.0173e-01,  3.9141e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.5614e-01,  3.8474e-01, -5.5029e-01,  ...,  1.8169e-01,\n",
      "           1.9762e-01, -2.7417e-02],\n",
      "         [-5.4764e-01,  4.7519e-01, -4.3391e-01,  ..., -8.0014e-02,\n",
      "           1.3478e-01, -8.6759e-02],\n",
      "         [-4.3985e-02, -9.5011e-02, -3.2000e-01,  ...,  9.6081e-02,\n",
      "          -1.4362e-01, -1.2021e-01],\n",
      "         ...,\n",
      "         [-2.4159e-01,  4.5916e-02, -6.5436e-01,  ...,  1.8774e-01,\n",
      "          -1.5590e-01,  6.6799e-02],\n",
      "         [ 7.6294e-02,  6.8948e-02, -5.2985e-01,  ...,  2.5413e-01,\n",
      "          -6.7428e-02,  1.2929e-01],\n",
      "         [-4.3533e-01,  1.8793e-01, -8.7824e-01,  ...,  8.6756e-02,\n",
      "          -1.1312e-01,  1.1911e-01]],\n",
      "\n",
      "        [[-4.5256e-01,  2.8974e-01, -4.7980e-01,  ..., -3.3535e-01,\n",
      "           6.7733e-02,  3.4224e-01],\n",
      "         [-3.8745e-01,  2.4584e-01, -4.9373e-01,  ..., -5.8150e-02,\n",
      "          -5.3779e-02, -4.0393e-02],\n",
      "         [-5.2766e-01, -1.0548e-01, -4.9202e-01,  ...,  3.3614e-02,\n",
      "           6.0480e-02,  1.5325e-01],\n",
      "         ...,\n",
      "         [-4.2826e-01,  2.4749e-01, -7.6651e-01,  ..., -5.3369e-02,\n",
      "           2.1742e-01,  3.2482e-01],\n",
      "         [ 1.2326e-01, -3.0359e-01, -5.6923e-01,  ...,  2.5864e-02,\n",
      "           8.5414e-02,  2.7725e-01],\n",
      "         [-6.2097e-01,  2.4114e-02, -5.7545e-01,  ..., -2.1395e-01,\n",
      "           1.3570e-01,  2.6042e-01]],\n",
      "\n",
      "        [[-2.3649e-01,  5.0483e-01, -7.0697e-01,  ..., -4.1508e-02,\n",
      "           2.2032e-01,  9.3914e-02],\n",
      "         [-2.7074e-01,  1.2230e-01, -4.4871e-01,  ..., -1.7228e-02,\n",
      "          -1.6517e-01,  2.1464e-01],\n",
      "         [-8.7938e-02,  2.3062e-01, -5.1706e-01,  ...,  2.5800e-01,\n",
      "           8.4501e-02, -6.3707e-02],\n",
      "         ...,\n",
      "         [-1.8719e-01,  3.0261e-01, -8.1237e-01,  ...,  1.4640e-01,\n",
      "           1.9059e-01,  3.3888e-01],\n",
      "         [ 2.4932e-01,  3.7098e-02, -6.9547e-01,  ...,  3.6694e-01,\n",
      "           4.8977e-02, -5.5328e-02],\n",
      "         [-2.6874e-02,  1.3831e-01, -8.2518e-01,  ...,  2.8581e-01,\n",
      "           4.4645e-02,  5.7834e-02]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9657251834869385\n",
      "Model outputs:  tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.0801,  0.0900, -0.1258,  ...,  0.0229, -0.2430,  0.2666],\n",
      "         [-0.0117, -0.1116, -0.1079,  ..., -0.0594, -0.1096,  0.1885],\n",
      "         ...,\n",
      "         [ 0.1643,  0.0654, -0.3816,  ..., -0.1500, -0.2893,  0.4169],\n",
      "         [-0.2367,  0.1409, -0.6668,  ..., -0.5468, -0.1684,  0.3640],\n",
      "         [ 0.0137,  0.0517, -0.3432,  ..., -0.0506, -0.1325,  0.2460]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.1313,  0.2577, -0.5940,  ...,  0.6792,  0.0900, -0.0609],\n",
      "         [-0.0466,  0.0524, -0.7229,  ...,  0.8221,  0.0870,  0.0049],\n",
      "         ...,\n",
      "         [-0.3318, -0.0074, -0.8818,  ...,  0.9157,  0.2564,  0.0202],\n",
      "         [-0.3102,  0.4607, -0.8181,  ...,  0.4634,  0.0366, -0.1156],\n",
      "         [-0.2622,  0.2665, -0.8562,  ...,  0.8015,  0.0655,  0.0022]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.1477,  0.2017, -0.7312,  ...,  0.3921, -0.3781,  0.2545],\n",
      "         [-0.1874, -0.0176, -0.6355,  ...,  0.4815,  0.0555, -0.0723],\n",
      "         ...,\n",
      "         [-0.2586,  0.2607, -0.8678,  ...,  0.4376,  0.2135, -0.1035],\n",
      "         [-0.3027,  0.2969, -0.7000,  ...,  0.1784,  0.0444, -0.1623],\n",
      "         [-0.2889,  0.2046, -0.7751,  ...,  0.2099, -0.1489, -0.0595]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.0125,  0.0240,  0.0185,  ...,  0.3681,  0.0690, -0.1837],\n",
      "         [-0.3404, -0.3411, -0.1503,  ...,  0.5240,  0.0164,  0.0958],\n",
      "         ...,\n",
      "         [-0.4852,  0.0657, -0.3891,  ...,  0.6958,  0.3926, -0.1312],\n",
      "         [-0.4729,  0.1493, -0.3310,  ...,  0.0417, -0.0314, -0.0341],\n",
      "         [-0.3346,  0.1611, -0.3298,  ...,  0.6118,  0.2968, -0.1437]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.1162,  0.1043, -0.4755,  ...,  0.6660, -0.2527, -0.1662],\n",
      "         [-0.0866, -0.0909, -0.2925,  ...,  0.4644, -0.0159,  0.0287],\n",
      "         ...,\n",
      "         [ 0.0805,  0.0865, -0.7812,  ...,  0.4888,  0.0933,  0.0951],\n",
      "         [-0.4459,  0.3710, -0.7169,  ...,  0.3582,  0.0687, -0.4386],\n",
      "         [-0.1025,  0.1404, -0.8760,  ...,  0.3252, -0.1444,  0.1616]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.2031,  0.0668, -0.1629,  ...,  0.1098, -0.1888,  0.1776],\n",
      "         [ 0.2192, -0.0574, -0.5738,  ..., -0.1093, -0.3488,  0.4367],\n",
      "         ...,\n",
      "         [ 0.2557,  0.0206, -0.2673,  ..., -0.1408, -0.0347,  0.3651],\n",
      "         [-0.0742, -0.1319, -0.4216,  ..., -0.2916, -0.1702,  0.4419],\n",
      "         [ 0.1269,  0.1575, -0.4983,  ..., -0.0502, -0.0500,  0.1232]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [5/10], Loss: 0.9657\n",
      "Model outputs:  tensor([[[-6.4435e-02,  2.6911e-01, -1.5453e-01,  ..., -5.8938e-01,\n",
      "           5.2087e-01,  5.2943e-01],\n",
      "         [ 1.6406e-01,  2.6361e-01, -3.1399e-01,  ..., -7.6129e-01,\n",
      "           4.8252e-01,  3.4144e-01],\n",
      "         [ 8.2128e-02, -1.0404e-02, -1.0769e-01,  ..., -9.0170e-01,\n",
      "           5.1167e-01,  2.7729e-01],\n",
      "         ...,\n",
      "         [-2.8010e-01, -6.1863e-02, -2.5140e-01,  ..., -4.0355e-01,\n",
      "           4.9941e-01,  2.5292e-01],\n",
      "         [-1.0710e-01,  3.9257e-01, -1.7011e-01,  ..., -6.1425e-01,\n",
      "           4.3622e-01,  6.1063e-01],\n",
      "         [-1.3278e-01,  4.8891e-02, -2.6782e-01,  ..., -9.0738e-01,\n",
      "           6.6078e-01,  2.3222e-01]],\n",
      "\n",
      "        [[-2.1438e-01,  9.5331e-02, -3.7755e-01,  ..., -1.0490e-01,\n",
      "           3.4607e-01, -1.7349e-01],\n",
      "         [-4.5713e-01,  4.0085e-01, -1.0294e-01,  ..., -1.2422e-01,\n",
      "           1.4928e-01, -1.4920e-01],\n",
      "         [-2.3472e-01,  2.4541e-01, -2.7264e-01,  ..., -2.3718e-01,\n",
      "           2.9343e-01, -2.6488e-01],\n",
      "         ...,\n",
      "         [-5.4123e-01, -2.5044e-01, -3.7880e-01,  ...,  2.0393e-01,\n",
      "           1.1709e-01,  1.2442e-01],\n",
      "         [-5.0965e-01,  5.1027e-01, -2.8171e-01,  ..., -9.1541e-02,\n",
      "          -4.2319e-02, -4.1145e-01],\n",
      "         [-3.5984e-01,  3.5426e-01, -2.9960e-01,  ...,  3.4589e-02,\n",
      "           2.4249e-01, -1.7143e-01]],\n",
      "\n",
      "        [[ 8.8349e-02, -2.0655e-02, -2.2383e-01,  ..., -4.7089e-01,\n",
      "          -1.7218e-01,  2.1276e-02],\n",
      "         [ 1.6797e-01,  2.3968e-02,  1.4619e-01,  ..., -5.9544e-01,\n",
      "          -1.7869e-01, -6.5594e-02],\n",
      "         [ 1.3689e-01, -1.0870e-01,  1.2983e-01,  ..., -6.1664e-01,\n",
      "          -1.5253e-01, -6.7810e-02],\n",
      "         ...,\n",
      "         [-1.7068e-01, -4.2953e-01, -2.2236e-01,  ..., -3.9199e-01,\n",
      "          -2.1041e-01,  3.4572e-01],\n",
      "         [ 4.3666e-02,  1.5814e-01, -3.1213e-01,  ..., -3.5377e-01,\n",
      "          -8.5063e-02,  1.9664e-01],\n",
      "         [-2.0892e-01,  4.9640e-03, -1.1858e-01,  ..., -2.4852e-01,\n",
      "          -3.8081e-01,  2.0255e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.9434e-01, -1.7326e-01, -2.3245e-01,  ..., -5.5933e-01,\n",
      "          -2.3991e-01,  1.9056e-01],\n",
      "         [ 1.8481e-01,  1.0699e-01, -2.6591e-02,  ..., -3.8548e-01,\n",
      "           7.3208e-03,  2.0086e-01],\n",
      "         [-9.0229e-03,  1.0795e-01,  2.6986e-02,  ..., -4.3500e-01,\n",
      "          -2.0734e-01,  1.3709e-01],\n",
      "         ...,\n",
      "         [-1.3562e-02, -2.9065e-01, -1.4236e-01,  ..., -2.9190e-01,\n",
      "          -2.3484e-01,  3.8157e-01],\n",
      "         [ 1.0020e-02,  1.6263e-01, -2.5855e-01,  ..., -4.0258e-01,\n",
      "          -1.7714e-01,  1.0692e-01],\n",
      "         [ 4.7844e-02, -4.8955e-03, -4.3450e-02,  ..., -6.4603e-01,\n",
      "          -1.4703e-02, -3.9973e-02]],\n",
      "\n",
      "        [[ 2.8776e-01, -1.0799e-01,  7.7098e-02,  ..., -3.9379e-01,\n",
      "          -4.1970e-02,  4.2650e-02],\n",
      "         [-8.1078e-03,  1.7454e-01,  5.2582e-02,  ..., -4.2525e-01,\n",
      "          -7.5458e-02, -7.4102e-02],\n",
      "         [ 1.4902e-01, -2.8103e-02,  1.6648e-01,  ..., -4.8385e-01,\n",
      "          -9.2010e-02, -9.2759e-02],\n",
      "         ...,\n",
      "         [-1.4000e-01,  4.0122e-02,  3.8769e-05,  ..., -3.5378e-01,\n",
      "          -1.7528e-01,  2.3706e-01],\n",
      "         [ 7.5496e-02,  9.0775e-03, -2.2965e-01,  ..., -4.2512e-01,\n",
      "           2.0818e-03,  1.3064e-01],\n",
      "         [ 1.6055e-01,  7.5138e-02, -1.1931e-01,  ..., -3.2021e-01,\n",
      "          -5.6273e-02,  6.8709e-02]],\n",
      "\n",
      "        [[-5.0863e-01,  3.5727e-01, -7.9071e-02,  ..., -5.9563e-02,\n",
      "           3.5429e-01, -1.2351e-01],\n",
      "         [-3.7873e-01,  3.7269e-01, -2.1546e-01,  ..., -5.0141e-01,\n",
      "           2.6802e-01,  3.3657e-02],\n",
      "         [-3.2532e-01,  2.8692e-01, -1.8386e-01,  ..., -2.7075e-01,\n",
      "           2.5991e-01, -2.8751e-01],\n",
      "         ...,\n",
      "         [-5.5680e-01, -1.8623e-01, -1.2722e-01,  ...,  1.6386e-01,\n",
      "           2.9741e-01,  2.0097e-01],\n",
      "         [-4.6839e-01,  2.2599e-01, -8.1952e-02,  ..., -3.2684e-02,\n",
      "           3.7527e-01, -2.5724e-02],\n",
      "         [-6.6914e-01,  1.4972e-01, -1.5270e-01,  ..., -1.4485e-01,\n",
      "           1.7225e-01, -3.5773e-02]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9477905631065369\n",
      "Model outputs:  tensor([[[-0.2340, -0.0276, -0.5120,  ...,  0.2588, -0.4241, -0.1264],\n",
      "         [-0.2143,  0.1315, -0.6177,  ...,  0.0098, -0.2335,  0.3410],\n",
      "         [-0.0287, -0.3233, -0.4363,  ...,  0.3257, -0.0345,  0.3672],\n",
      "         ...,\n",
      "         [ 0.2146, -0.0771, -0.1522,  ..., -0.1773, -0.1451, -0.0488],\n",
      "         [-0.4240,  0.2089, -0.6603,  ...,  0.2276, -0.1077, -0.1800],\n",
      "         [-0.1332,  0.4993, -0.8466,  ...,  0.3439, -0.0954,  0.1783]],\n",
      "\n",
      "        [[-0.1404,  0.2490, -0.9450,  ..., -0.0400, -0.0071,  0.2129],\n",
      "         [-0.5868,  0.0793, -0.5688,  ..., -0.1069, -0.1166,  0.3228],\n",
      "         [-0.2887, -0.3115, -0.2236,  ...,  0.0657, -0.1592,  0.3764],\n",
      "         ...,\n",
      "         [-0.2416,  0.1547, -0.2294,  ..., -0.2527, -0.0209,  0.0636],\n",
      "         [-0.4297,  0.3470, -0.4778,  ..., -0.2041,  0.1228, -0.0051],\n",
      "         [-0.4769,  0.1059, -0.7304,  ..., -0.4580, -0.1618,  0.2709]],\n",
      "\n",
      "        [[ 0.0953, -0.3885, -0.6323,  ..., -0.2386, -0.1071,  0.3766],\n",
      "         [-0.1651,  0.0714, -0.4443,  ..., -0.2701, -0.0786,  0.2219],\n",
      "         [ 0.1229, -0.3001, -0.1375,  ..., -0.0518, -0.1938,  0.3900],\n",
      "         ...,\n",
      "         [ 0.1885, -0.0022, -0.2241,  ..., -0.4197, -0.0320,  0.1107],\n",
      "         [-0.1462,  0.1511, -0.2379,  ..., -0.3188, -0.0491,  0.0694],\n",
      "         [-0.1653,  0.1389, -0.3919,  ..., -0.2108, -0.0146,  0.4020]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1089, -0.2029, -0.1874,  ..., -0.1432, -0.2426,  0.3703],\n",
      "         [ 0.1235, -0.2364, -0.2309,  ..., -0.3916, -0.3283,  0.3058],\n",
      "         [ 0.0516, -0.4365, -0.2605,  ..., -0.2066, -0.2249,  0.7702],\n",
      "         ...,\n",
      "         [-0.0037, -0.1719, -0.3098,  ..., -0.4208, -0.1461,  0.1355],\n",
      "         [-0.1203,  0.0396, -0.2256,  ..., -0.4414, -0.2779,  0.3545],\n",
      "         [ 0.1045, -0.1633, -0.5595,  ..., -0.6100, -0.2612,  0.5344]],\n",
      "\n",
      "        [[-0.3681, -0.1153, -0.2772,  ..., -0.3270, -0.2977,  0.2749],\n",
      "         [-0.1650, -0.0526, -0.4972,  ..., -0.2699, -0.3679,  0.2502],\n",
      "         [ 0.1576, -0.5633, -0.1961,  ..., -0.3621, -0.2118,  0.4896],\n",
      "         ...,\n",
      "         [ 0.1560, -0.0948, -0.1686,  ..., -0.4943, -0.1794,  0.0303],\n",
      "         [-0.2388,  0.1329, -0.4014,  ..., -0.2955, -0.2411,  0.0940],\n",
      "         [ 0.0509, -0.2159, -0.3870,  ..., -0.1840, -0.1744,  0.4359]],\n",
      "\n",
      "        [[-0.3784,  0.2099, -0.3226,  ...,  0.5391,  0.2007, -0.1024],\n",
      "         [-0.5789,  0.2562, -0.0775,  ...,  0.4057,  0.2123, -0.2680],\n",
      "         [-0.1841, -0.3826, -0.1117,  ...,  0.6521,  0.2453,  0.1294],\n",
      "         ...,\n",
      "         [-0.0998, -0.1632,  0.1130,  ...,  0.3369,  0.2716, -0.0966],\n",
      "         [-0.6294,  0.1846, -0.1900,  ...,  0.1238,  0.0681, -0.0927],\n",
      "         [-0.3488,  0.0924, -0.1543,  ...,  0.4793,  0.4094, -0.1009]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.044920563697815\n",
      "Model outputs:  tensor([[[-2.0024e-01,  9.6290e-02, -4.2759e-01,  ...,  8.6739e-01,\n",
      "           6.2525e-01,  2.1070e-01],\n",
      "         [-2.5781e-01, -7.5459e-02, -2.0909e-01,  ...,  4.3817e-01,\n",
      "          -8.7224e-02, -2.5508e-01],\n",
      "         [-5.0916e-01,  4.8319e-01, -2.3578e-01,  ...,  9.0304e-01,\n",
      "           5.0565e-01,  1.2234e-01],\n",
      "         ...,\n",
      "         [-3.4574e-01,  5.6072e-01, -1.9589e-01,  ...,  4.7346e-01,\n",
      "           3.4773e-01,  1.3948e-02],\n",
      "         [ 9.1405e-02,  3.9408e-02, -2.4921e-01,  ...,  6.6240e-01,\n",
      "           1.8050e-01,  8.6814e-02],\n",
      "         [-1.4908e-01,  2.9469e-01, -3.5259e-01,  ...,  9.1179e-01,\n",
      "           9.7807e-03,  2.2197e-01]],\n",
      "\n",
      "        [[-4.3165e-02, -4.7674e-02, -5.3357e-01,  ...,  2.1882e-01,\n",
      "           2.3684e-02,  5.3411e-01],\n",
      "         [ 1.4358e-01,  4.5501e-02, -4.9866e-01,  ..., -1.9629e-01,\n",
      "          -9.6427e-02, -7.9235e-03],\n",
      "         [-1.0423e-01,  1.0262e-01, -6.7447e-01,  ...,  1.3043e-01,\n",
      "          -1.1980e-01,  2.3591e-01],\n",
      "         ...,\n",
      "         [ 1.7600e-01,  2.8984e-01, -3.7399e-01,  ..., -6.5915e-02,\n",
      "          -1.3404e-01,  6.5114e-02],\n",
      "         [ 2.8206e-01, -1.1164e-01, -6.0012e-01,  ..., -1.7219e-01,\n",
      "           2.2483e-01,  1.2300e-01],\n",
      "         [ 9.6160e-02,  1.0626e-02, -5.5086e-01,  ..., -1.2203e-02,\n",
      "           1.4597e-02,  3.8091e-01]],\n",
      "\n",
      "        [[-2.0846e-01,  1.3589e-01, -4.6711e-01,  ...,  7.7711e-01,\n",
      "           3.6716e-01,  4.8771e-02],\n",
      "         [-1.6608e-01, -2.4576e-01, -1.8884e-01,  ...,  7.0148e-01,\n",
      "           4.4096e-01, -4.6826e-01],\n",
      "         [-3.2175e-01,  3.8028e-01, -5.3221e-01,  ...,  7.6743e-01,\n",
      "           2.3926e-01, -4.3464e-02],\n",
      "         ...,\n",
      "         [-1.1968e-01,  3.5919e-01, -4.4594e-02,  ...,  5.2400e-01,\n",
      "           1.9167e-01, -3.8672e-01],\n",
      "         [-6.8293e-02,  2.6791e-01, -3.4881e-01,  ...,  8.4112e-01,\n",
      "           2.6304e-01, -1.0478e-01],\n",
      "         [-1.8227e-01,  3.1863e-01, -2.6145e-01,  ...,  1.0006e+00,\n",
      "           5.8793e-01, -2.0981e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.6090e-02, -1.1068e-01, -6.1619e-01,  ...,  5.8962e-02,\n",
      "          -1.1380e-01,  6.6755e-01],\n",
      "         [ 2.8070e-01, -1.9094e-01, -4.1159e-01,  ..., -3.2696e-03,\n",
      "          -4.0295e-01,  1.6836e-01],\n",
      "         [ 2.9176e-01, -2.3986e-01, -3.4952e-01,  ..., -8.9303e-02,\n",
      "          -1.4215e-01,  4.1119e-01],\n",
      "         ...,\n",
      "         [ 3.3584e-01, -9.7472e-04, -5.0619e-01,  ..., -4.5651e-01,\n",
      "           1.3658e-02,  4.4575e-01],\n",
      "         [ 3.7253e-01,  7.3875e-02, -2.6503e-01,  ...,  1.1142e-01,\n",
      "          -1.3620e-02,  3.5760e-01],\n",
      "         [-2.1387e-03,  3.6278e-02, -3.0858e-01,  ...,  8.7487e-02,\n",
      "          -6.1849e-02,  3.1171e-01]],\n",
      "\n",
      "        [[-1.2601e-02, -5.9867e-02, -3.7276e-01,  ..., -1.1521e-02,\n",
      "          -5.8125e-02,  6.6745e-01],\n",
      "         [ 2.5045e-01,  1.1622e-01, -6.1652e-01,  ..., -8.5184e-02,\n",
      "          -4.1606e-01,  1.6255e-01],\n",
      "         [-6.1313e-03, -2.0458e-01, -4.6842e-01,  ..., -2.2829e-01,\n",
      "          -2.4772e-01,  3.6074e-01],\n",
      "         ...,\n",
      "         [-8.1616e-03, -1.8242e-02, -5.4476e-01,  ..., -4.9577e-01,\n",
      "          -1.7939e-01,  2.2237e-01],\n",
      "         [ 4.4462e-02, -9.8995e-02, -5.9726e-01,  ..., -1.9054e-01,\n",
      "          -2.6667e-01,  4.4951e-01],\n",
      "         [-3.9252e-02, -3.5844e-02, -4.2941e-01,  ..., -1.8692e-02,\n",
      "          -9.8285e-03,  3.3108e-01]],\n",
      "\n",
      "        [[-7.2313e-02, -2.7528e-01, -3.7802e-01,  ..., -2.6986e-01,\n",
      "          -5.8357e-02,  6.0151e-01],\n",
      "         [ 6.7820e-02, -9.0662e-02, -4.8779e-01,  ..., -3.1349e-01,\n",
      "          -5.3933e-01,  3.2481e-01],\n",
      "         [ 8.3723e-02,  1.6965e-01, -3.7568e-01,  ..., -3.9487e-02,\n",
      "          -1.7115e-01,  3.3925e-01],\n",
      "         ...,\n",
      "         [ 1.7380e-02, -1.8324e-01, -4.1572e-01,  ..., -6.5691e-01,\n",
      "          -7.1702e-02,  2.8202e-01],\n",
      "         [ 5.3861e-01,  8.0729e-02, -6.8768e-01,  ..., -2.7684e-02,\n",
      "          -1.7929e-02,  3.7193e-01],\n",
      "         [-2.4844e-01,  2.1755e-01, -5.6930e-01,  ..., -1.2567e-01,\n",
      "          -4.0733e-02,  3.4985e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.260387659072876\n",
      "Model outputs:  tensor([[[ 1.5694e-01, -1.8665e-01, -8.1542e-02,  ..., -5.9366e-01,\n",
      "          -1.1330e-01,  1.5955e-01],\n",
      "         [ 1.6943e-01, -1.3148e-01, -4.5051e-02,  ..., -4.8922e-01,\n",
      "          -5.5487e-03,  1.5471e-01],\n",
      "         [-1.3647e-01, -1.9229e-01, -9.4867e-02,  ..., -6.3304e-01,\n",
      "           6.4022e-02,  1.8270e-01],\n",
      "         ...,\n",
      "         [ 1.2476e-01, -1.2505e-01,  3.0989e-04,  ..., -7.2426e-01,\n",
      "           4.3688e-02,  1.9272e-01],\n",
      "         [ 9.6119e-02,  9.8703e-02,  3.5476e-01,  ..., -5.0536e-01,\n",
      "          -7.3099e-02, -1.0501e-01],\n",
      "         [ 2.0832e-02,  3.2433e-02, -6.6629e-02,  ..., -5.3443e-01,\n",
      "          -1.7684e-01,  2.3606e-02]],\n",
      "\n",
      "        [[-2.9330e-01,  1.8537e-01, -1.5627e-01,  ..., -2.3656e-02,\n",
      "           3.5265e-01, -2.3927e-01],\n",
      "         [-6.0976e-01,  2.8708e-01, -2.0530e-01,  ...,  6.0158e-02,\n",
      "           7.1820e-01, -1.9131e-01],\n",
      "         [-3.1242e-01,  4.8687e-02, -3.2531e-01,  ...,  6.6203e-02,\n",
      "           4.3754e-01, -1.5925e-01],\n",
      "         ...,\n",
      "         [-3.8667e-01,  3.4216e-01, -1.9562e-01,  ...,  3.4343e-03,\n",
      "           3.8883e-01, -1.8737e-01],\n",
      "         [-3.2452e-01,  8.5331e-02,  2.2181e-01,  ..., -4.7468e-02,\n",
      "           3.4955e-01, -5.0557e-01],\n",
      "         [-2.6600e-01,  2.4017e-01, -4.7501e-01,  ...,  2.3168e-02,\n",
      "           3.8245e-01, -1.5762e-01]],\n",
      "\n",
      "        [[-3.7153e-01,  1.6996e-01, -2.2600e-01,  ...,  1.4355e-01,\n",
      "           1.1592e-01, -1.4659e-01],\n",
      "         [-3.8631e-01,  2.9732e-01, -1.3435e-01,  ...,  3.2496e-01,\n",
      "           5.8950e-01, -1.5368e-01],\n",
      "         [-1.2777e-01,  3.2999e-01, -4.3994e-01,  ...,  2.6378e-01,\n",
      "           2.4945e-01, -3.3658e-01],\n",
      "         ...,\n",
      "         [-3.3174e-01,  4.3671e-01,  3.7419e-02,  ...,  1.6705e-01,\n",
      "           3.0184e-01, -5.5826e-01],\n",
      "         [-4.4838e-01,  4.3547e-01,  3.7870e-02,  ...,  3.1591e-01,\n",
      "           4.2504e-01, -1.4550e-01],\n",
      "         [-3.3704e-01,  4.7268e-01, -7.9214e-02,  ...,  3.0288e-01,\n",
      "           4.2844e-01, -4.8484e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.9344e-01,  1.4836e-01, -1.8622e-01,  ..., -3.3431e-01,\n",
      "          -2.5843e-02, -1.2007e-01],\n",
      "         [-9.3457e-02,  5.1248e-01, -3.9815e-01,  ..., -3.1132e-01,\n",
      "           2.2292e-01,  1.0838e-01],\n",
      "         [ 1.1069e-01,  4.1782e-02, -8.7490e-01,  ..., -1.9357e-01,\n",
      "           1.2839e-01,  1.9343e-02],\n",
      "         ...,\n",
      "         [-1.0266e-01,  3.4692e-01, -2.9222e-01,  ..., -1.4329e-01,\n",
      "           7.9590e-02, -4.7443e-01],\n",
      "         [ 4.8895e-03,  3.3588e-01, -3.4495e-01,  ..., -2.0078e-01,\n",
      "           7.9490e-02, -2.4981e-01],\n",
      "         [ 2.2687e-01,  4.5663e-02, -1.8491e-01,  ..., -2.2726e-01,\n",
      "          -4.0267e-02,  3.1271e-02]],\n",
      "\n",
      "        [[-2.8640e-01,  2.1649e-02, -6.8462e-03,  ...,  1.6886e-01,\n",
      "           1.6324e-01, -4.1838e-01],\n",
      "         [-4.9728e-01,  1.8422e-01, -5.9764e-02,  ...,  2.7897e-01,\n",
      "           1.3312e-01, -2.5563e-01],\n",
      "         [-2.1847e-01,  2.1739e-01, -1.2441e-01,  ...,  2.8625e-01,\n",
      "           2.5315e-01, -3.8147e-01],\n",
      "         ...,\n",
      "         [-4.0342e-01,  4.0718e-01, -2.7039e-01,  ...,  5.2453e-02,\n",
      "           3.4854e-01, -4.8503e-01],\n",
      "         [-3.3722e-01,  2.5224e-01, -2.4661e-01,  ...,  5.0639e-02,\n",
      "           4.3596e-01, -6.7824e-02],\n",
      "         [-3.8961e-01,  4.1919e-01, -1.7179e-01,  ..., -1.6497e-01,\n",
      "           3.5264e-01, -2.6407e-01]],\n",
      "\n",
      "        [[-2.9872e-01,  3.1589e-01, -1.4281e-01,  ...,  1.8462e-02,\n",
      "           1.4398e-01, -1.7166e-01],\n",
      "         [-5.5624e-01,  1.8695e-01, -1.9040e-01,  ...,  3.2573e-02,\n",
      "           1.8637e-01,  7.4810e-02],\n",
      "         [-3.0845e-01,  2.0936e-01, -3.0683e-01,  ...,  1.6036e-01,\n",
      "           3.6688e-01, -1.3423e-01],\n",
      "         ...,\n",
      "         [-3.3679e-01,  2.4403e-01, -1.2342e-02,  ..., -2.2119e-01,\n",
      "           2.6218e-01, -3.1499e-01],\n",
      "         [-4.3522e-01,  2.9466e-01, -2.3262e-01,  ...,  3.3237e-01,\n",
      "           4.5294e-01, -2.0982e-01],\n",
      "         [-3.4616e-01,  2.8085e-01, -1.4495e-01,  ..., -2.0099e-01,\n",
      "           8.7657e-02, -5.1380e-02]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1060640811920166\n",
      "Model outputs:  tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.1362, -0.0939, -0.5165,  ...,  0.6187, -0.3447, -0.1299],\n",
      "         [-0.1959, -0.2003, -0.7662,  ...,  0.5032, -0.1949,  0.1264],\n",
      "         ...,\n",
      "         [ 0.2069, -0.2399, -0.5963,  ...,  0.7912, -0.1713, -0.0881],\n",
      "         [-0.0588, -0.2790, -0.8042,  ...,  0.5393, -0.2178, -0.0384],\n",
      "         [ 0.1421,  0.0961, -0.6127,  ...,  0.8452,  0.0348,  0.0253]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.3697, -0.0829, -0.1927,  ..., -0.3198,  0.1693,  0.4877],\n",
      "         [ 0.1724, -0.1304, -0.4234,  ..., -0.3328,  0.4574,  0.6357],\n",
      "         ...,\n",
      "         [-0.0328, -0.2204, -0.2838,  ..., -0.2901,  0.4414,  0.5816],\n",
      "         [ 0.2450, -0.2894, -0.5802,  ..., -0.6861,  0.4574,  0.5608],\n",
      "         [ 0.1479, -0.1809, -0.3203,  ..., -0.4352,  0.3703,  0.5339]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.1890, -0.0062, -0.3120,  ...,  0.2205, -0.5476,  0.1025],\n",
      "         [ 0.0536, -0.0162, -0.7623,  ...,  0.1496, -0.2102,  0.6429],\n",
      "         ...,\n",
      "         [-0.0968, -0.2829, -0.5718,  ...,  0.2914, -0.1038,  0.3255],\n",
      "         [ 0.1089,  0.1117, -0.7901,  ...,  0.0103, -0.1331,  0.3619],\n",
      "         [-0.0464, -0.1825, -0.5596,  ...,  0.1164, -0.0198,  0.3902]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.4269,  0.1422, -0.2540,  ...,  0.3171, -0.1028, -0.0230],\n",
      "         [-0.4483, -0.0610, -0.3925,  ...,  0.3198, -0.0782,  0.2050],\n",
      "         ...,\n",
      "         [-0.3426, -0.1603, -0.4180,  ...,  0.4052,  0.0681,  0.0998],\n",
      "         [-0.2538,  0.0412, -0.6307,  ...,  0.4166,  0.0185,  0.1992],\n",
      "         [-0.3534,  0.1330, -0.5815,  ...,  0.5627,  0.2178,  0.0757]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.3591,  0.0128, -0.3606,  ...,  0.3415, -0.3005,  0.2370],\n",
      "         [ 0.0431,  0.0065, -0.8459,  ...,  0.2639,  0.0010,  0.2750],\n",
      "         ...,\n",
      "         [ 0.3893, -0.2101, -0.5780,  ...,  0.3491,  0.0087,  0.3547],\n",
      "         [ 0.1003,  0.1742, -0.6482,  ...,  0.0318, -0.0104,  0.1697],\n",
      "         [-0.0447, -0.1259, -0.6848,  ...,  0.3773, -0.3922,  0.2632]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.1792,  0.0446,  0.0100,  ...,  0.7449,  0.2592, -0.0626],\n",
      "         [-0.1654,  0.1861, -0.2337,  ...,  0.9040,  0.2331,  0.2942],\n",
      "         ...,\n",
      "         [-0.2635, -0.0954, -0.2487,  ...,  0.6395, -0.0803,  0.1159],\n",
      "         [ 0.0071,  0.3468, -0.4079,  ...,  0.5151,  0.0301, -0.0805],\n",
      "         [-0.0882, -0.0218, -0.2928,  ...,  0.5511,  0.1898,  0.1032]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [6/10], Loss: 1.1061\n",
      "Model outputs:  tensor([[[-0.6736,  0.3014, -0.5625,  ...,  0.4262,  0.3024,  0.0702],\n",
      "         [-0.3224,  0.1803, -0.3215,  ...,  0.6155,  0.1991,  0.0358],\n",
      "         [-0.5454,  0.2792, -0.6678,  ...,  0.8107,  0.3361,  0.2618],\n",
      "         ...,\n",
      "         [-0.5047,  0.2204, -0.5532,  ...,  0.5223,  0.2392,  0.2043],\n",
      "         [-0.5413,  0.3580, -0.2782,  ...,  0.2968,  0.4808,  0.1851],\n",
      "         [-0.5420,  0.0138, -0.3551,  ...,  0.4372,  0.2542,  0.3109]],\n",
      "\n",
      "        [[-0.1642,  0.1840, -0.6771,  ...,  0.1147, -0.0968,  0.0288],\n",
      "         [ 0.0017,  0.1679, -0.7633,  ...,  0.6982, -0.0569,  0.1206],\n",
      "         [ 0.0502, -0.0240, -0.8468,  ...,  0.5219,  0.1242,  0.3211],\n",
      "         ...,\n",
      "         [-0.1594, -0.1668, -0.7538,  ...,  0.5232, -0.1142,  0.4164],\n",
      "         [-0.0949,  0.1522, -0.5857,  ...,  0.0429, -0.1337, -0.1017],\n",
      "         [-0.1124, -0.0216, -0.6355,  ...,  0.2485, -0.0237,  0.3024]],\n",
      "\n",
      "        [[-0.6116,  0.3896, -0.5524,  ...,  0.4301,  0.0334,  0.0854],\n",
      "         [-0.1426,  0.1143, -0.1973,  ...,  0.3522,  0.2946,  0.1185],\n",
      "         [-0.3341,  0.1039, -0.3363,  ...,  0.5978,  0.2067,  0.4171],\n",
      "         ...,\n",
      "         [-0.4847,  0.2112, -0.4472,  ...,  0.3905,  0.1967,  0.1825],\n",
      "         [-0.4003,  0.2532, -0.3141,  ...,  0.4998,  0.4032,  0.0868],\n",
      "         [-0.4627,  0.0851, -0.5184,  ...,  0.6252,  0.4780,  0.3496]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6283,  0.1598, -0.9190,  ...,  0.2290,  0.0830,  0.0220],\n",
      "         [-0.3276,  0.0890, -0.6936,  ...,  0.4633, -0.4251, -0.0385],\n",
      "         [-0.3580, -0.1975, -0.8448,  ...,  0.3489,  0.1289,  0.1933],\n",
      "         ...,\n",
      "         [-0.2146,  0.2588, -1.0016,  ...,  0.7042, -0.0067,  0.2134],\n",
      "         [-0.4056,  0.2573, -0.7355,  ...,  0.2969,  0.0039, -0.0655],\n",
      "         [-0.3861, -0.1414, -0.6382,  ...,  0.3628, -0.0108,  0.0408]],\n",
      "\n",
      "        [[-0.4042,  0.2223, -0.0938,  ...,  0.5857,  0.3779, -0.0157],\n",
      "         [-0.0356,  0.2787, -0.1500,  ...,  0.8991,  0.3858, -0.1451],\n",
      "         [-0.1580,  0.1137, -0.2755,  ...,  0.6052,  0.3809, -0.0831],\n",
      "         ...,\n",
      "         [-0.5311,  0.3318, -0.1284,  ...,  0.5067,  0.4023, -0.1387],\n",
      "         [-0.2563,  0.3205, -0.2819,  ...,  0.6077,  0.3782, -0.1079],\n",
      "         [-0.1991, -0.0228, -0.0915,  ...,  0.9262,  0.4036,  0.0580]],\n",
      "\n",
      "        [[-0.5101,  0.4413, -0.5839,  ...,  0.8604,  0.1459,  0.0069],\n",
      "         [-0.2887,  0.4088, -0.7560,  ...,  0.8925,  0.0401, -0.1384],\n",
      "         [-0.2631,  0.2146, -0.7166,  ...,  0.9067,  0.2715,  0.1583],\n",
      "         ...,\n",
      "         [-0.4715,  0.1426, -0.8484,  ...,  0.8542,  0.4074,  0.0707],\n",
      "         [-0.5968,  0.1338, -0.5152,  ...,  0.5205,  0.5877,  0.2068],\n",
      "         [-0.4259,  0.0375, -0.7079,  ...,  0.6869,  0.1516,  0.2656]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0404618978500366\n",
      "Model outputs:  tensor([[[-4.3440e-01,  3.1414e-02, -2.7821e-01,  ...,  5.3855e-01,\n",
      "           6.4208e-01, -4.5817e-02],\n",
      "         [-4.3785e-01, -4.0266e-01, -2.2032e-01,  ...,  4.8115e-01,\n",
      "           5.3709e-01, -1.5555e-01],\n",
      "         [-5.1397e-01,  1.8129e-01, -4.9592e-01,  ...,  4.8028e-01,\n",
      "           2.6348e-01, -2.4443e-02],\n",
      "         ...,\n",
      "         [-6.4216e-01, -6.2970e-02, -3.1265e-01,  ...,  4.5191e-01,\n",
      "          -7.6000e-02, -1.6349e-02],\n",
      "         [-5.7352e-01,  8.5310e-03, -3.5822e-01,  ...,  4.2647e-01,\n",
      "           3.2157e-01, -4.7206e-02],\n",
      "         [-7.6907e-01,  3.2583e-02, -3.1358e-01,  ...,  3.9309e-01,\n",
      "           1.5454e-01,  2.2989e-01]],\n",
      "\n",
      "        [[-4.2713e-01,  1.9838e-01, -8.9820e-02,  ...,  5.4502e-01,\n",
      "           2.0741e-01, -3.3969e-02],\n",
      "         [-5.0534e-01,  1.9916e-02, -1.6044e-01,  ...,  9.5315e-01,\n",
      "           5.3826e-01,  6.8082e-02],\n",
      "         [-5.6960e-01,  1.0038e-01, -4.5203e-01,  ...,  4.9923e-01,\n",
      "           3.0113e-01, -3.6994e-02],\n",
      "         ...,\n",
      "         [-3.9707e-01,  8.1263e-02, -3.9706e-01,  ...,  7.5194e-01,\n",
      "          -1.6142e-01,  8.9892e-03],\n",
      "         [-5.8280e-01,  2.4540e-01, -2.4030e-01,  ...,  8.6552e-01,\n",
      "           3.1675e-01,  1.6589e-02],\n",
      "         [-4.2299e-01, -1.2924e-01, -7.8432e-02,  ...,  5.9162e-01,\n",
      "           1.0786e-01,  6.1591e-02]],\n",
      "\n",
      "        [[ 2.9174e-01, -1.1745e-01, -4.3660e-01,  ..., -6.4899e-01,\n",
      "           3.8090e-01,  5.8183e-01],\n",
      "         [ 3.0813e-01, -3.7298e-01, -1.9709e-01,  ..., -1.8529e-01,\n",
      "           3.3192e-01,  4.7793e-01],\n",
      "         [ 3.1667e-01,  1.7719e-02, -4.1496e-01,  ..., -5.2780e-01,\n",
      "           6.0996e-01,  3.9682e-01],\n",
      "         ...,\n",
      "         [-1.2939e-01, -9.2157e-02, -2.6028e-01,  ..., -5.0139e-01,\n",
      "          -4.0822e-01, -2.4719e-01],\n",
      "         [ 3.0552e-01, -2.1524e-01, -4.6346e-01,  ..., -3.7588e-01,\n",
      "          -6.6819e-02,  2.7693e-01],\n",
      "         [ 1.8361e-01, -4.7302e-01,  2.1280e-02,  ..., -6.4552e-01,\n",
      "           2.1633e-02,  5.5527e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.9582e-01,  1.5938e-01, -6.6824e-01,  ...,  4.8193e-02,\n",
      "           2.3639e-01, -6.4300e-02],\n",
      "         [-1.2245e-01, -4.0155e-01, -5.6640e-01,  ...,  1.1987e-01,\n",
      "           8.2057e-02,  3.3967e-01],\n",
      "         [-1.9703e-01,  1.8506e-01, -7.3997e-01,  ...,  2.9644e-04,\n",
      "           8.1430e-02,  2.5044e-01],\n",
      "         ...,\n",
      "         [-2.8730e-01,  1.1976e-01, -9.1946e-01,  ...,  2.4523e-01,\n",
      "          -3.5798e-01,  2.3377e-01],\n",
      "         [-3.5798e-02, -9.6729e-02, -8.0762e-01,  ...,  1.3247e-01,\n",
      "          -9.0751e-02,  3.6093e-01],\n",
      "         [-7.7280e-02,  1.7020e-01, -6.6299e-01,  ...,  8.1140e-02,\n",
      "          -3.2013e-01,  6.2335e-01]],\n",
      "\n",
      "        [[-8.3548e-01,  3.2470e-01, -4.4374e-01,  ...,  4.5704e-01,\n",
      "           4.2063e-01, -1.2444e-02],\n",
      "         [-3.4762e-01, -7.3171e-02, -3.3977e-01,  ...,  5.1456e-01,\n",
      "           1.1944e-01,  2.7629e-01],\n",
      "         [-5.8512e-01,  3.5665e-01, -5.5029e-01,  ...,  3.9249e-01,\n",
      "           1.7335e-01,  4.2869e-01],\n",
      "         ...,\n",
      "         [-6.0034e-01,  1.4302e-01, -4.7125e-01,  ...,  2.6467e-01,\n",
      "          -4.2477e-01,  1.5004e-01],\n",
      "         [-6.7832e-01,  1.1478e-01, -3.7767e-01,  ...,  2.6589e-01,\n",
      "           2.0212e-01,  8.2591e-02],\n",
      "         [-6.8474e-01, -1.5311e-01, -5.1521e-01,  ...,  1.9396e-01,\n",
      "           2.2101e-01,  3.0279e-01]],\n",
      "\n",
      "        [[-2.1319e-01,  1.9744e-01, -6.2513e-01,  ...,  5.5111e-01,\n",
      "           4.2153e-02,  2.0957e-01],\n",
      "         [-9.1523e-02, -6.2486e-02, -6.8464e-01,  ...,  7.8731e-01,\n",
      "           2.5170e-01,  3.1559e-01],\n",
      "         [-2.9049e-01,  2.7091e-01, -6.8141e-01,  ...,  6.6978e-01,\n",
      "           5.8694e-01, -9.9395e-02],\n",
      "         ...,\n",
      "         [-5.2235e-01,  1.5759e-01, -8.8783e-01,  ...,  5.8694e-01,\n",
      "          -4.8724e-01,  1.2908e-01],\n",
      "         [-4.2098e-01,  1.1332e-01, -8.9766e-01,  ...,  6.6467e-01,\n",
      "           9.0384e-04,  3.2366e-01],\n",
      "         [-4.4262e-01, -4.2458e-03, -8.4253e-01,  ...,  8.5718e-01,\n",
      "           2.5422e-02,  1.9250e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.6022956371307373\n",
      "Model outputs:  tensor([[[ 0.2760,  0.0153, -0.2011,  ...,  0.3725,  0.0077, -0.6150],\n",
      "         [ 0.0876, -0.1560,  0.1835,  ...,  0.0738, -0.2142, -0.6204],\n",
      "         [-0.0120,  0.0094,  0.2704,  ...,  0.3459, -0.3183, -0.7445],\n",
      "         ...,\n",
      "         [ 0.0666,  0.0879, -0.0890,  ...,  0.2849,  0.0245, -0.8255],\n",
      "         [ 0.1547,  0.1375,  0.0816,  ...,  0.3681, -0.0388, -0.6963],\n",
      "         [ 0.0499,  0.2136,  0.1143,  ...,  0.1722,  0.0748, -0.6230]],\n",
      "\n",
      "        [[ 0.1598,  0.3002, -0.5770,  ..., -0.4858,  0.2722,  0.1884],\n",
      "         [ 0.1115,  0.2395, -0.2709,  ..., -0.5455,  0.3112,  0.2604],\n",
      "         [-0.0318, -0.0574, -0.2367,  ..., -0.5155,  0.2081,  0.3229],\n",
      "         ...,\n",
      "         [ 0.2905, -0.2394, -0.2326,  ..., -0.8027,  0.3822, -0.1504],\n",
      "         [ 0.4603,  0.0714, -0.4085,  ..., -0.7108,  0.4599,  0.0452],\n",
      "         [-0.0315, -0.1584, -0.0869,  ..., -0.7267,  0.4291,  0.0553]],\n",
      "\n",
      "        [[-0.2909,  0.2632, -0.2485,  ...,  0.5944,  0.1547, -0.0264],\n",
      "         [-0.4470,  0.5024,  0.0130,  ...,  0.3250,  0.4817, -0.4446],\n",
      "         [-0.4324,  0.5442,  0.0181,  ...,  0.4073,  0.1707, -0.4223],\n",
      "         ...,\n",
      "         [-0.4265,  0.6027, -0.0409,  ...,  0.4367,  0.4195, -0.1869],\n",
      "         [-0.3257,  0.3748, -0.4397,  ...,  0.4006,  0.0922,  0.0189],\n",
      "         [-0.2211,  0.3620, -0.2799,  ...,  0.1684,  0.2322, -0.2450]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5650,  0.0608, -0.2561,  ...,  0.4215,  0.2228, -0.1109],\n",
      "         [-0.3423,  0.2498, -0.1761,  ...,  0.1681,  0.3322, -0.0498],\n",
      "         [-0.3500,  0.4685,  0.0563,  ...,  0.4743,  0.2293, -0.1500],\n",
      "         ...,\n",
      "         [-0.2810,  0.4260, -0.5761,  ...,  0.3884,  0.3560, -0.1017],\n",
      "         [-0.3201, -0.0094,  0.0926,  ...,  0.1711,  0.3364, -0.2603],\n",
      "         [-0.2722,  0.2058, -0.2567,  ...,  0.2028,  0.3157, -0.0593]],\n",
      "\n",
      "        [[-0.5060,  0.3683, -0.0833,  ...,  0.5318,  0.2447, -0.0904],\n",
      "         [-0.1634,  0.3519, -0.0073,  ...,  0.5099,  0.3711, -0.0720],\n",
      "         [-0.1763,  0.7176,  0.2920,  ...,  0.4184,  0.3419, -0.3449],\n",
      "         ...,\n",
      "         [-0.2620,  0.2517, -0.1606,  ...,  0.2677,  0.2808, -0.1130],\n",
      "         [-0.5819,  0.3960, -0.0717,  ...,  0.6425,  0.1795, -0.4103],\n",
      "         [-0.4242,  0.3345, -0.4234,  ...,  0.0736,  0.5519, -0.5084]],\n",
      "\n",
      "        [[-0.2273,  0.1218, -0.4103,  ..., -0.3351, -0.0737,  0.3496],\n",
      "         [ 0.1172, -0.0821, -0.0565,  ..., -0.3886, -0.1381,  0.3350],\n",
      "         [-0.0498,  0.1522, -0.0806,  ..., -0.3461, -0.0571,  0.2263],\n",
      "         ...,\n",
      "         [ 0.1084, -0.1061, -0.3480,  ..., -0.5541, -0.1721,  0.3970],\n",
      "         [ 0.0096, -0.0192, -0.3049,  ..., -0.3342, -0.3142,  0.3073],\n",
      "         [-0.0372,  0.0041, -0.1716,  ..., -0.4479, -0.1238,  0.3113]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2532066106796265\n",
      "Model outputs:  tensor([[[-1.9311e-01,  3.3181e-02, -9.2385e-01,  ...,  6.9007e-01,\n",
      "           1.7543e-01,  2.2517e-01],\n",
      "         [-7.4679e-02,  3.1700e-01, -8.3084e-01,  ...,  8.5356e-01,\n",
      "           1.0334e-01,  5.3551e-03],\n",
      "         [-2.5050e-01,  6.9176e-01, -5.0561e-01,  ...,  5.7151e-01,\n",
      "          -6.3086e-02, -1.3663e-01],\n",
      "         ...,\n",
      "         [-1.6710e-01,  4.3666e-01, -3.4374e-01,  ...,  8.5266e-01,\n",
      "           4.3334e-02, -7.8184e-03],\n",
      "         [-3.1873e-01,  5.0269e-01, -4.0633e-01,  ...,  5.6035e-01,\n",
      "          -1.2220e-01, -1.6684e-01],\n",
      "         [-9.0588e-01,  6.7681e-01, -8.5268e-01,  ...,  6.1767e-01,\n",
      "          -2.8664e-01,  1.6629e-01]],\n",
      "\n",
      "        [[ 4.2603e-01, -1.2542e-01, -6.4499e-02,  ...,  6.4477e-01,\n",
      "           8.6786e-02, -3.6078e-01],\n",
      "         [ 8.3314e-02, -1.4432e-01,  1.2693e-02,  ...,  6.1784e-01,\n",
      "          -2.5185e-01, -4.7719e-01],\n",
      "         [ 1.5065e-01, -4.1736e-01,  4.4808e-01,  ...,  3.3244e-01,\n",
      "          -4.6280e-01, -5.7419e-01],\n",
      "         ...,\n",
      "         [ 4.4220e-01, -4.4568e-02,  5.8148e-01,  ...,  5.9987e-01,\n",
      "          -2.9701e-01, -5.1804e-01],\n",
      "         [ 3.3827e-01,  5.9981e-02,  3.2708e-01,  ...,  4.9415e-01,\n",
      "          -3.2976e-01, -6.0947e-01],\n",
      "         [-1.9470e-02,  4.5728e-02,  1.5890e-01,  ...,  3.1887e-01,\n",
      "          -1.9268e-01, -5.9113e-01]],\n",
      "\n",
      "        [[ 7.5362e-02, -3.3641e-01, -6.5231e-01,  ...,  1.4292e-01,\n",
      "           1.8046e-02,  3.0325e-01],\n",
      "         [ 1.7001e-01,  3.1242e-02, -6.5003e-01,  ..., -1.1796e-01,\n",
      "           1.5316e-01,  4.2544e-01],\n",
      "         [ 2.9888e-01,  3.1761e-01, -2.0284e-01,  ..., -4.8955e-01,\n",
      "          -2.1532e-01,  1.4335e-02],\n",
      "         ...,\n",
      "         [-4.6575e-04,  6.1060e-02, -2.1584e-01,  ..., -1.7347e-01,\n",
      "          -3.5794e-01,  3.2807e-01],\n",
      "         [-6.1552e-03, -1.1441e-02, -4.3835e-01,  ...,  5.5610e-02,\n",
      "          -4.1622e-01,  2.1887e-01],\n",
      "         [-2.3086e-01,  7.2819e-02, -4.3754e-01,  ..., -2.2562e-01,\n",
      "          -3.4734e-01,  6.1151e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.9348e-01,  2.4048e-01, -8.7826e-01,  ...,  1.0242e+00,\n",
      "           4.2578e-01,  1.7866e-01],\n",
      "         [-1.4686e-01,  1.6811e-01, -8.9096e-01,  ...,  7.6817e-01,\n",
      "           2.7900e-01,  4.8757e-02],\n",
      "         [-1.8456e-01,  5.1011e-01, -3.8966e-01,  ...,  5.5993e-01,\n",
      "           4.4702e-02,  2.9813e-02],\n",
      "         ...,\n",
      "         [-1.6890e-01,  2.8487e-01, -2.6454e-01,  ...,  8.0914e-01,\n",
      "          -1.0763e-01, -3.1866e-02],\n",
      "         [-4.4303e-01,  4.5792e-01, -3.0093e-01,  ...,  7.3218e-01,\n",
      "          -1.2404e-01, -3.7399e-01],\n",
      "         [-5.5708e-01,  7.7142e-01, -6.6400e-01,  ...,  5.0844e-01,\n",
      "          -4.2843e-02,  2.7135e-01]],\n",
      "\n",
      "        [[-2.5212e-01, -3.4745e-01, -3.3416e-01,  ...,  7.2044e-01,\n",
      "           4.0457e-01,  1.8104e-01],\n",
      "         [-3.8222e-01,  2.1661e-01, -3.8136e-01,  ...,  6.0559e-01,\n",
      "           2.7639e-01, -6.6201e-02],\n",
      "         [-3.6144e-01,  4.9464e-01, -2.0389e-03,  ...,  2.3197e-01,\n",
      "          -1.8629e-01, -1.5372e-01],\n",
      "         ...,\n",
      "         [-3.8444e-01,  1.2606e-02,  1.7316e-01,  ...,  4.2722e-01,\n",
      "           1.9671e-01,  6.1693e-02],\n",
      "         [-5.3997e-01,  3.7241e-01, -4.6785e-02,  ...,  4.7111e-01,\n",
      "          -1.2783e-01, -3.0946e-02],\n",
      "         [-4.8588e-01,  5.0128e-01, -1.4488e-01,  ...,  4.4607e-01,\n",
      "          -1.3016e-02,  3.5981e-01]],\n",
      "\n",
      "        [[-9.8178e-02, -4.1363e-01, -4.6474e-01,  ...,  8.0422e-05,\n",
      "           1.4893e-01, -3.8874e-02],\n",
      "         [ 9.3277e-02,  8.5222e-03, -6.4793e-01,  ...,  1.6228e-01,\n",
      "           4.6624e-02, -2.8043e-02],\n",
      "         [ 2.8606e-02, -2.3954e-02, -3.7279e-01,  ..., -5.8296e-01,\n",
      "           4.9639e-02, -2.9531e-01],\n",
      "         ...,\n",
      "         [ 1.0738e-02, -4.5819e-02, -1.3558e-01,  ..., -4.1929e-02,\n",
      "          -4.8468e-02, -1.5709e-01],\n",
      "         [ 1.0372e-01, -1.1978e-01, -9.2683e-02,  ..., -4.6530e-02,\n",
      "           8.1264e-03, -2.2634e-01],\n",
      "         [ 8.6693e-02,  1.0068e-01, -5.3465e-01,  ..., -3.1571e-01,\n",
      "          -3.5641e-01,  7.0421e-02]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2295793294906616\n",
      "Model outputs:  tensor([[[-0.3680,  0.7766, -0.2230,  ...,  0.3757,  0.3779, -0.2724],\n",
      "         [-0.6509,  0.3941, -0.2765,  ...,  0.3109,  0.4502, -0.2021],\n",
      "         [-0.4972,  0.2729, -0.4295,  ...,  0.8254,  0.4607, -0.1555],\n",
      "         ...,\n",
      "         [-0.0509, -0.2331, -0.2683,  ...,  0.4340,  0.2156, -0.0979],\n",
      "         [-0.3858,  0.2837, -0.3068,  ...,  0.6459,  0.1823,  0.2310],\n",
      "         [-0.4812,  0.2242, -0.1973,  ...,  0.4662,  0.2585, -0.0946]],\n",
      "\n",
      "        [[-0.2127,  0.1332,  0.1794,  ..., -0.2045, -0.1161, -0.0242],\n",
      "         [-0.0819,  0.2185, -0.2054,  ..., -0.5226, -0.2305,  0.3688],\n",
      "         [-0.2287,  0.2490, -0.6142,  ..., -0.0686, -0.0122,  0.4256],\n",
      "         ...,\n",
      "         [ 0.3555,  0.0049, -0.2633,  ..., -0.1991,  0.0629,  0.3350],\n",
      "         [-0.1015, -0.0708, -0.6089,  ..., -0.1019, -0.0898,  0.4459],\n",
      "         [ 0.1149,  0.0106, -0.5133,  ...,  0.1789,  0.0219,  0.4086]],\n",
      "\n",
      "        [[-0.2982,  0.4578,  0.0312,  ...,  0.5103,  0.3178, -0.1662],\n",
      "         [-0.7634,  0.5651, -0.1744,  ...,  0.4682,  0.4027, -0.1058],\n",
      "         [-0.3244,  0.2132, -0.2010,  ...,  0.4946,  0.6805, -0.2110],\n",
      "         ...,\n",
      "         [-0.2269, -0.2373, -0.2991,  ...,  0.3961,  0.1772, -0.0495],\n",
      "         [-0.3259,  0.3794, -0.1718,  ...,  0.7674,  0.3078, -0.0339],\n",
      "         [-0.3137,  0.3964,  0.0212,  ...,  0.4575,  0.3024, -0.1643]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4387,  0.3970, -0.1900,  ...,  0.2600,  0.3856, -0.0889],\n",
      "         [-0.8235,  0.3966,  0.0129,  ...,  0.4706,  0.0464, -0.0127],\n",
      "         [-0.2801,  0.0836, -0.0190,  ...,  0.4109,  0.4726, -0.0075],\n",
      "         ...,\n",
      "         [-0.0577,  0.1200, -0.3290,  ...,  0.2802,  0.2926,  0.0370],\n",
      "         [-0.3722,  0.1137, -0.1229,  ...,  0.6478,  0.3335,  0.0085],\n",
      "         [-0.6899,  0.0955, -0.0853,  ...,  0.9389,  0.1669, -0.0836]],\n",
      "\n",
      "        [[-0.2812,  0.6641, -0.4456,  ...,  0.2651,  0.0602, -0.2570],\n",
      "         [-0.7636,  0.7607, -0.7380,  ...,  0.5331,  0.3698, -0.3195],\n",
      "         [-0.5048,  0.2499, -0.8529,  ...,  0.9006,  0.1796, -0.0870],\n",
      "         ...,\n",
      "         [-0.1227, -0.1052, -0.5957,  ...,  0.7197,  0.1141, -0.2005],\n",
      "         [-0.4799,  0.2256, -0.9391,  ...,  0.8328,  0.3148,  0.0561],\n",
      "         [-0.3124,  0.2451, -0.7339,  ...,  0.8524,  0.3357,  0.0721]],\n",
      "\n",
      "        [[ 0.0573,  0.1697, -0.5223,  ..., -0.6917,  0.3294, -0.2220],\n",
      "         [ 0.0702,  0.1239, -0.1124,  ..., -0.3731,  0.1603, -0.7512],\n",
      "         [ 0.1560, -0.2694,  0.0730,  ..., -0.0504,  0.3243, -0.7228],\n",
      "         ...,\n",
      "         [-0.1491, -0.0785, -0.4510,  ..., -0.2945,  0.5189,  0.4254],\n",
      "         [ 0.0726,  0.2674, -0.5805,  ..., -0.4304,  0.7324,  0.2144],\n",
      "         [ 0.2201,  0.0280, -0.4467,  ..., -0.2501,  0.6019,  0.2290]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9793075919151306\n",
      "Model outputs:  tensor([[[ 0.2565, -0.1199, -0.4455,  ...,  0.4843,  0.1801, -0.6277],\n",
      "         [ 0.3549, -0.2133, -0.0173,  ...,  0.3403,  0.1765, -0.8945],\n",
      "         [ 0.2497, -0.0370, -0.1318,  ...,  0.3653,  0.0709, -0.6106],\n",
      "         ...,\n",
      "         [ 0.1899, -0.2256, -0.4210,  ..., -0.1269,  0.2089, -0.3156],\n",
      "         [ 0.1584, -0.0958, -0.3738,  ..., -0.1109,  0.1021, -0.6651],\n",
      "         [-0.0869,  0.0742, -0.2098,  ...,  0.1073, -0.0782, -0.9440]],\n",
      "\n",
      "        [[-0.3387,  0.2075, -1.0874,  ...,  0.5075,  0.2853, -0.0483],\n",
      "         [-0.0577,  0.1675, -0.8550,  ...,  0.2820,  0.4847, -0.4516],\n",
      "         [-0.3248,  0.3873, -0.9877,  ...,  0.4706,  0.7661, -0.4116],\n",
      "         ...,\n",
      "         [ 0.0556, -0.2998, -0.4971,  ...,  0.6691,  0.0541, -0.0962],\n",
      "         [-0.2938,  0.0994, -1.0308,  ...,  0.4479,  0.0513, -0.0778],\n",
      "         [-0.3130,  0.3484, -0.6315,  ...,  0.1418,  0.2779, -0.2071]],\n",
      "\n",
      "        [[ 0.4789, -0.1409, -0.1265,  ...,  0.3550,  0.0225, -0.4778],\n",
      "         [ 0.5554, -0.2304,  0.0931,  ...,  0.4895,  0.3136, -0.8511],\n",
      "         [ 0.2384,  0.0804, -0.2324,  ...,  0.3850,  0.2111, -0.6454],\n",
      "         ...,\n",
      "         [ 0.2300, -0.3664,  0.0041,  ...,  0.4080, -0.2321, -0.3970],\n",
      "         [ 0.1000, -0.1712,  0.2667,  ...,  0.3640, -0.0141, -0.7314],\n",
      "         [ 0.2116,  0.1579,  0.1312,  ...,  0.1922, -0.1203, -0.6691]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4267,  0.3986, -0.6717,  ...,  0.5733,  0.3954,  0.0346],\n",
      "         [-0.2108,  0.1524, -0.5085,  ...,  0.4247,  0.6168, -0.3096],\n",
      "         [-0.6401,  0.3068, -0.7050,  ...,  0.5619,  0.7294, -0.1006],\n",
      "         ...,\n",
      "         [-0.3286,  0.4911, -0.8597,  ...,  0.6514,  0.3748, -0.1161],\n",
      "         [-0.4389,  0.3216, -0.6296,  ...,  0.2812,  0.3426, -0.0526],\n",
      "         [-0.6520,  0.5671, -0.2707,  ...,  0.0732,  0.2649, -0.1411]],\n",
      "\n",
      "        [[ 0.0424, -0.0158, -0.6730,  ..., -0.0734,  0.3258,  0.2452],\n",
      "         [ 0.0931,  0.1187, -0.5820,  ..., -0.0934,  0.2243,  0.1718],\n",
      "         [-0.0303,  0.2464, -0.6377,  ..., -0.1316,  0.0258,  0.0944],\n",
      "         ...,\n",
      "         [ 0.0373, -0.1061, -0.7860,  ..., -0.1184,  0.1538,  0.1389],\n",
      "         [-0.2664,  0.2051, -0.5850,  ..., -0.2282,  0.0201, -0.0824],\n",
      "         [-0.3060,  0.2347, -0.4916,  ..., -0.3462, -0.0834, -0.1166]],\n",
      "\n",
      "        [[ 0.0715,  0.1491, -0.4224,  ...,  0.1366,  0.1116, -0.0708],\n",
      "         [ 0.1518,  0.0762, -0.5837,  ...,  0.0451,  0.2222, -0.4374],\n",
      "         [ 0.0647, -0.0838, -0.6068,  ..., -0.0796,  0.3797, -0.3573],\n",
      "         ...,\n",
      "         [ 0.2008, -0.1939, -0.5207,  ...,  0.0551,  0.1103, -0.1598],\n",
      "         [-0.2837,  0.0881, -0.5428,  ..., -0.2811, -0.1294, -0.2622],\n",
      "         [-0.0113,  0.1370, -0.3720,  ..., -0.3580, -0.1123, -0.4220]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.47505521774292\n",
      "Model outputs:  tensor([[[-0.3347,  0.2925, -0.5607,  ...,  0.5627, -0.1449, -0.0907],\n",
      "         [-0.2678,  0.2678, -0.4959,  ...,  0.2775,  0.2656, -0.2495],\n",
      "         [-0.3819,  0.4064, -0.7654,  ...,  0.4834,  0.1316, -0.2094],\n",
      "         ...,\n",
      "         [-0.4234,  0.1023, -0.5616,  ...,  0.4938, -0.2498, -0.1506],\n",
      "         [-0.0877, -0.0240, -0.3660,  ...,  0.3418,  0.0926, -0.3559],\n",
      "         [-0.2290,  0.3470, -0.5534,  ...,  0.2854,  0.1441, -0.2234]],\n",
      "\n",
      "        [[-0.4250,  0.1673, -0.4311,  ...,  0.0268,  0.3220,  0.3440],\n",
      "         [-0.4551,  0.0388, -0.3348,  ..., -0.0058,  0.2731,  0.1258],\n",
      "         [-0.7107,  0.2381, -0.1361,  ...,  0.1374,  0.2206, -0.0313],\n",
      "         ...,\n",
      "         [-0.3518,  0.2234, -0.2884,  ...,  0.3594, -0.0351,  0.2416],\n",
      "         [-0.4774, -0.0531, -0.1400,  ..., -0.0689,  0.0365, -0.1299],\n",
      "         [-0.7634,  0.4378, -0.3363,  ...,  0.0163,  0.2378,  0.0557]],\n",
      "\n",
      "        [[-0.3434,  0.1280, -0.5756,  ..., -0.4403,  0.0568,  0.0542],\n",
      "         [-0.2538,  0.1684, -0.4809,  ..., -0.4871, -0.0782,  0.0777],\n",
      "         [-0.4907,  0.0899, -0.5237,  ..., -0.3312, -0.0908,  0.1453],\n",
      "         ...,\n",
      "         [-0.5021,  0.1680, -0.6723,  ..., -0.3231, -0.2151,  0.3257],\n",
      "         [-0.5550, -0.0170, -0.3986,  ..., -0.4637, -0.0154, -0.0495],\n",
      "         [-0.2859, -0.0393, -0.4521,  ..., -0.2873, -0.1674,  0.0365]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2205,  0.3033, -0.4966,  ..., -0.4147, -0.4383,  0.3926],\n",
      "         [ 0.0820, -0.1207, -0.1038,  ..., -0.3435, -0.1838,  0.5294],\n",
      "         [-0.0235, -0.3257, -0.0671,  ..., -0.6701, -0.6396,  0.1023],\n",
      "         ...,\n",
      "         [-0.1044, -0.0633, -0.3530,  ..., -0.1534, -0.3400,  0.6037],\n",
      "         [ 0.2100, -0.3211, -0.0067,  ..., -0.8743, -0.4045,  0.3612],\n",
      "         [ 0.0691, -0.3040, -0.2213,  ..., -0.4469, -0.3272,  0.3985]],\n",
      "\n",
      "        [[-0.6501,  0.3290, -0.4126,  ..., -0.2826,  0.0518,  0.1189],\n",
      "         [-0.3612,  0.4103, -0.3791,  ..., -0.3473, -0.0547,  0.0129],\n",
      "         [-0.3814,  0.0614, -0.5648,  ..., -0.2090,  0.1044,  0.0549],\n",
      "         ...,\n",
      "         [-0.2634,  0.1565, -0.7107,  ..., -0.1211, -0.2662,  0.6592],\n",
      "         [-0.2183, -0.1472, -0.4228,  ..., -0.2114, -0.0216, -0.1557],\n",
      "         [-0.4082,  0.2430, -0.6361,  ..., -0.1596, -0.0199,  0.2089]],\n",
      "\n",
      "        [[ 0.0951, -0.2520, -0.3038,  ..., -0.3506, -0.3426,  0.3935],\n",
      "         [ 0.2100, -0.2085, -0.1489,  ..., -0.3579, -0.1215,  0.2796],\n",
      "         [ 0.0239, -0.2132, -0.2169,  ..., -0.4427, -0.5384,  0.3955],\n",
      "         ...,\n",
      "         [-0.0123, -0.2969, -0.2462,  ...,  0.1100, -0.3582,  0.4952],\n",
      "         [ 0.2809, -0.2793,  0.0183,  ..., -0.3912, -0.1936,  0.1726],\n",
      "         [ 0.1166, -0.1769, -0.0854,  ..., -0.3772, -0.0551,  0.4474]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1607567071914673\n",
      "Model outputs:  tensor([[[-0.3178,  0.0059, -0.6674,  ...,  0.1134, -0.2887,  0.5498],\n",
      "         [-0.1534, -0.0853, -0.8696,  ...,  0.0864, -0.0662,  0.3608],\n",
      "         [-0.2819, -0.1186, -0.8196,  ..., -0.0442, -0.1933,  0.5516],\n",
      "         ...,\n",
      "         [-0.1178,  0.0715, -0.6960,  ...,  0.3269, -0.0264,  0.0061],\n",
      "         [ 0.1805,  0.0253, -0.9370,  ...,  0.2939, -0.0339,  0.3452],\n",
      "         [-0.3226,  0.3305, -0.5628,  ...,  0.1282,  0.1383,  0.2300]],\n",
      "\n",
      "        [[-0.9445, -0.0193, -0.5687,  ...,  0.2788,  0.1442,  0.4219],\n",
      "         [-0.5662,  0.0644, -0.6675,  ...,  0.5748,  0.2295,  0.3286],\n",
      "         [-0.6351,  0.2055, -0.7036,  ...,  0.4458,  0.4945,  0.5482],\n",
      "         ...,\n",
      "         [-0.6359,  0.4335, -0.6298,  ...,  0.4264,  0.4278,  0.1615],\n",
      "         [-0.5279,  0.1420, -0.7313,  ...,  0.4010,  0.3232,  0.3962],\n",
      "         [-0.7875,  0.3932, -0.6690,  ...,  0.5891,  0.3394,  0.1539]],\n",
      "\n",
      "        [[ 0.0093, -0.3006, -0.6948,  ..., -0.2269, -0.3042,  0.6437],\n",
      "         [ 0.1773, -0.0226, -0.4860,  ..., -0.2665, -0.0735,  0.6166],\n",
      "         [-0.0346, -0.3063, -0.5048,  ..., -0.0368, -0.3603,  0.9442],\n",
      "         ...,\n",
      "         [-0.1314, -0.0470, -0.5628,  ..., -0.1703, -0.0872,  0.6948],\n",
      "         [ 0.0216, -0.1175, -0.5170,  ..., -0.0886, -0.2982,  0.4428],\n",
      "         [-0.1295, -0.3482, -0.5299,  ..., -0.1296, -0.3377,  0.3748]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6274,  0.3245, -0.2712,  ...,  0.8276,  0.0448,  0.1671],\n",
      "         [-0.3726, -0.0018, -0.5415,  ...,  0.8312,  0.3464,  0.0382],\n",
      "         [-0.4685,  0.2239, -0.5002,  ...,  0.6247,  0.6279,  0.0970],\n",
      "         ...,\n",
      "         [-0.4276,  0.4978, -0.6000,  ...,  0.7694,  0.3551, -0.2471],\n",
      "         [-0.5676,  0.0870, -0.5031,  ...,  0.5821,  0.3767,  0.1933],\n",
      "         [-0.4599,  0.2033, -0.3714,  ...,  0.6761,  0.4140, -0.1599]],\n",
      "\n",
      "        [[-0.4531,  0.3341, -0.8086,  ...,  0.4618, -0.2109,  0.3586],\n",
      "         [-0.1669,  0.1572, -0.7684,  ...,  0.4516, -0.0564,  0.0149],\n",
      "         [-0.3131,  0.2181, -0.8220,  ...,  0.6251, -0.1699,  0.3711],\n",
      "         ...,\n",
      "         [-0.2892,  0.2748, -0.8405,  ...,  0.3283, -0.1525,  0.2276],\n",
      "         [-0.2396,  0.0044, -1.0068,  ...,  0.2027, -0.0590,  0.3052],\n",
      "         [-0.1994,  0.1903, -0.9635,  ...,  0.7500,  0.0818, -0.0137]],\n",
      "\n",
      "        [[-0.3637,  0.4036, -1.2105,  ...,  0.8250,  0.1255,  0.3445],\n",
      "         [-0.5387,  0.1821, -0.9112,  ...,  0.9732,  0.4016,  0.0791],\n",
      "         [-0.5836,  0.1281, -0.9615,  ...,  1.0523,  0.1839,  0.2812],\n",
      "         ...,\n",
      "         [-0.3114,  0.4995, -0.9190,  ...,  1.0060,  0.0934, -0.1551],\n",
      "         [-0.2917,  0.2297, -0.9052,  ...,  0.8939,  0.3172,  0.1198],\n",
      "         [-0.4751,  0.3300, -0.8811,  ...,  0.8691,  0.0287, -0.0458]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3876253366470337\n",
      "Model outputs:  tensor([[[-4.7565e-01,  2.5009e-01,  1.1937e-01,  ...,  5.0439e-01,\n",
      "          -1.0259e-01, -2.2785e-01],\n",
      "         [-4.1561e-01,  7.2357e-02, -7.1566e-02,  ...,  6.8016e-01,\n",
      "           2.2952e-01,  1.2791e-01],\n",
      "         [-5.0873e-01,  3.7130e-01, -1.2035e-01,  ...,  8.4488e-01,\n",
      "           3.6374e-01, -2.7053e-01],\n",
      "         ...,\n",
      "         [-3.6784e-01, -4.3172e-02, -1.4570e-01,  ...,  6.9899e-01,\n",
      "           1.0483e-01, -1.8139e-01],\n",
      "         [ 4.5959e-02,  9.0651e-02,  1.8641e-01,  ...,  3.7223e-01,\n",
      "           1.2780e-01, -3.4722e-01],\n",
      "         [-4.0897e-02, -1.5038e-01, -1.6437e-01,  ...,  5.0271e-01,\n",
      "           9.0254e-02, -3.8503e-01]],\n",
      "\n",
      "        [[-2.7136e-01,  1.3766e-01, -6.2102e-01,  ..., -1.2595e-02,\n",
      "          -1.0083e-01, -7.4856e-02],\n",
      "         [-5.5246e-01,  1.1835e-01, -6.3829e-01,  ...,  8.3704e-02,\n",
      "          -1.8361e-01, -1.2623e-01],\n",
      "         [-1.7742e-02,  1.7655e-01, -5.6675e-01,  ...,  3.9713e-01,\n",
      "           2.1439e-01,  2.1415e-01],\n",
      "         ...,\n",
      "         [-5.0464e-02,  2.1317e-02, -6.5407e-01,  ...,  1.6974e-01,\n",
      "          -1.0558e-01, -1.0054e-01],\n",
      "         [ 2.1442e-01,  7.5637e-02, -2.6623e-01,  ...,  4.2818e-02,\n",
      "          -1.2552e-02, -3.0264e-01],\n",
      "         [ 1.5305e-01,  6.8957e-02, -8.4249e-01,  ...,  9.2505e-02,\n",
      "          -4.3757e-01, -1.7795e-01]],\n",
      "\n",
      "        [[-6.3336e-01,  1.7779e-01, -8.7306e-02,  ...,  2.7696e-01,\n",
      "           2.9704e-02,  3.2722e-02],\n",
      "         [-4.3705e-01,  1.0691e-01, -1.7306e-01,  ...,  2.4354e-01,\n",
      "           3.1177e-01, -2.3347e-02],\n",
      "         [-4.0084e-01, -2.6275e-01, -3.4219e-01,  ...,  4.6195e-01,\n",
      "           3.4729e-01, -2.5440e-02],\n",
      "         ...,\n",
      "         [-2.6846e-01, -1.8355e-01, -1.6964e-01,  ...,  4.2466e-01,\n",
      "           3.9862e-01,  1.1459e-01],\n",
      "         [-1.1024e-03,  1.0967e-01, -1.4627e-01,  ...,  4.6176e-03,\n",
      "           9.4595e-02, -1.6504e-01],\n",
      "         [-2.3352e-01, -2.4929e-01, -1.9626e-01,  ..., -7.8696e-03,\n",
      "          -1.2348e-01, -2.3023e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-7.4476e-02,  1.0895e-01, -6.2654e-01,  ...,  6.1677e-02,\n",
      "           8.3340e-02,  2.6750e-01],\n",
      "         [-2.6691e-01,  4.7054e-02, -5.5863e-01,  ..., -8.8403e-02,\n",
      "           1.4233e-02,  2.7856e-01],\n",
      "         [ 5.0687e-03, -3.0748e-01, -5.6977e-01,  ..., -7.7952e-02,\n",
      "          -1.6133e-01,  1.6209e-01],\n",
      "         ...,\n",
      "         [ 1.7035e-01, -2.7486e-01, -4.3976e-01,  ...,  3.9177e-02,\n",
      "          -2.7044e-04,  5.3232e-01],\n",
      "         [ 1.8914e-01, -1.8595e-01, -1.9233e-01,  ..., -1.7630e-01,\n",
      "          -2.1008e-01, -4.8289e-02],\n",
      "         [ 3.5266e-01, -3.3131e-01, -5.5639e-01,  ..., -1.0186e-01,\n",
      "           7.0197e-02,  2.1944e-02]],\n",
      "\n",
      "        [[-5.6369e-01,  6.6107e-02, -2.1235e-01,  ...,  1.8385e-01,\n",
      "          -5.4740e-02, -1.2126e-01],\n",
      "         [-6.8412e-01,  6.9174e-02, -1.9451e-01,  ...,  3.6691e-01,\n",
      "          -1.7109e-03, -1.9241e-02],\n",
      "         [-4.2480e-01,  1.9504e-01, -4.3007e-01,  ...,  6.3052e-01,\n",
      "           1.2700e-01,  4.6532e-02],\n",
      "         ...,\n",
      "         [ 3.4482e-03, -1.7088e-01, -4.1343e-01,  ...,  3.6683e-01,\n",
      "           2.0351e-02, -4.2595e-02],\n",
      "         [-7.6359e-03, -1.0747e-01, -7.2330e-02,  ...,  3.0817e-01,\n",
      "           4.0019e-02, -4.0456e-01],\n",
      "         [-1.5786e-01, -3.9918e-02, -1.0120e-01,  ...,  2.6220e-01,\n",
      "           1.7569e-01, -5.4323e-01]],\n",
      "\n",
      "        [[ 1.2227e-01, -7.8879e-02, -1.7401e-01,  ..., -2.9746e-01,\n",
      "          -2.6295e-01,  1.9907e-01],\n",
      "         [ 6.2996e-02, -1.4104e-01, -5.3204e-01,  ..., -3.7260e-01,\n",
      "          -3.9600e-01,  4.2183e-01],\n",
      "         [ 6.3479e-02, -2.3964e-01, -4.6083e-01,  ..., -4.5868e-02,\n",
      "           1.9431e-01,  2.6560e-01],\n",
      "         ...,\n",
      "         [-3.7624e-02, -1.0957e-01, -5.8461e-01,  ...,  1.4638e-01,\n",
      "          -2.6322e-01,  3.2527e-01],\n",
      "         [ 1.8789e-01, -1.6156e-01, -5.9049e-02,  ..., -4.5915e-01,\n",
      "          -2.2674e-01,  1.6223e-01],\n",
      "         [ 1.9691e-01, -2.0778e-01, -4.5553e-01,  ..., -3.8258e-01,\n",
      "          -1.4913e-01,  1.8755e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9335013628005981\n",
      "Model outputs:  tensor([[[ 0.0805,  0.1953, -0.5907,  ...,  0.5257,  0.2560,  0.0419],\n",
      "         [-0.2154,  0.1404, -0.6875,  ...,  0.3401,  0.1035, -0.0564],\n",
      "         [ 0.3060,  0.0834, -0.5119,  ...,  0.1017,  0.1528, -0.1544],\n",
      "         ...,\n",
      "         [-0.1495,  0.1641, -0.6165,  ...,  0.1675, -0.0384, -0.0498],\n",
      "         [-0.1278,  0.0409, -0.5172,  ...,  0.0506,  0.2967, -0.1325],\n",
      "         [ 0.1993, -0.0299, -0.5708,  ...,  0.6707,  0.0768, -0.0307]],\n",
      "\n",
      "        [[-0.1477,  0.0538, -0.4053,  ...,  0.4367,  0.3748, -0.0111],\n",
      "         [-0.7203,  0.3522, -0.2525,  ...,  0.3038,  0.0651, -0.0168],\n",
      "         [-0.2407,  0.3493, -0.4269,  ...,  0.0859,  0.1040,  0.0344],\n",
      "         ...,\n",
      "         [-0.7324,  0.3043, -0.3856,  ...,  0.2625,  0.1284, -0.1345],\n",
      "         [-0.0991,  0.0254, -0.3607,  ...,  0.4143,  0.4468,  0.1058],\n",
      "         [-0.2047, -0.0830, -0.2322,  ...,  0.4242,  0.4217,  0.0226]],\n",
      "\n",
      "        [[-0.1598,  0.0528, -0.4664,  ...,  0.3613,  0.6995,  0.0378],\n",
      "         [-0.5094,  0.3751, -0.3806,  ...,  0.4356,  0.2992, -0.0768],\n",
      "         [-0.1511,  0.4438, -0.3544,  ...,  0.0368,  0.4927, -0.1356],\n",
      "         ...,\n",
      "         [-0.3750,  0.1250, -0.3751,  ...,  0.3710,  0.2354,  0.2756],\n",
      "         [-0.3977, -0.0047, -0.5694,  ...,  0.5580,  0.5791,  0.1802],\n",
      "         [-0.3564, -0.0289, -0.3488,  ...,  0.3585,  0.4918,  0.1611]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2692, -0.1151, -0.3299,  ..., -0.1263, -0.0093,  0.1935],\n",
      "         [-0.1615, -0.0444, -0.5225,  ..., -0.1477, -0.2106,  0.3471],\n",
      "         [ 0.1961,  0.1417, -0.3251,  ..., -0.1309,  0.0815,  0.0585],\n",
      "         ...,\n",
      "         [ 0.3500, -0.1634, -0.3558,  ..., -0.1001,  0.1071,  0.2706],\n",
      "         [ 0.2771, -0.1339, -0.2013,  ..., -0.1050,  0.0167,  0.6647],\n",
      "         [ 0.1740,  0.0139, -0.4122,  ...,  0.0861,  0.3133,  0.3543]],\n",
      "\n",
      "        [[ 0.1796, -0.0697, -0.7688,  ...,  0.4395,  0.1213,  0.1737],\n",
      "         [-0.1232,  0.2531, -0.6689,  ...,  0.2144, -0.1661, -0.2310],\n",
      "         [ 0.0499,  0.2369, -0.5996,  ...,  0.0811, -0.0640, -0.1493],\n",
      "         ...,\n",
      "         [ 0.0181, -0.0106, -0.6449,  ...,  0.4385, -0.0128, -0.1262],\n",
      "         [-0.0834, -0.1451, -0.6277,  ...,  0.2065, -0.0643, -0.3509],\n",
      "         [ 0.1710, -0.2224, -0.6143,  ...,  0.5320,  0.2407,  0.2510]],\n",
      "\n",
      "        [[ 0.0185,  0.0538, -0.4659,  ...,  0.1108,  0.1335,  0.3221],\n",
      "         [ 0.1605,  0.1053, -0.5653,  ..., -0.0946,  0.2296,  0.2310],\n",
      "         [ 0.0925,  0.0654, -0.5000,  ...,  0.0986,  0.1719, -0.0581],\n",
      "         ...,\n",
      "         [ 0.0815,  0.2000, -0.5119,  ..., -0.2261, -0.1331,  0.2955],\n",
      "         [ 0.2535,  0.0039, -0.4589,  ..., -0.1564, -0.0302,  0.3482],\n",
      "         [-0.1224, -0.2963, -0.2483,  ...,  0.2231,  0.1245,  0.0457]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.144214391708374\n",
      "Model outputs:  tensor([[[-4.4851e-01,  6.3978e-02, -3.6585e-01,  ...,  4.2040e-01,\n",
      "           1.1902e-01, -9.2276e-03],\n",
      "         [-2.7808e-01,  1.7907e-01, -2.5152e-01,  ...,  6.9863e-01,\n",
      "           6.4866e-01,  1.9240e-02],\n",
      "         [-3.6501e-01,  1.2885e-01, -4.4396e-01,  ...,  3.8454e-01,\n",
      "           8.3471e-02,  4.2740e-01],\n",
      "         ...,\n",
      "         [-3.3308e-01,  1.3170e-01, -4.9648e-02,  ...,  5.3556e-01,\n",
      "           1.8710e-01,  1.4465e-01],\n",
      "         [-3.5983e-01,  1.9898e-01, -4.4792e-01,  ...,  7.0204e-01,\n",
      "           2.2727e-01,  8.2020e-02],\n",
      "         [-5.4199e-01,  1.9250e-01, -6.1475e-01,  ...,  6.2894e-01,\n",
      "           1.5518e-01,  2.7208e-01]],\n",
      "\n",
      "        [[-2.1203e-01,  3.0957e-01, -7.7929e-01,  ...,  9.7843e-01,\n",
      "           3.0102e-01,  2.9243e-02],\n",
      "         [-3.4869e-01,  1.3637e-01, -8.5121e-01,  ...,  9.9565e-01,\n",
      "           2.1316e-01, -6.9458e-02],\n",
      "         [-2.6369e-01, -5.9639e-02, -9.2381e-01,  ...,  6.7095e-01,\n",
      "          -5.8278e-02,  3.3833e-01],\n",
      "         ...,\n",
      "         [-4.7919e-01,  5.1165e-01, -7.5631e-01,  ...,  7.5826e-01,\n",
      "           1.7634e-02, -1.3160e-01],\n",
      "         [-4.0015e-02,  5.1006e-01, -1.1025e+00,  ...,  8.6587e-01,\n",
      "           1.3240e-01,  6.2246e-02],\n",
      "         [-3.9828e-01,  3.6510e-01, -9.3246e-01,  ...,  8.7794e-01,\n",
      "           2.1709e-01,  1.2884e-01]],\n",
      "\n",
      "        [[ 1.1156e-02, -2.5676e-01, -5.8040e-01,  ...,  2.5597e-02,\n",
      "          -4.4882e-01,  7.5526e-01],\n",
      "         [-1.7426e-01, -1.6835e-01, -3.9055e-01,  ..., -3.7748e-02,\n",
      "          -2.9628e-01,  3.1728e-01],\n",
      "         [ 9.4708e-02, -1.1091e-01, -3.1819e-01,  ..., -1.1041e-01,\n",
      "          -4.2958e-01,  5.3574e-01],\n",
      "         ...,\n",
      "         [-1.1645e-04, -4.8950e-02, -2.3775e-01,  ..., -1.4785e-01,\n",
      "          -3.5256e-01,  6.7560e-01],\n",
      "         [ 1.2253e-01, -3.3652e-01, -6.7047e-01,  ...,  8.2526e-03,\n",
      "          -2.3696e-01,  4.4917e-01],\n",
      "         [-7.4636e-02, -3.3568e-01, -3.8268e-01,  ...,  8.9631e-02,\n",
      "          -2.9325e-01,  6.2934e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2936e-01, -4.9642e-02, -5.2608e-01,  ...,  1.7411e-02,\n",
      "          -1.2305e-01,  3.6924e-01],\n",
      "         [-9.1417e-02, -8.1576e-02, -4.4401e-01,  ..., -8.8866e-02,\n",
      "          -3.0956e-01,  6.0548e-01],\n",
      "         [ 3.5645e-02, -1.6130e-01, -2.9872e-01,  ..., -1.2477e-01,\n",
      "          -4.9081e-01,  9.4025e-01],\n",
      "         ...,\n",
      "         [-2.5247e-01, -1.2305e-01, -3.2858e-01,  ..., -9.3078e-03,\n",
      "          -6.7989e-01,  4.4595e-01],\n",
      "         [ 2.0857e-01,  1.9154e-02, -4.8570e-01,  ...,  1.4757e-01,\n",
      "          -2.7139e-01,  4.8445e-01],\n",
      "         [-2.3783e-01, -3.9978e-01, -6.3224e-01,  ..., -1.3607e-01,\n",
      "          -4.3018e-01,  6.0511e-01]],\n",
      "\n",
      "        [[-2.2065e-01, -3.7417e-02, -9.5120e-01,  ...,  1.0467e-01,\n",
      "           6.2583e-02,  3.1103e-01],\n",
      "         [-5.4406e-01, -1.3173e-01, -5.4867e-01,  ...,  5.1132e-02,\n",
      "           1.1043e-01,  2.0910e-02],\n",
      "         [-3.4299e-01,  1.2426e-01, -7.6265e-01,  ..., -8.0951e-02,\n",
      "           1.6876e-01,  4.8616e-01],\n",
      "         ...,\n",
      "         [-5.6013e-01,  3.0697e-02, -9.4359e-01,  ...,  5.1687e-01,\n",
      "           9.2915e-02,  3.1571e-01],\n",
      "         [-5.2080e-01,  6.4015e-02, -1.2460e+00,  ...,  3.4838e-01,\n",
      "           2.2966e-01,  2.0075e-01],\n",
      "         [-5.5474e-01,  3.5206e-01, -6.6622e-01,  ...,  1.3047e-01,\n",
      "           6.3117e-02,  3.2505e-01]],\n",
      "\n",
      "        [[-1.9882e-01,  7.2635e-02, -8.5867e-01,  ...,  7.0371e-01,\n",
      "          -5.0287e-02,  1.5857e-01],\n",
      "         [ 1.4425e-02,  6.5390e-02, -5.8345e-01,  ...,  5.8960e-01,\n",
      "           2.0926e-02,  1.4833e-01],\n",
      "         [-1.9865e-01,  1.1897e-01, -1.0343e+00,  ...,  5.2295e-01,\n",
      "          -7.9367e-02,  3.3329e-01],\n",
      "         ...,\n",
      "         [-5.2139e-01,  1.2589e-01, -9.0015e-01,  ...,  4.4905e-01,\n",
      "          -3.1622e-01,  4.8052e-02],\n",
      "         [-4.2639e-01,  1.6918e-01, -9.8790e-01,  ...,  3.7831e-01,\n",
      "          -2.3928e-02, -2.0552e-03],\n",
      "         [-2.5753e-01,  2.9528e-01, -9.0038e-01,  ...,  3.6947e-01,\n",
      "           5.2962e-03,  2.4134e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9324751496315002\n",
      "Model outputs:  tensor([[[-0.5646,  0.1486, -0.7581,  ...,  0.1257, -0.1250,  0.1271],\n",
      "         [-0.3805,  0.2531, -0.3828,  ..., -0.0313, -0.0577, -0.1292],\n",
      "         [ 0.2335, -0.1335, -0.4946,  ...,  0.3121, -0.2067, -0.0188],\n",
      "         ...,\n",
      "         [-0.2413,  0.2237, -0.8675,  ...,  0.1727, -0.0464,  0.2661],\n",
      "         [ 0.0367, -0.0153, -0.7836,  ...,  0.3754,  0.1894,  0.2036],\n",
      "         [-0.1692,  0.0289, -0.7920,  ...,  0.1683,  0.1007,  0.0681]],\n",
      "\n",
      "        [[-0.3483,  0.1978, -0.7788,  ...,  0.2118, -0.0380,  0.1543],\n",
      "         [-0.3365,  0.2488, -0.5879,  ..., -0.0323,  0.1004, -0.0537],\n",
      "         [ 0.2386, -0.1033, -0.4376,  ..., -0.0093, -0.3215, -0.1298],\n",
      "         ...,\n",
      "         [-0.3532,  0.4214, -0.8368,  ...,  0.2557, -0.2296,  0.1198],\n",
      "         [-0.2363,  0.0091, -0.7302,  ...,  0.5420,  0.1151,  0.1089],\n",
      "         [-0.0267,  0.0213, -0.4154,  ...,  0.3302, -0.1774,  0.0327]],\n",
      "\n",
      "        [[-0.4697,  0.3797,  0.0528,  ...,  0.7075,  0.4585,  0.0011],\n",
      "         [-0.5173,  0.2475, -0.2189,  ...,  0.3224,  0.3526, -0.0287],\n",
      "         [-0.2537, -0.3967,  0.1791,  ...,  0.2239,  0.0758, -0.2683],\n",
      "         ...,\n",
      "         [-0.6328,  0.3710, -0.1868,  ...,  0.5706,  0.3587,  0.0274],\n",
      "         [-0.4435, -0.0843, -0.3325,  ...,  0.6819,  0.3081,  0.1209],\n",
      "         [-0.3255,  0.1759,  0.0227,  ...,  0.7891,  0.2974, -0.0090]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1755,  0.1564, -0.7230,  ...,  0.0360, -0.1397,  0.1388],\n",
      "         [-0.4006,  0.4245, -0.6575,  ...,  0.1091, -0.0962, -0.0397],\n",
      "         [ 0.1245,  0.1395, -0.3558,  ...,  0.1203, -0.3321, -0.1093],\n",
      "         ...,\n",
      "         [-0.2190,  0.3726, -0.8188,  ...,  0.2584, -0.0733,  0.0357],\n",
      "         [-0.0891,  0.0754, -0.6624,  ...,  0.3018, -0.1908,  0.3160],\n",
      "         [-0.0922, -0.0741, -0.7849,  ...,  0.2839, -0.0046,  0.2130]],\n",
      "\n",
      "        [[-0.6812,  0.1963, -0.4869,  ...,  0.4015,  0.0806, -0.0528],\n",
      "         [-0.3159,  0.3301, -0.4158,  ..., -0.0292, -0.0690,  0.1009],\n",
      "         [-0.3509, -0.0507,  0.0633,  ..., -0.0332,  0.1534, -0.1579],\n",
      "         ...,\n",
      "         [-0.5815,  0.2588, -0.2796,  ...,  0.5214,  0.2917, -0.0141],\n",
      "         [-0.4475,  0.1773, -0.4330,  ...,  0.3476,  0.2762,  0.1676],\n",
      "         [-0.4919,  0.0977, -0.4005,  ...,  0.5887,  0.3109,  0.1693]],\n",
      "\n",
      "        [[-0.3983,  0.2802, -0.7588,  ...,  0.5779,  0.2642, -0.4364],\n",
      "         [-0.2591,  0.2798, -0.3855,  ...,  0.4671,  0.0886, -0.1521],\n",
      "         [-0.0419,  0.2312, -0.5583,  ...,  0.5265,  0.1523, -0.1439],\n",
      "         ...,\n",
      "         [-0.3401,  0.4461, -0.8448,  ...,  0.4629, -0.2346, -0.0369],\n",
      "         [ 0.1319, -0.0146, -0.8031,  ...,  0.6313,  0.2501,  0.4262],\n",
      "         [-0.0738,  0.0898, -0.8283,  ...,  0.6143,  0.0804,  0.0455]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1850041151046753\n",
      "Model outputs:  tensor([[[ 0.0595, -0.1785, -0.5578,  ..., -0.3844, -0.1836,  0.2633],\n",
      "         [ 0.0017, -0.3355, -0.4323,  ..., -0.7936, -0.1265,  0.5570],\n",
      "         [-0.2338, -0.0918, -0.5544,  ..., -0.6780, -0.2786,  0.1299],\n",
      "         ...,\n",
      "         [ 0.0485, -0.3256, -0.2737,  ..., -0.2446, -0.2329,  0.2145],\n",
      "         [-0.1371, -0.1756, -0.4033,  ..., -0.5123, -0.5550,  0.1711],\n",
      "         [-0.1028, -0.1966, -0.3306,  ..., -0.3234, -0.2900,  0.5485]],\n",
      "\n",
      "        [[-0.0084, -0.3684, -0.4577,  ..., -0.3566, -0.1326,  0.4618],\n",
      "         [ 0.2120, -0.2210, -0.3224,  ..., -0.7357, -0.2508,  0.0975],\n",
      "         [ 0.1765,  0.1384, -0.0990,  ..., -0.6353, -0.5112,  0.0634],\n",
      "         ...,\n",
      "         [-0.1707, -0.1505, -0.3999,  ..., -0.3714, -0.3620,  0.3380],\n",
      "         [-0.0040, -0.2669, -0.5533,  ..., -0.2825, -0.7648,  0.1989],\n",
      "         [ 0.1855, -0.1711, -0.3173,  ..., -0.4162, -0.2706,  0.2119]],\n",
      "\n",
      "        [[ 0.1028, -0.2264, -0.4828,  ..., -0.2263, -0.0932,  0.3793],\n",
      "         [ 0.0442, -0.1000, -0.3660,  ..., -0.4540, -0.1672, -0.0744],\n",
      "         [-0.0767,  0.0724, -0.3526,  ..., -0.8646, -0.0503,  0.1464],\n",
      "         ...,\n",
      "         [-0.0888, -0.1691, -0.5051,  ..., -0.4604, -0.4119,  0.2612],\n",
      "         [ 0.0579, -0.0365, -0.3525,  ..., -0.4882, -0.4484,  0.2834],\n",
      "         [-0.0680, -0.1561, -0.5549,  ..., -0.2300, -0.2075,  0.4896]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4119,  0.3422, -1.0078,  ...,  0.4762,  0.0477, -0.0734],\n",
      "         [-0.2156,  0.4449, -0.4023,  ...,  0.3440,  0.2173, -0.2684],\n",
      "         [-0.1816,  0.0872, -0.5858,  ...,  0.3072,  0.0776, -0.1745],\n",
      "         ...,\n",
      "         [-0.2734,  0.2004, -0.9426,  ...,  0.1818,  0.0805,  0.2466],\n",
      "         [-0.2374,  0.3572, -0.9554,  ...,  0.3512, -0.2040,  0.0703],\n",
      "         [-0.4992,  0.2205, -0.7322,  ...,  0.3854, -0.0157,  0.0360]],\n",
      "\n",
      "        [[-0.0681, -0.2873, -0.3556,  ..., -0.5397,  0.5146,  0.6552],\n",
      "         [ 0.1443,  0.1359, -0.7066,  ..., -0.7618,  0.6427,  0.6900],\n",
      "         [-0.1843,  0.1459, -0.3811,  ..., -0.7778,  0.4444,  0.3740],\n",
      "         ...,\n",
      "         [-0.0608, -0.2016, -0.4507,  ..., -0.6860,  0.3950,  0.6482],\n",
      "         [-0.1044, -0.1007, -0.5539,  ..., -0.6215,  0.3448,  0.3123],\n",
      "         [-0.0350,  0.0438, -0.4594,  ..., -0.5531,  0.3168,  0.6991]],\n",
      "\n",
      "        [[ 0.1771, -0.1263, -0.4393,  ..., -0.2985,  0.1706,  0.5101],\n",
      "         [-0.0384,  0.0018, -0.2234,  ..., -0.3014, -0.3102,  0.1756],\n",
      "         [ 0.2612,  0.0150,  0.0911,  ..., -0.3500, -0.5071,  0.0088],\n",
      "         ...,\n",
      "         [-0.2359, -0.1216, -0.3337,  ..., -0.4257, -0.5975,  0.6900],\n",
      "         [-0.2687, -0.1815, -0.1458,  ..., -0.5544, -0.4519,  0.3406],\n",
      "         [-0.0415, -0.1988, -0.4550,  ..., -0.5301, -0.4765,  0.6059]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9550034999847412\n",
      "Model outputs:  tensor([[[-0.1400,  0.3264, -0.3002,  ...,  0.0858,  0.2250, -0.1366],\n",
      "         [-0.3620,  0.2607, -0.5583,  ...,  0.5137,  0.0281, -0.0411],\n",
      "         [-0.3921,  0.2403, -0.6236,  ...,  0.7076,  0.0525,  0.1018],\n",
      "         ...,\n",
      "         [ 0.0896,  0.1383, -0.5441,  ...,  0.2964,  0.3395, -0.2663],\n",
      "         [-0.3466,  0.5566, -0.7744,  ...,  0.4834,  0.2325,  0.0335],\n",
      "         [-0.4477,  0.3350, -0.4449,  ...,  0.3480,  0.0095, -0.0103]],\n",
      "\n",
      "        [[ 0.0754, -0.2515,  0.3178,  ..., -0.5171,  0.0680,  0.0931],\n",
      "         [ 0.3688, -0.1645, -0.2812,  ..., -0.1938, -0.3870,  0.1186],\n",
      "         [ 0.1407, -0.2936, -0.3734,  ..., -0.1231, -0.1316,  0.4487],\n",
      "         ...,\n",
      "         [ 0.2723, -0.2541, -0.1528,  ..., -0.1551, -0.1527, -0.1175],\n",
      "         [ 0.2393, -0.1154, -0.4740,  ..., -0.2832, -0.0265,  0.1754],\n",
      "         [ 0.2145, -0.3602, -0.3010,  ..., -0.5491, -0.1506,  0.2996]],\n",
      "\n",
      "        [[-0.4917,  0.4551, -0.0593,  ...,  0.3681,  0.2083, -0.4763],\n",
      "         [-0.3080,  0.3156, -0.5878,  ...,  0.3587,  0.0502, -0.1726],\n",
      "         [-0.4126, -0.0486, -0.3984,  ...,  0.7065,  0.0911,  0.0237],\n",
      "         ...,\n",
      "         [-0.0659,  0.1486, -0.6278,  ...,  0.2499,  0.3416, -0.3480],\n",
      "         [-0.4317,  0.4018, -0.4999,  ...,  0.5990,  0.0186, -0.0904],\n",
      "         [-0.3382,  0.2358, -0.8002,  ...,  0.4754,  0.2627, -0.2269]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5209,  0.1516,  0.0267,  ..., -0.0812,  0.3473, -0.3333],\n",
      "         [-0.1264,  0.2094, -0.3083,  ...,  0.3150,  0.0752, -0.1578],\n",
      "         [-0.3428, -0.0230,  0.0210,  ...,  0.5863,  0.1303,  0.0165],\n",
      "         ...,\n",
      "         [-0.0811, -0.0363, -0.2762,  ...,  0.3932,  0.1868, -0.2750],\n",
      "         [-0.2553,  0.1116, -0.1689,  ...,  0.4613,  0.1866, -0.3450],\n",
      "         [-0.3540,  0.1472, -0.2725,  ...,  0.3425,  0.1341, -0.1931]],\n",
      "\n",
      "        [[-0.1831,  0.1543,  0.0242,  ..., -0.0377, -0.0630, -0.2972],\n",
      "         [-0.1022,  0.4842, -0.7317,  ...,  0.1459, -0.1957,  0.0325],\n",
      "         [ 0.0185,  0.0910, -0.5362,  ...,  0.3931, -0.1033,  0.1106],\n",
      "         ...,\n",
      "         [ 0.4485, -0.0059, -0.4883,  ...,  0.2340, -0.0462, -0.0179],\n",
      "         [-0.0717,  0.3613, -0.6535,  ...,  0.0231, -0.0458, -0.1831],\n",
      "         [-0.0405,  0.2394, -0.6073,  ...,  0.1564, -0.2659, -0.1052]],\n",
      "\n",
      "        [[-0.5760,  0.2541,  0.1702,  ...,  0.0317,  0.2760, -0.6046],\n",
      "         [-0.2995,  0.3866, -0.2565,  ...,  0.4685,  0.0732, -0.1517],\n",
      "         [-0.0968,  0.1410, -0.1160,  ...,  0.5179,  0.2007, -0.2024],\n",
      "         ...,\n",
      "         [-0.0463,  0.1685, -0.3138,  ...,  0.4422,  0.3476, -0.3207],\n",
      "         [-0.2990,  0.2229, -0.2135,  ...,  0.6582,  0.3302, -0.2290],\n",
      "         [-0.1144,  0.4159, -0.1492,  ...,  0.4511,  0.2576, -0.2588]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.046479344367981\n",
      "Model outputs:  tensor([[[-1.7348e-01,  6.3183e-02, -4.5780e-01,  ...,  4.4974e-01,\n",
      "           1.9736e-01,  7.4503e-02],\n",
      "         [-4.2594e-01,  1.0559e-01, -1.7488e-01,  ...,  1.5667e-01,\n",
      "           2.1352e-01, -1.3449e-01],\n",
      "         [-4.1639e-01, -1.9814e-02, -1.9703e-01,  ...,  3.3884e-02,\n",
      "           5.9036e-02, -6.0533e-02],\n",
      "         ...,\n",
      "         [-4.1024e-01,  3.1976e-01, -5.7306e-01,  ...,  3.9463e-01,\n",
      "           6.5463e-01, -6.0958e-02],\n",
      "         [-5.6561e-01,  2.0789e-01, -2.2516e-01,  ...,  3.3553e-01,\n",
      "           2.2333e-01, -3.6190e-01],\n",
      "         [-4.8780e-01, -1.0947e-02, -3.6544e-01,  ...,  1.9128e-01,\n",
      "           4.4491e-01, -3.6363e-01]],\n",
      "\n",
      "        [[ 2.4927e-01, -2.6441e-01, -2.2298e-01,  ..., -6.5711e-01,\n",
      "          -2.5502e-01,  7.2665e-02],\n",
      "         [-2.0332e-01,  4.2460e-02, -2.7108e-01,  ..., -4.4080e-01,\n",
      "          -3.6581e-01,  1.5913e-01],\n",
      "         [ 9.2878e-02, -2.6262e-01, -3.6924e-01,  ..., -5.3651e-01,\n",
      "          -1.3392e-01,  4.7215e-01],\n",
      "         ...,\n",
      "         [-2.5764e-01,  4.9819e-02, -3.0400e-01,  ..., -2.6830e-01,\n",
      "          -1.3173e-01,  3.4904e-01],\n",
      "         [-1.7006e-02, -9.1356e-02, -1.1453e-01,  ..., -5.0923e-01,\n",
      "          -2.5006e-01,  1.6200e-01],\n",
      "         [ 3.2981e-02, -2.3520e-01, -4.3694e-01,  ..., -5.8334e-01,\n",
      "          -7.2591e-02,  1.0890e-01]],\n",
      "\n",
      "        [[-2.1250e-01, -1.0653e-01, -2.2730e-01,  ..., -4.9823e-01,\n",
      "          -1.7376e-01,  1.2689e-01],\n",
      "         [-1.2997e-01,  1.0174e-01, -2.9053e-01,  ..., -4.6818e-01,\n",
      "          -1.5825e-01, -1.4176e-01],\n",
      "         [ 1.5347e-01, -3.7658e-03, -5.2937e-01,  ..., -4.9001e-01,\n",
      "          -1.1608e-01,  5.3323e-02],\n",
      "         ...,\n",
      "         [-3.1609e-01, -2.1213e-01, -4.8518e-01,  ..., -2.5587e-01,\n",
      "           8.7180e-02,  5.5889e-02],\n",
      "         [-1.6922e-01,  7.9180e-02, -4.2720e-01,  ..., -2.3576e-01,\n",
      "           4.5461e-02,  3.6403e-01],\n",
      "         [ 9.8789e-02, -2.3018e-02, -5.4531e-01,  ..., -4.0951e-01,\n",
      "          -9.4859e-02, -4.4340e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.0270e-01, -2.4993e-01, -1.9271e-01,  ..., -8.3324e-01,\n",
      "           7.3616e-01,  5.4026e-01],\n",
      "         [-3.5094e-02, -1.3733e-02, -5.3337e-01,  ..., -6.5366e-01,\n",
      "           5.8503e-01,  6.0088e-01],\n",
      "         [ 2.4330e-01, -1.2933e-01, -4.3961e-01,  ..., -6.6060e-01,\n",
      "           5.0406e-01,  3.2636e-01],\n",
      "         ...,\n",
      "         [-4.0455e-01, -1.1657e-01, -6.5143e-01,  ..., -5.6734e-01,\n",
      "           6.9588e-01,  9.1740e-01],\n",
      "         [-1.3745e-03,  4.4752e-03, -5.3561e-01,  ..., -5.8336e-01,\n",
      "           8.4269e-01,  6.8637e-01],\n",
      "         [ 9.7607e-03, -1.2147e-01, -5.5514e-01,  ..., -8.3974e-01,\n",
      "           6.9314e-01,  4.0442e-01]],\n",
      "\n",
      "        [[-1.0224e-01,  2.9606e-01, -7.0063e-01,  ..., -1.1950e-02,\n",
      "           2.7803e-01, -3.0756e-01],\n",
      "         [-4.3434e-04,  1.1940e-01, -6.6649e-01,  ...,  2.6843e-01,\n",
      "           2.7377e-01, -3.4965e-01],\n",
      "         [-2.1571e-01,  7.8272e-02, -5.6235e-01,  ...,  1.4895e-01,\n",
      "           1.1000e-01, -3.1855e-01],\n",
      "         ...,\n",
      "         [-3.9939e-01,  2.0096e-01, -8.7566e-01,  ..., -7.2169e-02,\n",
      "           2.2047e-01, -1.3889e-01],\n",
      "         [-2.1542e-01,  3.7535e-01, -9.1016e-01,  ...,  2.1310e-01,\n",
      "           4.3463e-01, -1.9311e-01],\n",
      "         [-2.3980e-01,  5.0210e-02, -4.6409e-01,  ...,  1.5206e-01,\n",
      "           7.7459e-02, -3.1515e-01]],\n",
      "\n",
      "        [[-2.0294e-01,  5.4041e-02, -8.0088e-01,  ..., -2.4709e-01,\n",
      "           2.4237e-01, -1.5845e-01],\n",
      "         [-3.3384e-01, -1.4602e-01, -7.1893e-01,  ..., -3.1542e-01,\n",
      "           8.1014e-02,  2.3128e-02],\n",
      "         [-3.6225e-01,  8.9664e-02, -6.3281e-01,  ..., -1.5941e-01,\n",
      "           1.6724e-01, -7.8048e-02],\n",
      "         ...,\n",
      "         [-4.7822e-01,  1.8631e-01, -7.1175e-01,  ..., -1.4996e-01,\n",
      "           2.3418e-02,  2.8774e-01],\n",
      "         [-3.9386e-01,  1.2734e-01, -3.6950e-01,  ..., -1.3143e-01,\n",
      "          -1.6101e-01, -7.1386e-03],\n",
      "         [-4.9795e-01, -3.2425e-01, -3.6386e-01,  ..., -1.8430e-01,\n",
      "           1.0343e-01, -2.0108e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1121805906295776\n",
      "Model outputs:  tensor([[[-0.0434, -0.1533, -0.1327,  ..., -0.1993, -0.2658, -0.1114],\n",
      "         [-0.1769, -0.1037, -0.4119,  ..., -0.2266, -0.3998,  0.2809],\n",
      "         [-0.1402, -0.4550, -0.1764,  ..., -0.1919,  0.0882,  0.3755],\n",
      "         ...,\n",
      "         [ 0.0603, -0.3766, -0.3180,  ..., -0.0734, -0.0014,  0.4696],\n",
      "         [ 0.1703, -0.4680, -0.5529,  ...,  0.1636, -0.1150,  0.2354],\n",
      "         [-0.0183, -0.0550, -0.4425,  ..., -0.1281, -0.2282,  0.6853]],\n",
      "\n",
      "        [[-0.2450,  0.2496, -0.5297,  ...,  0.2488, -0.1244, -0.1915],\n",
      "         [-0.1092,  0.2043, -0.8009,  ...,  0.3002, -0.3312, -0.0954],\n",
      "         [-0.1388,  0.1170, -0.7208,  ...,  0.4802,  0.2602,  0.0082],\n",
      "         ...,\n",
      "         [ 0.1461,  0.0123, -0.3282,  ...,  0.1667,  0.1884, -0.1547],\n",
      "         [ 0.1948, -0.1621, -0.5148,  ...,  0.5089, -0.0985,  0.1200],\n",
      "         [ 0.0990, -0.0906, -0.6309,  ...,  0.3492,  0.1340,  0.0787]],\n",
      "\n",
      "        [[ 0.1174, -0.0289, -0.3143,  ..., -0.0135, -0.2186,  0.2437],\n",
      "         [-0.0634,  0.0340, -0.4558,  ..., -0.1581, -0.3795,  0.0096],\n",
      "         [ 0.2082, -0.3648, -0.4262,  ..., -0.0127,  0.0312,  0.6156],\n",
      "         ...,\n",
      "         [ 0.0811, -0.0620, -0.1962,  ..., -0.1894, -0.2366,  0.2811],\n",
      "         [ 0.3077, -0.1609, -0.2900,  ...,  0.0704, -0.2313,  0.3467],\n",
      "         [-0.1272, -0.2647, -0.4604,  ..., -0.1891, -0.1491,  0.6424]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0396,  0.0903, -0.4812,  ...,  0.1179, -0.0661,  0.0587],\n",
      "         [-0.1008,  0.1781, -0.6271,  ...,  0.0717, -0.0545, -0.0479],\n",
      "         [-0.1648, -0.1773, -0.4105,  ...,  0.2593,  0.2425,  0.3083],\n",
      "         ...,\n",
      "         [ 0.3800,  0.1086, -0.3740,  ..., -0.0139, -0.0488,  0.2533],\n",
      "         [-0.0061, -0.2159, -0.4511,  ...,  0.2475,  0.0127,  0.2448],\n",
      "         [ 0.0223,  0.0099, -0.6390,  ...,  0.2983, -0.0340,  0.4265]],\n",
      "\n",
      "        [[-0.4072,  0.3493, -0.4089,  ...,  0.1478, -0.2771, -0.2321],\n",
      "         [-0.0924,  0.1338, -0.8050,  ...,  0.2517, -0.2374, -0.1452],\n",
      "         [ 0.0670,  0.0833, -0.6900,  ...,  0.5125,  0.1812,  0.0553],\n",
      "         ...,\n",
      "         [-0.0029, -0.1611, -0.4649,  ...,  0.1939,  0.3014, -0.0041],\n",
      "         [-0.1927,  0.1421, -0.7393,  ...,  0.4513,  0.2686,  0.0094],\n",
      "         [-0.1614,  0.1203, -0.6666,  ...,  0.1789,  0.2381, -0.0226]],\n",
      "\n",
      "        [[-0.3368,  0.3902,  0.1128,  ...,  0.8244, -0.0088, -0.1740],\n",
      "         [-0.1000,  0.5043, -0.2590,  ...,  0.7939,  0.5223,  0.0485],\n",
      "         [-0.4340,  0.0840, -0.1346,  ...,  0.3417,  0.1866,  0.2451],\n",
      "         ...,\n",
      "         [ 0.1280,  0.1040,  0.0188,  ...,  0.8871,  0.4401,  0.0076],\n",
      "         [-0.1465, -0.0418, -0.1781,  ...,  0.8631,  0.2363, -0.2697],\n",
      "         [-0.2637,  0.0519, -0.1532,  ...,  0.4596,  0.1337, -0.0181]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2063730955123901\n",
      "Model outputs:  tensor([[[ 0.0027,  0.1940, -0.2280,  ..., -0.0778,  0.0226, -0.4066],\n",
      "         [-0.0948,  0.1261, -0.4062,  ...,  0.1295, -0.0480, -0.2033],\n",
      "         [ 0.0564,  0.0540, -0.3376,  ...,  0.1619, -0.2079, -0.0313],\n",
      "         ...,\n",
      "         [-0.0651, -0.0231, -0.3304,  ...,  0.0626,  0.1533, -0.1896],\n",
      "         [-0.0968,  0.1795, -0.4682,  ..., -0.0017,  0.0640, -0.0851],\n",
      "         [-0.1201,  0.0709, -0.4448,  ...,  0.0663,  0.1590, -0.1428]],\n",
      "\n",
      "        [[-0.3586,  0.3653, -0.4459,  ...,  0.2546,  0.2521, -0.4572],\n",
      "         [-0.2618,  0.0468, -0.3050,  ...,  0.5327,  0.1487, -0.2504],\n",
      "         [-0.4489,  0.1896, -0.5658,  ...,  0.1451,  0.2133, -0.1292],\n",
      "         ...,\n",
      "         [-0.2903,  0.2083, -0.6649,  ...,  0.1036,  0.2463, -0.2274],\n",
      "         [-0.0481,  0.2206, -0.5951,  ...,  0.1266,  0.1614, -0.0299],\n",
      "         [-0.3626, -0.0119, -0.4332,  ...,  0.1616,  0.1922, -0.4023]],\n",
      "\n",
      "        [[-0.0878,  0.0117, -0.4501,  ..., -0.1168,  0.0215, -0.0733],\n",
      "         [-0.2263,  0.0815, -0.3784,  ..., -0.0587, -0.0019,  0.1639],\n",
      "         [-0.2670,  0.1472, -0.4379,  ..., -0.0465, -0.1138, -0.1795],\n",
      "         ...,\n",
      "         [-0.1920,  0.2832, -0.4996,  ..., -0.0384,  0.0545, -0.1249],\n",
      "         [-0.0639,  0.2632, -0.2881,  ..., -0.2310, -0.0930,  0.0300],\n",
      "         [-0.2968,  0.1020, -0.5309,  ..., -0.0527,  0.0931, -0.1793]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3767, -0.1568, -0.1633,  ..., -0.0177,  0.1132, -0.3985],\n",
      "         [-0.3747,  0.0473, -0.3522,  ...,  0.2898,  0.0550, -0.0530],\n",
      "         [-0.3607, -0.0324, -0.2382,  ...,  0.1388,  0.3892, -0.0958],\n",
      "         ...,\n",
      "         [-0.4169,  0.1329,  0.1982,  ..., -0.1118,  0.3249, -0.2421],\n",
      "         [-0.2018, -0.0146, -0.0205,  ..., -0.0815,  0.2304, -0.0380],\n",
      "         [-0.4603,  0.0200, -0.3158,  ...,  0.0933, -0.0972, -0.0742]],\n",
      "\n",
      "        [[-0.1090,  0.5358, -0.0820,  ...,  0.3045,  0.1975, -0.5467],\n",
      "         [-0.2377,  0.2979, -0.0439,  ...,  0.4326,  0.3520, -0.2977],\n",
      "         [-0.1962,  0.4734, -0.0021,  ...,  0.3011,  0.1273, -0.2334],\n",
      "         ...,\n",
      "         [-0.1067,  0.2176,  0.0055,  ...,  0.4643,  0.2920, -0.4847],\n",
      "         [-0.2154,  0.3815, -0.3082,  ...,  0.1697,  0.2818, -0.2257],\n",
      "         [-0.2553,  0.2817, -0.2449,  ...,  0.0558,  0.2702, -0.4102]],\n",
      "\n",
      "        [[ 0.1738, -0.3879, -0.1858,  ..., -0.4387, -0.1436,  0.2187],\n",
      "         [ 0.2152, -0.0387, -0.0744,  ..., -0.3590, -0.0287,  0.1110],\n",
      "         [-0.0740, -0.0998, -0.1302,  ..., -0.5017, -0.2833,  0.1368],\n",
      "         ...,\n",
      "         [-0.1028, -0.0195, -0.1267,  ..., -0.5301, -0.1664,  0.1382],\n",
      "         [ 0.2482, -0.2181, -0.3686,  ..., -0.5236, -0.2714,  0.2906],\n",
      "         [ 0.0674, -0.0699, -0.1860,  ..., -0.4662, -0.3748,  0.4017]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9257405996322632\n",
      "Model outputs:  tensor([[[-5.2383e-01,  1.7419e-01, -6.5165e-01,  ...,  1.3953e-01,\n",
      "           1.9636e-01,  2.2184e-01],\n",
      "         [-4.1554e-01,  2.1600e-01, -5.5269e-01,  ...,  1.5029e-01,\n",
      "           2.1740e-01,  2.6245e-01],\n",
      "         [-7.2881e-01,  4.3484e-01, -6.1908e-01,  ...,  3.8486e-01,\n",
      "           4.0830e-01,  4.9258e-01],\n",
      "         ...,\n",
      "         [-4.4236e-01, -8.3337e-02, -2.5990e-01,  ...,  2.1482e-01,\n",
      "           4.8199e-01,  2.2237e-01],\n",
      "         [-5.7948e-01,  2.0669e-01, -3.5110e-01,  ...,  2.9727e-01,\n",
      "           3.7523e-01,  4.4727e-02],\n",
      "         [-6.9202e-01,  2.4534e-01, -5.6204e-01,  ...,  3.0565e-01,\n",
      "           2.8927e-01,  3.7818e-01]],\n",
      "\n",
      "        [[-4.3766e-01,  2.1371e-01, -2.8965e-01,  ...,  8.2528e-01,\n",
      "           4.3091e-01, -1.2531e-01],\n",
      "         [-5.0869e-01,  6.1385e-01, -3.4171e-01,  ...,  7.2331e-01,\n",
      "           6.3356e-01,  1.4350e-01],\n",
      "         [-2.7843e-01,  5.2266e-01, -4.2716e-01,  ...,  8.1614e-01,\n",
      "           4.5457e-01,  1.3783e-01],\n",
      "         ...,\n",
      "         [-3.7114e-01, -1.0137e-01,  1.4477e-01,  ...,  7.1353e-01,\n",
      "           3.5229e-01, -2.3075e-01],\n",
      "         [-2.1622e-01,  1.3879e-01, -2.6601e-01,  ...,  7.0276e-01,\n",
      "           4.8892e-01, -3.6507e-02],\n",
      "         [-4.8404e-01,  4.2655e-01, -2.0770e-01,  ...,  7.4547e-01,\n",
      "           5.5869e-01,  5.2836e-02]],\n",
      "\n",
      "        [[-6.5090e-02, -9.0580e-02, -9.9870e-01,  ...,  7.8244e-02,\n",
      "           8.5764e-02,  3.5685e-02],\n",
      "         [-5.2724e-01,  7.8515e-02, -9.1063e-01,  ...,  4.5628e-02,\n",
      "          -1.1235e-01,  1.6449e-01],\n",
      "         [-3.0478e-01,  2.8072e-01, -8.9451e-01,  ...,  3.1116e-01,\n",
      "           2.5926e-01,  1.0113e-01],\n",
      "         ...,\n",
      "         [-1.5513e-01, -1.5456e-01, -7.0268e-01,  ...,  3.7122e-01,\n",
      "           7.2290e-02,  2.6477e-01],\n",
      "         [-1.6466e-01,  8.7109e-02, -4.0651e-01,  ...,  1.7092e-02,\n",
      "           3.8135e-01,  2.8866e-02],\n",
      "         [-5.0746e-01,  3.1392e-01, -7.6289e-01,  ...,  2.1405e-01,\n",
      "           3.5663e-01,  1.2170e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.8698e-01, -1.4155e-01, -4.8473e-01,  ...,  6.9373e-01,\n",
      "           4.3640e-01, -2.4808e-01],\n",
      "         [-4.5585e-01,  2.8350e-01, -5.9387e-01,  ...,  7.1782e-01,\n",
      "           7.2881e-01,  1.4465e-01],\n",
      "         [-3.6220e-01,  3.1852e-01, -4.6559e-01,  ...,  7.3407e-01,\n",
      "           5.5994e-01, -6.5753e-02],\n",
      "         ...,\n",
      "         [-4.4641e-01, -2.1519e-01, -4.3978e-02,  ...,  7.1524e-01,\n",
      "           4.4824e-01, -6.9899e-02],\n",
      "         [-2.9760e-01, -1.1960e-02, -9.4267e-02,  ...,  7.9725e-01,\n",
      "           3.2975e-01,  2.6954e-02],\n",
      "         [-4.8937e-01,  1.0989e-01, -3.9680e-01,  ...,  8.2367e-01,\n",
      "           4.4060e-01,  8.4275e-02]],\n",
      "\n",
      "        [[-3.1660e-01, -3.6777e-02, -2.7623e-01,  ...,  6.2407e-01,\n",
      "           3.8347e-01, -1.1178e-01],\n",
      "         [-4.6452e-01,  5.7643e-01, -6.4895e-01,  ...,  6.6818e-01,\n",
      "           5.0900e-01,  1.6579e-01],\n",
      "         [-5.3163e-01,  3.4316e-01, -2.4498e-01,  ...,  9.1875e-01,\n",
      "           3.0118e-01,  3.0014e-02],\n",
      "         ...,\n",
      "         [-4.3037e-01, -1.0383e-01, -4.5452e-03,  ...,  6.2097e-01,\n",
      "           2.9879e-01,  1.1373e-01],\n",
      "         [-3.7033e-01,  1.6280e-01, -2.1241e-01,  ...,  6.2554e-01,\n",
      "           4.6397e-01, -5.3929e-04],\n",
      "         [-3.5169e-01,  3.5463e-01, -8.8033e-02,  ...,  7.7720e-01,\n",
      "           6.4996e-01,  1.8768e-01]],\n",
      "\n",
      "        [[ 5.6341e-01, -1.2888e-01, -1.2463e-01,  ...,  4.0535e-01,\n",
      "          -1.0065e-01, -5.4582e-01],\n",
      "         [ 3.1290e-01, -3.2615e-02, -4.0040e-01,  ...,  3.4784e-01,\n",
      "          -5.9853e-02, -2.9420e-01],\n",
      "         [ 3.1325e-01, -1.1470e-01, -2.6079e-01,  ...,  7.6849e-01,\n",
      "          -9.3030e-02, -4.9809e-01],\n",
      "         ...,\n",
      "         [ 3.1057e-01, -3.1718e-01, -2.1138e-01,  ...,  6.8379e-01,\n",
      "          -8.4144e-02, -4.7526e-01],\n",
      "         [ 2.5822e-01, -3.5275e-01,  4.4830e-03,  ...,  5.0274e-01,\n",
      "           5.9637e-02, -5.4312e-01],\n",
      "         [ 3.6880e-01, -2.4787e-01,  1.0843e-01,  ...,  5.6896e-01,\n",
      "           2.5246e-01, -5.8949e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2225863933563232\n",
      "Model outputs:  tensor([[[-0.1427,  0.1528, -0.5705,  ..., -0.2858,  0.0287,  0.1084],\n",
      "         [-0.2861,  0.1366, -0.7641,  ..., -0.0694,  0.2065, -0.0042],\n",
      "         [-0.1709,  0.2100, -0.4473,  ...,  0.0287, -0.0875,  0.0306],\n",
      "         ...,\n",
      "         [-0.1795,  0.1396, -0.5704,  ..., -0.0083,  0.0068, -0.1572],\n",
      "         [-0.2386,  0.2845, -0.4649,  ..., -0.1620,  0.0443, -0.0421],\n",
      "         [-0.2740,  0.1801, -0.8167,  ..., -0.0404, -0.1253, -0.1103]],\n",
      "\n",
      "        [[-0.6365,  0.3745, -0.6444,  ..., -0.1740,  0.3124, -0.0154],\n",
      "         [-0.1933, -0.0596, -0.5032,  ..., -0.2333,  0.1325, -0.0088],\n",
      "         [-0.5675, -0.0463, -0.7238,  ...,  0.0011,  0.1935, -0.0123],\n",
      "         ...,\n",
      "         [-0.3383,  0.4644, -0.5407,  ..., -0.1891,  0.0105,  0.2449],\n",
      "         [-0.4350,  0.4610, -0.5746,  ..., -0.2585,  0.1955, -0.1434],\n",
      "         [-0.2973,  0.2745, -0.8280,  ..., -0.1074,  0.3013,  0.0461]],\n",
      "\n",
      "        [[ 0.2000, -0.2169, -0.3757,  ..., -0.1897,  0.1457, -0.3981],\n",
      "         [-0.0034, -0.0362, -0.3267,  ..., -0.0104, -0.2477, -0.3082],\n",
      "         [ 0.4719, -0.1235, -0.3986,  ..., -0.1661,  0.0618, -0.0291],\n",
      "         ...,\n",
      "         [ 0.3243, -0.0780, -0.3240,  ..., -0.2422,  0.0258, -0.3362],\n",
      "         [ 0.0379, -0.1980, -0.3082,  ..., -0.1740,  0.0333, -0.5210],\n",
      "         [ 0.1420, -0.1660, -0.3954,  ..., -0.1282, -0.0198, -0.1519]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3823,  0.4855, -0.3419,  ...,  0.1539,  0.3426, -0.3788],\n",
      "         [-0.4326,  0.0083, -0.0975,  ...,  0.3554,  0.2842,  0.4121],\n",
      "         [-0.5988,  0.2203, -0.2596,  ...,  0.3248,  0.3759, -0.0587],\n",
      "         ...,\n",
      "         [-0.4536,  0.0812, -0.3044,  ...,  0.0672,  0.3660, -0.1806],\n",
      "         [-0.6673,  0.4044, -0.3422,  ...,  0.0263,  0.2865, -0.2968],\n",
      "         [-0.6174,  0.4617, -0.7215,  ...,  0.2595,  0.0191,  0.0896]],\n",
      "\n",
      "        [[-0.2587,  0.4318, -0.3725,  ...,  0.3180,  0.3495, -0.2912],\n",
      "         [-0.5138,  0.1309, -0.4147,  ...,  0.4433,  0.2171,  0.0133],\n",
      "         [-0.4831,  0.1405, -0.7830,  ...,  0.4878,  0.3869, -0.2309],\n",
      "         ...,\n",
      "         [-0.3779,  0.3784, -0.5686,  ...,  0.1023,  0.1593, -0.1269],\n",
      "         [-0.7344,  0.3995, -0.5201,  ..., -0.0097,  0.1992,  0.0258],\n",
      "         [-0.0958,  0.5076, -0.7716,  ...,  0.3241,  0.0891,  0.1322]],\n",
      "\n",
      "        [[-0.5445,  0.2564, -0.3534,  ..., -0.2236,  0.1036, -0.1501],\n",
      "         [-0.3554, -0.2121, -0.6316,  ..., -0.0377,  0.2412,  0.2013],\n",
      "         [-0.3039, -0.1105, -0.6709,  ..., -0.1351,  0.3238,  0.0387],\n",
      "         ...,\n",
      "         [-0.6009,  0.5304, -0.4059,  ...,  0.0515,  0.3096, -0.1750],\n",
      "         [-0.5177,  0.5302, -0.8077,  ..., -0.3579,  0.1364,  0.0506],\n",
      "         [-0.5873,  0.4679, -0.7808,  ..., -0.1180, -0.0394,  0.1614]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.5303176641464233\n",
      "Model outputs:  tensor([[[-0.2754,  0.0107, -0.4703,  ...,  0.5776,  0.1702, -0.1760],\n",
      "         [-0.1345,  0.2191, -0.7753,  ...,  0.5831,  0.1505, -0.0456],\n",
      "         [-0.2041,  0.2522, -0.7117,  ...,  0.3731,  0.2136, -0.2478],\n",
      "         ...,\n",
      "         [-0.1921,  0.2147, -0.6832,  ...,  0.3144,  0.4395, -0.4012],\n",
      "         [-0.2518,  0.2361, -0.6478,  ...,  0.2074,  0.2154, -0.3001],\n",
      "         [-0.1736,  0.2294, -1.0911,  ...,  0.4314,  0.1391, -0.2950]],\n",
      "\n",
      "        [[ 0.0765, -0.2906, -0.1012,  ..., -0.5709, -0.2305, -0.0310],\n",
      "         [-0.1506, -0.3553, -0.4711,  ..., -0.3209, -0.1729,  0.2167],\n",
      "         [-0.0163,  0.1223, -0.6178,  ..., -0.1901,  0.0740,  0.0377],\n",
      "         ...,\n",
      "         [-0.0157,  0.1043, -0.3090,  ..., -0.2578,  0.1185, -0.0874],\n",
      "         [-0.3243,  0.0232, -0.2260,  ..., -0.1365,  0.0176, -0.1050],\n",
      "         [ 0.0155, -0.0105, -0.4171,  ..., -0.2356,  0.0966,  0.2168]],\n",
      "\n",
      "        [[-0.1638,  0.0430, -0.2296,  ..., -0.4407,  0.1136,  0.1655],\n",
      "         [ 0.0433, -0.1201, -0.6134,  ..., -0.0570, -0.1390,  0.3612],\n",
      "         [-0.0831, -0.0962, -0.1101,  ..., -0.6339, -0.1988,  0.1270],\n",
      "         ...,\n",
      "         [ 0.0361, -0.0686, -0.3062,  ..., -0.4427, -0.0797,  0.1253],\n",
      "         [ 0.1289, -0.1603,  0.0487,  ..., -0.5041, -0.1545,  0.2748],\n",
      "         [ 0.0794, -0.2398, -0.4908,  ..., -0.3373, -0.0099,  0.4405]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1645, -0.0302, -0.6621,  ...,  0.4524,  0.2261, -0.1840],\n",
      "         [-0.1377,  0.1535, -0.7755,  ...,  0.5438,  0.3115, -0.1348],\n",
      "         [-0.2482,  0.5295, -0.6184,  ...,  0.1770,  0.4499, -0.2705],\n",
      "         ...,\n",
      "         [-0.1527,  0.3461, -0.7922,  ...,  0.3598,  0.1648, -0.4298],\n",
      "         [-0.4390,  0.1373, -0.7350,  ...,  0.1807,  0.3771, -0.1992],\n",
      "         [-0.4149,  0.3808, -0.6033,  ...,  0.5495,  0.2856, -0.4089]],\n",
      "\n",
      "        [[-0.0168,  0.0325, -0.1409,  ..., -0.2012, -0.1063,  0.4549],\n",
      "         [ 0.0555,  0.0333, -0.3376,  ...,  0.0436, -0.1333,  0.2315],\n",
      "         [ 0.1382, -0.0652, -0.1665,  ..., -0.3008, -0.0318,  0.1580],\n",
      "         ...,\n",
      "         [ 0.1196,  0.0218, -0.3167,  ..., -0.3837, -0.1091,  0.1162],\n",
      "         [-0.1202, -0.0385, -0.2208,  ..., -0.3684, -0.2943,  0.1217],\n",
      "         [ 0.0506, -0.1631, -0.3105,  ..., -0.2236, -0.0994,  0.0855]],\n",
      "\n",
      "        [[ 0.0499, -0.3675, -0.2253,  ..., -0.2916,  0.0335,  0.1319],\n",
      "         [-0.0098, -0.1252, -0.2977,  ..., -0.3031,  0.0912,  0.1598],\n",
      "         [ 0.1989,  0.1862, -0.1202,  ..., -0.4907,  0.0996,  0.1336],\n",
      "         ...,\n",
      "         [ 0.1691, -0.1857, -0.3223,  ..., -0.3454, -0.1700,  0.0845],\n",
      "         [ 0.0516, -0.0023, -0.1761,  ..., -0.2915, -0.2814,  0.1839],\n",
      "         [-0.2465, -0.1774, -0.2613,  ..., -0.4138, -0.3170,  0.2625]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3384954929351807\n",
      "Model outputs:  tensor([[[-0.1408, -0.1183, -0.6825,  ...,  0.1065, -0.3388, -0.2089],\n",
      "         [-0.1646,  0.0484, -0.6852,  ...,  0.2392, -0.1320, -0.0031],\n",
      "         [-0.4606,  0.0378, -0.7687,  ...,  0.0412, -0.0828,  0.2488],\n",
      "         ...,\n",
      "         [-0.2806, -0.1499, -0.6837,  ...,  0.7032, -0.0521,  0.2368],\n",
      "         [-0.1970,  0.2010, -0.8327,  ...,  0.0495,  0.0119, -0.0136],\n",
      "         [-0.0896, -0.0343, -0.6478,  ...,  0.1755, -0.0043, -0.0644]],\n",
      "\n",
      "        [[-0.3262,  0.0540, -1.0482,  ...,  0.5464, -0.0457, -0.2515],\n",
      "         [-0.0826,  0.0048, -0.9117,  ...,  0.5075,  0.2580, -0.0612],\n",
      "         [-0.0940,  0.0502, -0.3625,  ...,  0.0734,  0.0996, -0.2820],\n",
      "         ...,\n",
      "         [-0.2786, -0.1385, -0.7600,  ...,  0.8032,  0.4066, -0.2408],\n",
      "         [-0.2712,  0.3251, -0.6553,  ...,  0.2450, -0.0027, -0.0241],\n",
      "         [-0.3599,  0.3804, -0.6287,  ...,  0.4622, -0.1617, -0.0508]],\n",
      "\n",
      "        [[ 0.2368, -0.1866,  0.2332,  ...,  0.3604,  0.0673, -0.7368],\n",
      "         [ 0.3622, -0.5660,  0.1304,  ...,  0.2674,  0.1231, -0.7033],\n",
      "         [-0.0038, -0.0439,  0.1891,  ...,  0.3644, -0.1224, -0.6694],\n",
      "         ...,\n",
      "         [ 0.0993, -0.4063, -0.0586,  ...,  0.5930, -0.0215, -0.6216],\n",
      "         [ 0.0533, -0.1947,  0.3313,  ...,  0.1235,  0.0488, -0.6462],\n",
      "         [ 0.1406, -0.0596,  0.1967,  ...,  0.5192,  0.0069, -0.6168]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5498, -0.0091, -0.5203,  ...,  0.3108,  0.2851, -0.1394],\n",
      "         [-0.6152,  0.2384, -0.5136,  ...,  0.2212,  0.4176,  0.1351],\n",
      "         [-0.6550,  0.2546, -0.3233,  ...,  0.3240,  0.3007,  0.0118],\n",
      "         ...,\n",
      "         [-0.4586, -0.1525, -0.6091,  ...,  0.5533,  0.3770,  0.3224],\n",
      "         [-0.4323,  0.3330, -0.5405,  ...,  0.4212,  0.2540,  0.0614],\n",
      "         [-0.5474,  0.3009, -0.3296,  ...,  0.2424,  0.1951, -0.0688]],\n",
      "\n",
      "        [[-0.3802,  0.0991, -0.5365,  ...,  0.4306, -0.0971,  0.0891],\n",
      "         [-0.1328, -0.0968, -0.5441,  ...,  0.3258,  0.0186,  0.0717],\n",
      "         [-0.2464, -0.0324, -0.4936,  ...,  0.1301,  0.0318, -0.0870],\n",
      "         ...,\n",
      "         [-0.2294, -0.1929, -0.8724,  ...,  0.4524, -0.1445,  0.0436],\n",
      "         [-0.1730,  0.1054, -0.6840,  ...,  0.5220,  0.0327,  0.0231],\n",
      "         [-0.1249,  0.1669, -0.5443,  ...,  0.4844,  0.1723, -0.0538]],\n",
      "\n",
      "        [[-0.0285,  0.0155, -0.5242,  ..., -0.1523, -0.2645,  0.3966],\n",
      "         [ 0.0695, -0.1352, -0.3590,  ..., -0.3495, -0.1499,  0.2101],\n",
      "         [-0.1895, -0.2855, -0.2593,  ..., -0.4381, -0.3460,  0.2060],\n",
      "         ...,\n",
      "         [ 0.0412, -0.2145, -0.6703,  ..., -0.1969, -0.1050,  0.5754],\n",
      "         [ 0.2027, -0.1371, -0.3217,  ..., -0.2864, -0.2387,  0.4211],\n",
      "         [ 0.0220, -0.0241, -0.3511,  ..., -0.1978, -0.4258,  0.0084]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.061938762664795\n",
      "Model outputs:  tensor([[[ 2.8390e-01,  6.3195e-02, -2.4505e-01,  ..., -3.7926e-01,\n",
      "          -1.4565e-01,  1.9405e-01],\n",
      "         [ 1.5851e-01, -2.0278e-01, -1.3068e-02,  ..., -3.0129e-01,\n",
      "           1.0036e-02, -2.7367e-01],\n",
      "         [-4.2812e-02, -2.8055e-01, -1.7639e-01,  ..., -5.3124e-02,\n",
      "          -3.3774e-01,  1.1702e-01],\n",
      "         ...,\n",
      "         [-1.1892e-01, -1.1445e-01, -3.0712e-01,  ...,  5.7503e-02,\n",
      "           8.1106e-02,  1.7381e-01],\n",
      "         [ 7.4603e-02,  8.4936e-02, -6.4119e-01,  ..., -1.2552e-01,\n",
      "          -6.1423e-02,  2.6995e-01],\n",
      "         [-1.3125e-01,  1.8258e-01, -6.3673e-01,  ...,  1.2017e-01,\n",
      "           1.6286e-01,  1.1508e-01]],\n",
      "\n",
      "        [[-4.1258e-01,  4.3029e-02, -5.7493e-01,  ..., -1.6782e-01,\n",
      "          -6.0048e-02, -1.3109e-01],\n",
      "         [-2.5928e-01, -1.2701e-01, -2.5038e-01,  ..., -2.7140e-01,\n",
      "          -2.9398e-02, -2.7178e-01],\n",
      "         [-4.3110e-01,  8.0413e-02, -1.9919e-01,  ...,  5.4759e-02,\n",
      "          -2.3402e-01, -6.2877e-02],\n",
      "         ...,\n",
      "         [-4.3149e-01, -3.8530e-02, -5.8755e-01,  ...,  3.4778e-01,\n",
      "           2.7201e-01,  1.3094e-01],\n",
      "         [-7.4745e-01, -1.4455e-01, -6.3126e-01,  ...,  2.2766e-02,\n",
      "           3.7444e-01,  5.6859e-02],\n",
      "         [-4.8716e-01,  1.7347e-01, -9.5566e-01,  ...,  3.4572e-01,\n",
      "          -2.1445e-01, -1.3497e-02]],\n",
      "\n",
      "        [[ 2.4004e-01, -5.0855e-01, -6.2977e-02,  ..., -7.1896e-01,\n",
      "          -2.2919e-01, -3.5466e-01],\n",
      "         [ 2.3851e-01, -1.2233e-01,  2.5551e-01,  ..., -6.0924e-01,\n",
      "          -6.1573e-02,  4.6587e-02],\n",
      "         [ 4.5154e-01, -2.4291e-01,  1.7952e-01,  ..., -5.5492e-01,\n",
      "          -1.5297e-02,  1.5569e-01],\n",
      "         ...,\n",
      "         [-1.1916e-01, -4.7972e-01, -3.0596e-01,  ..., -1.3868e-01,\n",
      "           2.5261e-02, -2.0961e-01],\n",
      "         [-1.5452e-01, -3.1041e-01, -1.4019e-01,  ..., -1.4744e-01,\n",
      "           1.7360e-03, -5.8683e-02],\n",
      "         [-1.0727e-02, -2.7160e-02, -7.6480e-01,  ..., -1.6021e-01,\n",
      "          -3.9604e-02,  1.6183e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.4315e-02, -8.5029e-02, -6.1643e-02,  ...,  1.1048e-01,\n",
      "          -1.4647e-01, -1.4476e-01],\n",
      "         [ 7.9595e-02,  2.6117e-01, -2.6013e-01,  ..., -5.6979e-02,\n",
      "          -1.0773e-01, -3.3805e-01],\n",
      "         [ 9.4768e-02,  1.9517e-01,  2.9536e-03,  ...,  2.6750e-01,\n",
      "          -5.9785e-01, -2.8311e-01],\n",
      "         ...,\n",
      "         [-3.0233e-01,  1.9184e-01, -6.0841e-01,  ...,  5.3711e-01,\n",
      "           1.3906e-01,  4.5698e-02],\n",
      "         [-3.1478e-01,  6.3457e-02, -4.3637e-01,  ...,  3.6369e-01,\n",
      "           2.5554e-01, -1.0083e-01],\n",
      "         [-2.3683e-01,  2.8182e-01, -1.1012e+00,  ...,  3.6307e-01,\n",
      "          -7.3734e-02,  9.8349e-02]],\n",
      "\n",
      "        [[-2.7610e-02, -1.1909e-01,  1.1154e-01,  ...,  2.1455e-01,\n",
      "           1.5066e-01, -1.5914e-01],\n",
      "         [-5.8781e-02,  1.5495e-01,  2.3730e-01,  ...,  3.6598e-01,\n",
      "           1.9489e-01, -1.7230e-01],\n",
      "         [-2.6595e-01, -2.4638e-02,  6.4925e-01,  ...,  5.9923e-01,\n",
      "          -6.5960e-02, -9.2989e-02],\n",
      "         ...,\n",
      "         [ 4.9779e-02, -1.2629e-01, -8.0638e-02,  ...,  9.8564e-01,\n",
      "           4.6803e-01, -1.3671e-01],\n",
      "         [-5.1278e-01,  1.8165e-01,  4.2631e-02,  ...,  8.7035e-01,\n",
      "           3.8821e-01, -5.7533e-02],\n",
      "         [-2.7734e-01,  1.8303e-01, -1.6252e-01,  ...,  8.9320e-01,\n",
      "           4.2962e-01, -1.5136e-01]],\n",
      "\n",
      "        [[ 8.7408e-02, -2.9945e-01, -7.8114e-02,  ..., -3.4620e-01,\n",
      "          -2.3057e-01, -6.3370e-02],\n",
      "         [ 1.5444e-01,  2.9385e-05, -8.1588e-02,  ..., -4.6324e-01,\n",
      "          -2.2881e-03, -1.6213e-01],\n",
      "         [ 5.5529e-02,  1.4832e-01, -3.9990e-02,  ...,  2.9446e-02,\n",
      "          -2.8590e-01,  1.2485e-01],\n",
      "         ...,\n",
      "         [-1.6818e-01,  1.0966e-01, -7.0667e-01,  ...,  2.0763e-02,\n",
      "           1.4361e-01,  3.5934e-01],\n",
      "         [ 1.6614e-01,  1.0514e-01, -4.7860e-01,  ..., -1.4903e-01,\n",
      "           2.1457e-01,  3.8556e-01],\n",
      "         [-2.4035e-01,  1.3988e-01, -8.7526e-01,  ...,  3.8333e-01,\n",
      "           2.4907e-01,  4.3093e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3529272079467773\n",
      "Model outputs:  tensor([[[-5.4321e-01, -2.8318e-02, -8.2002e-01,  ...,  1.4912e-01,\n",
      "           2.0394e-01,  1.9333e-01],\n",
      "         [-5.7758e-01, -1.0177e-01, -9.3479e-01,  ...,  2.4720e-01,\n",
      "          -4.0337e-03,  4.0810e-01],\n",
      "         [-4.5849e-01,  2.3941e-01, -9.4687e-01,  ...,  5.3368e-04,\n",
      "           2.2979e-01,  2.7051e-01],\n",
      "         ...,\n",
      "         [-1.5348e-01,  2.8811e-01, -6.8164e-01,  ...,  8.6636e-02,\n",
      "           2.4084e-01,  2.0042e-01],\n",
      "         [-4.7924e-01,  1.6844e-01, -6.7507e-01,  ...,  8.8634e-02,\n",
      "           3.4402e-01,  5.0194e-02],\n",
      "         [-2.9476e-01,  1.4041e-01, -7.4384e-01,  ...,  1.2132e-01,\n",
      "           2.2098e-01,  9.8528e-02]],\n",
      "\n",
      "        [[-8.7665e-02,  1.3198e-01, -9.3008e-01,  ...,  4.4909e-01,\n",
      "           6.5066e-02,  1.1662e-01],\n",
      "         [-1.2541e-01,  1.6958e-01, -8.4961e-01,  ...,  3.3150e-01,\n",
      "           7.1198e-02,  4.3082e-01],\n",
      "         [ 3.6746e-03,  1.8603e-02, -9.0102e-01,  ...,  5.4026e-01,\n",
      "           7.8609e-02,  9.3774e-02],\n",
      "         ...,\n",
      "         [-1.2870e-01,  1.7193e-01, -9.2855e-01,  ...,  1.8230e-01,\n",
      "           1.0698e-02,  2.5489e-01],\n",
      "         [-5.1274e-02,  1.4561e-01, -7.6492e-01,  ...,  2.5852e-01,\n",
      "           6.7190e-02, -1.6496e-01],\n",
      "         [ 1.6464e-02,  1.7518e-01, -7.8882e-01,  ...,  2.5544e-01,\n",
      "           2.4115e-01,  2.8507e-01]],\n",
      "\n",
      "        [[-3.5678e-01,  9.8658e-02, -9.3603e-01,  ...,  3.1950e-01,\n",
      "           2.0133e-01, -1.3891e-02],\n",
      "         [-1.9028e-01,  2.1493e-01, -7.5757e-01,  ...,  4.8146e-01,\n",
      "           2.0769e-01,  3.2916e-01],\n",
      "         [ 8.6769e-02,  1.2383e-01, -9.2314e-01,  ...,  5.0521e-01,\n",
      "          -3.2602e-02,  1.6858e-01],\n",
      "         ...,\n",
      "         [-2.7111e-01,  3.1935e-01, -6.0014e-01,  ...,  2.3751e-01,\n",
      "           1.0463e-01, -1.2283e-01],\n",
      "         [-3.9184e-01,  3.1947e-01, -6.8646e-01,  ...,  3.1047e-01,\n",
      "           1.2512e-01, -2.6884e-02],\n",
      "         [-2.0976e-01,  1.7484e-01, -1.0036e+00,  ...,  3.2054e-01,\n",
      "          -1.0844e-01,  9.6009e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.3347e-02, -5.2161e-02, -3.5311e-01,  ..., -6.4233e-02,\n",
      "          -2.0726e-02,  4.4594e-01],\n",
      "         [ 3.1998e-02, -8.0208e-02, -5.3396e-01,  ...,  1.3668e-01,\n",
      "          -1.0657e-01,  2.0057e-01],\n",
      "         [-2.4014e-02, -1.5976e-01, -4.6373e-01,  ...,  1.0822e-01,\n",
      "          -4.9037e-02,  5.3194e-01],\n",
      "         ...,\n",
      "         [ 2.5620e-02,  2.6796e-02, -2.2870e-01,  ..., -2.5197e-01,\n",
      "          -1.0590e-01,  3.1518e-01],\n",
      "         [ 1.3713e-01, -2.0755e-02, -2.7378e-01,  ..., -8.4160e-02,\n",
      "           5.6652e-02,  2.5411e-01],\n",
      "         [ 6.6112e-03, -1.9152e-01, -3.9225e-01,  ..., -9.6387e-02,\n",
      "           1.2801e-01,  4.2568e-01]],\n",
      "\n",
      "        [[-7.0333e-01,  4.5146e-01, -7.0970e-01,  ...,  6.1350e-01,\n",
      "           1.5830e-01,  1.6297e-01],\n",
      "         [-6.5980e-01,  2.9746e-01, -7.7168e-01,  ...,  5.8100e-01,\n",
      "          -7.5957e-03,  1.8337e-01],\n",
      "         [-5.2451e-01,  7.2636e-02, -7.1563e-01,  ...,  5.7361e-01,\n",
      "           4.4359e-01,  5.9419e-02],\n",
      "         ...,\n",
      "         [-4.4957e-01,  3.1939e-01, -5.3376e-01,  ...,  2.6680e-01,\n",
      "           5.3884e-01, -4.8268e-02],\n",
      "         [-5.7064e-01,  4.1145e-01, -7.2768e-01,  ...,  4.8670e-01,\n",
      "           4.4242e-01, -1.2694e-01],\n",
      "         [-4.3459e-01,  3.3874e-01, -7.3458e-01,  ...,  7.1224e-01,\n",
      "           6.6734e-01, -2.1312e-02]],\n",
      "\n",
      "        [[ 1.0552e-01,  6.0493e-02, -4.3786e-01,  ..., -1.0479e-01,\n",
      "          -8.5422e-02,  3.2797e-01],\n",
      "         [-1.5034e-01, -8.3212e-02, -3.5611e-01,  ..., -3.7082e-02,\n",
      "          -1.5911e-01,  2.4966e-01],\n",
      "         [-1.6928e-02, -7.3524e-02, -4.7284e-01,  ..., -8.0032e-02,\n",
      "          -1.7919e-01,  4.8106e-01],\n",
      "         ...,\n",
      "         [-1.7427e-01, -4.7223e-02, -3.3101e-01,  ..., -2.8971e-01,\n",
      "           4.6115e-02,  2.5429e-01],\n",
      "         [-1.3508e-01, -7.5225e-02, -9.9462e-02,  ..., -2.9998e-01,\n",
      "          -1.0900e-01,  1.0418e-01],\n",
      "         [ 5.9662e-02,  2.1466e-01, -3.9540e-01,  ..., -1.7031e-01,\n",
      "           3.8061e-03,  3.4995e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3270138502120972\n",
      "Model outputs:  tensor([[[-0.1556, -0.3284, -0.4955,  ..., -0.2279, -0.1251,  0.2343],\n",
      "         [-0.0306,  0.0198, -0.6831,  ..., -0.0784, -0.1913,  0.3779],\n",
      "         [ 0.1898, -0.5507, -0.0604,  ..., -0.1899,  0.1273,  0.5265],\n",
      "         ...,\n",
      "         [-0.2344, -0.0739, -0.2468,  ..., -0.3037,  0.1791, -0.0436],\n",
      "         [ 0.0147, -0.0179, -0.3417,  ..., -0.3333, -0.0919,  0.0962],\n",
      "         [-0.0749,  0.1818, -0.2118,  ..., -0.4724, -0.2715,  0.0974]],\n",
      "\n",
      "        [[-0.1924, -0.6142, -0.3781,  ..., -0.3585, -0.2650,  0.3554],\n",
      "         [ 0.0132,  0.0235, -0.2821,  ..., -0.1200, -0.3994,  0.4091],\n",
      "         [-0.0821, -0.4552, -0.2885,  ..., -0.3031, -0.2658,  0.6074],\n",
      "         ...,\n",
      "         [-0.0310, -0.1572, -0.3111,  ..., -0.1561, -0.1136,  0.1889],\n",
      "         [-0.2235, -0.0471, -0.4488,  ..., -0.4425,  0.0128, -0.0176],\n",
      "         [-0.2525, -0.3607, -0.3505,  ..., -0.2974, -0.0196,  0.1356]],\n",
      "\n",
      "        [[ 0.0205, -0.4297, -0.3153,  ..., -0.2806, -0.2012,  0.2126],\n",
      "         [ 0.1713, -0.3401, -0.3516,  ..., -0.0969, -0.5349,  0.6303],\n",
      "         [ 0.1259, -0.7438,  0.0876,  ..., -0.2700, -0.0430,  0.4543],\n",
      "         ...,\n",
      "         [-0.1406, -0.1682, -0.7258,  ..., -0.3604, -0.0376,  0.1294],\n",
      "         [ 0.0055, -0.1245, -0.3350,  ..., -0.3811, -0.0437,  0.0232],\n",
      "         [-0.1709, -0.1797, -0.2561,  ..., -0.5571, -0.2938,  0.0997]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2461, -0.0200, -0.7316,  ...,  0.7562,  0.1015, -0.3271],\n",
      "         [-0.1729,  0.0236, -0.6627,  ...,  0.6586,  0.1281, -0.0256],\n",
      "         [-0.0654, -0.3989, -0.3350,  ...,  0.7275,  0.1200, -0.0568],\n",
      "         ...,\n",
      "         [-0.4293,  0.1166, -0.4348,  ...,  0.1991,  0.1467, -0.1666],\n",
      "         [-0.2092,  0.1621, -0.7217,  ...,  0.1989,  0.1718, -0.2902],\n",
      "         [-0.3444,  0.3067, -0.7029,  ...,  0.1859, -0.1016, -0.1331]],\n",
      "\n",
      "        [[-0.1079,  0.0207, -0.5456,  ...,  0.1075, -0.0679, -0.1965],\n",
      "         [-0.0321,  0.2722, -0.7459,  ...,  0.4632, -0.1096,  0.1649],\n",
      "         [ 0.0019, -0.1707, -0.0846,  ...,  0.4174,  0.0434,  0.0871],\n",
      "         ...,\n",
      "         [-0.2299,  0.1972, -0.6587,  ...,  0.2453,  0.2600, -0.2191],\n",
      "         [-0.2017,  0.0632, -0.4945,  ..., -0.1434, -0.0537,  0.0051],\n",
      "         [-0.0747,  0.2070, -0.5707,  ..., -0.0820, -0.0620, -0.1562]],\n",
      "\n",
      "        [[-0.4251,  0.0568, -0.4126,  ...,  0.5633, -0.1860,  0.0676],\n",
      "         [-0.5092,  0.1463, -0.5041,  ...,  0.5225,  0.1219,  0.2874],\n",
      "         [-0.4094, -0.0973, -0.4320,  ...,  0.6400,  0.4586, -0.1638],\n",
      "         ...,\n",
      "         [-0.5489,  0.2716, -0.8653,  ...,  0.2760,  0.3841,  0.2335],\n",
      "         [-0.3618,  0.1731, -0.8044,  ...,  0.1880,  0.2067, -0.2475],\n",
      "         [-0.4613,  0.5928, -0.6018,  ..., -0.1866,  0.1800, -0.1903]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9062953591346741\n",
      "Model outputs:  tensor([[[-0.4518,  0.2918, -0.1952,  ...,  0.1882, -0.0134, -0.5973],\n",
      "         [-0.3019,  0.3102,  0.0323,  ...,  0.1841,  0.1569, -0.2629],\n",
      "         [-0.3047,  0.0911, -0.1569,  ...,  0.2927,  0.2553, -0.2296],\n",
      "         ...,\n",
      "         [-0.2565,  0.4105, -0.0309,  ...,  0.2993,  0.1682, -0.2185],\n",
      "         [-0.2083,  0.3585, -0.1181,  ..., -0.1269,  0.2211, -0.2987],\n",
      "         [-0.2327,  0.6127, -0.1856,  ...,  0.2535,  0.1714, -0.3996]],\n",
      "\n",
      "        [[-0.2011,  0.4234, -0.3202,  ...,  0.2011,  0.0051, -0.5406],\n",
      "         [-0.1205,  0.2568, -0.5231,  ...,  0.3098, -0.1913, -0.2562],\n",
      "         [-0.1306,  0.2478, -0.6384,  ...,  0.1938,  0.1047, -0.1339],\n",
      "         ...,\n",
      "         [-0.3704,  0.3395, -0.3544,  ...,  0.0714,  0.1471, -0.3714],\n",
      "         [-0.3940,  0.3094, -0.7082,  ...,  0.0758,  0.2258, -0.1498],\n",
      "         [-0.3396,  0.3748, -0.4833,  ...,  0.3520,  0.0318, -0.1864]],\n",
      "\n",
      "        [[-0.2736,  0.1956, -0.4893,  ..., -0.0500, -0.1881, -0.4466],\n",
      "         [-0.4007,  0.1976, -0.2261,  ...,  0.1344, -0.1909, -0.1595],\n",
      "         [-0.3618,  0.5064, -0.5981,  ...,  0.1459,  0.1664, -0.1897],\n",
      "         ...,\n",
      "         [-0.1879,  0.4061, -0.5485,  ...,  0.1396, -0.1463, -0.5357],\n",
      "         [-0.1441,  0.3065, -0.5730,  ...,  0.1821,  0.0508, -0.0507],\n",
      "         [-0.1978,  0.3788, -0.4529,  ...,  0.1724, -0.1014, -0.3639]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0521, -0.4381, -0.1094,  ..., -0.5587,  0.1826, -0.3102],\n",
      "         [ 0.1186, -0.2151, -0.1750,  ..., -0.6209,  0.1400, -0.0457],\n",
      "         [ 0.3393, -0.1669, -0.1367,  ..., -0.6020,  0.2523, -0.0787],\n",
      "         ...,\n",
      "         [-0.0349,  0.2283, -0.4603,  ..., -0.5782, -0.1445,  0.0818],\n",
      "         [-0.1333,  0.0975, -0.0573,  ..., -0.3839, -0.0743, -0.2293],\n",
      "         [-0.2243, -0.1424, -0.4788,  ..., -0.4562,  0.1164, -0.0996]],\n",
      "\n",
      "        [[-0.2083,  0.2697, -0.2878,  ...,  0.2952,  0.0752, -0.7703],\n",
      "         [-0.3841,  0.4011, -0.6054,  ...,  0.5098,  0.0329, -0.1598],\n",
      "         [-0.2831,  0.0705, -0.3302,  ...,  0.3636,  0.1480, -0.2951],\n",
      "         ...,\n",
      "         [-0.4399,  0.3347, -0.5851,  ...,  0.4399, -0.0473, -0.4834],\n",
      "         [-0.2446,  0.2404, -0.4792,  ...,  0.4296,  0.0452, -0.4659],\n",
      "         [-0.1114,  0.1562, -0.2880,  ...,  0.3942,  0.1907, -0.3752]],\n",
      "\n",
      "        [[-0.3504,  0.1371, -0.2785,  ..., -0.3995, -0.1811, -0.2155],\n",
      "         [-0.3333, -0.0045, -0.4693,  ..., -0.1129, -0.2733,  0.1822],\n",
      "         [-0.3520,  0.2914, -0.3719,  ..., -0.2252,  0.0262, -0.1459],\n",
      "         ...,\n",
      "         [-0.1908,  0.3065, -0.0830,  ...,  0.0820, -0.0617, -0.2211],\n",
      "         [-0.1890,  0.1661, -0.3799,  ..., -0.0629,  0.0286, -0.1404],\n",
      "         [-0.3952,  0.1727, -0.2804,  ..., -0.2069, -0.0716, -0.0042]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0089030265808105\n",
      "Model outputs:  tensor([[[-0.0075, -0.0756, -0.5712,  ..., -0.1765, -0.0472,  0.0364],\n",
      "         [-0.1489, -0.0386, -0.4686,  ..., -0.1790,  0.1227,  0.0932],\n",
      "         [-0.2991,  0.0576, -0.6729,  ..., -0.0025, -0.2819,  0.4147],\n",
      "         ...,\n",
      "         [-0.0954,  0.0282, -0.5752,  ..., -0.0221, -0.0963,  0.1770],\n",
      "         [-0.3456, -0.1671, -0.1069,  ..., -0.0169, -0.0422,  0.1747],\n",
      "         [-0.1397,  0.0529, -0.4799,  ..., -0.0489, -0.0597,  0.0569]],\n",
      "\n",
      "        [[ 0.1315, -0.0117, -0.2999,  ..., -0.3672, -0.0749,  0.2821],\n",
      "         [-0.2144, -0.1660, -0.2817,  ..., -0.3418, -0.1860,  0.2028],\n",
      "         [-0.3151, -0.0539, -0.4803,  ..., -0.1450, -0.2807,  0.1424],\n",
      "         ...,\n",
      "         [-0.1098, -0.1529, -0.4012,  ...,  0.0384, -0.0534,  0.6753],\n",
      "         [-0.3457, -0.4429, -0.0296,  ..., -0.1238, -0.2692,  0.5090],\n",
      "         [-0.1840, -0.0491, -0.5087,  ..., -0.4589, -0.3321,  0.4094]],\n",
      "\n",
      "        [[-0.1145,  0.2126, -0.3445,  ..., -0.6494, -0.3125,  0.0127],\n",
      "         [ 0.2211,  0.0341, -0.5249,  ..., -0.3861, -0.2498,  0.0207],\n",
      "         [-0.1874, -0.0759, -0.5181,  ..., -0.3869,  0.0044,  0.2951],\n",
      "         ...,\n",
      "         [-0.1848, -0.1910, -0.4284,  ...,  0.0243,  0.1700, -0.1798],\n",
      "         [-0.2056, -0.1748, -0.2444,  ...,  0.0587, -0.2256, -0.0710],\n",
      "         [-0.0294, -0.3253, -0.2616,  ..., -0.3712,  0.1025, -0.1662]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5046,  0.4401, -0.2713,  ...,  0.6529,  0.2841, -0.2829],\n",
      "         [-0.3937,  0.3506, -0.0756,  ...,  0.4373,  0.4205, -0.1479],\n",
      "         [-0.3268,  0.2842, -0.3254,  ...,  0.4798,  0.2557,  0.0768],\n",
      "         ...,\n",
      "         [-0.4658, -0.0648,  0.1618,  ...,  0.7213,  0.5117, -0.2565],\n",
      "         [-0.3088, -0.1967, -0.0946,  ...,  0.6294,  0.1811,  0.1204],\n",
      "         [-0.6718,  0.2580, -0.1451,  ...,  0.2855,  0.2511, -0.0056]],\n",
      "\n",
      "        [[-0.1792,  0.0680, -0.4636,  ...,  0.0770, -0.1313, -0.0781],\n",
      "         [-0.4252,  0.2779, -0.7531,  ...,  0.1604, -0.0111,  0.1850],\n",
      "         [-0.1695,  0.0724, -0.8937,  ...,  0.2023,  0.1050, -0.0705],\n",
      "         ...,\n",
      "         [-0.4418, -0.1501, -0.4974,  ...,  0.1096,  0.0156,  0.0931],\n",
      "         [-0.2921,  0.0673, -0.4892,  ...,  0.3543,  0.0462,  0.2750],\n",
      "         [-0.2701,  0.0532, -0.7887,  ...,  0.3782,  0.0360, -0.1654]],\n",
      "\n",
      "        [[-0.3529,  0.1005, -0.7319,  ..., -0.1657, -0.2788,  0.0240],\n",
      "         [-0.5174,  0.1943, -0.5983,  ...,  0.1734,  0.0499, -0.2132],\n",
      "         [-0.1567,  0.1738, -0.8786,  ...,  0.3402,  0.1295,  0.1904],\n",
      "         ...,\n",
      "         [-0.3533,  0.0032, -0.4752,  ...,  0.0832,  0.0058, -0.0212],\n",
      "         [-0.4475, -0.2329, -0.6169,  ...,  0.1534, -0.1750,  0.0449],\n",
      "         [-0.1509,  0.3830, -0.6077,  ...,  0.3223,  0.1038, -0.0310]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9516991376876831\n",
      "Model outputs:  tensor([[[ 0.1534, -0.1391, -0.0732,  ..., -0.3428, -0.1044, -0.1032],\n",
      "         [ 0.2262, -0.2559, -0.2394,  ...,  0.0809, -0.0775,  0.6165],\n",
      "         [ 0.1031, -0.2106, -0.0797,  ..., -0.5589, -0.2385,  0.1505],\n",
      "         ...,\n",
      "         [ 0.0404, -0.0607, -0.1596,  ..., -0.2693, -0.4010,  0.1346],\n",
      "         [-0.0232, -0.1862, -0.1828,  ..., -0.4799, -0.3030,  0.2364],\n",
      "         [ 0.1150,  0.2749, -0.3364,  ..., -0.4181, -0.0934, -0.0544]],\n",
      "\n",
      "        [[-0.5549,  0.4822, -0.3907,  ...,  0.2787,  0.3365, -0.1858],\n",
      "         [-0.4658,  0.1928, -0.4228,  ...,  0.5500,  0.2436,  0.1172],\n",
      "         [-0.5852,  0.4774, -0.5138,  ...,  0.3655,  0.0949, -0.1102],\n",
      "         ...,\n",
      "         [-0.4774,  0.4816, -0.7951,  ...,  0.5267,  0.2680,  0.3316],\n",
      "         [-0.3698,  0.4020, -0.4349,  ...,  0.1540,  0.4322, -0.1277],\n",
      "         [-0.3728,  0.2995, -0.2500,  ...,  0.1354,  0.2680, -0.2011]],\n",
      "\n",
      "        [[-0.0767,  0.3740, -0.2671,  ..., -0.0463,  0.0063, -0.2850],\n",
      "         [-0.0747, -0.1952, -0.6954,  ...,  0.3266, -0.1447,  0.3450],\n",
      "         [-0.0684,  0.2787, -0.4702,  ...,  0.0656, -0.3066, -0.3876],\n",
      "         ...,\n",
      "         [-0.1962,  0.1063, -0.5881,  ...,  0.2821, -0.1719, -0.0138],\n",
      "         [-0.2453,  0.3225, -0.5234,  ..., -0.2633, -0.2399, -0.0866],\n",
      "         [ 0.1017,  0.4205, -0.5424,  ..., -0.0849,  0.0748, -0.1356]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2651,  0.3735, -0.2046,  ..., -0.0317, -0.0301, -0.2914],\n",
      "         [-0.3601, -0.0681, -0.3812,  ...,  0.7512,  0.2929,  0.0621],\n",
      "         [-0.6012,  0.3774, -0.1172,  ...,  0.2830,  0.2298, -0.3213],\n",
      "         ...,\n",
      "         [-0.3806,  0.3716, -0.3165,  ...,  0.6157,  0.3476, -0.0175],\n",
      "         [-0.6362,  0.3524, -0.2754,  ...,  0.0603,  0.0277, -0.1996],\n",
      "         [-0.1832,  0.0323, -0.3264,  ...,  0.1657,  0.3453, -0.1267]],\n",
      "\n",
      "        [[-0.4154,  0.2499, -0.2594,  ..., -0.0166,  0.3595, -0.2371],\n",
      "         [-0.6765,  0.3345, -0.3887,  ...,  0.5067,  0.3031,  0.1113],\n",
      "         [-0.4810,  0.3854, -0.1794,  ...,  0.0324, -0.1280,  0.1871],\n",
      "         ...,\n",
      "         [-0.5101,  0.1598, -0.1989,  ...,  0.2495,  0.1096,  0.1132],\n",
      "         [-0.8304,  0.4329, -0.5065,  ...,  0.0252,  0.1988,  0.1912],\n",
      "         [-0.3308,  0.4355, -0.2773,  ...,  0.0192,  0.0810,  0.0062]],\n",
      "\n",
      "        [[-0.5296,  0.2199, -0.1066,  ...,  0.0311,  0.3407, -0.1527],\n",
      "         [-0.7120, -0.0418, -0.0169,  ...,  0.4336,  0.3292, -0.1041],\n",
      "         [-0.4218,  0.1921,  0.0567,  ...,  0.2812,  0.2756, -0.0928],\n",
      "         ...,\n",
      "         [-0.4523,  0.1305, -0.1849,  ...,  0.4746,  0.1621,  0.0750],\n",
      "         [-0.4913,  0.3972, -0.4766,  ...,  0.3850,  0.2927,  0.0918],\n",
      "         [-0.2903,  0.1760, -0.2591,  ...,  0.0446,  0.1818, -0.3052]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0216257572174072\n",
      "Model outputs:  tensor([[[-0.3578, -0.3412, -0.3375,  ..., -0.2693, -0.7674,  0.1831],\n",
      "         [-0.1151, -0.5390, -0.2826,  ...,  0.1063, -0.3757,  0.6340],\n",
      "         [-0.0540, -0.2116, -0.3787,  ..., -0.0708,  0.1101,  0.3956],\n",
      "         ...,\n",
      "         [-0.1460, -0.0546, -0.6379,  ..., -0.1878, -0.3294,  0.3550],\n",
      "         [ 0.1266, -0.5332, -0.1532,  ..., -0.1045, -0.1003,  0.3945],\n",
      "         [-0.1873, -0.2793, -0.5464,  ..., -0.1571, -0.0324,  0.2296]],\n",
      "\n",
      "        [[-0.2993,  0.1621, -0.5262,  ...,  1.0331, -0.0675, -0.2216],\n",
      "         [-0.2459, -0.5247, -0.4391,  ...,  0.9205,  0.2672, -0.0308],\n",
      "         [-0.3136,  0.1738, -0.5479,  ...,  0.6802,  0.1079, -0.0884],\n",
      "         ...,\n",
      "         [-0.4075,  0.2030, -0.8571,  ...,  0.7725,  0.1871,  0.0664],\n",
      "         [-0.0772, -0.0162, -0.7268,  ...,  0.8725,  0.0614, -0.0189],\n",
      "         [-0.3475,  0.2687, -0.5838,  ...,  0.8049, -0.0061, -0.0373]],\n",
      "\n",
      "        [[-0.3540,  0.0683, -0.7731,  ...,  0.7054, -0.0342, -0.1661],\n",
      "         [-0.2265, -0.3836, -0.5909,  ...,  0.5723,  0.0855, -0.1424],\n",
      "         [-0.0811, -0.0464, -0.7065,  ...,  0.8951,  0.1108, -0.0503],\n",
      "         ...,\n",
      "         [-0.3740,  0.0817, -0.7275,  ...,  0.8858,  0.1068, -0.1796],\n",
      "         [-0.2721,  0.0791, -0.6913,  ...,  0.9214,  0.1119,  0.0213],\n",
      "         [-0.2605,  0.0131, -0.7218,  ...,  0.8082,  0.0680, -0.0415]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2983, -0.1713, -0.4123,  ..., -0.3418, -0.5110,  0.5082],\n",
      "         [-0.0187, -0.4896, -0.0526,  ...,  0.0625, -0.4788,  0.5933],\n",
      "         [-0.0072, -0.3829, -0.5036,  ..., -0.1685, -0.4251,  0.1222],\n",
      "         ...,\n",
      "         [-0.1041, -0.2457, -0.3610,  ..., -0.2698, -0.4751,  0.5721],\n",
      "         [-0.2182,  0.0300, -0.2813,  ...,  0.0756, -0.2322,  0.3580],\n",
      "         [ 0.1307, -0.4367, -0.2795,  ..., -0.0545, -0.2473,  0.4676]],\n",
      "\n",
      "        [[-0.6573,  0.1338, -0.4497,  ...,  0.5410, -0.0293,  0.1903],\n",
      "         [-0.5580, -0.5192, -0.4574,  ...,  0.7081,  0.2169,  0.2410],\n",
      "         [-0.5461, -0.0051, -0.6558,  ...,  0.8339,  0.3582,  0.2265],\n",
      "         ...,\n",
      "         [-0.5959,  0.2219, -0.5951,  ...,  0.5533,  0.2035,  0.2346],\n",
      "         [-0.5216, -0.1683, -0.2187,  ...,  0.5269,  0.0425,  0.2486],\n",
      "         [-0.4149,  0.3376, -0.6231,  ...,  0.7992,  0.2521,  0.1431]],\n",
      "\n",
      "        [[-0.1291,  0.0815, -0.7491,  ...,  0.4320, -0.0518,  0.2295],\n",
      "         [-0.3215, -0.3994, -0.3939,  ...,  0.3380,  0.0769,  0.1911],\n",
      "         [-0.2846, -0.0584, -0.6799,  ...,  0.2730, -0.0020,  0.1129],\n",
      "         ...,\n",
      "         [-0.3497,  0.0612, -0.8419,  ...,  0.4344, -0.1999,  0.3012],\n",
      "         [-0.1656, -0.0794, -0.7067,  ...,  0.3813, -0.0544, -0.0963],\n",
      "         [-0.2729, -0.0635, -0.5112,  ...,  0.3103, -0.0308,  0.2059]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8557137846946716\n",
      "Model outputs:  tensor([[[-0.2460,  0.1916, -0.1714,  ...,  0.7094,  0.3316,  0.1432],\n",
      "         [-0.3490, -0.0118, -0.0940,  ...,  0.5873, -0.0790, -0.0114],\n",
      "         [-0.0535, -0.2797, -0.1372,  ...,  0.8749,  0.4316,  0.0365],\n",
      "         ...,\n",
      "         [-0.4537,  0.4870, -0.1060,  ...,  0.3177,  0.2428, -0.1674],\n",
      "         [-0.4830,  0.1994, -0.2446,  ...,  0.5961,  0.2531,  0.0965],\n",
      "         [-0.4243,  0.3822, -0.1538,  ...,  0.3947,  0.1508,  0.0330]],\n",
      "\n",
      "        [[-0.2820,  0.1136, -0.2082,  ...,  0.9492, -0.0047,  0.3492],\n",
      "         [-0.1231,  0.3268, -0.3965,  ...,  0.8560,  0.1804, -0.0730],\n",
      "         [-0.0983, -0.2465, -0.0891,  ...,  0.8342,  0.4540,  0.2325],\n",
      "         ...,\n",
      "         [-0.4457,  0.3901,  0.0141,  ...,  0.3672,  0.0983, -0.4235],\n",
      "         [-0.2254,  0.2383, -0.4013,  ...,  0.9242,  0.3018, -0.1965],\n",
      "         [-0.7592,  0.3272, -0.2768,  ...,  0.4845, -0.0779, -0.3226]],\n",
      "\n",
      "        [[-0.1897, -0.2204, -0.1086,  ...,  0.9498,  0.1561,  0.2666],\n",
      "         [-0.3980,  0.0403, -0.1824,  ...,  0.8009,  0.2512, -0.2185],\n",
      "         [ 0.0484, -0.4845, -0.2157,  ...,  0.7586,  0.4806,  0.0122],\n",
      "         ...,\n",
      "         [-0.4120,  0.2919, -0.0477,  ...,  0.3365,  0.0801, -0.2857],\n",
      "         [-0.0610, -0.0955, -0.2291,  ...,  0.7652,  0.1882, -0.0287],\n",
      "         [-0.2002,  0.3169, -0.3328,  ...,  0.6122,  0.1935, -0.1400]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1587, -0.3594, -0.5198,  ..., -0.0104, -0.1536,  0.5196],\n",
      "         [-0.0318, -0.2890, -0.5826,  ..., -0.1675, -0.4864,  0.3261],\n",
      "         [ 0.1973, -0.5861, -0.2757,  ..., -0.0854, -0.1377,  0.6338],\n",
      "         ...,\n",
      "         [ 0.0756,  0.1626, -0.4088,  ..., -0.1159, -0.1079,  0.2562],\n",
      "         [ 0.0270, -0.4668, -0.4244,  ..., -0.2180, -0.1466,  0.6065],\n",
      "         [-0.0517, -0.1601, -0.5765,  ..., -0.2237, -0.1056,  0.0972]],\n",
      "\n",
      "        [[-0.4330, -0.0370, -0.1676,  ...,  0.7365,  0.3327,  0.0753],\n",
      "         [-0.3734, -0.0345, -0.1756,  ...,  0.6454, -0.0367,  0.1748],\n",
      "         [-0.0473, -0.1675,  0.0214,  ...,  0.5118,  0.2111, -0.2029],\n",
      "         ...,\n",
      "         [-0.5427,  0.1056,  0.0233,  ...,  0.2306,  0.1407, -0.1983],\n",
      "         [-0.2471, -0.1193, -0.1874,  ...,  0.5926,  0.4169,  0.0843],\n",
      "         [-0.7228,  0.3142, -0.0621,  ...,  0.3149,  0.1708, -0.3010]],\n",
      "\n",
      "        [[-0.5525,  0.0783, -0.5348,  ...,  0.4704,  0.2047,  0.3283],\n",
      "         [-0.4998,  0.1905, -0.3570,  ...,  0.2710, -0.0064,  0.1705],\n",
      "         [-0.4209, -0.3968, -0.1629,  ...,  0.3842,  0.4316,  0.4169],\n",
      "         ...,\n",
      "         [-0.6632,  0.2633, -0.3215,  ..., -0.1423,  0.0023,  0.1137],\n",
      "         [-0.3054,  0.1423, -0.4694,  ...,  0.3136,  0.0627,  0.2383],\n",
      "         [-0.5392,  0.0728, -0.3180,  ...,  0.1276,  0.3111,  0.0654]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9762939810752869\n",
      "Model outputs:  tensor([[[ 0.2263,  0.0053, -0.2711,  ..., -0.2134,  0.4755,  0.1726],\n",
      "         [-0.0828,  0.0619, -0.2637,  ...,  0.0458,  0.4610,  0.1350],\n",
      "         [-0.0252,  0.3590, -0.2637,  ..., -0.1258,  0.2435,  0.1745],\n",
      "         ...,\n",
      "         [ 0.2338, -0.1553, -0.4525,  ..., -0.2651,  0.2787,  0.5108],\n",
      "         [ 0.0650, -0.4659,  0.0333,  ..., -0.1763,  0.3719,  0.2100],\n",
      "         [-0.0030,  0.2385, -0.6124,  ..., -0.1283,  0.4716,  0.4381]],\n",
      "\n",
      "        [[ 0.0357, -0.1516, -0.3743,  ..., -0.3905,  0.6542,  0.2759],\n",
      "         [ 0.0470, -0.0676, -0.3414,  ..., -0.0848,  0.6820,  0.3172],\n",
      "         [ 0.0715, -0.0771, -0.2745,  ...,  0.1455,  0.6052,  0.4222],\n",
      "         ...,\n",
      "         [-0.2079, -0.0780, -0.3999,  ..., -0.2106,  0.2698,  0.0304],\n",
      "         [ 0.0025, -0.5724, -0.1620,  ..., -0.4352,  0.5712,  0.3198],\n",
      "         [-0.0639, -0.1881, -0.5431,  ..., -0.2846,  0.3823,  0.4214]],\n",
      "\n",
      "        [[ 0.1779, -0.0142,  0.1342,  ...,  0.2843, -0.0303, -0.4137],\n",
      "         [-0.1591, -0.3383, -0.0702,  ...,  0.5746, -0.0023, -0.5650],\n",
      "         [ 0.1104, -0.0836,  0.0617,  ...,  0.4408,  0.0736, -0.4090],\n",
      "         ...,\n",
      "         [ 0.1343, -0.1852,  0.1167,  ...,  0.5373, -0.1230, -0.4173],\n",
      "         [ 0.2364, -0.6114,  0.0116,  ...,  0.6041, -0.0669, -0.4085],\n",
      "         [ 0.1405, -0.0745,  0.1733,  ...,  0.6326, -0.0848, -0.2864]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1717,  0.1821, -0.8506,  ...,  0.4361,  0.0954,  0.0293],\n",
      "         [-0.2445, -0.0172, -0.7518,  ...,  0.3761,  0.2023, -0.1988],\n",
      "         [-0.2191,  0.2024, -0.5908,  ...,  0.3899, -0.0383, -0.1884],\n",
      "         ...,\n",
      "         [-0.4707, -0.0453, -0.7444,  ...,  0.3117, -0.2602,  0.1377],\n",
      "         [-0.2145, -0.2483, -0.3038,  ...,  0.1823,  0.0225,  0.2257],\n",
      "         [-0.4421,  0.0123, -0.7898,  ...,  0.4392, -0.0970,  0.1460]],\n",
      "\n",
      "        [[-0.0943, -0.0857, -0.4227,  ..., -0.4475,  0.8001,  0.5013],\n",
      "         [-0.1960, -0.1508, -0.4306,  ..., -0.2090,  0.6142,  0.3969],\n",
      "         [-0.0437,  0.0975, -0.1690,  ..., -0.0885,  0.8266,  0.7159],\n",
      "         ...,\n",
      "         [-0.0774, -0.3034, -0.4835,  ..., -0.2903,  0.4751,  0.1479],\n",
      "         [-0.0399, -0.2443, -0.0359,  ..., -0.4289,  0.4122,  0.3287],\n",
      "         [-0.2902,  0.0486, -0.6287,  ..., -0.2528,  0.0216,  0.4949]],\n",
      "\n",
      "        [[-0.0028, -0.1229, -0.6182,  ..., -0.2843, -0.0318,  0.3475],\n",
      "         [-0.4401, -0.1865, -0.3614,  ...,  0.2809,  0.1514,  0.3079],\n",
      "         [-0.1539, -0.1060, -0.2091,  ...,  0.0099, -0.0282,  0.3986],\n",
      "         ...,\n",
      "         [-0.0708, -0.1081, -0.3471,  ..., -0.0344, -0.0086,  0.4151],\n",
      "         [-0.4990, -0.5103, -0.2877,  ..., -0.3268, -0.2928,  0.3927],\n",
      "         [-0.1579,  0.1280, -0.7344,  ...,  0.0047, -0.3005,  0.3330]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.376729965209961\n",
      "Model outputs:  tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.1238, -0.1256, -0.2619,  ...,  0.7432,  0.0939, -0.1512],\n",
      "         [-0.2326, -0.1446, -0.3479,  ...,  0.7814,  0.1858,  0.1495],\n",
      "         ...,\n",
      "         [-0.3034, -0.4749, -0.0745,  ...,  0.9064,  0.3519, -0.2235],\n",
      "         [-0.1003, -0.1055, -0.2586,  ...,  0.8323,  0.0909,  0.0736],\n",
      "         [-0.3601, -0.0772, -0.2621,  ...,  0.9521,  0.3702, -0.2600]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.1347,  0.0250, -0.7554,  ...,  0.6081, -0.2647, -0.0888],\n",
      "         [ 0.1261, -0.2609, -0.6802,  ...,  0.6393,  0.0901,  0.0632],\n",
      "         ...,\n",
      "         [-0.0587, -0.2114, -0.5313,  ...,  0.7834,  0.1035,  0.1026],\n",
      "         [-0.2112, -0.0112, -0.9622,  ...,  0.3915, -0.1395,  0.3179],\n",
      "         [-0.0950,  0.0725, -0.5537,  ...,  0.6919, -0.0030, -0.2792]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.0077,  0.0169, -0.1411,  ...,  0.4055,  0.0443, -0.2530],\n",
      "         [-0.3779, -0.0536, -0.2944,  ...,  0.6361,  0.0932,  0.1357],\n",
      "         ...,\n",
      "         [-0.1929,  0.0657, -0.4012,  ...,  0.5621,  0.1841,  0.2092],\n",
      "         [-0.4085,  0.3861, -0.3513,  ...,  0.5139, -0.0424, -0.1699],\n",
      "         [-0.2443, -0.2870, -0.2930,  ...,  0.5322,  0.1312,  0.0709]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.2711, -0.0510, -0.4003,  ...,  0.0360, -0.3498,  0.3053],\n",
      "         [-0.3012, -0.3746, -0.3029,  ...,  0.1520, -0.2951,  0.4439],\n",
      "         ...,\n",
      "         [-0.1520, -0.5025, -0.4184,  ...,  0.1776, -0.1709,  0.6002],\n",
      "         [-0.1382, -0.2305, -0.5974,  ...,  0.0683, -0.2720,  0.3706],\n",
      "         [-0.1057,  0.1228, -0.4246,  ...,  0.0270, -0.4971,  0.4208]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.2287, -0.1814, -0.6474,  ...,  0.4141, -0.1701,  0.2806],\n",
      "         [-0.0975,  0.0188, -0.9580,  ...,  0.3132, -0.3039,  0.2658],\n",
      "         ...,\n",
      "         [-0.3555, -0.3203, -0.5392,  ...,  0.3908, -0.1999,  0.1950],\n",
      "         [-0.2869,  0.0243, -0.9103,  ...,  0.2992, -0.1491, -0.1691],\n",
      "         [-0.0634,  0.2629, -0.7054,  ...,  0.5503, -0.0394,  0.1570]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.1248, -0.1852,  0.0105,  ...,  0.7552,  0.0564, -0.0280],\n",
      "         [-0.1500, -0.1688, -0.1458,  ...,  0.9038,  0.1449, -0.1784],\n",
      "         ...,\n",
      "         [-0.4391, -0.1355, -0.1538,  ...,  0.7640,  0.1193,  0.1445],\n",
      "         [-0.5116,  0.3366, -0.5073,  ...,  0.5256,  0.0828, -0.1231],\n",
      "         [-0.1895,  0.0143, -0.2632,  ...,  0.8727,  0.4173, -0.0745]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [7/10], Loss: 1.3767\n",
      "Model outputs:  tensor([[[ 0.1452, -0.1047,  0.2862,  ..., -0.3735, -0.6295,  0.3260],\n",
      "         [ 0.0669, -0.0255, -0.3116,  ..., -0.4619, -0.0613,  0.4272],\n",
      "         [-0.1107, -0.0387, -0.2575,  ..., -0.5014, -0.2400,  0.3563],\n",
      "         ...,\n",
      "         [-0.1606, -0.0322, -0.4535,  ..., -0.4093, -0.3054,  0.2951],\n",
      "         [-0.2006, -0.0424, -0.3462,  ..., -0.5098, -0.4175,  0.3950],\n",
      "         [-0.3859,  0.0028, -0.4761,  ..., -0.5568, -0.3959,  0.5729]],\n",
      "\n",
      "        [[-0.1363, -0.0275, -0.2551,  ...,  0.1703, -0.1455, -0.0495],\n",
      "         [-0.1162, -0.0156, -0.4801,  ..., -0.0929,  0.0828, -0.0257],\n",
      "         [-0.0944,  0.1454, -0.7041,  ..., -0.1501,  0.0724,  0.1838],\n",
      "         ...,\n",
      "         [-0.3470,  0.1680, -0.3801,  ..., -0.0165, -0.0796,  0.1361],\n",
      "         [-0.3322,  0.2763, -0.6218,  ..., -0.3128,  0.2249,  0.1187],\n",
      "         [-0.3527,  0.0883, -0.7809,  ..., -0.1827,  0.0324,  0.2058]],\n",
      "\n",
      "        [[-0.5721,  0.3008, -0.0208,  ...,  0.4379,  0.0616, -0.2315],\n",
      "         [-0.3690,  0.1271, -0.1262,  ...,  0.0998,  0.2545, -0.2286],\n",
      "         [-0.4186,  0.4543, -0.2368,  ...,  0.0473,  0.2356, -0.2973],\n",
      "         ...,\n",
      "         [-0.6185,  0.2549, -0.4174,  ...,  0.1238,  0.4281, -0.2122],\n",
      "         [-0.6712,  0.2775, -0.4678,  ...,  0.1972,  0.1885, -0.0907],\n",
      "         [-0.4113,  0.3777, -0.4606,  ...,  0.3789,  0.0938,  0.0829]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2083,  0.0824, -0.1714,  ...,  0.0548,  0.2548,  0.1647],\n",
      "         [-0.4945,  0.2569, -0.5971,  ..., -0.2286,  0.2526, -0.0455],\n",
      "         [-0.6114,  0.3909, -0.3149,  ..., -0.1695,  0.1539,  0.1889],\n",
      "         ...,\n",
      "         [-0.7461,  0.3680, -0.6034,  ..., -0.1269,  0.0901,  0.3223],\n",
      "         [-0.5022,  0.2899, -0.5457,  ...,  0.1012,  0.1952,  0.0756],\n",
      "         [-0.6417,  0.3070, -0.5401,  ...,  0.0529,  0.1624, -0.0446]],\n",
      "\n",
      "        [[ 0.0114,  0.1011,  0.1419,  ..., -0.2647, -0.3739,  0.0718],\n",
      "         [-0.0589,  0.0573, -0.1745,  ..., -0.2643,  0.0562,  0.2470],\n",
      "         [-0.1059, -0.1055, -0.0586,  ..., -0.5445, -0.1899,  0.2492],\n",
      "         ...,\n",
      "         [-0.3321, -0.0805, -0.4079,  ..., -0.3246, -0.1551,  0.2502],\n",
      "         [-0.1853,  0.0442, -0.4359,  ..., -0.6952, -0.3203,  0.2466],\n",
      "         [-0.1466,  0.0031, -0.3854,  ..., -0.4852, -0.1068,  0.3941]],\n",
      "\n",
      "        [[-0.4742,  0.1848, -0.4416,  ...,  0.0608,  0.1449,  0.1923],\n",
      "         [-0.3131,  0.3662, -0.3073,  ...,  0.0809,  0.4552,  0.1325],\n",
      "         [-0.4440,  0.1885, -0.6651,  ..., -0.0292,  0.3891,  0.2208],\n",
      "         ...,\n",
      "         [-0.4637,  0.0502, -0.4546,  ..., -0.0273,  0.3031,  0.4599],\n",
      "         [-0.6750,  0.1371, -0.6185,  ..., -0.0286,  0.3600,  0.0910],\n",
      "         [-0.6271,  0.2801, -0.2424,  ..., -0.3554,  0.2403,  0.1426]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2025442123413086\n",
      "Model outputs:  tensor([[[ 0.3627,  0.0875, -0.3069,  ..., -0.2231, -0.1544, -0.6683],\n",
      "         [ 0.0858, -0.2496, -0.0081,  ...,  0.0632, -0.0183, -0.6312],\n",
      "         [ 0.2921, -0.3345, -0.1390,  ...,  0.1169,  0.0313, -0.5063],\n",
      "         ...,\n",
      "         [ 0.3705, -0.6306, -0.4265,  ...,  0.1401, -0.1107, -0.3442],\n",
      "         [ 0.1857, -0.2725, -0.2079,  ..., -0.0901,  0.2760, -0.4769],\n",
      "         [-0.0452, -0.3867, -0.0348,  ..., -0.1265,  0.0595, -0.2657]],\n",
      "\n",
      "        [[-0.4253,  0.1402, -0.3939,  ...,  0.0478,  0.1400, -0.3809],\n",
      "         [-0.6993,  0.0851, -0.2699,  ...,  0.3556, -0.0132,  0.1028],\n",
      "         [-0.3537,  0.0455, -0.1121,  ...,  0.2165,  0.3618, -0.1290],\n",
      "         ...,\n",
      "         [-0.1211, -0.2475, -0.1617,  ...,  0.5723,  0.4217, -0.2435],\n",
      "         [-0.4015, -0.0300, -0.3316,  ...,  0.5067,  0.4525, -0.1114],\n",
      "         [-0.5530,  0.1352, -0.1351,  ...,  0.4688,  0.3347,  0.0894]],\n",
      "\n",
      "        [[-0.3022, -0.2554, -0.1866,  ..., -0.4870, -0.3370, -0.0273],\n",
      "         [ 0.1398, -0.3476, -0.4230,  ..., -0.3889, -0.4205,  0.1411],\n",
      "         [-0.0801, -0.2137, -0.5819,  ..., -0.3813, -0.2590,  0.3397],\n",
      "         ...,\n",
      "         [ 0.2180, -0.3397, -0.5133,  ..., -0.0321, -0.2353,  0.2995],\n",
      "         [ 0.0535, -0.2254, -0.5285,  ...,  0.1278, -0.2136,  0.5402],\n",
      "         [ 0.1972, -0.1648, -0.2998,  ..., -0.1246, -0.4473,  0.5156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0890, -0.4270, -0.1977,  ..., -0.6307, -0.2344,  0.0942],\n",
      "         [ 0.1215, -0.0959, -0.3706,  ..., -0.3800, -0.2393,  0.1913],\n",
      "         [ 0.0835, -0.0408, -0.5906,  ..., -0.3193, -0.0681,  0.5900],\n",
      "         ...,\n",
      "         [ 0.2129, -0.4610, -0.5722,  ..., -0.1464, -0.1991,  0.4697],\n",
      "         [ 0.0666, -0.2430, -0.8131,  ...,  0.0827,  0.0441,  0.5815],\n",
      "         [-0.0613, -0.1723, -0.5001,  ..., -0.1907,  0.0414,  0.3951]],\n",
      "\n",
      "        [[ 0.0650,  0.1396, -0.1388,  ..., -0.7628, -0.3442,  0.2751],\n",
      "         [-0.1647, -0.1590, -0.2847,  ..., -0.3834, -0.1981,  0.1864],\n",
      "         [ 0.1213, -0.0976, -0.5449,  ..., -0.3051, -0.1790,  0.3830],\n",
      "         ...,\n",
      "         [ 0.1752, -0.2102, -0.4014,  ...,  0.0652,  0.0443,  0.1999],\n",
      "         [-0.0945, -0.1215, -0.7373,  ...,  0.0019, -0.1771,  0.4444],\n",
      "         [ 0.0355, -0.0673, -0.3093,  ..., -0.1893, -0.3707,  0.2315]],\n",
      "\n",
      "        [[ 0.1572, -0.1071, -0.4598,  ..., -0.8588,  0.6818,  0.4793],\n",
      "         [ 0.0619, -0.1157, -0.3683,  ..., -0.6117,  0.5507,  0.3508],\n",
      "         [-0.1343, -0.0630, -0.5523,  ..., -0.6022,  0.5489,  0.3438],\n",
      "         ...,\n",
      "         [ 0.2845, -0.4180, -0.3611,  ..., -0.1811,  0.4741,  0.4446],\n",
      "         [ 0.1730, -0.0389, -0.5059,  ..., -0.3735,  0.4035,  0.5549],\n",
      "         [-0.0067, -0.0284, -0.3189,  ..., -0.5316,  0.4749,  0.6247]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.212135910987854\n",
      "Model outputs:  tensor([[[-0.3460,  0.0423, -0.5375,  ...,  0.6511,  0.4298,  0.0491],\n",
      "         [-0.4606,  0.0929, -0.7925,  ...,  0.6081,  0.5399, -0.0728],\n",
      "         [-0.5015,  0.0034, -0.7938,  ...,  0.8198,  0.2743,  0.0929],\n",
      "         ...,\n",
      "         [-0.3926,  0.2676, -0.5583,  ...,  0.8008,  0.6036,  0.0115],\n",
      "         [-0.4021,  0.0769, -0.5988,  ...,  0.6985,  0.5334,  0.0754],\n",
      "         [-0.1705,  0.1302, -0.5789,  ...,  0.8340,  0.4874, -0.0136]],\n",
      "\n",
      "        [[-0.0479, -0.2848, -0.1102,  ..., -0.0188, -0.2503,  0.3338],\n",
      "         [-0.2148, -0.1264, -0.4686,  ..., -0.0306, -0.2494,  0.4008],\n",
      "         [-0.0073, -0.2593, -0.6419,  ...,  0.0165, -0.2920,  0.4855],\n",
      "         ...,\n",
      "         [-0.1220, -0.0068, -0.4392,  ...,  0.0016, -0.1102,  0.6210],\n",
      "         [-0.1085, -0.2957, -0.2155,  ..., -0.1597, -0.1150,  0.6681],\n",
      "         [ 0.1609, -0.2521, -0.3108,  ...,  0.0402, -0.2682,  0.3130]],\n",
      "\n",
      "        [[ 0.1234, -0.3718, -0.1260,  ...,  0.1564, -0.0331,  0.4533],\n",
      "         [-0.1430,  0.0179, -0.5753,  ...,  0.1922,  0.2507,  0.2911],\n",
      "         [-0.2030, -0.1304, -0.5361,  ..., -0.0490, -0.1641,  0.2992],\n",
      "         ...,\n",
      "         [-0.1636,  0.0530, -0.5606,  ...,  0.0557, -0.1588,  0.3644],\n",
      "         [-0.0334, -0.2323, -0.5149,  ...,  0.0789, -0.4146,  0.4619],\n",
      "         [-0.0531, -0.1633, -0.3556,  ..., -0.1612, -0.3288,  0.2922]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3556, -0.3465, -0.5755,  ...,  0.1424,  0.0379,  0.3258],\n",
      "         [-0.5399, -0.1978, -0.6117,  ..., -0.1829,  0.3168,  0.1713],\n",
      "         [-0.3886,  0.0463, -0.6888,  ...,  0.0445,  0.0495,  0.1240],\n",
      "         ...,\n",
      "         [-0.3800,  0.1669, -0.8076,  ...,  0.0118,  0.3111,  0.1455],\n",
      "         [-0.2130, -0.1378, -0.4685,  ...,  0.2303,  0.1304,  0.3376],\n",
      "         [-0.2196, -0.2183, -0.8041,  ...,  0.1281,  0.2503,  0.2444]],\n",
      "\n",
      "        [[ 0.1192, -0.1668, -0.6319,  ...,  0.4359,  0.0197, -0.1151],\n",
      "         [-0.1056, -0.0177, -0.7783,  ...,  0.2922,  0.1469,  0.1020],\n",
      "         [-0.2490,  0.1757, -0.8452,  ...,  0.5861,  0.1952,  0.0519],\n",
      "         ...,\n",
      "         [-0.0123, -0.1785, -0.7545,  ...,  0.6175,  0.2050,  0.1876],\n",
      "         [-0.1067,  0.0520, -0.4815,  ...,  0.3659,  0.1353,  0.1433],\n",
      "         [-0.1507,  0.1519, -0.6854,  ...,  0.2838, -0.1119,  0.1396]],\n",
      "\n",
      "        [[ 0.0625, -0.5795, -0.2900,  ..., -0.4769,  0.2722,  0.0376],\n",
      "         [ 0.1646, -0.1060, -0.4054,  ..., -0.3375,  0.2255,  0.1108],\n",
      "         [ 0.2211, -0.2386, -0.4829,  ..., -0.1895,  0.3595,  0.3069],\n",
      "         ...,\n",
      "         [-0.0627, -0.2640, -0.3396,  ..., -0.1261, -0.1353,  0.0741],\n",
      "         [-0.0459, -0.5391, -0.2276,  ..., -0.3274,  0.1168,  0.1597],\n",
      "         [ 0.0902, -0.3904, -0.2444,  ..., -0.3363, -0.1619,  0.1509]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3996154069900513\n",
      "Model outputs:  tensor([[[ 0.1667,  0.0949,  0.2522,  ...,  0.1862, -0.3062, -0.9617],\n",
      "         [-0.0554, -0.2410,  0.4043,  ...,  0.1090, -0.2823, -0.6402],\n",
      "         [ 0.3043, -0.3224, -0.2119,  ...,  0.4376,  0.0078, -0.4306],\n",
      "         ...,\n",
      "         [-0.0055, -0.1274,  0.3042,  ...,  0.3663, -0.4523, -0.8732],\n",
      "         [ 0.0923, -0.1832,  0.3966,  ...,  0.2339, -0.2312, -0.8834],\n",
      "         [-0.2204, -0.2270,  0.0707,  ...,  0.4756,  0.1342, -0.5551]],\n",
      "\n",
      "        [[-0.5059,  0.2906, -0.3044,  ...,  0.4875,  0.4338, -0.1490],\n",
      "         [-0.5604,  0.1704, -0.0712,  ...,  0.3093,  0.2431,  0.0122],\n",
      "         [-0.2584,  0.0768, -0.1648,  ...,  0.9169,  0.4907, -0.0605],\n",
      "         ...,\n",
      "         [-0.4977,  0.4993, -0.1965,  ...,  0.3393,  0.1544, -0.2862],\n",
      "         [-0.6216,  0.5490,  0.0170,  ...,  0.4526,  0.4193, -0.2854],\n",
      "         [-0.3756, -0.0043,  0.0423,  ...,  0.5396,  0.4598,  0.0439]],\n",
      "\n",
      "        [[-0.1321, -0.0390, -0.3613,  ..., -0.2508, -0.0267,  0.1209],\n",
      "         [-0.0279, -0.2430, -0.1905,  ..., -0.1539, -0.2010,  0.0547],\n",
      "         [-0.1861,  0.0023, -0.5939,  ..., -0.1144,  0.0194,  0.4912],\n",
      "         ...,\n",
      "         [-0.2170,  0.1800, -0.1001,  ..., -0.2478, -0.1633, -0.1560],\n",
      "         [-0.1063,  0.0567, -0.0796,  ..., -0.5151, -0.3049,  0.0021],\n",
      "         [-0.0295, -0.3888, -0.0975,  ..., -0.0562, -0.2601,  0.4690]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0863, -0.0139, -0.3081,  ..., -0.3850, -0.0660,  0.1531],\n",
      "         [-0.0505, -0.0094, -0.1653,  ..., -0.1921, -0.1660,  0.2229],\n",
      "         [ 0.1054, -0.0864, -0.4897,  ...,  0.0322, -0.0993,  0.2248],\n",
      "         ...,\n",
      "         [ 0.1259,  0.0451, -0.0881,  ..., -0.5703, -0.3826,  0.2141],\n",
      "         [ 0.1507,  0.0207, -0.1291,  ..., -0.1639, -0.0488,  0.0534],\n",
      "         [ 0.0159, -0.5709, -0.1827,  ...,  0.2622, -0.0976,  0.4815]],\n",
      "\n",
      "        [[-0.1867,  0.1435, -0.2842,  ...,  0.1142,  0.1676, -0.1150],\n",
      "         [-0.1274,  0.1275, -0.3948,  ..., -0.0867,  0.0431, -0.1397],\n",
      "         [-0.0985,  0.2541, -0.4830,  ...,  0.1728, -0.0246,  0.1756],\n",
      "         ...,\n",
      "         [-0.1299,  0.0868, -0.3049,  ..., -0.1190, -0.0932, -0.1614],\n",
      "         [-0.2914,  0.0948, -0.4422,  ..., -0.0635, -0.1444, -0.0127],\n",
      "         [-0.0766, -0.3357, -0.5780,  ...,  0.3361,  0.0211,  0.2932]],\n",
      "\n",
      "        [[ 0.0396, -0.1960, -0.6016,  ..., -0.2580,  0.0814,  0.1661],\n",
      "         [-0.2226,  0.1497, -0.4317,  ..., -0.3411,  0.2400,  0.1420],\n",
      "         [-0.1416,  0.0253, -0.5126,  ..., -0.0492,  0.2793,  0.5669],\n",
      "         ...,\n",
      "         [ 0.0119,  0.0622, -0.3644,  ..., -0.4104,  0.1034, -0.0483],\n",
      "         [ 0.0165,  0.1405, -0.3384,  ..., -0.3065, -0.2178,  0.0085],\n",
      "         [-0.1415, -0.4418, -0.3233,  ..., -0.2029, -0.0294,  0.2216]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8387795686721802\n",
      "Model outputs:  tensor([[[-0.6884,  0.1647, -0.2770,  ...,  0.3749,  0.2233,  0.0293],\n",
      "         [-0.4099, -0.0484, -0.5687,  ...,  0.1709, -0.0403, -0.0864],\n",
      "         [-0.5017,  0.3079, -0.6774,  ...,  0.0333,  0.1637, -0.0331],\n",
      "         ...,\n",
      "         [-0.4865, -0.1313, -0.7524,  ...,  0.3300,  0.0376, -0.1220],\n",
      "         [-0.5555,  0.1859, -0.5435,  ...,  0.3771,  0.3678,  0.3455],\n",
      "         [-0.4183,  0.2818, -0.4096,  ...,  0.0722,  0.2821,  0.1967]],\n",
      "\n",
      "        [[-0.2409,  0.1570, -0.1606,  ...,  0.0147, -0.1568,  0.2160],\n",
      "         [-0.1368,  0.1029, -0.3124,  ..., -0.0986, -0.1302, -0.1603],\n",
      "         [-0.0492, -0.0827, -0.2973,  ..., -0.1766, -0.1262,  0.4200],\n",
      "         ...,\n",
      "         [-0.1118, -0.0155, -0.2493,  ..., -0.1042, -0.1739,  0.3578],\n",
      "         [-0.1838, -0.1639, -0.3896,  ...,  0.1199, -0.1274,  0.4628],\n",
      "         [ 0.0520,  0.1318, -0.2962,  ..., -0.3036, -0.0185,  0.1244]],\n",
      "\n",
      "        [[-0.3529, -0.1374, -0.5447,  ...,  0.1906,  0.0423, -0.0370],\n",
      "         [-0.1721,  0.2980, -0.4488,  ...,  0.0790,  0.0376,  0.1415],\n",
      "         [-0.2255, -0.0158, -0.4306,  ...,  0.0473,  0.1289, -0.0297],\n",
      "         ...,\n",
      "         [-0.1642,  0.0714, -0.4790,  ...,  0.3043,  0.3317, -0.0659],\n",
      "         [-0.1737,  0.1352, -0.6721,  ...,  0.2459,  0.1382,  0.1459],\n",
      "         [-0.2813,  0.3945, -0.3051,  ..., -0.0983,  0.2805, -0.1122]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2513,  0.0908,  0.0228,  ..., -0.1277,  0.1562, -0.0991],\n",
      "         [-0.0864,  0.2036, -0.3448,  ...,  0.0812, -0.1114,  0.1927],\n",
      "         [-0.0930, -0.1487, -0.2524,  ..., -0.4014, -0.1003,  0.1138],\n",
      "         ...,\n",
      "         [-0.1179, -0.0530, -0.3163,  ..., -0.0967, -0.2612,  0.0896],\n",
      "         [ 0.1396,  0.0646, -0.4830,  ..., -0.0014, -0.0482,  0.3118],\n",
      "         [ 0.0706,  0.1489, -0.4354,  ..., -0.1439, -0.1937,  0.3142]],\n",
      "\n",
      "        [[-0.1632, -0.0211, -0.1314,  ..., -0.1215, -0.0041,  0.3283],\n",
      "         [-0.1341,  0.1942, -0.2932,  ..., -0.0395, -0.1933,  0.0821],\n",
      "         [ 0.0942, -0.0461, -0.2517,  ..., -0.1955, -0.0035,  0.0878],\n",
      "         ...,\n",
      "         [ 0.0309,  0.0263, -0.2627,  ...,  0.0787, -0.2896,  0.6088],\n",
      "         [-0.0248, -0.1195, -0.3812,  ..., -0.1543, -0.3641,  0.4550],\n",
      "         [ 0.0777,  0.0935, -0.4347,  ..., -0.0809, -0.2810,  0.3082]],\n",
      "\n",
      "        [[-0.5265,  0.1153, -0.6236,  ...,  0.3010, -0.1035, -0.2495],\n",
      "         [-0.1246, -0.0066, -0.5434,  ...,  0.2846, -0.1058, -0.1349],\n",
      "         [ 0.1300,  0.0449, -0.5239,  ...,  0.2882,  0.0667,  0.0641],\n",
      "         ...,\n",
      "         [-0.0210,  0.0236, -0.5253,  ...,  0.4815,  0.2567, -0.2531],\n",
      "         [-0.1234,  0.2752, -0.6582,  ...,  0.4343,  0.0264,  0.2358],\n",
      "         [-0.1484,  0.4232, -0.2249,  ...,  0.4567,  0.2129, -0.0603]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.739363431930542\n",
      "Model outputs:  tensor([[[-0.6823,  0.0615, -0.5442,  ..., -0.0382,  0.1727,  0.0967],\n",
      "         [-0.7318, -0.0039, -0.3954,  ...,  0.3258,  0.2508,  0.2384],\n",
      "         [-0.7881,  0.1028, -0.3956,  ...,  0.0192,  0.3725,  0.2505],\n",
      "         ...,\n",
      "         [-0.5226,  0.1739, -0.5606,  ..., -0.1624,  0.0625,  0.1182],\n",
      "         [-0.7837,  0.0821, -0.5572,  ...,  0.1516, -0.0305,  0.3624],\n",
      "         [-0.3863,  0.0894, -0.5432,  ...,  0.2123, -0.1230,  0.3604]],\n",
      "\n",
      "        [[-0.4751,  0.3797, -0.1547,  ...,  0.0844,  0.1473, -0.2723],\n",
      "         [-0.2811,  0.2068, -0.2636,  ...,  0.5355,  0.2371,  0.1170],\n",
      "         [-0.4905, -0.0189, -0.4890,  ...,  0.5245,  0.4778, -0.0803],\n",
      "         ...,\n",
      "         [-0.4473,  0.1436, -0.0322,  ...,  0.1793, -0.1580, -0.1241],\n",
      "         [-0.4630,  0.0011, -0.5126,  ...,  0.3433, -0.0182,  0.0412],\n",
      "         [-0.5840,  0.2421, -0.3852,  ...,  0.6674, -0.0312, -0.0037]],\n",
      "\n",
      "        [[ 0.2636, -0.1986, -0.3319,  ..., -0.3919, -0.2855,  0.3057],\n",
      "         [ 0.1916, -0.4264, -0.3316,  ..., -0.3097, -0.2462,  0.4752],\n",
      "         [ 0.0423, -0.3527, -0.4413,  ..., -0.2837, -0.3729,  0.4031],\n",
      "         ...,\n",
      "         [-0.0617, -0.1818, -0.2251,  ..., -0.4052, -0.3839,  0.3253],\n",
      "         [-0.2787, -0.2530, -0.6597,  ..., -0.2075, -0.6415,  0.8091],\n",
      "         [-0.2176, -0.3042, -0.4257,  ..., -0.3668, -0.7157,  0.8954]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6886,  0.2622, -0.4198,  ...,  0.3812,  0.2116,  0.1019],\n",
      "         [-0.4626,  0.1035, -0.1743,  ...,  0.3312,  0.1623,  0.1030],\n",
      "         [-0.3784, -0.1805, -0.4234,  ...,  0.3110,  0.0417,  0.2011],\n",
      "         ...,\n",
      "         [-0.5727,  0.2138, -0.3287,  ...,  0.2881, -0.0678,  0.1531],\n",
      "         [-0.6306,  0.0768, -0.6093,  ...,  0.2345, -0.1698,  0.2895],\n",
      "         [-0.6529, -0.1547, -0.5244,  ...,  0.4861, -0.0757,  0.1710]],\n",
      "\n",
      "        [[ 0.0906, -0.1159, -0.7148,  ..., -0.4117, -0.3071,  0.4739],\n",
      "         [ 0.0069, -0.2716, -0.4315,  ..., -0.4194, -0.4803,  0.4404],\n",
      "         [-0.1529, -0.2275, -0.4415,  ..., -0.4740, -0.4561,  0.0593],\n",
      "         ...,\n",
      "         [-0.0406, -0.0555, -0.5631,  ..., -0.4324, -0.2559, -0.1058],\n",
      "         [-0.3710, -0.0821, -0.7546,  ..., -0.3861, -0.7064,  0.5648],\n",
      "         [-0.3062, -0.4776, -0.4044,  ..., -0.4602, -0.6199,  0.6797]],\n",
      "\n",
      "        [[-0.5808, -0.1506, -0.7761,  ...,  0.1421,  0.0206,  0.1264],\n",
      "         [-0.2419,  0.1474, -0.8174,  ...,  0.1352, -0.1154,  0.0318],\n",
      "         [-0.4247,  0.1116, -0.5163,  ...,  0.0882,  0.0625,  0.0644],\n",
      "         ...,\n",
      "         [-0.5582,  0.2700, -0.5004,  ..., -0.0131, -0.2195,  0.0172],\n",
      "         [-0.4325,  0.1120, -0.9047,  ...,  0.0334, -0.5121,  0.4612],\n",
      "         [-0.6720, -0.2051, -0.6707,  ...,  0.0720, -0.4178,  0.3224]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.103259563446045\n",
      "Model outputs:  tensor([[[-7.7715e-01,  2.8293e-01, -1.6930e-01,  ...,  3.5281e-01,\n",
      "           2.9572e-01, -3.4103e-01],\n",
      "         [-5.9209e-01, -6.0649e-02, -3.5204e-01,  ...,  6.1446e-01,\n",
      "           4.0342e-01, -4.3234e-02],\n",
      "         [-3.9394e-01, -3.3500e-01, -8.5779e-02,  ...,  4.1007e-01,\n",
      "           8.4101e-02,  3.4627e-02],\n",
      "         ...,\n",
      "         [-2.7921e-01, -2.1969e-01,  2.5979e-01,  ...,  4.6791e-01,\n",
      "           1.5232e-02, -3.2237e-01],\n",
      "         [-3.4922e-01,  1.0094e-01,  2.8858e-01,  ...,  5.8875e-01,\n",
      "           4.9494e-02, -1.2189e-01],\n",
      "         [-4.1242e-01, -9.6575e-02, -8.3982e-02,  ...,  4.4693e-01,\n",
      "           1.4358e-01, -3.4687e-01]],\n",
      "\n",
      "        [[-2.3681e-01,  4.3681e-01, -2.9706e-01,  ...,  5.2074e-01,\n",
      "           1.7816e-01, -1.8593e-01],\n",
      "         [-8.3086e-02,  1.6064e-01, -8.9310e-02,  ...,  8.9251e-01,\n",
      "           4.4738e-01, -1.9130e-01],\n",
      "         [-3.7426e-01,  4.0057e-02, -1.0938e-01,  ...,  8.9297e-01,\n",
      "           2.8387e-01,  9.5005e-02],\n",
      "         ...,\n",
      "         [-4.0285e-02, -4.8982e-02,  1.8939e-01,  ...,  6.8905e-01,\n",
      "          -1.4148e-01, -2.0589e-02],\n",
      "         [-1.8041e-01,  1.6826e-01,  2.7762e-01,  ...,  5.0532e-01,\n",
      "          -1.6054e-01, -2.8882e-01],\n",
      "         [-5.2406e-01,  9.3145e-02, -2.6835e-01,  ...,  7.5682e-01,\n",
      "           5.2076e-02, -3.4495e-01]],\n",
      "\n",
      "        [[-5.6301e-01,  2.8782e-01, -7.0149e-01,  ...,  7.0010e-01,\n",
      "           2.1195e-01, -3.0138e-01],\n",
      "         [-5.1428e-01,  1.0856e-02, -5.4416e-01,  ...,  5.6730e-01,\n",
      "           2.2954e-01,  2.0568e-01],\n",
      "         [-3.5693e-01, -2.2691e-01, -5.5619e-01,  ...,  6.3109e-01,\n",
      "           9.1901e-02,  1.0518e-01],\n",
      "         ...,\n",
      "         [-3.0259e-01, -1.3212e-01, -1.9784e-01,  ...,  4.2926e-01,\n",
      "          -2.4398e-01, -1.6976e-01],\n",
      "         [-3.9250e-01,  2.2477e-01,  2.5812e-02,  ...,  6.5060e-01,\n",
      "          -4.6536e-04, -1.1434e-01],\n",
      "         [-5.6993e-01, -8.0291e-02, -5.0985e-01,  ...,  4.5942e-01,\n",
      "          -8.3530e-02,  4.1982e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.6208e-02,  1.4468e-01, -5.5216e-01,  ...,  4.3828e-01,\n",
      "          -5.2025e-02, -3.4735e-01],\n",
      "         [-3.1049e-01, -7.6087e-03, -6.6967e-01,  ...,  3.9421e-01,\n",
      "          -6.1543e-02, -1.8447e-01],\n",
      "         [-2.0264e-01, -1.8234e-01, -3.7156e-01,  ...,  4.3254e-01,\n",
      "          -1.3487e-01,  8.0866e-02],\n",
      "         ...,\n",
      "         [-1.6499e-02,  5.5668e-03, -7.6402e-02,  ...,  3.1811e-01,\n",
      "          -5.4299e-01,  1.2872e-01],\n",
      "         [-2.4536e-01,  2.2739e-01, -2.4885e-01,  ...,  3.5019e-01,\n",
      "          -3.0336e-01, -3.0472e-01],\n",
      "         [-5.3644e-02, -1.2106e-02, -4.0934e-01,  ...,  3.4172e-01,\n",
      "          -2.8898e-01, -1.9026e-01]],\n",
      "\n",
      "        [[-6.6016e-01,  3.6817e-01, -5.3788e-01,  ...,  3.5622e-01,\n",
      "           1.9177e-01,  1.1548e-02],\n",
      "         [-4.5536e-01,  3.5244e-03, -4.5525e-01,  ...,  5.1810e-01,\n",
      "           4.7448e-01,  2.2670e-01],\n",
      "         [-4.2301e-01, -2.0673e-01, -2.9558e-01,  ...,  3.0084e-01,\n",
      "           1.6120e-01,  4.6347e-02],\n",
      "         ...,\n",
      "         [-2.7949e-01, -2.4422e-01,  1.9771e-01,  ...,  1.7110e-01,\n",
      "          -1.9064e-01,  1.8910e-01],\n",
      "         [-2.9843e-01,  1.8051e-01,  3.5925e-01,  ..., -1.0175e-01,\n",
      "          -5.6245e-02,  2.4168e-02],\n",
      "         [-5.1175e-01,  9.8788e-02, -3.9365e-01,  ...,  4.1908e-01,\n",
      "           2.5252e-01, -2.0966e-01]],\n",
      "\n",
      "        [[ 1.6684e-02, -7.5349e-02, -6.4137e-01,  ..., -3.3191e-01,\n",
      "          -7.8266e-02,  1.6712e-01],\n",
      "         [ 4.0298e-02, -1.6979e-01, -5.4929e-01,  ..., -2.8199e-02,\n",
      "          -5.7233e-02,  2.1858e-01],\n",
      "         [ 5.9809e-02, -3.8261e-01, -4.6084e-01,  ...,  1.5262e-01,\n",
      "          -7.7229e-02,  3.2848e-01],\n",
      "         ...,\n",
      "         [ 1.6687e-01, -2.4295e-01, -2.2330e-02,  ..., -1.2673e-01,\n",
      "          -1.3431e-01, -2.4992e-01],\n",
      "         [ 1.7904e-01, -1.4908e-01,  2.6990e-01,  ..., -8.6743e-02,\n",
      "          -1.0064e-01, -6.6235e-02],\n",
      "         [ 8.6780e-02, -2.0063e-02, -2.6836e-01,  ...,  6.6228e-02,\n",
      "          -2.2799e-01,  6.2479e-02]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1018937826156616\n",
      "Model outputs:  tensor([[[ 0.0022, -0.3503, -0.4251,  ...,  0.0547, -0.4882,  0.5803],\n",
      "         [ 0.1411, -0.2203, -0.3787,  ..., -0.2350, -0.1969,  0.4835],\n",
      "         [-0.1255,  0.1439, -0.4669,  ..., -0.0398, -0.2362,  0.3727],\n",
      "         ...,\n",
      "         [ 0.0272, -0.3485, -0.6646,  ..., -0.0322, -0.2282,  0.4856],\n",
      "         [ 0.0639, -0.4469, -0.2385,  ..., -0.0840, -0.2500,  0.0816],\n",
      "         [-0.0636, -0.2290, -0.5671,  ...,  0.0102, -0.2373,  0.6314]],\n",
      "\n",
      "        [[-0.3983,  0.1551, -0.8074,  ...,  0.5878,  0.0591,  0.0349],\n",
      "         [-0.0889,  0.0300, -0.6947,  ...,  0.7908,  0.3229, -0.0627],\n",
      "         [-0.5817,  0.0977, -0.8137,  ...,  1.0352,  0.3692, -0.0102],\n",
      "         ...,\n",
      "         [-0.1572, -0.3822, -0.6244,  ...,  0.7907,  0.1748, -0.1198],\n",
      "         [-0.3150, -0.0312, -0.7092,  ...,  0.6943, -0.0185, -0.1314],\n",
      "         [-0.1835,  0.1326, -0.9588,  ...,  0.9986,  0.2565,  0.0318]],\n",
      "\n",
      "        [[-0.1087, -0.3277, -0.9281,  ...,  0.4979, -0.0052,  0.2189],\n",
      "         [-0.0532, -0.0784, -0.7081,  ...,  0.9537,  0.1765,  0.1330],\n",
      "         [-0.1266, -0.1389, -0.8019,  ...,  0.4539,  0.1774, -0.0375],\n",
      "         ...,\n",
      "         [-0.3668, -0.3362, -0.9253,  ...,  0.5342,  0.1116, -0.1201],\n",
      "         [-0.5002, -0.1668, -0.6762,  ...,  0.7141, -0.0549, -0.0599],\n",
      "         [-0.1202, -0.0529, -0.8097,  ...,  0.6184,  0.1609,  0.0115]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4509, -0.2748, -0.8402,  ...,  0.1465, -0.1178,  0.1442],\n",
      "         [-0.1936, -0.0265, -0.8224,  ...,  0.2705,  0.0185,  0.2127],\n",
      "         [-0.6948, -0.1866, -0.8261,  ...,  0.0656,  0.2160,  0.2416],\n",
      "         ...,\n",
      "         [-0.6947, -0.4020, -0.5987,  ...,  0.2300, -0.1884,  0.3713],\n",
      "         [-0.4920, -0.4115, -0.5477,  ...,  0.4625, -0.0814,  0.1485],\n",
      "         [-0.7056, -0.0414, -0.9437,  ...,  0.1382, -0.2485,  0.2180]],\n",
      "\n",
      "        [[ 0.0434, -0.3958, -0.6072,  ..., -0.1504, -0.3337,  0.4059],\n",
      "         [-0.1620, -0.2811, -0.3177,  ..., -0.1450, -0.1957,  0.4554],\n",
      "         [-0.1140, -0.0128, -0.3064,  ...,  0.0300, -0.0402,  0.6288],\n",
      "         ...,\n",
      "         [-0.1004, -0.3625, -0.3698,  ...,  0.0087, -0.1214,  0.2095],\n",
      "         [-0.1452, -0.3326, -0.2964,  ..., -0.2813, -0.4561,  0.3402],\n",
      "         [ 0.1807, -0.4761, -0.7899,  ..., -0.1136, -0.4282,  0.5999]],\n",
      "\n",
      "        [[-0.0731, -0.0402, -0.6001,  ...,  0.2316, -0.0633,  0.1511],\n",
      "         [ 0.1693, -0.3253, -0.6619,  ...,  0.3542,  0.2766,  0.2115],\n",
      "         [-0.2002, -0.2256, -0.8129,  ...,  0.3612,  0.0084,  0.4119],\n",
      "         ...,\n",
      "         [-0.0644, -0.1008, -0.9440,  ...,  0.4068, -0.1843, -0.0586],\n",
      "         [-0.0072,  0.0246, -0.7699,  ...,  0.4096, -0.2228,  0.0959],\n",
      "         [-0.1523, -0.0484, -0.6866,  ...,  0.1725, -0.2770,  0.1714]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0016329288482666\n",
      "Model outputs:  tensor([[[-0.2444,  0.0303, -0.6640,  ..., -0.0210, -0.1366, -0.1670],\n",
      "         [-0.2677,  0.1552, -0.3792,  ..., -0.1318, -0.1864, -0.3087],\n",
      "         [ 0.0168,  0.0955, -0.1741,  ..., -0.4265, -0.3836,  0.0539],\n",
      "         ...,\n",
      "         [-0.2244,  0.0740, -0.2657,  ..., -0.1456, -0.2179,  0.0274],\n",
      "         [-0.2358,  0.0773, -0.4725,  ..., -0.1258, -0.2510, -0.1190],\n",
      "         [-0.0499, -0.0596, -0.4116,  ..., -0.0358, -0.1789,  0.1173]],\n",
      "\n",
      "        [[-0.4647,  0.0432, -0.7282,  ...,  0.0990,  0.2648, -0.0670],\n",
      "         [-0.0740,  0.1610, -0.6187,  ..., -0.1234,  0.1110, -0.1373],\n",
      "         [-0.1085,  0.1459, -0.4939,  ..., -0.4383, -0.3391, -0.0537],\n",
      "         ...,\n",
      "         [-0.4150,  0.2581, -0.5338,  ..., -0.3999, -0.0392, -0.1271],\n",
      "         [-0.6182,  0.1738, -0.5265,  ..., -0.2853, -0.1966,  0.0300],\n",
      "         [-0.5058,  0.2980, -0.4870,  ..., -0.1208,  0.2221, -0.0187]],\n",
      "\n",
      "        [[-0.0529, -0.0322, -0.2432,  ..., -0.2590, -0.0545, -0.0294],\n",
      "         [-0.2125, -0.0139, -0.2099,  ..., -0.2717, -0.0564, -0.0241],\n",
      "         [ 0.1335, -0.0596, -0.2580,  ..., -0.3512,  0.0263,  0.0867],\n",
      "         ...,\n",
      "         [-0.2381,  0.1877, -0.7727,  ..., -0.3437, -0.0159,  0.0084],\n",
      "         [-0.2320,  0.0770, -0.3085,  ..., -0.2543, -0.0752, -0.0300],\n",
      "         [ 0.0417, -0.0542, -0.1801,  ..., -0.2177, -0.4027, -0.0056]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4035,  0.1123, -0.2293,  ...,  0.2244,  0.2034, -0.0623],\n",
      "         [-0.2121,  0.1164, -0.1816,  ...,  0.0883, -0.0140, -0.0684],\n",
      "         [-0.5252,  0.2219, -0.3183,  ...,  0.1681,  0.1469, -0.2203],\n",
      "         ...,\n",
      "         [-0.3579,  0.3391, -0.4078,  ..., -0.0183,  0.1435, -0.2842],\n",
      "         [-0.4914,  0.1614, -0.3834,  ...,  0.0069,  0.1168, -0.0096],\n",
      "         [-0.6324,  0.1106, -0.4425,  ...,  0.2657, -0.1230, -0.0079]],\n",
      "\n",
      "        [[-0.1905,  0.3396, -0.6759,  ..., -0.3596, -0.2343,  0.0400],\n",
      "         [-0.3455,  0.0354, -0.3547,  ..., -0.1470, -0.0631,  0.1205],\n",
      "         [-0.0644,  0.1543, -0.4411,  ..., -0.3985, -0.1148,  0.0512],\n",
      "         ...,\n",
      "         [ 0.0503,  0.2868, -0.3340,  ..., -0.4192, -0.0639,  0.2338],\n",
      "         [ 0.0414,  0.1522, -0.4790,  ..., -0.3867, -0.0536,  0.1688],\n",
      "         [-0.0759,  0.0322, -0.3058,  ..., -0.2931, -0.2909, -0.0882]],\n",
      "\n",
      "        [[ 0.0437, -0.1850, -0.1807,  ..., -0.3536,  0.1772, -0.4736],\n",
      "         [ 0.2170, -0.1623, -0.0956,  ..., -0.5240,  0.0022, -0.1375],\n",
      "         [-0.0308, -0.2559, -0.1494,  ..., -0.7875,  0.2076, -0.0500],\n",
      "         ...,\n",
      "         [ 0.1658, -0.0241, -0.2843,  ..., -0.4638, -0.2961, -0.4584],\n",
      "         [ 0.1250,  0.0733, -0.2670,  ..., -0.1014, -0.1479,  0.0063],\n",
      "         [-0.0036, -0.4401, -0.3290,  ..., -0.4594, -0.1950, -0.2668]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3672597408294678\n",
      "Model outputs:  tensor([[[-2.7537e-01, -3.5583e-03, -1.5726e-01,  ...,  2.6737e-01,\n",
      "           6.2718e-02, -5.1334e-02],\n",
      "         [-3.1643e-01,  7.2411e-01,  6.4694e-02,  ...,  5.2125e-01,\n",
      "           1.0594e-01, -9.8559e-02],\n",
      "         [-2.8595e-01,  4.0735e-01, -3.3973e-01,  ...,  4.3602e-01,\n",
      "           9.5265e-02,  6.8079e-02],\n",
      "         ...,\n",
      "         [-5.3565e-01,  1.4464e-01,  1.5374e-01,  ...,  2.0605e-01,\n",
      "          -2.4234e-01, -8.4532e-02],\n",
      "         [-4.4164e-02, -8.7471e-02,  3.8045e-02,  ...,  5.4498e-01,\n",
      "           3.1704e-02,  4.0037e-02],\n",
      "         [-5.8195e-01,  3.8332e-01, -1.2941e-01,  ...,  2.6029e-01,\n",
      "           4.0871e-01, -1.4143e-01]],\n",
      "\n",
      "        [[-3.6328e-01,  3.2462e-01, -6.7681e-01,  ...,  8.3538e-01,\n",
      "           2.6454e-01, -3.1044e-03],\n",
      "         [-6.0836e-01,  4.8731e-01, -6.1095e-01,  ...,  4.8831e-01,\n",
      "           8.8649e-02,  2.2095e-01],\n",
      "         [-2.0906e-01,  1.3764e-01, -5.7259e-01,  ...,  4.0434e-01,\n",
      "           3.1032e-01, -6.5405e-02],\n",
      "         ...,\n",
      "         [-4.5047e-01,  6.1467e-01, -5.6469e-01,  ...,  4.1097e-01,\n",
      "          -3.0439e-02, -8.3591e-02],\n",
      "         [-8.5925e-02,  1.2150e-02, -3.0221e-01,  ...,  4.6656e-01,\n",
      "           8.7198e-02,  8.2499e-02],\n",
      "         [-4.7763e-01,  5.4116e-01, -6.4268e-01,  ...,  2.2582e-01,\n",
      "           9.4795e-02,  2.6089e-03]],\n",
      "\n",
      "        [[-2.3031e-01,  2.3533e-01, -6.3310e-01,  ...,  2.0062e-01,\n",
      "          -1.6495e-01,  1.4318e-01],\n",
      "         [-4.7898e-01,  3.8673e-01, -7.1408e-01,  ...,  1.2594e-01,\n",
      "          -7.6040e-02, -1.0312e-01],\n",
      "         [-1.9389e-01,  1.5862e-01, -7.6273e-01,  ...,  3.6585e-01,\n",
      "          -1.1432e-01,  1.0672e-01],\n",
      "         ...,\n",
      "         [-2.8055e-01,  2.6654e-01, -5.7369e-01,  ...,  1.7862e-01,\n",
      "          -3.7753e-01,  1.6850e-01],\n",
      "         [-1.3647e-01, -5.2038e-02, -4.8324e-01,  ...,  2.7492e-01,\n",
      "          -4.7164e-01,  8.3533e-02],\n",
      "         [-5.9834e-01,  3.9151e-01, -2.5163e-01,  ...,  1.3214e-01,\n",
      "          -3.6349e-03, -1.0908e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.9190e-02, -1.6597e-01, -4.9098e-01,  ..., -1.3498e-02,\n",
      "           9.6126e-02,  3.2409e-01],\n",
      "         [-2.7389e-01,  1.5162e-01, -5.5692e-01,  ..., -1.7985e-01,\n",
      "          -1.2112e-01, -1.6315e-02],\n",
      "         [-5.2501e-02, -1.5075e-01, -3.8412e-01,  ...,  3.9704e-02,\n",
      "           9.2887e-02,  2.0278e-03],\n",
      "         ...,\n",
      "         [-1.6029e-01, -1.2376e-01, -1.9779e-01,  ..., -1.7579e-01,\n",
      "          -7.5455e-02,  8.7022e-02],\n",
      "         [-1.0422e-02, -2.7895e-01, -2.0521e-01,  ..., -1.5018e-01,\n",
      "          -1.8838e-01,  1.8355e-01],\n",
      "         [-4.4786e-01,  9.4488e-02, -1.4797e-01,  ..., -3.0009e-01,\n",
      "           9.0991e-03, -1.8000e-01]],\n",
      "\n",
      "        [[-2.5939e-01,  1.2209e-01, -4.2659e-01,  ...,  4.1512e-01,\n",
      "          -1.8429e-01, -1.8184e-01],\n",
      "         [-3.9187e-01,  5.5719e-01, -8.9225e-01,  ..., -3.7946e-02,\n",
      "           1.1782e-01,  1.9495e-01],\n",
      "         [-2.8027e-01,  1.0627e-01, -9.0410e-01,  ...,  1.2327e-01,\n",
      "          -2.1732e-01, -9.5568e-02],\n",
      "         ...,\n",
      "         [-1.7283e-01,  2.4881e-01, -7.0893e-01,  ...,  4.3725e-03,\n",
      "          -1.3134e-01,  8.1171e-02],\n",
      "         [-2.0856e-01, -8.0460e-02, -3.5244e-01,  ...,  1.4070e-01,\n",
      "          -1.3684e-01, -4.9696e-02],\n",
      "         [-2.7319e-01,  3.3124e-01, -3.2846e-01,  ..., -8.6334e-04,\n",
      "          -1.7177e-01,  1.3675e-02]],\n",
      "\n",
      "        [[ 3.0842e-01, -1.8037e-01, -2.4726e-01,  ..., -2.1645e-02,\n",
      "           1.1138e-01, -4.0038e-01],\n",
      "         [-1.1370e-01,  4.6180e-02, -5.0480e-01,  ..., -2.6455e-01,\n",
      "          -3.3416e-01, -4.2145e-01],\n",
      "         [ 2.2171e-01,  1.9459e-02, -3.9587e-02,  ...,  4.4471e-03,\n",
      "           1.0305e-01, -6.1819e-01],\n",
      "         ...,\n",
      "         [ 3.6869e-01, -1.8990e-01, -6.0674e-02,  ..., -4.7558e-01,\n",
      "           8.8799e-02, -3.6628e-01],\n",
      "         [ 9.3422e-02, -3.2316e-01, -1.1727e-02,  ..., -8.4515e-03,\n",
      "           4.9945e-02, -4.3434e-01],\n",
      "         [ 2.5898e-03,  3.8033e-02, -1.3576e-01,  ..., -1.1454e-01,\n",
      "           1.6926e-01, -6.5905e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1411961317062378\n",
      "Model outputs:  tensor([[[-0.2456, -0.0428, -0.8332,  ...,  0.2732, -0.0996, -0.0431],\n",
      "         [ 0.0860, -0.6744, -0.7242,  ...,  0.6972, -0.0130,  0.2623],\n",
      "         [-0.1525, -0.1426, -0.8717,  ...,  0.4684,  0.2593,  0.0596],\n",
      "         ...,\n",
      "         [-0.3578, -0.0804, -0.4508,  ...,  0.3766, -0.1507,  0.2030],\n",
      "         [-0.3004, -0.6443, -0.3811,  ...,  0.3267, -0.2972,  0.0867],\n",
      "         [-0.5448, -0.2061, -0.7925,  ...,  0.0247,  0.1507, -0.1188]],\n",
      "\n",
      "        [[ 0.2442, -0.3223,  0.0546,  ...,  0.3613, -0.1815, -0.5219],\n",
      "         [ 0.3649, -0.3916,  0.1601,  ...,  0.4607,  0.4137, -0.4974],\n",
      "         [ 0.1332, -0.3804, -0.0630,  ...,  0.3842,  0.1072, -0.4736],\n",
      "         ...,\n",
      "         [ 0.1666, -0.4349,  0.1617,  ...,  0.4027, -0.0733, -0.2514],\n",
      "         [ 0.1198, -0.6922,  0.2070,  ...,  0.6023, -0.0302, -0.2368],\n",
      "         [ 0.3578, -0.2586, -0.0780,  ...,  0.3384, -0.2127, -0.5829]],\n",
      "\n",
      "        [[-0.6851, -0.1550, -0.6068,  ..., -0.0347, -0.1854,  0.0847],\n",
      "         [-0.5556, -0.1419, -0.7963,  ..., -0.0792, -0.1515,  0.0370],\n",
      "         [-0.2554, -0.4693, -0.5036,  ...,  0.1056,  0.0601,  0.2441],\n",
      "         ...,\n",
      "         [-0.4936, -0.3108, -0.5949,  ..., -0.0855, -0.0733,  0.2326],\n",
      "         [-0.1665, -0.6238, -0.5802,  ...,  0.0176, -0.3595,  0.1863],\n",
      "         [-0.3143, -0.2851, -0.6599,  ..., -0.0222, -0.1085,  0.0176]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7227,  0.0551, -0.4232,  ...,  0.2823,  0.1585,  0.0251],\n",
      "         [-0.5233, -0.3910, -0.4827,  ..., -0.0250,  0.1347,  0.2169],\n",
      "         [-0.2303, -0.3209, -0.5144,  ...,  0.1483,  0.2986, -0.1025],\n",
      "         ...,\n",
      "         [-0.6166, -0.2201, -0.3587,  ...,  0.3773, -0.1211,  0.1527],\n",
      "         [-0.3752, -0.7133, -0.2775,  ...,  0.4094, -0.0408,  0.2902],\n",
      "         [-0.5877,  0.0518, -0.5408,  ...,  0.0710, -0.2461,  0.0671]],\n",
      "\n",
      "        [[-0.2188, -0.2398, -0.4384,  ..., -0.1136, -0.3716,  0.1516],\n",
      "         [-0.0478, -0.2704, -0.5434,  ..., -0.0881, -0.0009,  0.2024],\n",
      "         [ 0.1338, -0.2031, -0.4292,  ..., -0.2210,  0.0786,  0.2465],\n",
      "         ...,\n",
      "         [-0.0044, -0.2360, -0.5267,  ..., -0.1362, -0.2993,  0.0837],\n",
      "         [-0.0305, -0.7529, -0.3041,  ...,  0.2731, -0.2220,  0.5286],\n",
      "         [-0.0128, -0.3565, -0.4370,  ..., -0.1861, -0.2218,  0.1806]],\n",
      "\n",
      "        [[ 0.0898, -0.5569, -0.4234,  ..., -0.2522, -0.5277,  0.4668],\n",
      "         [ 0.0065, -0.0215, -0.3118,  ..., -0.3160, -0.0707,  0.1714],\n",
      "         [ 0.0197, -0.3142, -0.4908,  ...,  0.0537, -0.0758,  0.2350],\n",
      "         ...,\n",
      "         [-0.1624, -0.3340, -0.2081,  ..., -0.0966, -0.2901,  0.2043],\n",
      "         [-0.0491, -0.7630, -0.0278,  ..., -0.2399, -0.3639,  0.5889],\n",
      "         [-0.0329, -0.4202, -0.5965,  ..., -0.3239, -0.2597,  0.4146]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.6942002773284912\n",
      "Model outputs:  tensor([[[ 0.1943, -0.1138, -0.4164,  ..., -0.1366,  0.2503,  0.5114],\n",
      "         [ 0.3880, -0.1062, -0.4759,  ..., -0.3127,  0.1905,  0.3902],\n",
      "         [ 0.4102, -0.2797, -0.2626,  ..., -0.3061,  0.2615,  0.6317],\n",
      "         ...,\n",
      "         [ 0.1877, -0.1090, -0.3867,  ..., -0.0391,  0.5721,  0.1991],\n",
      "         [ 0.3765, -0.1539, -0.4735,  ..., -0.0978,  0.5055,  0.2729],\n",
      "         [ 0.2999, -0.1128, -0.4077,  ..., -0.0977,  0.5093,  0.3098]],\n",
      "\n",
      "        [[-0.0279, -0.2626, -0.7080,  ...,  0.3768, -0.0014,  0.1560],\n",
      "         [ 0.1963, -0.2618, -0.5161,  ...,  0.0142, -0.3552, -0.0883],\n",
      "         [ 0.0591, -0.1265, -0.4204,  ...,  0.2777,  0.0985,  0.2851],\n",
      "         ...,\n",
      "         [-0.0240, -0.2762, -0.5961,  ...,  0.1009, -0.0146,  0.2191],\n",
      "         [ 0.0745, -0.1805, -0.7031,  ...,  0.3001, -0.0010, -0.0756],\n",
      "         [-0.0498,  0.1155, -0.6805,  ...,  0.2725,  0.3086,  0.0520]],\n",
      "\n",
      "        [[-0.0964, -0.4480, -0.2287,  ..., -0.3898,  0.3508,  0.1804],\n",
      "         [ 0.4051, -0.2668, -0.3660,  ..., -0.6257,  0.1414,  0.1320],\n",
      "         [ 0.2114, -0.3408, -0.1655,  ..., -0.5434,  0.2267,  0.1533],\n",
      "         ...,\n",
      "         [-0.0614, -0.1487, -0.8258,  ...,  0.8365,  0.1672, -0.0913],\n",
      "         [ 0.1826, -0.1010, -0.7668,  ...,  0.6278,  0.3701,  0.1046],\n",
      "         [-0.1390,  0.0209, -0.6541,  ...,  0.8605,  0.2688, -0.2657]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0890, -0.3622, -0.2021,  ..., -0.0905,  0.2745, -0.0570],\n",
      "         [ 0.0921, -0.1975, -0.2902,  ..., -0.1346, -0.1383, -0.2768],\n",
      "         [ 0.0618, -0.5685, -0.1020,  ..., -0.2360,  0.1379,  0.1157],\n",
      "         ...,\n",
      "         [ 0.0980, -0.5152, -0.3087,  ..., -0.1473,  0.0662, -0.2586],\n",
      "         [ 0.1623, -0.4055, -0.1246,  ..., -0.0462, -0.0867, -0.4508],\n",
      "         [-0.1537, -0.3363, -0.3222,  ...,  0.0161, -0.0141, -0.0209]],\n",
      "\n",
      "        [[-0.1636, -0.3795, -0.4403,  ...,  0.6352,  0.0778, -0.0178],\n",
      "         [ 0.0624, -0.3536, -0.2117,  ...,  0.5405,  0.0191, -0.1618],\n",
      "         [-0.2524, -0.5275,  0.0782,  ...,  0.4576,  0.0776,  0.0579],\n",
      "         ...,\n",
      "         [-0.4096, -0.3711, -0.0332,  ...,  0.5056,  0.1736, -0.0371],\n",
      "         [-0.0819, -0.0698, -0.2981,  ...,  0.7510,  0.5511, -0.2670],\n",
      "         [-0.5342,  0.0631, -0.3224,  ...,  0.6469,  0.4014, -0.0144]],\n",
      "\n",
      "        [[-0.2518, -0.1534, -0.6247,  ...,  0.2753,  0.1063,  0.1458],\n",
      "         [ 0.1743, -0.1682, -0.2421,  ...,  0.2791, -0.3403, -0.0931],\n",
      "         [ 0.0607, -0.2399, -0.4987,  ...,  0.4173, -0.1055,  0.0651],\n",
      "         ...,\n",
      "         [-0.0862, -0.1104, -0.5904,  ...,  0.3652,  0.2246, -0.1029],\n",
      "         [ 0.1140, -0.1973, -0.3625,  ...,  0.4475,  0.1607, -0.0656],\n",
      "         [-0.0644, -0.2211, -0.6394,  ...,  0.3760, -0.0575,  0.1596]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2075334787368774\n",
      "Model outputs:  tensor([[[ 0.0578, -0.3409, -0.2561,  ...,  0.1663, -0.2287, -0.4199],\n",
      "         [ 0.2636, -0.4403, -0.0062,  ...,  0.4673, -0.2265, -0.3362],\n",
      "         [ 0.2885, -0.4366, -0.1409,  ...,  0.3345, -0.1945, -0.4652],\n",
      "         ...,\n",
      "         [-0.1186, -0.3701, -0.0539,  ...,  0.2632, -0.1041, -0.5686],\n",
      "         [ 0.2281, -0.4348, -0.0112,  ...,  0.5351, -0.1042, -0.4593],\n",
      "         [ 0.0906, -0.3846,  0.1580,  ...,  0.5363, -0.1821, -0.6564]],\n",
      "\n",
      "        [[-0.3325, -0.0962, -0.1682,  ...,  0.2654, -0.0729,  0.0582],\n",
      "         [-0.4644, -0.2140, -0.6863,  ...,  0.9119,  0.1703,  0.1299],\n",
      "         [-0.2541, -0.1494, -0.3228,  ...,  0.4458,  0.2269, -0.1064],\n",
      "         ...,\n",
      "         [-0.3026, -0.3143, -0.0943,  ...,  0.6344,  0.1142, -0.0582],\n",
      "         [-0.2123, -0.1915, -0.2212,  ...,  0.4619,  0.0642, -0.3242],\n",
      "         [-0.3542, -0.0052, -0.1393,  ...,  0.5237, -0.0851, -0.1266]],\n",
      "\n",
      "        [[-0.4587,  0.0554, -0.3821,  ...,  0.2581,  0.0622,  0.0539],\n",
      "         [-0.2664, -0.2799, -0.6282,  ...,  0.4587,  0.1667,  0.2588],\n",
      "         [-0.2961, -0.0420, -0.5540,  ...,  0.2996,  0.0572,  0.0028],\n",
      "         ...,\n",
      "         [-0.3246, -0.0090, -0.3717,  ...,  0.2955,  0.0318,  0.1781],\n",
      "         [-0.4893, -0.0101, -0.6574,  ...,  0.3509, -0.0936,  0.1537],\n",
      "         [-0.3748,  0.1410, -0.3023,  ...,  0.3884, -0.0776,  0.0107]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3354,  0.2029, -0.2602,  ...,  0.5506,  0.4993, -0.1139],\n",
      "         [-0.4406, -0.2753,  0.0152,  ...,  0.7814,  0.2156, -0.0995],\n",
      "         [-0.1667, -0.0433, -0.3185,  ...,  0.4012,  0.3323, -0.1037],\n",
      "         ...,\n",
      "         [-0.3348, -0.4198, -0.2567,  ...,  0.2142,  0.1446, -0.0235],\n",
      "         [-0.2448, -0.2167, -0.1562,  ...,  0.3966,  0.3222,  0.0434],\n",
      "         [-0.3125, -0.0552,  0.3880,  ...,  0.4311,  0.0508, -0.2114]],\n",
      "\n",
      "        [[-0.5105,  0.1287, -0.5026,  ...,  0.1091,  0.3358,  0.1561],\n",
      "         [-0.1343, -0.3724, -0.2771,  ...,  0.3213,  0.0861,  0.3859],\n",
      "         [-0.5188,  0.3432, -0.5828,  ...,  0.2542,  0.1882, -0.0475],\n",
      "         ...,\n",
      "         [-0.4097, -0.2738, -0.2165,  ...,  0.3257,  0.2730,  0.0151],\n",
      "         [-0.2100,  0.0740, -0.4227,  ...,  0.5755,  0.3993,  0.1214],\n",
      "         [-0.4655,  0.1989, -0.3798,  ...,  0.3340, -0.2308, -0.0367]],\n",
      "\n",
      "        [[-0.1122, -0.2026, -0.2078,  ..., -0.1208, -0.2258,  0.1859],\n",
      "         [ 0.0542, -0.5208, -0.3705,  ..., -0.0568, -0.3776,  0.3355],\n",
      "         [ 0.1453, -0.3842, -0.3446,  ..., -0.1840, -0.1193,  0.5235],\n",
      "         ...,\n",
      "         [ 0.1171, -0.5260, -0.1524,  ..., -0.1849, -0.1728,  0.5302],\n",
      "         [ 0.1397, -0.3726, -0.1860,  ..., -0.1357, -0.1892,  0.5370],\n",
      "         [ 0.3474, -0.4017, -0.2442,  ..., -0.3822, -0.2240,  0.4378]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.31509268283844\n",
      "Model outputs:  tensor([[[-0.3947,  0.0246, -0.5036,  ..., -0.0377,  0.3628, -0.2380],\n",
      "         [-0.3566, -0.2858, -0.7633,  ..., -0.0826,  0.0810,  0.0193],\n",
      "         [-0.4397, -0.1625, -0.8319,  ..., -0.0050,  0.3777, -0.1414],\n",
      "         ...,\n",
      "         [-0.5416,  0.1198, -0.8424,  ...,  0.0219,  0.1037, -0.0500],\n",
      "         [-0.5127, -0.0731, -0.6111,  ...,  0.1325,  0.2737,  0.0920],\n",
      "         [-0.4951,  0.0320, -0.8680,  ...,  0.0886,  0.1367, -0.3622]],\n",
      "\n",
      "        [[-0.1215, -0.1517, -0.3541,  ..., -0.3118, -0.0880,  0.0046],\n",
      "         [ 0.0052, -0.2145, -0.2795,  ..., -0.2256, -0.3545,  0.2974],\n",
      "         [-0.2220, -0.3270, -0.5563,  ..., -0.4635, -0.0927, -0.0565],\n",
      "         ...,\n",
      "         [-0.2906, -0.0770, -0.5268,  ..., -0.1607,  0.0556,  0.0407],\n",
      "         [-0.0495, -0.4648, -0.2600,  ..., -0.2455,  0.0708,  0.4739],\n",
      "         [-0.2768, -0.2032, -0.3521,  ..., -0.4455,  0.1484,  0.1736]],\n",
      "\n",
      "        [[ 0.3271, -0.1325, -0.0743,  ..., -0.1194,  0.0807, -0.6207],\n",
      "         [-0.0976,  0.1597,  0.0888,  ...,  0.3490, -0.1209, -0.9110],\n",
      "         [ 0.0973, -0.2977,  0.0854,  ...,  0.3821,  0.1146, -0.9020],\n",
      "         ...,\n",
      "         [ 0.2575, -0.2023, -0.0814,  ...,  0.3838,  0.0224, -0.5143],\n",
      "         [-0.1185, -0.5983,  0.0448,  ...,  0.3236,  0.2581, -0.6195],\n",
      "         [ 0.2167, -0.1534,  0.1297,  ...,  0.5340,  0.2027, -0.6149]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1667, -0.0400, -0.4723,  ..., -0.2313,  0.2708, -0.1776],\n",
      "         [-0.0360, -0.1623, -0.3925,  ..., -0.0950, -0.0223,  0.1213],\n",
      "         [ 0.1431, -0.1012, -0.5047,  ..., -0.2038,  0.1225,  0.0675],\n",
      "         ...,\n",
      "         [-0.0962, -0.0440, -0.4124,  ...,  0.2019,  0.0913,  0.1142],\n",
      "         [-0.0275, -0.2085, -0.5294,  ..., -0.0030,  0.0256,  0.4614],\n",
      "         [-0.0218,  0.0529, -0.7637,  ..., -0.2404,  0.0510,  0.2232]],\n",
      "\n",
      "        [[-0.3260,  0.2584, -0.5305,  ..., -0.1390, -0.1151,  0.1459],\n",
      "         [ 0.0993, -0.1708, -0.5487,  ..., -0.3681, -0.1824,  0.2591],\n",
      "         [-0.3973, -0.0149, -0.3025,  ..., -0.3741, -0.1693, -0.0465],\n",
      "         ...,\n",
      "         [-0.1217,  0.0922, -0.3654,  ..., -0.1348, -0.0808,  0.1390],\n",
      "         [ 0.1645, -0.5561, -0.3202,  ..., -0.1509, -0.4014,  0.2811],\n",
      "         [ 0.1218, -0.0816, -0.6263,  ..., -0.0046, -0.0477,  0.3717]],\n",
      "\n",
      "        [[-0.1750,  0.3513, -0.4898,  ...,  0.0879,  0.0851, -0.1745],\n",
      "         [-0.0701,  0.2389, -0.6707,  ...,  0.0881,  0.0663, -0.0683],\n",
      "         [-0.0873,  0.2707, -0.7856,  ...,  0.2979,  0.2147, -0.0985],\n",
      "         ...,\n",
      "         [ 0.1095,  0.1970, -0.6004,  ...,  0.4136,  0.2673,  0.0550],\n",
      "         [-0.1187, -0.0023, -0.4980,  ...,  0.3172,  0.0593,  0.1469],\n",
      "         [-0.1746,  0.2030, -0.8061,  ...,  0.3818,  0.1056,  0.0490]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3194903135299683\n",
      "Model outputs:  tensor([[[-4.8623e-01,  1.6384e-01, -5.8338e-02,  ...,  5.1030e-01,\n",
      "           1.6073e-01, -1.5882e-01],\n",
      "         [-5.4797e-01,  7.5019e-03, -6.6040e-02,  ...,  6.0813e-01,\n",
      "           4.2593e-01, -1.2287e-01],\n",
      "         [-4.9289e-01, -1.8529e-01, -2.6171e-01,  ...,  5.2192e-01,\n",
      "           3.0915e-01, -3.1168e-02],\n",
      "         ...,\n",
      "         [-7.3569e-01,  4.2206e-02,  3.1301e-02,  ...,  6.2990e-01,\n",
      "          -1.6397e-01,  5.8060e-03],\n",
      "         [-4.1433e-01, -7.7670e-02, -9.0261e-02,  ...,  4.9141e-01,\n",
      "           1.5182e-01, -2.5800e-01],\n",
      "         [-5.9056e-01,  7.6525e-02, -1.7999e-01,  ...,  4.7674e-01,\n",
      "           4.1414e-01, -2.2753e-01]],\n",
      "\n",
      "        [[-9.0630e-01,  4.6556e-01, -6.3826e-01,  ...,  4.6214e-01,\n",
      "           7.6716e-02,  2.2456e-01],\n",
      "         [-4.1972e-01,  1.9119e-01, -5.6864e-01,  ...,  5.1485e-01,\n",
      "           1.4264e-01,  3.0589e-01],\n",
      "         [-6.3709e-01,  3.1679e-01, -6.6983e-01,  ...,  6.5609e-01,\n",
      "           5.2554e-01,  2.6697e-02],\n",
      "         ...,\n",
      "         [-4.9096e-01,  2.4216e-01, -5.2239e-01,  ...,  6.1180e-01,\n",
      "          -4.0577e-01, -5.1271e-02],\n",
      "         [-4.2507e-01,  1.4838e-01, -4.5110e-01,  ...,  3.9494e-01,\n",
      "           1.4821e-01, -1.5483e-01],\n",
      "         [-6.7700e-01, -8.0775e-04, -4.9495e-01,  ...,  4.3997e-01,\n",
      "           2.6593e-01,  1.9971e-01]],\n",
      "\n",
      "        [[-4.3443e-01,  1.7791e-01, -5.3763e-03,  ...,  4.7166e-01,\n",
      "           4.2019e-01, -9.4062e-02],\n",
      "         [-4.8495e-01, -2.6555e-01,  2.5638e-02,  ...,  5.9778e-01,\n",
      "           1.4423e-01, -1.9744e-02],\n",
      "         [-4.5559e-01,  5.6019e-02, -7.6819e-02,  ...,  6.7759e-01,\n",
      "           3.1286e-01, -4.3608e-02],\n",
      "         ...,\n",
      "         [-4.4030e-01, -6.7796e-02, -2.8291e-01,  ...,  5.8124e-01,\n",
      "          -3.3830e-01,  1.5120e-01],\n",
      "         [-7.5026e-01,  2.8415e-01, -2.1473e-01,  ...,  4.0309e-01,\n",
      "           3.4888e-01, -4.2760e-01],\n",
      "         [-7.9352e-01, -8.4531e-02, -2.6341e-01,  ...,  4.1554e-01,\n",
      "           2.1096e-01,  8.5017e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.5765e-01,  1.3117e-01, -5.4233e-01,  ...,  1.6045e-02,\n",
      "          -9.5094e-02,  7.3169e-02],\n",
      "         [-6.7347e-01, -2.0995e-01, -4.0699e-01,  ...,  1.1396e-01,\n",
      "           6.0530e-02,  4.6866e-01],\n",
      "         [-3.8628e-01, -1.1525e-01, -6.1254e-01,  ..., -5.3664e-02,\n",
      "           2.9190e-01,  1.8279e-01],\n",
      "         ...,\n",
      "         [-4.9468e-01,  1.1652e-01, -6.8424e-01,  ..., -3.1540e-01,\n",
      "          -6.0566e-01,  9.9497e-02],\n",
      "         [-4.2512e-01,  2.9879e-01, -3.2435e-01,  ..., -4.6674e-02,\n",
      "           8.2920e-02, -6.4443e-02],\n",
      "         [-6.6110e-01,  1.3735e-01, -4.1110e-01,  ..., -6.9987e-02,\n",
      "           1.8651e-01,  2.0855e-01]],\n",
      "\n",
      "        [[-4.2608e-01, -7.4971e-02, -2.8054e-01,  ...,  6.6372e-01,\n",
      "           1.4354e-01, -1.6073e-02],\n",
      "         [-3.9038e-01, -1.1345e-01,  6.1044e-02,  ...,  6.4569e-01,\n",
      "           4.2491e-01, -1.5518e-01],\n",
      "         [-2.6017e-01,  9.3467e-02, -2.7998e-01,  ...,  6.3968e-01,\n",
      "           3.6408e-01, -2.4829e-01],\n",
      "         ...,\n",
      "         [-7.1341e-01,  9.4261e-02,  7.9672e-02,  ...,  6.2340e-01,\n",
      "          -3.5785e-01, -7.9481e-02],\n",
      "         [-4.9806e-01,  2.4990e-01, -1.1832e-01,  ...,  5.5788e-01,\n",
      "           3.5603e-01, -3.0851e-02],\n",
      "         [-6.8825e-01,  1.3710e-01, -4.3618e-01,  ...,  5.4497e-01,\n",
      "           2.7727e-01, -2.3156e-01]],\n",
      "\n",
      "        [[-1.7536e-01, -1.5762e-01, -4.1758e-01,  ..., -2.3723e-02,\n",
      "          -1.1768e-02, -2.1955e-01],\n",
      "         [-1.9746e-01, -3.8360e-01, -3.7856e-02,  ...,  4.4480e-02,\n",
      "           7.5526e-02, -4.0185e-01],\n",
      "         [-1.6093e-01, -2.8663e-01, -3.4101e-01,  ...,  2.8562e-02,\n",
      "           3.2469e-01, -3.4135e-01],\n",
      "         ...,\n",
      "         [-7.9990e-02, -4.4950e-02, -2.1889e-01,  ...,  8.9980e-02,\n",
      "          -6.2763e-01, -4.3907e-01],\n",
      "         [-3.4817e-01, -9.5299e-02, -3.3532e-01,  ..., -1.2759e-01,\n",
      "          -1.9692e-01, -4.9018e-01],\n",
      "         [-9.1826e-02, -2.4508e-01, -2.8672e-01,  ..., -7.9733e-02,\n",
      "           1.3963e-02, -3.0190e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1278618574142456\n",
      "Model outputs:  tensor([[[ 1.7107e-01, -1.1635e-01, -4.6177e-02,  ..., -5.5709e-01,\n",
      "          -4.1774e-01, -1.3846e-01],\n",
      "         [-3.0145e-03, -9.2368e-02, -2.2625e-01,  ..., -3.1847e-01,\n",
      "          -2.9591e-01,  2.3954e-01],\n",
      "         [-1.4708e-02, -1.8837e-01,  6.5539e-02,  ..., -5.7842e-01,\n",
      "          -3.3009e-01,  1.7304e-03],\n",
      "         ...,\n",
      "         [-3.5082e-01, -2.7026e-01, -1.8928e-01,  ..., -5.5271e-01,\n",
      "          -1.6836e-01,  3.0597e-01],\n",
      "         [ 8.0082e-03, -3.6203e-01, -4.9983e-01,  ..., -5.1721e-01,\n",
      "          -1.7817e-01,  2.9128e-01],\n",
      "         [-9.1088e-02, -1.4993e-01, -1.8184e-01,  ..., -5.3156e-01,\n",
      "          -2.3433e-01,  1.0194e-01]],\n",
      "\n",
      "        [[-2.9049e-01,  2.2647e-01, -1.9472e-01,  ...,  4.5439e-01,\n",
      "          -3.3984e-02, -1.3326e-01],\n",
      "         [-1.6257e-01,  4.7871e-01, -3.0761e-01,  ...,  2.7147e-01,\n",
      "           7.1827e-02, -2.4277e-01],\n",
      "         [-4.2039e-01,  1.7124e-01, -2.1198e-01,  ...,  2.9130e-01,\n",
      "           1.2485e-01, -2.1172e-01],\n",
      "         ...,\n",
      "         [-2.9949e-01, -1.7048e-01, -1.3934e-01,  ...,  5.5333e-01,\n",
      "          -1.8578e-02, -1.4144e-01],\n",
      "         [-2.6051e-01,  6.6249e-02, -8.8477e-03,  ...,  5.3503e-01,\n",
      "           1.5105e-01, -1.3896e-01],\n",
      "         [-2.0934e-01,  1.3978e-01, -2.1305e-01,  ...,  6.0228e-01,\n",
      "           2.8522e-01, -2.9189e-01]],\n",
      "\n",
      "        [[-2.5110e-01, -1.3743e-02, -5.8802e-01,  ..., -7.7826e-02,\n",
      "          -2.4036e-01,  1.9490e-02],\n",
      "         [-3.4635e-01,  7.2977e-02, -4.6241e-01,  ...,  3.4353e-01,\n",
      "           2.6579e-02, -1.2328e-01],\n",
      "         [-3.3233e-01,  1.8801e-01, -2.6149e-01,  ...,  2.1603e-01,\n",
      "           1.1145e-01, -3.3613e-01],\n",
      "         ...,\n",
      "         [-3.3463e-01, -1.0770e-02, -9.1164e-01,  ...,  2.6817e-01,\n",
      "          -9.8731e-02, -6.7144e-02],\n",
      "         [-3.3489e-01,  1.3458e-01, -5.4023e-01,  ...,  4.2310e-01,\n",
      "           6.6808e-02, -1.2578e-01],\n",
      "         [-1.6743e-01, -3.9266e-02, -4.9380e-01,  ...,  3.5001e-01,\n",
      "          -1.3925e-01, -1.0436e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.2203e-02, -1.0913e-01, -2.7065e-01,  ..., -3.2285e-01,\n",
      "          -2.7394e-01, -6.5537e-01],\n",
      "         [ 6.0649e-02, -1.3157e-01, -2.3804e-01,  ..., -2.2481e-01,\n",
      "           7.7925e-02, -4.3404e-01],\n",
      "         [ 7.7857e-02, -2.5983e-02, -3.8305e-01,  ..., -3.7295e-01,\n",
      "          -4.0795e-02, -4.8184e-01],\n",
      "         ...,\n",
      "         [-9.6358e-02, -3.2878e-01, -3.3036e-01,  ...,  1.1130e-01,\n",
      "          -2.8897e-01, -2.7752e-01],\n",
      "         [-8.5049e-02, -9.2007e-02, -1.1245e-01,  ..., -9.1390e-02,\n",
      "          -8.0436e-02, -4.9997e-01],\n",
      "         [-1.0202e-01, -8.3548e-02, -8.0338e-02,  ..., -1.5044e-01,\n",
      "          -1.3317e-01, -2.4688e-01]],\n",
      "\n",
      "        [[ 1.2796e-01, -3.4295e-01, -2.9451e-01,  ..., -7.0472e-01,\n",
      "          -3.2031e-01,  3.3744e-01],\n",
      "         [ 1.9346e-01, -3.0825e-01, -2.9129e-01,  ..., -3.2854e-01,\n",
      "          -3.6643e-03, -9.9793e-02],\n",
      "         [ 9.5801e-02, -3.6542e-01, -1.1504e-01,  ..., -5.6412e-01,\n",
      "          -4.5815e-01,  1.7708e-01],\n",
      "         ...,\n",
      "         [ 2.9799e-02, -3.9338e-01, -4.9303e-02,  ..., -3.2565e-01,\n",
      "          -5.3380e-01,  6.0534e-01],\n",
      "         [ 8.6536e-03, -3.0227e-01, -2.7746e-01,  ..., -3.1919e-01,\n",
      "          -3.0540e-01,  2.7025e-01],\n",
      "         [ 3.7386e-01, -1.8917e-01, -4.3243e-02,  ..., -6.2045e-01,\n",
      "          -4.1799e-01,  4.4128e-01]],\n",
      "\n",
      "        [[-2.3275e-01, -2.2374e-01,  9.1732e-03,  ..., -5.7060e-01,\n",
      "          -9.6299e-02, -1.0461e-01],\n",
      "         [-8.3189e-02,  6.6281e-02, -1.5974e-01,  ..., -4.7054e-01,\n",
      "          -4.4129e-01,  1.1664e-01],\n",
      "         [-2.3979e-01, -1.4572e-01, -1.6731e-01,  ..., -6.9059e-01,\n",
      "          -4.7690e-01,  2.8696e-01],\n",
      "         ...,\n",
      "         [ 1.2984e-01, -3.6422e-01, -1.6367e-01,  ..., -2.2174e-01,\n",
      "          -1.9535e-01,  3.9050e-01],\n",
      "         [-2.4359e-01, -1.4214e-01, -3.0466e-01,  ..., -2.5571e-01,\n",
      "          -3.3244e-01,  4.4872e-01],\n",
      "         [ 4.0540e-04, -2.1515e-01, -1.3710e-01,  ..., -5.5391e-01,\n",
      "          -4.7005e-01,  1.7029e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9401884078979492\n",
      "Model outputs:  tensor([[[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 2.2441e-01,  5.5093e-02, -9.9051e-02,  ..., -3.5072e-02,\n",
      "          -1.9623e-01,  2.6954e-01],\n",
      "         [-2.2620e-01, -8.2336e-02, -5.4259e-01,  ...,  9.8852e-02,\n",
      "          -2.9782e-01,  4.6589e-01],\n",
      "         ...,\n",
      "         [-1.6492e-01, -2.1622e-01, -4.9046e-01,  ..., -6.5521e-02,\n",
      "          -1.6736e-01,  4.8883e-01],\n",
      "         [-1.1271e-01,  4.4949e-03, -7.2243e-01,  ...,  1.6859e-01,\n",
      "          -3.3590e-01,  2.1844e-01],\n",
      "         [ 1.4404e-01,  2.0476e-01, -4.9413e-01,  ...,  4.2125e-02,\n",
      "           3.9922e-02,  4.7566e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 3.5271e-01, -2.2049e-01, -2.8210e-01,  ..., -1.9319e-01,\n",
      "           3.1219e-01,  5.7831e-01],\n",
      "         [-5.2641e-02, -1.1271e-01, -6.2705e-01,  ..., -1.2675e-01,\n",
      "           2.9821e-01,  3.0103e-01],\n",
      "         ...,\n",
      "         [ 1.0783e-01, -1.6244e-01, -3.6349e-01,  ..., -1.4254e-01,\n",
      "           3.1780e-01,  7.0784e-01],\n",
      "         [ 1.9665e-02,  3.4983e-02, -8.6305e-01,  ..., -4.7024e-01,\n",
      "           4.1511e-01,  4.3618e-01],\n",
      "         [ 1.2634e-01,  2.5749e-02, -3.8690e-01,  ..., -1.8449e-01,\n",
      "           6.3577e-01,  2.6780e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [-5.2183e-02,  3.7604e-01, -6.2952e-01,  ...,  8.1319e-01,\n",
      "          -2.6834e-01, -5.2419e-02],\n",
      "         [-2.5400e-01, -1.2670e-01, -6.8856e-01,  ...,  8.1036e-01,\n",
      "          -1.2175e-01,  2.1840e-01],\n",
      "         ...,\n",
      "         [-5.0382e-01,  3.3972e-01, -7.3563e-01,  ...,  9.4800e-01,\n",
      "          -3.1518e-02,  1.7783e-01],\n",
      "         [-5.1340e-01,  4.8409e-01, -1.0402e+00,  ...,  8.3922e-01,\n",
      "           2.2418e-01, -1.1563e-01],\n",
      "         [-1.4608e-01,  1.7268e-01, -8.7052e-01,  ...,  8.5282e-01,\n",
      "           2.2825e-02, -1.6235e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 4.1760e-01, -3.7022e-01,  2.7034e-01,  ...,  5.6991e-01,\n",
      "          -2.8998e-01, -5.8333e-01],\n",
      "         [ 1.5462e-01, -2.1460e-01, -2.4106e-01,  ...,  6.9359e-01,\n",
      "          -3.8069e-01, -3.3293e-01],\n",
      "         ...,\n",
      "         [ 1.2461e-02, -1.8084e-01, -2.9266e-02,  ...,  6.1414e-01,\n",
      "          -1.4881e-01, -4.2735e-01],\n",
      "         [ 3.3413e-01, -6.6649e-02, -1.1205e-01,  ...,  6.1927e-01,\n",
      "          -1.5141e-01, -2.9680e-01],\n",
      "         [ 3.7863e-01, -2.9513e-01, -8.2259e-02,  ...,  6.0027e-01,\n",
      "          -1.2292e-02, -4.5836e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [-2.3577e-01,  8.5255e-02, -7.9841e-02,  ...,  8.0385e-01,\n",
      "           1.7213e-01, -1.3384e-01],\n",
      "         [-4.3094e-01, -2.0431e-01, -2.2395e-01,  ...,  9.2426e-01,\n",
      "           9.9927e-02, -1.8431e-01],\n",
      "         ...,\n",
      "         [-6.7372e-01,  1.2745e-01, -3.7759e-01,  ...,  6.2714e-01,\n",
      "           8.7421e-02, -7.6050e-04],\n",
      "         [-2.9776e-01,  4.1575e-01, -4.4933e-01,  ...,  7.6603e-01,\n",
      "           2.8529e-01, -7.6705e-02],\n",
      "         [-4.5305e-01,  1.9206e-01, -3.4151e-01,  ...,  5.8901e-01,\n",
      "           2.0267e-01, -1.2348e-01]],\n",
      "\n",
      "        [[        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 4.8048e-01, -1.2202e-01,  1.1654e-01,  ...,  6.6411e-01,\n",
      "          -1.9331e-01, -5.1500e-01],\n",
      "         [ 5.4353e-03, -3.4519e-01,  6.9552e-02,  ...,  6.0094e-01,\n",
      "          -4.4165e-01, -5.1216e-01],\n",
      "         ...,\n",
      "         [-2.0416e-02, -3.3188e-01, -1.3578e-01,  ...,  8.1679e-01,\n",
      "          -2.8489e-01, -2.6561e-01],\n",
      "         [ 3.3420e-01, -2.8496e-01, -1.8560e-02,  ...,  4.9645e-01,\n",
      "          -1.4793e-01, -4.4038e-01],\n",
      "         [ 2.7955e-02, -9.1691e-02, -1.3325e-01,  ...,  4.4143e-01,\n",
      "          -2.0551e-01, -5.7233e-01]]], grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [8/10], Loss: 0.9402\n",
      "Model outputs:  tensor([[[ 0.2457, -0.0638, -0.1050,  ..., -0.1872, -0.1401, -0.1011],\n",
      "         [-0.0633,  0.0524, -0.1020,  ...,  0.0461, -0.2693, -0.2191],\n",
      "         [-0.0959,  0.1454, -0.0485,  ...,  0.0346, -0.4836,  0.0193],\n",
      "         ...,\n",
      "         [ 0.3557, -0.2044, -0.3179,  ...,  0.3355, -0.3390,  0.1837],\n",
      "         [-0.0150, -0.4340, -0.0945,  ...,  0.3555, -0.0641, -0.0777],\n",
      "         [-0.1906,  0.1082, -0.7948,  ...,  0.0555, -0.2926, -0.1004]],\n",
      "\n",
      "        [[-0.4275, -0.1969,  0.5897,  ...,  0.1736, -0.1391, -0.1640],\n",
      "         [-0.0396, -0.2183,  0.1545,  ...,  0.1090, -0.0992, -0.3572],\n",
      "         [-0.3184, -0.1961,  0.3600,  ...,  0.5234, -0.1217, -0.2547],\n",
      "         ...,\n",
      "         [-0.1533, -0.3859,  0.3131,  ...,  0.3902,  0.2990, -0.1372],\n",
      "         [-0.2431, -0.1536, -0.1589,  ...,  0.6102,  0.2741,  0.0602],\n",
      "         [-0.1739, -0.0086, -0.2400,  ...,  0.4898,  0.2122, -0.2178]],\n",
      "\n",
      "        [[ 0.3684, -0.0547,  0.0389,  ..., -0.8472, -0.3104, -0.1382],\n",
      "         [ 0.1822, -0.1655, -0.0465,  ..., -0.5673,  0.0046, -0.1259],\n",
      "         [-0.0591, -0.3250, -0.0161,  ..., -0.0696, -0.6026,  0.1846],\n",
      "         ...,\n",
      "         [ 0.0834, -0.4203,  0.1571,  ..., -0.1563, -0.1647,  0.2118],\n",
      "         [ 0.1321, -0.5505, -0.3530,  ..., -0.1210, -0.0744,  0.2097],\n",
      "         [ 0.0381, -0.1372, -0.4776,  ..., -0.4179, -0.2830,  0.3312]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0087, -0.3113,  0.0872,  ..., -0.8000, -0.5382,  0.3765],\n",
      "         [ 0.0945, -0.2479,  0.2057,  ..., -0.5100, -0.4469,  0.1124],\n",
      "         [ 0.0671, -0.3066,  0.2201,  ..., -0.5254, -0.4979,  0.2587],\n",
      "         ...,\n",
      "         [ 0.2132, -0.3817, -0.2152,  ..., -0.1076, -0.0626,  0.4483],\n",
      "         [-0.0023, -0.5482,  0.0604,  ..., -0.2225, -0.1457,  0.1306],\n",
      "         [-0.2202, -0.0448,  0.0901,  ..., -0.3054, -0.2925, -0.0126]],\n",
      "\n",
      "        [[ 0.0514, -0.0707, -0.1021,  ..., -0.1411, -0.3903, -0.0366],\n",
      "         [ 0.0419, -0.2330, -0.1513,  ..., -0.0269, -0.3903, -0.1050],\n",
      "         [-0.4922, -0.1271,  0.0778,  ...,  0.3568, -0.0636, -0.2279],\n",
      "         ...,\n",
      "         [ 0.0020, -0.4223, -0.4391,  ...,  0.2636, -0.1210,  0.0884],\n",
      "         [-0.1490, -0.2036, -0.4949,  ...,  0.3321, -0.3054,  0.1346],\n",
      "         [-0.0511,  0.2622, -0.7508,  ...,  0.0204, -0.2796, -0.0968]],\n",
      "\n",
      "        [[-0.3595, -0.4841, -0.0449,  ..., -0.3576,  0.0212, -0.1083],\n",
      "         [-0.2773, -0.3316,  0.1983,  ..., -0.2698,  0.0516, -0.0646],\n",
      "         [-0.4649, -0.2145,  0.1909,  ...,  0.0156,  0.1113,  0.0264],\n",
      "         ...,\n",
      "         [-0.3882, -0.3409, -0.0500,  ...,  0.2669,  0.2038,  0.1043],\n",
      "         [-0.1469, -0.1791, -0.0047,  ...,  0.2177,  0.0370,  0.1029],\n",
      "         [-0.5525,  0.0899, -0.2164,  ..., -0.1353,  0.0165,  0.2126]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.016822338104248\n",
      "Model outputs:  tensor([[[-6.8331e-02,  1.9449e-01, -6.0342e-01,  ..., -2.3023e-01,\n",
      "           5.1560e-02,  2.1500e-01],\n",
      "         [-1.5287e-02,  1.0804e-01, -5.5038e-01,  ..., -2.3367e-01,\n",
      "           1.0817e-01,  6.2216e-02],\n",
      "         [-6.4523e-02,  1.6110e-01, -4.3656e-01,  ..., -5.7212e-02,\n",
      "           9.8980e-02,  1.3892e-01],\n",
      "         ...,\n",
      "         [-4.1913e-01, -6.6062e-02, -1.6935e-01,  ..., -3.5752e-01,\n",
      "          -4.8642e-02,  2.0662e-02],\n",
      "         [ 2.9464e-02,  2.0795e-01, -3.1123e-01,  ..., -2.2716e-01,\n",
      "           6.6366e-02,  1.5905e-01],\n",
      "         [-1.6142e-01,  2.0987e-01, -5.9597e-01,  ..., -2.4136e-01,\n",
      "          -1.8572e-01,  2.0491e-01]],\n",
      "\n",
      "        [[-8.4421e-02,  2.9894e-01, -6.7214e-01,  ..., -2.0798e-01,\n",
      "           9.0482e-03,  4.6992e-01],\n",
      "         [-1.3187e-01,  1.2001e-01, -7.3229e-01,  ...,  6.9631e-02,\n",
      "           1.1546e-01,  1.5323e-01],\n",
      "         [-2.9484e-01,  3.0097e-01, -7.4234e-01,  ..., -3.0062e-01,\n",
      "          -7.6435e-02,  3.9986e-01],\n",
      "         ...,\n",
      "         [-3.1411e-01, -1.0744e-01, -1.8338e-01,  ..., -2.5171e-01,\n",
      "          -5.7508e-02, -3.7100e-02],\n",
      "         [-1.9845e-01,  1.4413e-01, -4.1259e-01,  ..., -5.5471e-02,\n",
      "           5.3378e-02, -8.2972e-02],\n",
      "         [-6.0708e-02, -8.3686e-02, -5.1707e-01,  ..., -3.4408e-02,\n",
      "          -1.4501e-02,  1.2985e-01]],\n",
      "\n",
      "        [[-2.7854e-02, -1.9158e-01, -6.6795e-01,  ...,  1.8109e-03,\n",
      "          -1.0940e-01,  1.2274e-01],\n",
      "         [ 2.0340e-01, -3.4345e-02, -6.7943e-01,  ..., -3.2893e-01,\n",
      "          -7.2424e-02,  4.1476e-01],\n",
      "         [-1.7251e-01, -8.7128e-03, -3.9211e-01,  ..., -3.1925e-01,\n",
      "          -1.4012e-01,  2.7524e-01],\n",
      "         ...,\n",
      "         [-6.8755e-02, -2.7134e-01, -2.3469e-01,  ..., -3.0290e-01,\n",
      "          -1.3406e-01,  4.0014e-01],\n",
      "         [-1.6376e-01,  2.0087e-01, -3.2813e-01,  ..., -2.0991e-01,\n",
      "           3.1979e-01,  2.2357e-01],\n",
      "         [-1.4760e-02,  2.6560e-01, -3.7651e-01,  ..., -2.5854e-01,\n",
      "          -1.7649e-01,  1.1176e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.4657e-01, -4.4256e-01, -6.5692e-02,  ...,  3.0274e-01,\n",
      "          -9.5613e-02, -5.1418e-01],\n",
      "         [ 3.9057e-01, -6.8426e-02, -1.6765e-01,  ...,  3.4457e-01,\n",
      "           1.6448e-02, -6.7546e-01],\n",
      "         [-1.5461e-01, -8.1771e-02, -2.0426e-01,  ...,  3.2403e-01,\n",
      "          -1.1230e-01, -6.0205e-01],\n",
      "         ...,\n",
      "         [ 1.9907e-02, -2.7440e-01, -1.5913e-01,  ..., -1.9216e-01,\n",
      "           1.5981e-01, -5.9594e-01],\n",
      "         [-6.1612e-02,  2.8636e-02, -4.0435e-01,  ..., -4.6880e-02,\n",
      "           2.0171e-01, -4.7297e-01],\n",
      "         [ 5.5306e-02, -1.2482e-01, -4.9652e-01,  ...,  2.8985e-01,\n",
      "           3.1713e-02, -8.4879e-01]],\n",
      "\n",
      "        [[-1.4846e-01, -1.7473e-01, -5.9545e-01,  ..., -2.9853e-01,\n",
      "          -3.8083e-02,  1.1494e-01],\n",
      "         [-4.1895e-02, -6.6601e-02, -5.7036e-01,  ..., -2.0275e-01,\n",
      "          -7.5509e-02,  1.2780e-01],\n",
      "         [-2.6804e-01, -4.2289e-03, -5.9529e-01,  ..., -1.4828e-01,\n",
      "          -1.4172e-01,  2.1748e-01],\n",
      "         ...,\n",
      "         [-2.2931e-01, -2.4392e-01, -1.4992e-01,  ..., -7.9143e-04,\n",
      "          -3.8260e-02,  3.8616e-01],\n",
      "         [-1.0007e-01,  2.2847e-02, -5.3685e-01,  ..., -2.1132e-01,\n",
      "          -7.9896e-03,  1.2488e-01],\n",
      "         [-1.8121e-01,  4.0148e-01, -6.0589e-01,  ..., -1.7780e-01,\n",
      "           1.2384e-02, -1.2287e-01]],\n",
      "\n",
      "        [[-2.9698e-01,  3.7773e-01, -9.4371e-01,  ...,  5.6395e-01,\n",
      "           3.0478e-01, -1.0171e-01],\n",
      "         [-5.2320e-01,  4.7854e-01, -8.2080e-01,  ...,  6.8703e-01,\n",
      "           3.1678e-01, -1.2867e-01],\n",
      "         [-4.3378e-01,  4.0600e-01, -8.3276e-01,  ...,  7.3042e-01,\n",
      "           3.5859e-01, -2.3025e-01],\n",
      "         ...,\n",
      "         [-7.8229e-02,  3.0028e-01, -5.7608e-01,  ...,  3.4234e-01,\n",
      "           2.8564e-01, -1.6031e-01],\n",
      "         [-3.2367e-01,  5.1548e-01, -7.1334e-01,  ...,  4.5303e-01,\n",
      "           5.3406e-01, -2.3554e-01],\n",
      "         [-4.4263e-01,  4.4951e-01, -9.3194e-01,  ...,  6.3528e-01,\n",
      "           3.6479e-01, -2.2971e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.5663957595825195\n",
      "Model outputs:  tensor([[[-0.1131,  0.0510, -0.9955,  ...,  0.9273,  0.1889,  0.0249],\n",
      "         [-0.4506,  0.0560, -0.7596,  ...,  0.6382,  0.1935, -0.2461],\n",
      "         [-0.3640,  0.1175, -0.7541,  ...,  0.5812,  0.3899,  0.0065],\n",
      "         ...,\n",
      "         [-0.1939, -0.2312, -0.5570,  ...,  0.7176,  0.1835,  0.0504],\n",
      "         [-0.3946, -0.0011, -0.5298,  ...,  0.7323,  0.0099,  0.0550],\n",
      "         [-0.2251, -0.0125, -0.8097,  ...,  0.7779,  0.0586,  0.0765]],\n",
      "\n",
      "        [[-0.3479,  0.0664, -0.9053,  ...,  0.1731,  0.0075,  0.1716],\n",
      "         [-0.0533, -0.4059, -0.7847,  ...,  0.1135,  0.1107, -0.1421],\n",
      "         [-0.4562, -0.2243, -0.7167,  ...,  0.5285,  0.0550,  0.1559],\n",
      "         ...,\n",
      "         [-0.1296, -0.1727, -0.6591,  ...,  0.4746,  0.0582,  0.0320],\n",
      "         [-0.5138, -0.0451, -0.7153,  ...,  0.3467,  0.2367, -0.1534],\n",
      "         [-0.2805, -0.3033, -0.6281,  ...,  0.3058, -0.0470, -0.0536]],\n",
      "\n",
      "        [[ 0.2584, -0.1832, -0.7446,  ...,  0.3206,  0.1336, -0.0678],\n",
      "         [ 0.0365, -0.2147, -0.5528,  ...,  0.3365, -0.1792,  0.0578],\n",
      "         [-0.1025, -0.0891, -0.6204,  ...,  0.1911,  0.1709,  0.0150],\n",
      "         ...,\n",
      "         [ 0.1897, -0.3885, -0.5479,  ...,  0.5196, -0.0979, -0.0760],\n",
      "         [ 0.0776, -0.2574, -0.7492,  ...,  0.5772,  0.0485, -0.1446],\n",
      "         [-0.1260, -0.2404, -0.8821,  ...,  0.2856,  0.0026,  0.2024]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6963, -0.0080, -0.5203,  ...,  0.2737,  0.2762,  0.3716],\n",
      "         [-0.6836,  0.2160, -0.2878,  ...,  0.1589,  0.2204, -0.0217],\n",
      "         [-0.6134, -0.1019, -0.4149,  ...,  0.1155, -0.0098,  0.2899],\n",
      "         ...,\n",
      "         [-0.4519, -0.2044, -0.2006,  ...,  0.4127,  0.4577,  0.0999],\n",
      "         [-0.3918,  0.0557, -0.4179,  ...,  0.3435,  0.1064,  0.0990],\n",
      "         [-0.4758, -0.2431, -0.1559,  ...,  0.1998,  0.2512,  0.2260]],\n",
      "\n",
      "        [[ 0.1159, -0.1798, -0.7023,  ...,  0.0480, -0.0522,  0.3687],\n",
      "         [ 0.0655, -0.3057, -0.5306,  ..., -0.0927,  0.1479,  0.5210],\n",
      "         [ 0.1795, -0.2505, -0.6432,  ...,  0.1143, -0.0584,  0.1697],\n",
      "         ...,\n",
      "         [-0.0322, -0.1377, -0.4577,  ...,  0.1256, -0.0037,  0.0294],\n",
      "         [-0.0603,  0.0275, -0.5801,  ...,  0.1829,  0.1109,  0.3574],\n",
      "         [ 0.1367,  0.0171, -0.6349,  ...,  0.2413,  0.2087,  0.3582]],\n",
      "\n",
      "        [[ 0.2538, -0.4134, -0.3174,  ..., -0.7310,  0.2124,  0.2534],\n",
      "         [ 0.0785, -0.2976, -0.5153,  ..., -0.6483,  0.0300,  0.4288],\n",
      "         [ 0.2046, -0.3145, -0.3086,  ..., -0.5333,  0.3885,  0.1845],\n",
      "         ...,\n",
      "         [-0.0638, -0.2479, -0.5568,  ...,  0.0811, -0.1565,  0.2417],\n",
      "         [-0.1953, -0.2148, -0.6381,  ..., -0.2384, -0.1272,  0.4138],\n",
      "         [ 0.0028, -0.3125, -0.5808,  ..., -0.0491, -0.2562,  0.4331]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9870648384094238\n",
      "Model outputs:  tensor([[[-3.4858e-01,  4.3862e-01, -1.5819e-01,  ...,  3.9241e-01,\n",
      "           3.0369e-01, -4.5787e-02],\n",
      "         [-4.1383e-01,  4.3804e-01, -2.3132e-01,  ...,  2.7389e-01,\n",
      "           2.4156e-01, -1.2307e-01],\n",
      "         [-2.6234e-01,  1.0419e-01, -2.9642e-01,  ...,  4.7715e-01,\n",
      "           2.7814e-01,  7.2158e-03],\n",
      "         ...,\n",
      "         [-3.7245e-01,  1.6720e-01,  2.7826e-02,  ...,  7.0538e-01,\n",
      "           2.9923e-01, -3.3733e-01],\n",
      "         [-5.0859e-01,  1.1798e-01,  4.6640e-03,  ...,  8.6850e-01,\n",
      "           2.8154e-01, -3.3270e-01],\n",
      "         [-8.3396e-01,  7.3047e-01,  4.8214e-02,  ...,  5.6469e-01,\n",
      "           7.5875e-02, -3.1578e-01]],\n",
      "\n",
      "        [[-4.8086e-01,  5.5919e-01, -5.7136e-01,  ...,  4.1398e-01,\n",
      "           2.7378e-01, -2.2911e-01],\n",
      "         [-2.4428e-01,  2.2989e-01, -4.1558e-01,  ...,  3.0420e-01,\n",
      "           2.6510e-02, -1.4996e-01],\n",
      "         [-1.9127e-01,  3.2777e-01, -4.5120e-01,  ...,  7.3816e-01,\n",
      "           2.8716e-01, -7.8649e-02],\n",
      "         ...,\n",
      "         [-3.3788e-01,  2.5636e-01, -5.7162e-01,  ...,  6.0864e-01,\n",
      "           1.1704e-01, -4.7785e-02],\n",
      "         [-3.0585e-01,  9.0171e-02, -5.0713e-01,  ...,  7.9914e-01,\n",
      "           2.8727e-01, -2.8303e-01],\n",
      "         [-4.9982e-01,  3.0347e-01, -3.7879e-01,  ...,  5.0301e-01,\n",
      "          -4.2873e-02, -5.5180e-03]],\n",
      "\n",
      "        [[-1.5909e-01, -2.6075e-01, -5.0581e-02,  ..., -3.2044e-01,\n",
      "          -5.0970e-01,  1.1259e-01],\n",
      "         [ 4.2839e-02, -1.8511e-01, -1.8305e-01,  ..., -4.8253e-01,\n",
      "          -2.8939e-01,  1.8025e-01],\n",
      "         [-4.1005e-02, -1.9364e-02, -2.3969e-01,  ..., -3.3804e-01,\n",
      "          -2.1199e-01,  3.1951e-01],\n",
      "         ...,\n",
      "         [-4.4840e-02, -1.4738e-01, -4.7618e-01,  ..., -4.9717e-02,\n",
      "          -3.7867e-01,  2.4795e-01],\n",
      "         [-4.8860e-03,  3.0550e-03, -3.9679e-01,  ..., -2.7728e-01,\n",
      "           2.9236e-03,  8.1579e-02],\n",
      "         [-3.6243e-01,  6.3624e-02, -1.5460e-01,  ..., -3.5669e-01,\n",
      "          -1.3907e-01,  3.0122e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.3116e-01,  2.7530e-01, -2.4888e-01,  ...,  2.6205e-01,\n",
      "          -2.3309e-02,  2.5928e-03],\n",
      "         [-1.2948e-01,  3.2321e-01,  1.9376e-02,  ...,  1.7214e-01,\n",
      "           8.1795e-04, -1.0214e-01],\n",
      "         [-3.5223e-01,  1.8641e-01, -1.0617e-01,  ...,  4.6038e-01,\n",
      "           1.1110e-01, -5.7804e-02],\n",
      "         ...,\n",
      "         [-3.9700e-01,  3.7512e-01,  2.8541e-02,  ...,  7.2584e-01,\n",
      "           3.3806e-01,  3.3065e-02],\n",
      "         [-5.9045e-01,  2.6528e-01,  1.2813e-02,  ...,  6.2638e-01,\n",
      "           1.6497e-01, -1.5699e-01],\n",
      "         [-4.7878e-01,  4.5298e-01, -9.5543e-02,  ...,  4.1244e-01,\n",
      "           7.3678e-02, -1.7823e-01]],\n",
      "\n",
      "        [[-5.7601e-01,  3.9554e-01, -5.0510e-01,  ..., -1.2416e-01,\n",
      "           1.8778e-01, -1.6252e-01],\n",
      "         [-3.5936e-01,  1.1304e-01, -5.0372e-01,  ..., -2.3221e-01,\n",
      "          -2.9134e-01,  3.5443e-02],\n",
      "         [-3.6432e-01, -4.1720e-03, -2.6105e-01,  ...,  3.7848e-02,\n",
      "          -2.3323e-01,  1.7804e-01],\n",
      "         ...,\n",
      "         [-6.3874e-01,  1.0250e-01, -5.7993e-01,  ...,  2.8911e-01,\n",
      "          -3.6121e-02,  2.1314e-01],\n",
      "         [-5.0802e-01,  5.5869e-02, -3.8391e-01,  ..., -8.2946e-02,\n",
      "          -5.5162e-02, -2.1648e-01],\n",
      "         [-6.0472e-01,  1.8403e-01, -6.6605e-01,  ..., -7.7983e-02,\n",
      "          -6.1178e-02, -8.0298e-02]],\n",
      "\n",
      "        [[-3.3111e-01,  3.0568e-01, -5.1474e-01,  ...,  4.0898e-02,\n",
      "           2.8445e-01, -1.2580e-01],\n",
      "         [-1.7654e-01,  2.2624e-01, -4.6436e-01,  ...,  4.6661e-01,\n",
      "           1.1510e-02, -4.0325e-01],\n",
      "         [-3.5438e-01,  2.2252e-01, -4.1913e-01,  ...,  4.2740e-01,\n",
      "           2.8060e-01, -2.2875e-01],\n",
      "         ...,\n",
      "         [-3.3663e-01, -8.4866e-02, -4.5774e-01,  ...,  5.9520e-01,\n",
      "          -5.2417e-02, -1.2430e-01],\n",
      "         [-5.1877e-01,  1.2271e-01, -4.1621e-01,  ...,  5.2240e-01,\n",
      "           1.7208e-01, -1.0401e-01],\n",
      "         [-5.2633e-01,  2.3480e-01, -2.1781e-01,  ...,  4.0988e-01,\n",
      "          -2.6960e-02, -1.2801e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8280311822891235\n",
      "Model outputs:  tensor([[[-0.2658, -0.0831, -0.6277,  ..., -0.2860, -0.0705,  0.2480],\n",
      "         [-0.1572, -0.2573, -0.2749,  ..., -0.0218, -0.0709,  0.4510],\n",
      "         [ 0.1084, -0.1171, -0.6309,  ..., -0.3099, -0.1480,  0.3285],\n",
      "         ...,\n",
      "         [ 0.1009,  0.0707, -0.2519,  ..., -0.3207, -0.2235,  0.0034],\n",
      "         [-0.0026, -0.0739, -0.2436,  ..., -0.4637, -0.0082, -0.1031],\n",
      "         [-0.1363, -0.0840, -0.3922,  ..., -0.3357, -0.1353,  0.0376]],\n",
      "\n",
      "        [[-0.1812,  0.1264, -0.8112,  ...,  0.1521, -0.3718, -0.1313],\n",
      "         [-0.1105,  0.0077, -0.7439,  ...,  0.5674,  0.1688,  0.2419],\n",
      "         [-0.1292,  0.1375, -0.7554,  ...,  0.3731,  0.2044, -0.2723],\n",
      "         ...,\n",
      "         [-0.1498,  0.2063, -0.8351,  ...,  0.2398,  0.1173, -0.2755],\n",
      "         [ 0.0061,  0.2583, -0.5960,  ..., -0.0618,  0.1037, -0.0995],\n",
      "         [-0.1871,  0.0272, -0.6160,  ...,  0.1199,  0.1791, -0.1555]],\n",
      "\n",
      "        [[-0.4540,  0.3883, -1.0725,  ...,  0.5438,  0.1446, -0.1865],\n",
      "         [-0.2389,  0.1525, -0.8981,  ...,  0.7032,  0.1267,  0.0577],\n",
      "         [-0.2832,  0.2048, -0.8297,  ...,  0.5149,  0.2878,  0.0173],\n",
      "         ...,\n",
      "         [-0.3529,  0.1965, -0.6099,  ...,  0.4327,  0.3854, -0.4328],\n",
      "         [-0.1911,  0.5122, -0.7310,  ...,  0.4382,  0.1362, -0.1230],\n",
      "         [-0.3457,  0.4162, -0.5413,  ...,  0.1728,  0.0847, -0.2315]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0336, -0.1279, -0.4758,  ..., -0.3289, -0.2471,  0.2278],\n",
      "         [ 0.1651, -0.2475, -0.3484,  ...,  0.0222, -0.1431,  0.4136],\n",
      "         [-0.0572, -0.0374, -0.3778,  ..., -0.0692,  0.0139,  0.4364],\n",
      "         ...,\n",
      "         [ 0.2403, -0.1224, -0.4232,  ..., -0.4583,  0.0718,  0.1732],\n",
      "         [-0.0489, -0.1878, -0.0815,  ..., -0.4825, -0.1318, -0.2095],\n",
      "         [-0.0020, -0.0195, -0.3173,  ..., -0.2233,  0.0040, -0.0660]],\n",
      "\n",
      "        [[-0.4572,  0.1680, -0.4456,  ...,  0.0167,  0.3992,  0.0861],\n",
      "         [-0.7386, -0.2487, -0.3446,  ...,  0.2724,  0.2966,  0.2349],\n",
      "         [-0.5580,  0.2150, -0.3636,  ...,  0.1347,  0.4285,  0.1611],\n",
      "         ...,\n",
      "         [-0.6872,  0.0490, -0.2413,  ..., -0.0723,  0.3340, -0.2349],\n",
      "         [-0.2432,  0.3964, -0.4038,  ..., -0.2015,  0.3747, -0.0254],\n",
      "         [-0.1913,  0.1969, -0.4008,  ...,  0.1243,  0.3274,  0.2017]],\n",
      "\n",
      "        [[ 0.1025, -0.1619, -0.2513,  ..., -0.1468, -0.0744,  0.1952],\n",
      "         [-0.0943, -0.1801, -0.2166,  ..., -0.1756, -0.1772,  0.3514],\n",
      "         [-0.0028,  0.0095, -0.3833,  ..., -0.1447, -0.0228,  0.4484],\n",
      "         ...,\n",
      "         [ 0.0314,  0.0502,  0.0451,  ..., -0.4540, -0.1147, -0.0652],\n",
      "         [-0.0583, -0.0429, -0.2389,  ..., -0.4933,  0.1360,  0.2911],\n",
      "         [ 0.1341, -0.0955, -0.0169,  ..., -0.6676,  0.0642,  0.1634]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2062084674835205\n",
      "Model outputs:  tensor([[[ 0.1327, -0.2101, -0.4810,  ..., -0.0755, -0.1792,  0.3610],\n",
      "         [-0.2795, -0.4094, -0.3155,  ..., -0.0020, -0.3398,  0.2555],\n",
      "         [ 0.0346, -0.3884, -0.4470,  ..., -0.0273, -0.3153,  0.4289],\n",
      "         ...,\n",
      "         [ 0.0813, -0.4661, -0.2781,  ...,  0.0209, -0.3352,  0.2475],\n",
      "         [-0.1779,  0.0496, -0.5405,  ..., -0.1861, -0.4300,  0.1326],\n",
      "         [-0.0915, -0.5521, -0.4155,  ..., -0.1296, -0.4518,  0.3940]],\n",
      "\n",
      "        [[-0.3123,  0.0411, -0.7122,  ...,  0.8730,  0.0670, -0.0149],\n",
      "         [-0.4179, -0.1626, -0.6337,  ...,  0.7551, -0.1328, -0.0763],\n",
      "         [ 0.0103, -0.2332, -0.6135,  ...,  0.9026,  0.0651, -0.0431],\n",
      "         ...,\n",
      "         [ 0.1057, -0.2169, -0.6011,  ...,  0.5809,  0.0213, -0.0035],\n",
      "         [-0.0870,  0.0850, -0.7964,  ...,  0.8581,  0.0542, -0.2360],\n",
      "         [-0.2746, -0.5411, -0.5167,  ...,  0.7143,  0.0090, -0.0304]],\n",
      "\n",
      "        [[-0.4283, -0.1938, -0.2656,  ...,  0.6810,  0.2918, -0.1071],\n",
      "         [-0.4675,  0.0087, -0.2885,  ...,  0.7731,  0.3627, -0.2138],\n",
      "         [-0.2512, -0.3645, -0.2252,  ...,  0.6686,  0.0247,  0.0166],\n",
      "         ...,\n",
      "         [-0.2057, -0.2185, -0.2245,  ...,  0.7843,  0.1001, -0.1292],\n",
      "         [-0.3293,  0.0254, -0.1977,  ...,  0.8121,  0.3798, -0.0228],\n",
      "         [-0.4545, -0.4668, -0.3290,  ...,  0.9082,  0.1721,  0.0436]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3792,  0.1727, -0.5352,  ...,  0.6663,  0.1371,  0.4010],\n",
      "         [-0.6273, -0.0701, -0.3357,  ...,  0.3717,  0.3135, -0.1760],\n",
      "         [-0.2185, -0.0301, -0.2664,  ...,  0.4115,  0.0878,  0.0656],\n",
      "         ...,\n",
      "         [-0.3192, -0.2154, -0.5503,  ...,  0.4042,  0.1091,  0.1629],\n",
      "         [-0.6900,  0.3787, -0.1232,  ...,  0.5377,  0.3531,  0.1675],\n",
      "         [-0.5752, -0.3578, -0.1644,  ...,  0.3469,  0.0261,  0.1498]],\n",
      "\n",
      "        [[-0.0754,  0.2052, -0.4356,  ...,  0.7243,  0.0801,  0.2474],\n",
      "         [-0.4923,  0.0862,  0.0289,  ...,  0.8267,  0.4099, -0.0864],\n",
      "         [ 0.0329, -0.3121, -0.1534,  ...,  0.8809,  0.2998,  0.0832],\n",
      "         ...,\n",
      "         [-0.1183, -0.1629, -0.1367,  ...,  0.8896,  0.2288, -0.1214],\n",
      "         [-0.2040,  0.2126, -0.0443,  ...,  0.7928,  0.4591,  0.0177],\n",
      "         [-0.3567, -0.2976,  0.0401,  ...,  0.3497,  0.0716,  0.0816]],\n",
      "\n",
      "        [[-0.5399,  0.0311, -0.5092,  ...,  0.2964,  0.0785,  0.1803],\n",
      "         [-0.4977,  0.1186, -0.4970,  ...,  0.1775,  0.2163,  0.2701],\n",
      "         [-0.3676, -0.3104, -0.2974,  ...,  0.4763,  0.2896,  0.1182],\n",
      "         ...,\n",
      "         [-0.2543, -0.3461, -0.6651,  ...,  0.1945,  0.0815,  0.0857],\n",
      "         [-0.6335,  0.0385, -0.3932,  ...,  0.5959,  0.3849,  0.2279],\n",
      "         [-0.6522, -0.6468, -0.3249,  ...,  0.2351, -0.0982,  0.3980]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8155056834220886\n",
      "Model outputs:  tensor([[[-0.5255,  0.0559, -0.2640,  ...,  0.0840,  0.4262,  0.0962],\n",
      "         [-0.6398,  0.1401, -0.4962,  ..., -0.0147,  0.2092,  0.2365],\n",
      "         [-0.4913,  0.0396, -0.2949,  ...,  0.3166,  0.3346,  0.1409],\n",
      "         ...,\n",
      "         [-0.7131,  0.5074, -0.1616,  ...,  0.3170,  0.5334,  0.2499],\n",
      "         [-0.6560, -0.3059, -0.0839,  ...,  0.0452,  0.1234,  0.0576],\n",
      "         [-0.6832,  0.2087, -0.3725,  ...,  0.0629,  0.1529,  0.0721]],\n",
      "\n",
      "        [[-0.2690,  0.2366, -0.4816,  ...,  0.0430,  0.1323, -0.0294],\n",
      "         [-0.1907,  0.0263, -0.2350,  ..., -0.3713, -0.0176, -0.3021],\n",
      "         [-0.3542, -0.2457, -0.3857,  ...,  0.0031, -0.1344,  0.1004],\n",
      "         ...,\n",
      "         [-0.1106,  0.0304, -0.1949,  ..., -0.3846, -0.2468,  0.1680],\n",
      "         [-0.2647, -0.3822, -0.1367,  ...,  0.1844,  0.1470,  0.2821],\n",
      "         [-0.0098, -0.2304, -0.2712,  ..., -0.1652, -0.0746,  0.2453]],\n",
      "\n",
      "        [[-0.2535, -0.1111, -0.3213,  ...,  0.0139, -0.0395, -0.1053],\n",
      "         [-0.1717, -0.0522, -0.1835,  ..., -0.0444,  0.0994,  0.2482],\n",
      "         [-0.2541,  0.0706, -0.2430,  ..., -0.2114, -0.3332,  0.1646],\n",
      "         ...,\n",
      "         [-0.2710, -0.0925, -0.4361,  ...,  0.0867, -0.2382,  0.0607],\n",
      "         [-0.4073, -0.7419, -0.1357,  ..., -0.0980, -0.4322,  0.2217],\n",
      "         [-0.1444, -0.1116, -0.1317,  ..., -0.0125, -0.1129,  0.1110]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1635,  0.1862, -0.1218,  ..., -0.3531, -0.0633,  0.0514],\n",
      "         [-0.1629, -0.2494, -0.1004,  ..., -0.1992, -0.2974,  0.1799],\n",
      "         [-0.0372,  0.1497, -0.2932,  ...,  0.0299, -0.0606,  0.2637],\n",
      "         ...,\n",
      "         [ 0.1771, -0.1486, -0.0926,  ..., -0.1081, -0.2104,  0.0783],\n",
      "         [-0.2301, -0.3756, -0.3314,  ...,  0.0385, -0.3002,  0.6364],\n",
      "         [-0.0245, -0.3577, -0.0627,  ..., -0.4608, -0.2295,  0.3468]],\n",
      "\n",
      "        [[ 0.0793, -0.2567, -0.1921,  ..., -0.3958,  0.0047,  0.0961],\n",
      "         [ 0.0177, -0.3444, -0.3019,  ..., -0.1964, -0.2933,  0.0254],\n",
      "         [-0.2265, -0.0478, -0.5689,  ..., -0.2103, -0.2308,  0.3043],\n",
      "         ...,\n",
      "         [-0.1202, -0.2041, -0.1530,  ..., -0.3003, -0.2583,  0.2411],\n",
      "         [-0.1239, -0.3298, -0.0093,  ..., -0.0130,  0.0586,  0.6619],\n",
      "         [-0.0425, -0.3662,  0.0342,  ..., -0.2623, -0.3369,  0.1766]],\n",
      "\n",
      "        [[-0.5617,  0.0781, -0.1662,  ...,  0.5465,  0.0079, -0.2984],\n",
      "         [-0.2107,  0.0404,  0.0431,  ...,  0.2553,  0.0051, -0.3073],\n",
      "         [-0.3056,  0.2039, -0.0575,  ...,  0.4235,  0.0978, -0.3900],\n",
      "         ...,\n",
      "         [-0.4437,  0.2864, -0.0297,  ...,  0.4796,  0.2277, -0.3595],\n",
      "         [-0.7204, -0.2055,  0.2176,  ...,  0.5639,  0.3238, -0.3213],\n",
      "         [-0.6543,  0.2669,  0.0313,  ...,  0.5670,  0.4071, -0.3452]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8997766375541687\n",
      "Model outputs:  tensor([[[-0.3311,  0.0565, -0.0771,  ...,  0.6736,  0.2531,  0.0156],\n",
      "         [-0.3114, -0.2967, -0.1471,  ...,  0.5263,  0.3031,  0.1976],\n",
      "         [-0.3723,  0.0858, -0.2110,  ...,  0.7473,  0.2960, -0.0659],\n",
      "         ...,\n",
      "         [-0.4127, -0.2598, -0.1093,  ...,  0.8352, -0.1960,  0.3753],\n",
      "         [-0.7344, -0.0859, -0.1590,  ...,  0.4416,  0.1289,  0.1161],\n",
      "         [-0.8498,  0.1271, -0.1600,  ...,  0.6496,  0.1323, -0.2225]],\n",
      "\n",
      "        [[-0.2435,  0.3392, -0.6240,  ...,  0.5837,  0.1857, -0.1193],\n",
      "         [-0.1504, -0.0432, -0.5449,  ...,  0.7055,  0.2304,  0.0146],\n",
      "         [-0.3059, -0.1326, -0.5910,  ...,  0.7656, -0.1455, -0.3415],\n",
      "         ...,\n",
      "         [-0.2784,  0.0407, -0.7945,  ...,  0.8315, -0.6936,  0.1269],\n",
      "         [-0.5206,  0.3312, -0.9235,  ...,  0.8465,  0.1159, -0.0135],\n",
      "         [-0.7093, -0.0310, -0.8408,  ...,  0.8009,  0.1165, -0.0671]],\n",
      "\n",
      "        [[-0.0224, -0.0023, -0.2960,  ..., -0.2217, -0.2557,  0.1168],\n",
      "         [-0.1558, -0.1892, -0.3380,  ...,  0.1109, -0.0254,  0.3400],\n",
      "         [-0.0039, -0.1524, -0.3023,  ...,  0.0325, -0.1261,  0.2223],\n",
      "         ...,\n",
      "         [-0.2631, -0.0254, -0.3648,  ..., -0.2995, -0.8650,  0.2571],\n",
      "         [-0.0238, -0.0534, -0.3445,  ..., -0.2313, -0.3729,  0.1636],\n",
      "         [-0.4927, -0.3158, -0.3898,  ...,  0.1387, -0.4009,  0.4032]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5530,  0.1765, -0.8366,  ...,  0.1286, -0.3096,  0.0428],\n",
      "         [-0.1556, -0.0709, -0.6369,  ...,  0.1783,  0.1885,  0.1591],\n",
      "         [-0.2703,  0.1217, -0.9647,  ...,  0.0656, -0.0020, -0.1133],\n",
      "         ...,\n",
      "         [-0.5050, -0.1391, -0.9029,  ...,  0.1627, -0.7534,  0.2462],\n",
      "         [-0.4382,  0.0989, -0.5797,  ...,  0.0369, -0.0106, -0.0344],\n",
      "         [-0.5163,  0.0504, -0.8108,  ...,  0.2911, -0.3212,  0.1454]],\n",
      "\n",
      "        [[-0.5448,  0.0095, -0.3536,  ...,  0.5200,  0.4923,  0.0482],\n",
      "         [-0.3035, -0.0156, -0.4551,  ...,  0.6719,  0.1530,  0.2521],\n",
      "         [-0.4326,  0.1194, -0.6477,  ...,  0.4273,  0.1118,  0.0777],\n",
      "         ...,\n",
      "         [-0.4942, -0.1965, -0.3141,  ...,  0.3464, -0.5398,  0.4507],\n",
      "         [-0.5302,  0.0951, -0.5589,  ...,  0.3554,  0.1342,  0.2190],\n",
      "         [-0.7431,  0.2179, -0.8226,  ...,  0.4945,  0.3971,  0.1221]],\n",
      "\n",
      "        [[-0.3339,  0.2367,  0.1175,  ...,  0.4699,  0.2162,  0.0062],\n",
      "         [-0.5292,  0.0662,  0.0095,  ...,  0.8723,  0.2552,  0.0329],\n",
      "         [-0.3725,  0.2196, -0.2546,  ...,  0.5783,  0.2242, -0.3687],\n",
      "         ...,\n",
      "         [-0.4740, -0.1321, -0.1113,  ...,  0.6621, -0.3863,  0.3443],\n",
      "         [-0.3718,  0.4132, -0.1375,  ...,  0.7785,  0.4385, -0.1901],\n",
      "         [-0.5327,  0.0752, -0.2882,  ...,  0.5009,  0.4210, -0.1771]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0962891578674316\n",
      "Model outputs:  tensor([[[-0.3063, -0.1272, -0.3480,  ...,  0.6957,  0.3042,  0.1286],\n",
      "         [-0.3685, -0.0425, -0.4331,  ...,  0.4268, -0.1899, -0.3336],\n",
      "         [-0.5587,  0.0181, -0.6815,  ...,  0.5376,  0.2376, -0.0119],\n",
      "         ...,\n",
      "         [-0.3404,  0.1365, -0.1443,  ...,  0.2392,  0.3142, -0.2083],\n",
      "         [-0.3617, -0.3606, -0.2132,  ...,  0.5280,  0.2238, -0.1226],\n",
      "         [-0.4800,  0.0787, -0.4624,  ...,  0.7412,  0.2776,  0.0380]],\n",
      "\n",
      "        [[ 0.0569, -0.3564, -0.3475,  ..., -0.3889,  0.4272, -0.0030],\n",
      "         [ 0.4758, -0.5670, -0.2446,  ..., -0.5984, -0.1859, -0.1510],\n",
      "         [ 0.1754, -0.4072, -0.4352,  ..., -0.5399,  0.1229,  0.1045],\n",
      "         ...,\n",
      "         [ 0.0256, -0.0967, -0.2654,  ..., -0.6170,  0.1545, -0.2981],\n",
      "         [ 0.1694, -0.3693, -0.0437,  ..., -0.4868,  0.3466, -0.0765],\n",
      "         [ 0.0879, -0.3469, -0.4649,  ..., -0.5471,  0.0329,  0.1954]],\n",
      "\n",
      "        [[-0.3737,  0.3269, -0.5126,  ...,  0.4458,  0.2455,  0.0949],\n",
      "         [-0.4353, -0.2227, -0.5232,  ...,  0.3882,  0.1926, -0.2454],\n",
      "         [-0.6555,  0.2590, -0.3076,  ...,  0.6951,  0.3612,  0.2502],\n",
      "         ...,\n",
      "         [-0.6609,  0.5068, -0.4831,  ...,  0.1437,  0.3264, -0.2324],\n",
      "         [-0.3808,  0.1020, -0.6405,  ...,  0.3975,  0.2000,  0.0937],\n",
      "         [-0.5702,  0.0554, -0.5438,  ...,  0.6879,  0.4456,  0.1537]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3498,  0.1379, -0.8446,  ...,  0.8062,  0.5081,  0.2398],\n",
      "         [-0.0541,  0.0029, -0.6353,  ...,  0.6539,  0.0330, -0.3713],\n",
      "         [-0.4861,  0.1440, -0.8351,  ...,  0.8747,  0.3408,  0.2222],\n",
      "         ...,\n",
      "         [-0.4577,  0.3493, -0.3759,  ...,  0.3179,  0.2707, -0.2790],\n",
      "         [-0.1190, -0.0690, -0.7837,  ...,  0.8992,  0.3512, -0.2381],\n",
      "         [-0.3674,  0.3867, -0.7116,  ...,  0.6568,  0.3243,  0.0898]],\n",
      "\n",
      "        [[-0.0399,  0.0285, -0.9122,  ...,  0.4439, -0.0083,  0.1283],\n",
      "         [ 0.0376, -0.3170, -0.5783,  ...,  0.2648, -0.2814, -0.1748],\n",
      "         [-0.0721, -0.1763, -0.7136,  ...,  0.2515, -0.1208,  0.2265],\n",
      "         ...,\n",
      "         [-0.1783,  0.2207, -0.5129,  ..., -0.1511,  0.3137,  0.0434],\n",
      "         [-0.1713,  0.1489, -0.5016,  ...,  0.1496, -0.0507,  0.0209],\n",
      "         [ 0.0260, -0.0163, -0.9241,  ...,  0.4365,  0.1288,  0.1385]],\n",
      "\n",
      "        [[-0.4881,  0.2100, -0.5515,  ...,  0.5673,  0.3823,  0.2656],\n",
      "         [-0.1833, -0.0248, -0.2904,  ...,  0.3300,  0.0891, -0.0738],\n",
      "         [-0.6436, -0.0868, -0.6583,  ...,  0.2399,  0.2482,  0.1932],\n",
      "         ...,\n",
      "         [-0.4521,  0.3341, -0.1673,  ...,  0.2384,  0.4599,  0.0456],\n",
      "         [-0.2700, -0.0101, -0.5737,  ...,  0.6091,  0.1865,  0.2653],\n",
      "         [-0.7984,  0.3139, -0.3899,  ...,  0.2275,  0.0741,  0.3818]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0827432870864868\n",
      "Model outputs:  tensor([[[-0.1876, -0.3013, -0.2334,  ..., -0.1175, -0.1626,  0.1964],\n",
      "         [-0.0202, -0.3189, -0.5471,  ..., -0.1283, -0.3784,  0.1818],\n",
      "         [ 0.0760, -0.3101, -0.5278,  ..., -0.1884, -0.1642,  0.2873],\n",
      "         ...,\n",
      "         [-0.1430, -0.4305, -0.4709,  ..., -0.3689, -0.3754,  0.4925],\n",
      "         [-0.0475, -0.1639, -0.4324,  ..., -0.4015, -0.2871,  0.3431],\n",
      "         [-0.4030, -0.2566,  0.0268,  ...,  0.1141, -0.2286,  0.2138]],\n",
      "\n",
      "        [[ 0.1309, -0.2916, -0.2817,  ..., -0.2842, -0.2649,  0.2845],\n",
      "         [ 0.0176, -0.0467, -0.5972,  ..., -0.0880,  0.0017,  0.3402],\n",
      "         [ 0.0047, -0.1218, -0.2537,  ...,  0.1662, -0.0389,  0.5037],\n",
      "         ...,\n",
      "         [-0.2910, -0.3738, -0.5260,  ...,  0.0743, -0.3678,  0.6474],\n",
      "         [-0.1857, -0.3189, -0.4390,  ..., -0.3153, -0.2693,  0.2057],\n",
      "         [-0.2190, -0.7330,  0.0834,  ..., -0.0655, -0.4592,  0.4229]],\n",
      "\n",
      "        [[-0.6052, -0.0971, -0.3237,  ...,  0.0791,  0.1695,  0.2050],\n",
      "         [-0.4947,  0.0588, -0.3782,  ...,  0.2844,  0.3546, -0.0236],\n",
      "         [-0.7960,  0.0437, -0.1038,  ...,  0.2714,  0.2186, -0.2246],\n",
      "         ...,\n",
      "         [-0.6398,  0.1884, -0.5093,  ...,  0.3846, -0.2605,  0.1520],\n",
      "         [-0.7670,  0.0826, -0.2248,  ...,  0.2943,  0.2671,  0.0611],\n",
      "         [-0.6430, -0.2006, -0.1127,  ...,  0.2979,  0.0866, -0.0175]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5141, -0.0523, -0.4060,  ...,  0.4240, -0.0992, -0.2469],\n",
      "         [-0.4924,  0.1124, -0.7002,  ...,  0.4534, -0.0335,  0.1579],\n",
      "         [-0.6674,  0.1777, -0.8140,  ...,  0.3416, -0.0940, -0.1115],\n",
      "         ...,\n",
      "         [-0.5383,  0.1191, -0.8443,  ...,  0.4271, -0.0644,  0.0985],\n",
      "         [-0.4893,  0.1090, -0.4459,  ...,  0.2115, -0.3111, -0.1998],\n",
      "         [-0.3846, -0.4443, -0.3752,  ...,  0.3452, -0.1198,  0.0551]],\n",
      "\n",
      "        [[-0.5637, -0.1097, -0.1973,  ...,  0.5962,  0.1427, -0.0365],\n",
      "         [-0.3005,  0.3539, -0.1249,  ...,  0.6685,  0.3210, -0.1971],\n",
      "         [-0.3861,  0.1938, -0.1737,  ...,  0.7632,  0.3779, -0.0105],\n",
      "         ...,\n",
      "         [-0.4713,  0.1661, -0.1707,  ...,  0.8203,  0.0052, -0.0741],\n",
      "         [-0.2883,  0.0670,  0.1953,  ...,  0.3494,  0.1305, -0.1075],\n",
      "         [-0.4045,  0.0210,  0.2833,  ...,  0.7418,  0.0463, -0.0869]],\n",
      "\n",
      "        [[-0.0055, -0.2659, -0.1586,  ..., -0.2458, -0.1220,  0.3538],\n",
      "         [ 0.1532,  0.0901, -0.3109,  ..., -0.1474, -0.0934,  0.3269],\n",
      "         [ 0.0279, -0.0486, -0.5658,  ...,  0.0281, -0.3938,  0.5627],\n",
      "         ...,\n",
      "         [ 0.0073, -0.3919, -0.4562,  ..., -0.0183, -0.2334,  0.5293],\n",
      "         [ 0.0261, -0.1900, -0.0888,  ..., -0.1995, -0.2028,  0.0575],\n",
      "         [-0.1427, -0.1506, -0.0779,  ..., -0.0062, -0.3742,  0.7591]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9280582666397095\n",
      "Model outputs:  tensor([[[-0.1289, -0.3868, -0.2512,  ...,  0.6632,  0.5019, -0.0136],\n",
      "         [-0.5561, -0.2622, -0.0639,  ...,  0.5780,  0.0429, -0.1545],\n",
      "         [-0.4181, -0.0214, -0.0261,  ...,  0.5597,  0.1636, -0.3627],\n",
      "         ...,\n",
      "         [-0.1176,  0.2621, -0.2067,  ...,  0.5040,  0.3479, -0.2047],\n",
      "         [-0.3691, -0.0931, -0.1121,  ...,  0.4755, -0.0830, -0.0681],\n",
      "         [-0.3368,  0.0757,  0.0183,  ...,  0.6355,  0.1934, -0.2461]],\n",
      "\n",
      "        [[-0.2024, -0.4920, -0.5162,  ...,  0.2566, -0.2007,  0.2641],\n",
      "         [-0.3622, -0.1706, -0.7088,  ...,  0.1512, -0.3415,  0.0811],\n",
      "         [-0.0718, -0.1752, -0.6878,  ...,  0.1101, -0.1451,  0.1341],\n",
      "         ...,\n",
      "         [-0.0844, -0.0963, -0.1694,  ...,  0.3072,  0.0976, -0.0574],\n",
      "         [-0.2260,  0.1032, -0.6612,  ..., -0.1168, -0.1595,  0.1175],\n",
      "         [-0.0969, -0.2024, -0.4050,  ...,  0.0022, -0.0365,  0.1764]],\n",
      "\n",
      "        [[ 0.1138, -0.6776, -0.3231,  ...,  0.2267, -0.2566,  0.1965],\n",
      "         [ 0.0373, -0.3213, -0.3451,  ..., -0.1246, -0.2931,  0.4562],\n",
      "         [ 0.0777, -0.4078, -0.5938,  ..., -0.0616, -0.2806,  0.3470],\n",
      "         ...,\n",
      "         [-0.0957, -0.2932, -0.4029,  ..., -0.0170, -0.3030,  0.1578],\n",
      "         [ 0.0153, -0.3352, -0.2783,  ..., -0.1642, -0.4062,  0.2237],\n",
      "         [ 0.0774, -0.3003, -0.2613,  ..., -0.1718, -0.3447,  0.2850]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0709, -0.4292, -0.3456,  ..., -0.3060,  0.5402,  0.6026],\n",
      "         [ 0.1419,  0.0483, -0.4611,  ..., -0.2405,  0.4928,  0.5911],\n",
      "         [ 0.1050, -0.2220, -0.4711,  ..., -0.1759,  0.5244,  0.5521],\n",
      "         ...,\n",
      "         [-0.2025, -0.1663, -0.4832,  ..., -0.4405,  0.4577,  0.3417],\n",
      "         [-0.1287, -0.2132, -0.2178,  ..., -0.6167,  0.5193,  0.3203],\n",
      "         [-0.0821, -0.0151, -0.2926,  ..., -0.4693,  0.4102,  0.6148]],\n",
      "\n",
      "        [[ 0.2270, -0.4522, -0.4344,  ...,  0.1841, -0.1898,  0.5617],\n",
      "         [-0.2756, -0.1652, -0.3648,  ...,  0.0010, -0.2880,  0.4172],\n",
      "         [-0.0682, -0.2384, -0.4922,  ..., -0.1565, -0.2656,  0.2023],\n",
      "         ...,\n",
      "         [ 0.2295, -0.4876, -0.3502,  ..., -0.1187, -0.1706,  0.3064],\n",
      "         [ 0.1349, -0.3714, -0.1104,  ..., -0.2416,  0.0355,  0.3017],\n",
      "         [ 0.2611, -0.4339, -0.2737,  ..., -0.3721, -0.3900,  0.2133]],\n",
      "\n",
      "        [[ 0.3653, -0.4975, -0.4726,  ...,  0.1917, -0.2370,  0.4471],\n",
      "         [-0.2684,  0.0682, -0.4561,  ..., -0.0963, -0.4433,  0.5972],\n",
      "         [ 0.0519, -0.2461, -0.2602,  ...,  0.1719, -0.1463,  0.1715],\n",
      "         ...,\n",
      "         [-0.1491, -0.0414, -0.4575,  ..., -0.0515, -0.1482,  0.2246],\n",
      "         [-0.2159, -0.3315, -0.4632,  ..., -0.4142, -0.4966, -0.0888],\n",
      "         [ 0.0120, -0.1396, -0.3159,  ..., -0.4247, -0.3381,  0.4645]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.17207932472229\n",
      "Model outputs:  tensor([[[-0.1053, -0.0162, -0.2334,  ..., -0.3342, -0.3240, -0.2099],\n",
      "         [-0.0382,  0.1862, -0.4056,  ..., -0.2878, -0.0167, -0.2774],\n",
      "         [-0.0631, -0.1075, -0.1876,  ..., -0.2835, -0.1077, -0.2901],\n",
      "         ...,\n",
      "         [-0.1538,  0.2085, -0.2278,  ..., -0.3452,  0.0899, -0.1632],\n",
      "         [-0.1165,  0.1985, -0.2634,  ..., -0.2111, -0.0802, -0.2559],\n",
      "         [-0.1250,  0.0738, -0.4409,  ...,  0.0951, -0.0517, -0.2033]],\n",
      "\n",
      "        [[-0.1417,  0.0835, -0.0759,  ..., -0.2154, -0.1715, -0.3618],\n",
      "         [-0.2083,  0.3165, -0.3806,  ..., -0.0490, -0.0875, -0.3952],\n",
      "         [-0.0316,  0.1441, -0.3125,  ..., -0.1903, -0.0620, -0.2695],\n",
      "         ...,\n",
      "         [-0.1681,  0.1504, -0.6078,  ..., -0.1413, -0.2346, -0.3781],\n",
      "         [-0.1068,  0.0774, -0.4615,  ..., -0.4078, -0.0896, -0.3584],\n",
      "         [-0.2577,  0.0828, -0.5422,  ..., -0.0361, -0.2336, -0.2821]],\n",
      "\n",
      "        [[-0.0993, -0.1207, -0.2178,  ..., -0.4731, -0.3656,  0.0200],\n",
      "         [ 0.1972, -0.1189,  0.0411,  ..., -0.5206, -0.1112,  0.0313],\n",
      "         [-0.1549,  0.0113, -0.1824,  ..., -0.2347, -0.2191, -0.1623],\n",
      "         ...,\n",
      "         [ 0.0394,  0.0202, -0.0539,  ..., -0.4354,  0.0701, -0.2303],\n",
      "         [-0.1362, -0.3225,  0.0511,  ..., -0.4321, -0.1699,  0.0464],\n",
      "         [-0.1197, -0.0620, -0.1053,  ..., -0.2934, -0.4353,  0.0703]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0213, -0.1906,  0.0320,  ..., -0.5009, -0.2470, -0.0718],\n",
      "         [-0.0311, -0.0235, -0.0749,  ..., -0.4107, -0.3649, -0.1274],\n",
      "         [ 0.1450, -0.1696,  0.0311,  ..., -0.4836, -0.3047, -0.0773],\n",
      "         ...,\n",
      "         [-0.0397, -0.2957, -0.1928,  ..., -0.4999, -0.1510, -0.0551],\n",
      "         [-0.0953, -0.1887, -0.2653,  ..., -0.4633, -0.2606,  0.1345],\n",
      "         [-0.1527,  0.0171, -0.3868,  ..., -0.1841, -0.2857,  0.2608]],\n",
      "\n",
      "        [[-0.2429,  0.0544, -0.1309,  ..., -0.2892, -0.1424, -0.1134],\n",
      "         [ 0.3331, -0.1716, -0.0818,  ..., -0.7623, -0.3225,  0.1154],\n",
      "         [ 0.0276, -0.2325, -0.0277,  ..., -0.4143, -0.0378, -0.0698],\n",
      "         ...,\n",
      "         [-0.1255, -0.0369, -0.1687,  ..., -0.4812, -0.1558,  0.0098],\n",
      "         [-0.1529, -0.1310, -0.0603,  ..., -0.3588, -0.3300, -0.0613],\n",
      "         [-0.1175, -0.0804,  0.0333,  ..., -0.1639, -0.4946,  0.2745]],\n",
      "\n",
      "        [[-0.4402,  0.2363, -0.1221,  ...,  0.0026,  0.1471, -0.3705],\n",
      "         [-0.3895,  0.2651, -0.2906,  ...,  0.0785,  0.1825, -0.3568],\n",
      "         [-0.3066, -0.0336, -0.0841,  ..., -0.0951,  0.1289, -0.4711],\n",
      "         ...,\n",
      "         [-0.5455,  0.1037, -0.3019,  ...,  0.0294,  0.4844, -0.1268],\n",
      "         [-0.2418,  0.0999, -0.5444,  ...,  0.1334,  0.2448, -0.2492],\n",
      "         [-0.4323,  0.3607, -0.1789,  ...,  0.0942,  0.0010,  0.0039]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0312283039093018\n",
      "Model outputs:  tensor([[[ 1.1343e-01, -1.5785e-01, -2.5693e-01,  ..., -2.4438e-01,\n",
      "          -1.3476e-01,  7.3319e-02],\n",
      "         [ 6.2763e-02, -1.7155e-01, -2.7770e-01,  ..., -4.4308e-01,\n",
      "          -8.9712e-02,  2.3338e-01],\n",
      "         [ 2.9514e-01, -2.8304e-01, -2.1334e-01,  ..., -3.2397e-01,\n",
      "          -4.2625e-02,  2.0920e-01],\n",
      "         ...,\n",
      "         [ 2.6774e-01, -2.0659e-01, -7.0399e-02,  ..., -3.3511e-01,\n",
      "          -3.4894e-01,  1.0877e-02],\n",
      "         [-5.6111e-02, -1.9626e-01,  2.0421e-01,  ..., -1.2754e-01,\n",
      "          -1.8572e-01, -1.0447e-01],\n",
      "         [ 3.7609e-02, -1.9098e-01, -7.4090e-02,  ..., -6.0000e-01,\n",
      "          -2.7762e-01,  4.6148e-02]],\n",
      "\n",
      "        [[-6.3416e-04, -3.2264e-01,  3.2197e-03,  ..., -3.0177e-01,\n",
      "          -4.5411e-01,  1.9983e-03],\n",
      "         [-6.3783e-02, -1.6586e-01, -2.1217e-01,  ..., -4.0154e-01,\n",
      "          -2.0130e-01,  3.6813e-01],\n",
      "         [ 2.1777e-02, -1.3964e-01, -3.4357e-01,  ..., -6.2359e-01,\n",
      "          -2.7786e-01,  3.0019e-01],\n",
      "         ...,\n",
      "         [ 2.0480e-01, -5.2039e-01,  3.6247e-02,  ..., -4.2728e-01,\n",
      "          -2.5574e-01,  3.6030e-02],\n",
      "         [ 6.9027e-02, -2.7352e-01,  1.0360e-01,  ..., -4.5355e-01,\n",
      "          -5.3468e-02,  3.8529e-02],\n",
      "         [ 4.1595e-02, -1.1607e-01, -1.3485e-01,  ..., -6.4485e-01,\n",
      "          -2.0952e-01,  3.0270e-01]],\n",
      "\n",
      "        [[-3.9314e-01,  1.7394e-01, -1.2326e-01,  ...,  2.4008e-01,\n",
      "           3.2028e-01, -1.3341e-01],\n",
      "         [-4.8843e-01,  2.0854e-01, -3.0446e-01,  ...,  4.0118e-01,\n",
      "           1.3988e-01,  3.0751e-02],\n",
      "         [-2.4211e-02, -6.7562e-03,  5.9071e-02,  ...,  4.2161e-01,\n",
      "           3.6724e-01, -3.8058e-01],\n",
      "         ...,\n",
      "         [-2.3484e-01,  3.4436e-01, -7.2351e-02,  ...,  1.9780e-01,\n",
      "           2.1539e-01, -6.9135e-01],\n",
      "         [-1.2084e-01,  2.2714e-01,  7.1267e-02,  ...,  3.0585e-01,\n",
      "           2.9859e-01, -4.8063e-01],\n",
      "         [-1.1759e-01,  3.3376e-01, -1.6897e-01,  ...,  1.9297e-01,\n",
      "           6.3738e-02, -1.5763e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6094e-01,  4.5940e-02, -4.0408e-01,  ...,  3.8899e-01,\n",
      "          -6.1092e-02, -4.1939e-01],\n",
      "         [-2.0488e-01,  1.3750e-01, -6.1985e-01,  ...,  4.9965e-01,\n",
      "           1.2123e-01, -2.6996e-01],\n",
      "         [-2.5734e-01,  1.9635e-01, -5.0570e-01,  ...,  2.4974e-01,\n",
      "           2.7460e-02, -3.3900e-02],\n",
      "         ...,\n",
      "         [-1.3357e-01,  3.2583e-01, -3.4281e-01,  ...,  1.2145e-01,\n",
      "           5.1336e-02, -2.3667e-01],\n",
      "         [-2.1628e-01,  8.6839e-02, -3.4584e-01,  ...,  1.5901e-01,\n",
      "           1.9688e-01, -3.0931e-01],\n",
      "         [-1.2583e-01,  1.5942e-02, -3.2051e-01,  ...,  1.1709e-01,\n",
      "           1.6552e-01, -5.5357e-01]],\n",
      "\n",
      "        [[-2.3323e-01,  1.3017e-01, -3.7403e-01,  ...,  3.5386e-02,\n",
      "           1.7948e-01, -6.9618e-02],\n",
      "         [-4.9626e-01,  1.5404e-01, -2.9105e-01,  ...,  2.1747e-01,\n",
      "           1.1605e-01,  1.4021e-01],\n",
      "         [-5.6578e-01,  4.3951e-02, -5.4479e-01,  ...,  2.6579e-01,\n",
      "           5.4768e-01, -1.9669e-01],\n",
      "         ...,\n",
      "         [-3.0519e-01, -3.1868e-02, -3.8233e-01,  ..., -1.1618e-01,\n",
      "          -4.9987e-02, -2.9606e-01],\n",
      "         [-5.3350e-01, -1.6202e-02, -8.3458e-02,  ..., -7.3049e-02,\n",
      "           2.1604e-01, -1.9387e-01],\n",
      "         [-2.4140e-01, -1.2983e-02, -1.6580e-01,  ...,  8.3487e-04,\n",
      "           3.0407e-01, -5.1962e-02]],\n",
      "\n",
      "        [[-1.6869e-01, -6.4777e-02, -5.2621e-01,  ..., -2.2076e-02,\n",
      "           9.1746e-02, -1.3473e-01],\n",
      "         [-2.8442e-01,  2.5074e-01, -4.2431e-01,  ...,  5.4884e-02,\n",
      "          -1.9004e-01, -2.7421e-01],\n",
      "         [-2.0514e-01, -1.0533e-01, -6.3616e-01,  ..., -8.6458e-04,\n",
      "           1.3713e-01, -1.5096e-01],\n",
      "         ...,\n",
      "         [-2.8466e-01,  2.0378e-02, -5.2657e-01,  ...,  4.0563e-02,\n",
      "          -7.1257e-02, -2.6302e-02],\n",
      "         [-3.2267e-01,  6.8731e-02, -2.7917e-01,  ..., -1.3571e-01,\n",
      "          -1.3761e-01, -2.6001e-01],\n",
      "         [-1.9723e-01, -1.1866e-01, -2.5872e-01,  ..., -4.8905e-02,\n",
      "          -1.0585e-02, -1.9739e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9584513306617737\n",
      "Model outputs:  tensor([[[ 0.0884, -0.3167, -0.1208,  ..., -0.1930, -0.1485,  0.0732],\n",
      "         [ 0.1112, -0.2947, -0.3939,  ..., -0.1144,  0.0123,  0.5032],\n",
      "         [-0.0150, -0.3396, -0.3908,  ..., -0.0206, -0.1507,  0.2786],\n",
      "         ...,\n",
      "         [-0.2687, -0.3109, -0.2096,  ..., -0.2504, -0.1745, -0.0187],\n",
      "         [-0.3194, -0.2858, -0.5217,  ..., -0.4430, -0.1124, -0.0334],\n",
      "         [-0.2679, -0.2020, -0.3802,  ..., -0.2031, -0.3328,  0.4644]],\n",
      "\n",
      "        [[ 0.0797,  0.0537, -0.3528,  ...,  0.1490, -0.2178, -0.0142],\n",
      "         [-0.2785,  0.1350, -0.6679,  ...,  0.2373,  0.2773, -0.0761],\n",
      "         [-0.0683, -0.1666, -0.5512,  ...,  0.2104,  0.1218,  0.0236],\n",
      "         ...,\n",
      "         [-0.2927,  0.0269, -0.5087,  ...,  0.1319, -0.1679, -0.0623],\n",
      "         [-0.2745, -0.1111, -0.3873,  ..., -0.0385, -0.3923,  0.0459],\n",
      "         [-0.4900, -0.1518, -1.0191,  ...,  0.1169, -0.1926,  0.0580]],\n",
      "\n",
      "        [[ 0.2177, -0.4310, -0.2721,  ..., -0.0480, -0.3136,  0.0199],\n",
      "         [-0.0085, -0.3360, -0.5349,  ..., -0.0046, -0.1768,  0.3572],\n",
      "         [-0.0874, -0.5312, -0.4096,  ..., -0.3463, -0.0573,  0.0942],\n",
      "         ...,\n",
      "         [-0.2195, -0.2384, -0.2727,  ..., -0.3133, -0.1151, -0.0071],\n",
      "         [-0.4039, -0.1219, -0.1492,  ..., -0.3038, -0.0866,  0.1128],\n",
      "         [-0.5295, -0.3290, -0.6002,  ..., -0.4124, -0.4430,  0.1983]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0225, -0.3536, -0.3306,  ...,  0.1718, -0.3953, -0.0190],\n",
      "         [-0.0284, -0.3200, -0.7316,  ...,  0.0412, -0.0998,  0.0855],\n",
      "         [-0.0880, -0.1640, -0.5082,  ...,  0.3587,  0.2238, -0.4225],\n",
      "         ...,\n",
      "         [-0.1753, -0.1228, -0.3386,  ..., -0.0484, -0.2370, -0.1879],\n",
      "         [-0.3233,  0.1798, -0.4839,  ...,  0.0673, -0.1171, -0.1675],\n",
      "         [-0.4712, -0.1935, -0.5137,  ...,  0.1234, -0.4967, -0.2251]],\n",
      "\n",
      "        [[ 0.3603, -0.7361,  0.0922,  ..., -0.0915, -0.3421,  0.2728],\n",
      "         [ 0.0080, -0.2777, -0.4353,  ..., -0.2881, -0.1699,  0.4353],\n",
      "         [-0.0429, -0.5358, -0.3675,  ..., -0.2432, -0.1043,  0.3580],\n",
      "         ...,\n",
      "         [-0.1383, -0.2593, -0.0675,  ..., -0.3633, -0.6417,  0.1440],\n",
      "         [-0.1628, -0.3901, -0.2617,  ..., -0.4394, -0.4518,  0.3320],\n",
      "         [-0.1237, -0.2385, -0.0573,  ..., -0.5406, -0.4775,  0.3461]],\n",
      "\n",
      "        [[-0.0198, -0.2437, -0.0833,  ..., -0.4305,  0.4747,  0.2962],\n",
      "         [-0.0176, -0.3182, -0.3108,  ..., -0.5885,  0.5220,  0.2982],\n",
      "         [-0.0291, -0.3461, -0.4107,  ..., -0.4892,  0.5162,  0.5325],\n",
      "         ...,\n",
      "         [-0.1379, -0.1763, -0.3009,  ..., -0.4241,  0.3141,  0.2982],\n",
      "         [-0.0033, -0.1146, -0.3165,  ..., -0.4122,  0.3413,  0.4568],\n",
      "         [ 0.0257, -0.4269, -0.4499,  ..., -0.5865,  0.5296,  0.5077]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1465054750442505\n",
      "Model outputs:  tensor([[[-0.1480, -0.4577, -0.5715,  ..., -0.4561,  0.4529,  0.6517],\n",
      "         [-0.2131, -0.2080, -0.4685,  ..., -0.6684,  0.3650,  0.4843],\n",
      "         [-0.2293, -0.4821, -0.1974,  ..., -0.2679,  0.3426,  0.6087],\n",
      "         ...,\n",
      "         [-0.0498, -0.1577, -0.6620,  ..., -0.3892,  0.5613,  0.2850],\n",
      "         [-0.2691,  0.0730, -0.6343,  ..., -0.4215,  0.6662,  0.7750],\n",
      "         [-0.4281, -0.2417, -0.5348,  ..., -0.6171,  0.6196,  0.4803]],\n",
      "\n",
      "        [[ 0.0051, -0.2418, -0.4694,  ..., -0.0859, -0.1501,  0.1927],\n",
      "         [-0.1511, -0.0632, -0.3185,  ..., -0.3494, -0.3608,  0.0745],\n",
      "         [-0.2650, -0.3551, -0.2755,  ...,  0.0105, -0.1849,  0.4736],\n",
      "         ...,\n",
      "         [-0.0727, -0.3506, -0.7356,  ..., -0.0039, -0.0354,  0.3567],\n",
      "         [-0.1940, -0.2158, -0.5346,  ..., -0.3409, -0.6453,  0.2764],\n",
      "         [-0.3547, -0.1147, -0.4996,  ..., -0.1860, -0.1952,  0.1070]],\n",
      "\n",
      "        [[-0.8902,  0.0149, -0.6455,  ...,  0.4733, -0.1190,  0.1085],\n",
      "         [-0.6310,  0.0278, -0.4907,  ...,  0.3879, -0.0992, -0.1331],\n",
      "         [-0.7098, -0.1168, -0.5715,  ...,  0.6303, -0.0730,  0.1425],\n",
      "         ...,\n",
      "         [-0.6142,  0.1326, -0.8131,  ...,  0.6637,  0.3565,  0.1108],\n",
      "         [-0.6767,  0.0616, -1.0330,  ...,  0.6501,  0.2553,  0.2856],\n",
      "         [-0.7165,  0.1932, -0.7907,  ...,  0.2696,  0.2595,  0.0019]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1050, -0.4760, -0.4827,  ..., -0.1688, -0.4186,  0.5037],\n",
      "         [-0.3282, -0.1139, -0.2162,  ..., -0.4446, -0.5388,  0.3102],\n",
      "         [-0.1390, -0.3686, -0.1600,  ...,  0.0783, -0.1585,  0.4987],\n",
      "         ...,\n",
      "         [ 0.0495, -0.5414, -0.1755,  ..., -0.0594,  0.0093,  0.1913],\n",
      "         [-0.0782, -0.5322, -0.2605,  ..., -0.1093, -0.3628,  0.5958],\n",
      "         [-0.0604, -0.4296, -0.3795,  ..., -0.2902, -0.4070,  0.4552]],\n",
      "\n",
      "        [[-0.8219, -0.0032, -0.5843,  ...,  0.2532, -0.1353, -0.0531],\n",
      "         [-0.6870, -0.0776, -0.3485,  ...,  0.1715,  0.0773, -0.0425],\n",
      "         [-0.5496, -0.2279, -0.2234,  ...,  0.4927,  0.2586,  0.3226],\n",
      "         ...,\n",
      "         [-0.5442,  0.2855, -1.0359,  ...,  0.3461,  0.1946,  0.3694],\n",
      "         [-0.6209, -0.0577, -0.6996,  ...,  0.4301,  0.3078,  0.1878],\n",
      "         [-0.6823,  0.0873, -0.8233,  ...,  0.3182,  0.0949,  0.1763]],\n",
      "\n",
      "        [[-0.6004, -0.2274, -0.8537,  ...,  0.1874, -0.1965,  0.1624],\n",
      "         [-0.3649,  0.0085, -0.6524,  ..., -0.2377, -0.1984,  0.0742],\n",
      "         [-0.3182, -0.4013, -0.3289,  ...,  0.2645, -0.1093,  0.3208],\n",
      "         ...,\n",
      "         [-0.3882, -0.1740, -0.9107,  ...,  0.2326, -0.2046,  0.1792],\n",
      "         [-0.3928, -0.2036, -0.8468,  ...,  0.3131, -0.0207,  0.3470],\n",
      "         [-0.7186, -0.2159, -0.8560,  ...,  0.0287,  0.0844,  0.3578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8540160059928894\n",
      "Model outputs:  tensor([[[-0.5049,  0.1102, -0.2581,  ...,  0.6289,  0.4709,  0.1168],\n",
      "         [-0.7537,  0.3684, -0.5373,  ...,  0.7828,  0.3611, -0.2398],\n",
      "         [-0.5519,  0.1301, -0.4857,  ...,  1.0292,  0.0466, -0.0353],\n",
      "         ...,\n",
      "         [-0.5717,  0.0057, -0.2469,  ...,  1.0385,  0.2597, -0.2867],\n",
      "         [-0.3778,  0.3099, -0.5499,  ...,  1.1625,  0.2902, -0.0133],\n",
      "         [-0.4542,  0.0377, -0.2789,  ...,  1.0067,  0.5289,  0.0133]],\n",
      "\n",
      "        [[-0.5611, -0.1366, -0.3669,  ...,  0.5458,  0.2915,  0.3569],\n",
      "         [-0.4872,  0.2234, -0.3906,  ...,  0.4407,  0.1560,  0.1366],\n",
      "         [-0.8422, -0.0595, -0.2624,  ...,  0.7049, -0.1370, -0.2906],\n",
      "         ...,\n",
      "         [-0.4761,  0.2280, -0.2221,  ...,  0.6993,  0.4261, -0.0545],\n",
      "         [-0.6465, -0.0147, -0.3156,  ...,  0.8119,  0.4569,  0.2103],\n",
      "         [-0.5103,  0.2389, -0.1465,  ...,  0.9029,  0.5518, -0.0829]],\n",
      "\n",
      "        [[-0.9425,  0.1329, -0.2939,  ...,  0.7808,  0.2544, -0.1848],\n",
      "         [-0.7166,  0.1323, -0.3451,  ...,  0.4915,  0.3462, -0.2027],\n",
      "         [-0.4969, -0.1028, -0.3377,  ...,  0.6139, -0.3048,  0.0761],\n",
      "         ...,\n",
      "         [-0.8043,  0.1711, -0.2211,  ...,  0.6596,  0.2847, -0.3606],\n",
      "         [-0.7180, -0.0974, -0.2031,  ...,  0.8573,  0.1693, -0.1642],\n",
      "         [-0.5016,  0.1048, -0.3794,  ...,  0.8427,  0.6122, -0.2471]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6844,  0.0518, -0.3332,  ...,  0.7818,  0.3654, -0.2450],\n",
      "         [-0.7450,  0.2235, -0.3344,  ...,  0.5665,  0.2092,  0.0405],\n",
      "         [-0.4071, -0.0811, -0.2210,  ...,  0.5646, -0.2343,  0.0399],\n",
      "         ...,\n",
      "         [-0.4542,  0.0472, -0.0913,  ...,  0.8423,  0.3032, -0.2180],\n",
      "         [-0.4369,  0.0045, -0.2146,  ...,  0.9589,  0.6044, -0.3486],\n",
      "         [-0.5494,  0.1993, -0.0773,  ...,  0.8786,  0.7215, -0.1677]],\n",
      "\n",
      "        [[-0.7108,  0.0684, -0.4117,  ...,  0.3818,  0.4379,  0.0326],\n",
      "         [-0.7695,  0.1269, -0.3378,  ...,  0.3820,  0.2885,  0.0585],\n",
      "         [-0.7522, -0.0089, -0.5507,  ...,  0.7363,  0.0260,  0.2512],\n",
      "         ...,\n",
      "         [-0.5663,  0.4007, -0.5939,  ...,  0.4557,  0.3065,  0.0883],\n",
      "         [-0.7488,  0.2066, -0.4223,  ...,  0.4767,  0.3764,  0.2588],\n",
      "         [-0.7743,  0.3955, -0.3434,  ...,  0.4272,  0.6244,  0.0688]],\n",
      "\n",
      "        [[-0.6336,  0.1950, -0.3905,  ...,  0.9065,  0.4930, -0.0590],\n",
      "         [-0.5548,  0.0161, -0.3131,  ...,  0.8940,  0.3851,  0.0368],\n",
      "         [-0.5000, -0.0367, -0.3327,  ...,  0.9885, -0.0564,  0.1026],\n",
      "         ...,\n",
      "         [-0.7755, -0.1369, -0.2615,  ...,  1.0175,  0.2695, -0.1627],\n",
      "         [-0.5805, -0.0013, -0.2153,  ...,  0.9728,  0.2909,  0.1094],\n",
      "         [-0.3098,  0.2134, -0.2070,  ...,  1.0459,  0.6263, -0.0474]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.20265793800354\n",
      "Model outputs:  tensor([[[-0.0433, -0.2191, -0.2216,  ..., -0.3041, -0.3758,  0.4178],\n",
      "         [-0.0046, -0.5492, -0.1441,  ...,  0.0572, -0.1491,  0.5613],\n",
      "         [ 0.0104, -0.4452, -0.6502,  ...,  0.1032, -0.2701,  0.6228],\n",
      "         ...,\n",
      "         [ 0.0599, -0.0691, -0.1273,  ..., -0.4397,  0.1245,  0.4667],\n",
      "         [-0.2213, -0.3681, -0.1050,  ..., -0.5651, -0.2581,  0.2877],\n",
      "         [-0.1646, -0.3104, -0.3102,  ..., -0.3942, -0.2435,  0.5625]],\n",
      "\n",
      "        [[-0.0938, -0.0812, -0.4841,  ..., -0.0115, -0.2837,  0.2977],\n",
      "         [-0.1786, -0.2526, -0.0686,  ...,  0.0051, -0.1110,  0.3430],\n",
      "         [-0.2630, -0.1212, -0.5728,  ...,  0.0777, -0.2122,  0.2431],\n",
      "         ...,\n",
      "         [ 0.0252, -0.3213, -0.4358,  ...,  0.0383,  0.0196,  0.1405],\n",
      "         [-0.0648, -0.2840, -0.4782,  ..., -0.2682, -0.4448,  0.2499],\n",
      "         [-0.1687, -0.2574, -0.5599,  ..., -0.2000, -0.1102,  0.1931]],\n",
      "\n",
      "        [[ 0.1334, -0.4349, -0.5285,  ..., -0.0714, -0.4244,  0.3221],\n",
      "         [-0.1470, -0.3503, -0.4424,  ...,  0.0612, -0.4614,  0.5122],\n",
      "         [ 0.0905, -0.2622, -0.4007,  ..., -0.2350, -0.3139,  0.5721],\n",
      "         ...,\n",
      "         [ 0.1681, -0.4312, -0.2249,  ..., -0.3352, -0.2183,  0.4801],\n",
      "         [-0.1298, -0.1895, -0.2319,  ..., -0.3519, -0.4550,  0.2122],\n",
      "         [-0.3056, -0.2826, -0.3374,  ..., -0.3693, -0.5371,  0.2799]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6278, -0.0831, -0.2080,  ...,  0.4306, -0.0556,  0.0855],\n",
      "         [-0.3143, -0.2227,  0.0269,  ...,  0.6239,  0.2119,  0.0861],\n",
      "         [-0.3936,  0.1663, -0.5135,  ...,  0.4771,  0.2722,  0.0060],\n",
      "         ...,\n",
      "         [-0.4259,  0.0739, -0.3773,  ...,  0.4459,  0.5514, -0.1316],\n",
      "         [-0.4623,  0.0731, -0.3983,  ...,  0.3076,  0.0871, -0.1666],\n",
      "         [-0.7685, -0.1363, -0.2366,  ...,  0.4255,  0.2491, -0.0095]],\n",
      "\n",
      "        [[-0.1623, -0.7517, -0.2906,  ..., -0.1516, -0.2285, -0.1152],\n",
      "         [-0.3527, -0.5062,  0.0132,  ..., -0.0011, -0.0657,  0.0319],\n",
      "         [-0.0335, -0.3572, -0.4472,  ..., -0.5191,  0.0467,  0.3444],\n",
      "         ...,\n",
      "         [ 0.0674, -0.3345, -0.3019,  ...,  0.0576, -0.0891, -0.3572],\n",
      "         [-0.0130, -0.0553, -0.3871,  ..., -0.1831, -0.2360, -0.2490],\n",
      "         [-0.0052, -0.1419, -0.4829,  ..., -0.2975, -0.1319, -0.0605]],\n",
      "\n",
      "        [[ 0.0896, -0.0115, -0.4941,  ..., -0.3174,  0.1069,  0.4326],\n",
      "         [ 0.0397, -0.1794, -0.1574,  ..., -0.0442,  0.2540,  0.2323],\n",
      "         [ 0.0053,  0.0071, -0.6607,  ..., -0.3447,  0.1686,  0.3500],\n",
      "         ...,\n",
      "         [ 0.2742, -0.1097, -0.3931,  ..., -0.3409,  0.4480,  0.4606],\n",
      "         [-0.1135,  0.2041, -0.4206,  ..., -0.5188,  0.2147,  0.1707],\n",
      "         [-0.0047, -0.1829, -0.7170,  ..., -0.6704,  0.1069,  0.2324]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8150163292884827\n",
      "Model outputs:  tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.2699, -0.3571, -0.1535,  ...,  0.0931, -0.4856,  0.5744],\n",
      "         [ 0.0379, -0.2830, -0.4096,  ...,  0.0808, -0.1760,  0.4157],\n",
      "         ...,\n",
      "         [-0.0481, -0.4249, -0.5027,  ...,  0.1623, -0.1379,  0.3077],\n",
      "         [ 0.0748, -0.6201, -0.6739,  ..., -0.0768, -0.3616,  0.0295],\n",
      "         [-0.1381, -0.2921, -0.3694,  ..., -0.0756, -0.3737,  0.1456]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.3148, -0.1890, -0.5169,  ..., -0.1934,  0.3297,  0.4297],\n",
      "         [ 0.0082, -0.3270, -0.5445,  ..., -0.1432,  0.4012,  0.4388],\n",
      "         ...,\n",
      "         [ 0.0348, -0.3035, -0.5387,  ..., -0.3712,  0.2453,  0.7289],\n",
      "         [ 0.0231, -0.2222, -0.4634,  ..., -0.4022,  0.3020,  0.5406],\n",
      "         [ 0.2005, -0.3186, -0.4209,  ...,  0.0150,  0.3925,  0.4021]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.0078, -0.3126, -0.3473,  ...,  0.1644, -0.2885,  0.0871],\n",
      "         [-0.1977, -0.4616, -0.4762,  ...,  0.2806, -0.1851,  0.0531],\n",
      "         ...,\n",
      "         [-0.2121, -0.4038, -0.6027,  ...,  0.0203, -0.2981,  0.1843],\n",
      "         [-0.1302, -0.5687, -0.7316,  ...,  0.0813, -0.3786,  0.1279],\n",
      "         [-0.2576, -0.2487, -0.5001,  ...,  0.2787, -0.1677, -0.0886]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.1997, -0.4118, -0.2305,  ...,  0.6056, -0.0424, -0.0038],\n",
      "         [-0.2496, -0.2709, -0.5503,  ...,  0.6306,  0.1370, -0.1732],\n",
      "         ...,\n",
      "         [-0.0906, -0.3113, -0.3409,  ...,  0.5474, -0.0818,  0.0661],\n",
      "         [-0.5005, -0.3584, -0.4369,  ...,  0.4090, -0.2388, -0.0860],\n",
      "         [-0.1651,  0.1197, -0.2811,  ...,  0.7122, -0.1354, -0.2054]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.4246, -0.3773, -0.1074,  ...,  0.2490, -0.5579,  0.4732],\n",
      "         [ 0.0435, -0.3375, -0.2950,  ...,  0.0202, -0.3726,  0.6744],\n",
      "         ...,\n",
      "         [ 0.1668, -0.5491, -0.3671,  ..., -0.2116, -0.1744,  0.5217],\n",
      "         [ 0.1638, -0.5841, -0.4389,  ..., -0.0962, -0.2890,  0.5697],\n",
      "         [ 0.1127, -0.4342, -0.2710,  ..., -0.0255, -0.4047,  0.4558]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.5613,  0.0292, -0.1947,  ..., -0.0435,  0.2148,  0.2655],\n",
      "         [ 0.3979, -0.3024, -0.4232,  ..., -0.0279,  0.1901,  0.3818],\n",
      "         ...,\n",
      "         [ 0.5498, -0.1931, -0.3263,  ..., -0.1933,  0.2719,  0.5100],\n",
      "         [ 0.2871, -0.4806, -0.5847,  ..., -0.1324,  0.3234,  0.3175],\n",
      "         [ 0.5040, -0.0719, -0.2776,  ...,  0.0350,  0.2412,  0.1252]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [9/10], Loss: 0.8150\n",
      "Model outputs:  tensor([[[ 0.2973, -0.2792, -0.3803,  ..., -0.0231, -0.2665,  0.3260],\n",
      "         [-0.0532, -0.1407, -0.3157,  ..., -0.2637, -0.3382,  0.1629],\n",
      "         [-0.1029, -0.0767, -0.4472,  ..., -0.0745, -0.1947,  0.2528],\n",
      "         ...,\n",
      "         [-0.1306, -0.1993, -0.4220,  ..., -0.0162, -0.2303,  0.1705],\n",
      "         [-0.0821, -0.2765, -0.5064,  ..., -0.4026, -0.5237,  0.2616],\n",
      "         [-0.1393, -0.5318, -0.3778,  ...,  0.0506, -0.0502,  0.1367]],\n",
      "\n",
      "        [[ 0.1382, -0.3117, -0.2980,  ..., -0.1446, -0.1342,  0.2200],\n",
      "         [-0.2771, -0.2460, -0.4525,  ..., -0.1535, -0.1567,  0.2465],\n",
      "         [ 0.1199, -0.5376, -0.5992,  ..., -0.3210, -0.1287,  0.1014],\n",
      "         ...,\n",
      "         [-0.2670, -0.3158, -0.3236,  ..., -0.2706, -0.2932,  0.2567],\n",
      "         [-0.0652, -0.3085, -0.3604,  ..., -0.2860, -0.2652,  0.1838],\n",
      "         [ 0.1028, -0.6808, -0.4737,  ...,  0.0193, -0.0576,  0.2767]],\n",
      "\n",
      "        [[-0.0984, -0.1399, -0.8603,  ...,  0.1304, -0.4245, -0.0013],\n",
      "         [-0.2069,  0.2469, -0.5613,  ...,  0.1054, -0.1079, -0.0329],\n",
      "         [-0.1283,  0.1163, -0.5388,  ..., -0.0282, -0.2698, -0.1605],\n",
      "         ...,\n",
      "         [-0.1365, -0.0494, -0.5602,  ...,  0.1754, -0.4116,  0.1287],\n",
      "         [-0.1911, -0.0846, -0.5070,  ...,  0.0794, -0.2646, -0.1460],\n",
      "         [-0.3591, -0.2177, -0.4511,  ...,  0.4301, -0.2812, -0.0950]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3663, -0.1509, -0.4593,  ...,  0.4852,  0.3971, -0.3399],\n",
      "         [-0.6882,  0.1740,  0.0579,  ...,  0.2552,  0.3898, -0.1270],\n",
      "         [-0.3895,  0.0578, -0.4266,  ...,  0.4131, -0.0581,  0.0057],\n",
      "         ...,\n",
      "         [-0.1828, -0.0026, -0.1769,  ...,  0.4653,  0.1652, -0.1119],\n",
      "         [-0.4713,  0.1615, -0.0722,  ...,  0.5838, -0.0475, -0.2562],\n",
      "         [-0.5614, -0.2033, -0.1390,  ...,  0.6236,  0.0733, -0.1475]],\n",
      "\n",
      "        [[-0.2567, -0.2610, -0.3670,  ...,  0.2447,  0.1884,  0.0741],\n",
      "         [-0.6105,  0.0737, -0.2056,  ...,  0.2449,  0.0777,  0.0139],\n",
      "         [-0.4732,  0.0033, -0.4396,  ...,  0.2886,  0.5025,  0.0915],\n",
      "         ...,\n",
      "         [-0.4311,  0.0485, -0.3182,  ...,  0.1956,  0.2492,  0.0942],\n",
      "         [-0.5652,  0.2559, -0.4033,  ...,  0.1886,  0.1645,  0.0755],\n",
      "         [-0.3499, -0.1579, -0.4146,  ...,  0.4406,  0.1996,  0.1035]],\n",
      "\n",
      "        [[ 0.2417, -0.5704, -0.3221,  ..., -0.0693,  0.0613, -0.2216],\n",
      "         [-0.0683, -0.0499, -0.2552,  ..., -0.3720, -0.3284, -0.1039],\n",
      "         [ 0.1597, -0.5299, -0.2410,  ..., -0.2807, -0.1399, -0.2696],\n",
      "         ...,\n",
      "         [-0.0686, -0.2163, -0.2051,  ...,  0.0954, -0.0989, -0.4257],\n",
      "         [-0.0843, -0.2820, -0.1400,  ..., -0.0729, -0.0840, -0.1056],\n",
      "         [-0.1169, -0.4906, -0.2328,  ..., -0.1144, -0.1248, -0.2431]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.137489914894104\n",
      "Model outputs:  tensor([[[-2.6325e-01,  1.1771e-01, -5.6543e-01,  ..., -3.2645e-01,\n",
      "          -2.1461e-01,  1.8965e-01],\n",
      "         [-4.6566e-01, -1.1756e-01, -8.0131e-01,  ..., -1.1839e-01,\n",
      "          -4.2822e-02, -1.0902e-01],\n",
      "         [-3.1421e-01, -1.5871e-02, -7.0583e-01,  ..., -2.6074e-01,\n",
      "           1.0730e-01,  1.3175e-01],\n",
      "         ...,\n",
      "         [-7.3595e-01, -2.2809e-01, -4.5967e-01,  ..., -2.7622e-01,\n",
      "          -5.2136e-01,  3.2366e-02],\n",
      "         [-7.3547e-01, -1.7582e-01, -8.2043e-01,  ..., -1.3575e-01,\n",
      "          -7.1182e-01,  3.2543e-01],\n",
      "         [-5.8783e-01, -6.9508e-02, -6.9772e-01,  ..., -5.1187e-01,\n",
      "          -4.6571e-01,  1.1067e-01]],\n",
      "\n",
      "        [[ 9.8986e-02, -1.3470e-01, -9.5425e-02,  ..., -4.6041e-01,\n",
      "          -3.1337e-01, -3.9972e-02],\n",
      "         [ 1.7312e-01, -3.5120e-01, -4.4397e-01,  ..., -2.0072e-01,\n",
      "          -2.6645e-01,  2.1277e-01],\n",
      "         [ 1.9772e-01, -2.9852e-01, -2.8394e-01,  ..., -2.7826e-01,\n",
      "           1.0758e-01, -2.4692e-03],\n",
      "         ...,\n",
      "         [-3.3977e-01, -1.7937e-01, -4.5296e-01,  ..., -3.5429e-01,\n",
      "          -4.5759e-01,  4.6404e-01],\n",
      "         [-2.6742e-01, -4.9470e-01, -2.7593e-01,  ..., -3.8684e-01,\n",
      "          -9.4882e-01,  6.0222e-01],\n",
      "         [-3.6449e-01, -4.1747e-01, -4.9400e-01,  ..., -4.9996e-01,\n",
      "          -3.9480e-01,  4.5464e-01]],\n",
      "\n",
      "        [[-3.0576e-01,  1.4948e-01, -4.6465e-01,  ...,  2.8112e-01,\n",
      "          -2.2823e-01, -2.8775e-01],\n",
      "         [-3.9163e-01,  1.9034e-01, -2.6106e-01,  ...,  4.8944e-01,\n",
      "           1.9916e-01, -4.7509e-01],\n",
      "         [-4.3504e-01,  4.2781e-04, -1.9248e-01,  ...,  1.9389e-01,\n",
      "           2.1594e-02, -2.3520e-01],\n",
      "         ...,\n",
      "         [-5.8511e-01,  3.5954e-02, -1.9332e-01,  ...,  4.7450e-01,\n",
      "          -1.2261e-01,  2.6673e-02],\n",
      "         [-9.8393e-01,  2.0084e-01, -2.6295e-01,  ...,  7.8877e-01,\n",
      "          -2.5047e-01,  9.9879e-02],\n",
      "         [-6.6191e-01,  8.2692e-02, -1.7920e-01,  ...,  4.2796e-01,\n",
      "           3.5130e-01, -3.3325e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.4189e-01,  2.6026e-01, -7.3524e-01,  ...,  2.0891e-01,\n",
      "           6.3196e-02, -9.7407e-02],\n",
      "         [-3.5659e-01, -1.5303e-02, -5.5896e-01,  ...,  6.0033e-01,\n",
      "           7.0064e-02,  2.8335e-02],\n",
      "         [-4.8101e-01,  6.4064e-02, -6.1138e-01,  ...,  4.1501e-01,\n",
      "           2.1545e-01, -2.1123e-01],\n",
      "         ...,\n",
      "         [-4.7991e-01, -1.8657e-02, -4.1134e-01,  ...,  3.0455e-01,\n",
      "          -1.7171e-01, -1.2846e-01],\n",
      "         [-5.7791e-01,  9.3234e-02, -8.3417e-01,  ...,  6.3020e-01,\n",
      "          -6.3699e-01, -9.9196e-02],\n",
      "         [-7.5759e-01, -5.5239e-02, -9.5018e-01,  ...,  2.9061e-01,\n",
      "          -4.1598e-01, -1.3562e-01]],\n",
      "\n",
      "        [[ 2.1685e-01, -3.7858e-02,  1.1071e-01,  ...,  2.8689e-01,\n",
      "          -1.7270e-01, -4.0474e-01],\n",
      "         [ 1.2249e-01, -2.4032e-02, -7.3010e-02,  ...,  3.6396e-01,\n",
      "          -1.1348e-01, -4.2015e-01],\n",
      "         [ 1.8774e-01, -1.9418e-01, -3.1594e-02,  ...,  3.5828e-01,\n",
      "          -2.7927e-02, -6.2765e-01],\n",
      "         ...,\n",
      "         [-3.3128e-02, -2.7060e-01,  2.1295e-01,  ...,  4.0814e-01,\n",
      "          -2.6669e-01, -4.7253e-01],\n",
      "         [-4.1128e-01, -4.5611e-01,  1.2516e-01,  ...,  5.8954e-01,\n",
      "          -7.6914e-01, -4.1223e-01],\n",
      "         [ 1.6158e-01, -2.4858e-01, -2.6690e-02,  ...,  2.7639e-01,\n",
      "          -4.7347e-01, -2.6100e-01]],\n",
      "\n",
      "        [[-1.4000e-01, -3.7097e-01, -1.6795e-01,  ..., -4.3059e-01,\n",
      "          -4.0979e-01,  3.0658e-01],\n",
      "         [ 8.4553e-02, -4.1146e-01, -3.4384e-01,  ..., -2.5692e-01,\n",
      "          -3.5544e-01,  3.3976e-01],\n",
      "         [-1.2522e-02, -3.4275e-01, -4.6368e-01,  ..., -3.5125e-01,\n",
      "          -9.6510e-02, -8.6090e-03],\n",
      "         ...,\n",
      "         [-2.5203e-01, -3.0474e-01, -3.4406e-01,  ..., -2.6882e-01,\n",
      "          -3.9539e-01,  4.8632e-01],\n",
      "         [-3.0168e-01, -4.1639e-01,  2.9843e-02,  ..., -3.3458e-01,\n",
      "          -1.0689e+00,  5.1542e-01],\n",
      "         [-4.0147e-01, -4.5886e-01, -1.9359e-01,  ..., -4.1094e-01,\n",
      "          -5.9772e-01,  5.1247e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.257691502571106\n",
      "Model outputs:  tensor([[[ 0.0186, -0.2361, -0.4088,  ..., -0.1509, -0.5501,  0.4953],\n",
      "         [ 0.0242, -0.5563, -0.5351,  ..., -0.0444, -0.2436,  0.4985],\n",
      "         [ 0.1841, -0.4869, -0.4446,  ..., -0.3179, -0.5430,  0.4377],\n",
      "         ...,\n",
      "         [-0.0320, -0.3430, -0.4758,  ..., -0.1423, -0.5409,  0.6587],\n",
      "         [ 0.1441, -0.4650, -0.4047,  ..., -0.0937, -0.5647,  0.4521],\n",
      "         [-0.0188, -0.4248, -0.5348,  ..., -0.1289, -0.4426,  0.3658]],\n",
      "\n",
      "        [[-0.7868, -0.1619, -0.6246,  ...,  0.1516,  0.0618,  0.2478],\n",
      "         [-0.6346, -0.1544, -0.5045,  ...,  0.2718,  0.3047,  0.0169],\n",
      "         [-0.6137,  0.1165, -0.7411,  ...,  0.1085,  0.0762,  0.2029],\n",
      "         ...,\n",
      "         [-0.4748, -0.0101, -0.7801,  ...,  0.3307,  0.3241,  0.2795],\n",
      "         [-0.6234, -0.0223, -0.4255,  ...,  0.1073,  0.2008,  0.2669],\n",
      "         [-0.3723,  0.0695, -0.6370,  ...,  0.4521,  0.4961,  0.2093]],\n",
      "\n",
      "        [[-0.1971, -0.1288, -0.8918,  ...,  0.1364, -0.1016, -0.1379],\n",
      "         [-0.2759, -0.0073, -0.8851,  ...,  0.3484, -0.1933,  0.1483],\n",
      "         [-0.2594, -0.1538, -1.0032,  ...,  0.2622,  0.0419,  0.0577],\n",
      "         ...,\n",
      "         [-0.4512, -0.1652, -0.7964,  ...,  0.2528, -0.0912, -0.1061],\n",
      "         [-0.1205, -0.0361, -0.9833,  ...,  0.2706,  0.1450, -0.0034],\n",
      "         [ 0.0115, -0.0463, -0.8949,  ...,  0.5165,  0.0381,  0.0055]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6060, -0.1468, -1.0089,  ...,  0.1038, -0.0587,  0.5466],\n",
      "         [-0.2076, -0.2351, -0.8608,  ...,  0.4785,  0.0767,  0.1654],\n",
      "         [-0.5772, -0.1908, -0.9066,  ...,  0.1219, -0.1977,  0.2739],\n",
      "         ...,\n",
      "         [-0.5681, -0.0872, -0.8580,  ...,  0.0073, -0.3044,  0.2084],\n",
      "         [-0.4800, -0.1524, -0.9098,  ..., -0.0045, -0.2012,  0.0221],\n",
      "         [-0.2420,  0.0441, -0.7747,  ...,  0.3866,  0.0317,  0.1765]],\n",
      "\n",
      "        [[-0.3979, -0.0040, -1.0561,  ...,  0.6237, -0.1004, -0.0247],\n",
      "         [-0.1412, -0.3130, -0.8304,  ...,  0.6743, -0.1241, -0.1674],\n",
      "         [-0.3270, -0.0654, -1.1039,  ...,  0.4937,  0.0509,  0.2293],\n",
      "         ...,\n",
      "         [-0.4402,  0.0156, -0.8338,  ...,  0.6022, -0.0717,  0.1344],\n",
      "         [-0.2601, -0.2329, -0.7707,  ...,  0.6448, -0.0171, -0.0109],\n",
      "         [-0.2200,  0.1147, -0.6570,  ...,  0.9593, -0.0666, -0.1088]],\n",
      "\n",
      "        [[-0.0069, -0.3670, -0.4895,  ..., -0.3056, -0.6079,  0.4492],\n",
      "         [-0.2145, -0.2211, -0.6827,  ...,  0.0017, -0.5351,  0.3738],\n",
      "         [ 0.0551, -0.4722, -0.5463,  ..., -0.5745, -0.3325,  0.4711],\n",
      "         ...,\n",
      "         [-0.3178, -0.1148, -0.4120,  ..., -0.1627, -0.5343,  0.4577],\n",
      "         [ 0.1632, -0.4856, -0.4488,  ..., -0.1810, -0.2816,  0.4281],\n",
      "         [ 0.1182, -0.3888, -0.6073,  ...,  0.0261, -0.3503,  0.1398]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0156781673431396\n",
      "Model outputs:  tensor([[[-0.2112, -0.0894, -0.5468,  ...,  0.0941,  0.0099, -0.5115],\n",
      "         [-0.2387,  0.0484, -0.6545,  ...,  0.7566,  0.1095, -0.0632],\n",
      "         [-0.6290,  0.3227, -0.4306,  ...,  0.1451,  0.2452, -0.5968],\n",
      "         ...,\n",
      "         [-0.2754, -0.0750, -0.6957,  ...,  0.5880,  0.2422,  0.3664],\n",
      "         [-0.3275, -0.2011, -0.8889,  ...,  0.6194, -0.0131, -0.1552],\n",
      "         [-0.1717, -0.0180, -0.7850,  ...,  0.4038, -0.0121,  0.0543]],\n",
      "\n",
      "        [[-0.2369,  0.0589, -0.4270,  ..., -0.0668,  0.1936,  0.0921],\n",
      "         [-0.7178, -0.3706, -0.2916,  ...,  0.4944,  0.1686, -0.0442],\n",
      "         [-0.6227,  0.0116,  0.0085,  ...,  0.1175,  0.2305, -0.0714],\n",
      "         ...,\n",
      "         [-0.6048,  0.2649, -0.4508,  ...,  0.2129,  0.1280,  0.0737],\n",
      "         [-0.5429, -0.0304, -0.4612,  ...,  0.3240,  0.6538, -0.0062],\n",
      "         [-0.4214,  0.1483, -0.3923,  ...,  0.1088,  0.3089,  0.0476]],\n",
      "\n",
      "        [[ 0.1960, -0.2282, -0.2889,  ..., -0.0039, -0.0151,  0.2185],\n",
      "         [-0.1980, -0.4445, -0.6612,  ...,  0.2908,  0.0284,  0.2911],\n",
      "         [-0.0201, -0.3186, -0.4836,  ..., -0.3463, -0.0904,  0.1757],\n",
      "         ...,\n",
      "         [ 0.2002, -0.3039, -0.5949,  ..., -0.2054, -0.3234,  0.2207],\n",
      "         [ 0.1891, -0.3332, -0.4649,  ..., -0.0693,  0.0029,  0.3271],\n",
      "         [-0.0067,  0.0155, -0.3943,  ..., -0.4469, -0.1109,  0.3401]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1575,  0.0487, -0.4760,  ..., -0.1054, -0.1079,  0.1888],\n",
      "         [ 0.0051, -0.1564, -0.8107,  ...,  0.4097, -0.1400,  0.3567],\n",
      "         [-0.0641,  0.2656, -0.2632,  ..., -0.3067, -0.1016,  0.0311],\n",
      "         ...,\n",
      "         [-0.0560, -0.2836, -0.6098,  ..., -0.0273,  0.1644,  0.4614],\n",
      "         [ 0.0295, -0.3313, -0.4450,  ..., -0.1729, -0.1225,  0.3811],\n",
      "         [-0.1635, -0.1613, -0.3285,  ..., -0.3549, -0.2306,  0.0125]],\n",
      "\n",
      "        [[-0.3985,  0.2515, -0.3542,  ...,  0.5341,  0.2883, -0.2154],\n",
      "         [-0.2624, -0.0954, -0.4824,  ...,  0.9150,  0.3764,  0.0093],\n",
      "         [-0.2720,  0.3359,  0.1744,  ...,  0.3027,  0.0343, -0.2165],\n",
      "         ...,\n",
      "         [-0.2112, -0.0148, -0.1560,  ...,  0.5868,  0.2538, -0.2218],\n",
      "         [-0.4175, -0.0949, -0.2533,  ...,  0.6983,  0.1746, -0.1342],\n",
      "         [-0.6604,  0.1927, -0.0209,  ...,  0.5405,  0.2247, -0.2279]],\n",
      "\n",
      "        [[-0.2404,  0.1919, -0.0502,  ...,  0.4778,  0.0824, -0.2267],\n",
      "         [-0.2925,  0.1907, -0.2747,  ...,  0.8488,  0.1263, -0.0600],\n",
      "         [-0.3201,  0.0834,  0.0393,  ...,  0.4406,  0.2997,  0.0196],\n",
      "         ...,\n",
      "         [-0.2176,  0.0626, -0.2343,  ...,  0.5451,  0.2147,  0.1859],\n",
      "         [-0.3007,  0.1446, -0.2213,  ...,  0.5994,  0.2908, -0.1188],\n",
      "         [-0.4927,  0.2099, -0.1220,  ...,  0.5344,  0.1308, -0.4308]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.4258784055709839\n",
      "Model outputs:  tensor([[[-0.2500, -0.0253, -0.7872,  ...,  0.7405,  0.1413, -0.2191],\n",
      "         [-0.3950,  0.2887, -0.7323,  ...,  0.6135,  0.2628,  0.0855],\n",
      "         [-0.2180, -0.4353, -0.4485,  ...,  0.7914,  0.1820, -0.0157],\n",
      "         ...,\n",
      "         [-0.1731,  0.0712, -0.7465,  ...,  0.8545,  0.3679, -0.0993],\n",
      "         [-0.3025,  0.0926, -0.6046,  ...,  0.8508,  0.4634, -0.0828],\n",
      "         [-0.1110, -0.1463, -0.3602,  ...,  0.6319,  0.2377, -0.1693]],\n",
      "\n",
      "        [[-0.5736,  0.2152, -0.3002,  ...,  0.3803,  0.2279, -0.1042],\n",
      "         [-0.4383,  0.1027, -0.4387,  ...,  0.7983,  0.3243, -0.2291],\n",
      "         [-0.1721, -0.3157, -0.1297,  ...,  0.6938,  0.6610, -0.0907],\n",
      "         ...,\n",
      "         [-0.3807, -0.4633, -0.4532,  ...,  0.6245,  0.1088, -0.0362],\n",
      "         [-0.1452,  0.0372, -0.1754,  ...,  0.5911,  0.1861,  0.0629],\n",
      "         [-0.4216, -0.2031, -0.0147,  ...,  0.7096,  0.3479, -0.0958]],\n",
      "\n",
      "        [[-0.7084,  0.1771, -0.5113,  ...,  0.4053,  0.1711,  0.1093],\n",
      "         [-0.5126, -0.0537, -0.5726,  ...,  0.3198,  0.0736,  0.2798],\n",
      "         [-0.3950, -0.5577, -0.0817,  ...,  0.4491,  0.3676,  0.1211],\n",
      "         ...,\n",
      "         [-0.6480, -0.2253, -0.6318,  ...,  0.1986,  0.0720,  0.1220],\n",
      "         [-0.5469,  0.2275, -0.4274,  ...,  0.2995,  0.3269,  0.2221],\n",
      "         [-0.6603, -0.0582, -0.3721,  ...,  0.2538,  0.1132,  0.5123]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0416,  0.0147, -0.3792,  ..., -0.4039,  0.4354,  0.4528],\n",
      "         [ 0.3403, -0.2444, -0.4015,  ..., -0.5317,  0.1416,  0.2694],\n",
      "         [ 0.1263, -0.0792, -0.3397,  ..., -0.1112,  0.3394,  0.1868],\n",
      "         ...,\n",
      "         [ 0.2809, -0.4224, -0.2277,  ..., -0.1898,  0.0455, -0.0538],\n",
      "         [ 0.2967, -0.3135, -0.3344,  ..., -0.4823,  0.2354,  0.0818],\n",
      "         [ 0.1718, -0.1612, -0.2246,  ..., -0.1158,  0.3480,  0.1609]],\n",
      "\n",
      "        [[ 0.0718, -0.0793, -0.6583,  ...,  0.0770,  0.0853,  0.1737],\n",
      "         [ 0.0270, -0.0890, -0.4338,  ..., -0.0681,  0.0980,  0.1770],\n",
      "         [-0.0128, -0.1932, -0.3693,  ...,  0.1655, -0.1583,  0.2461],\n",
      "         ...,\n",
      "         [-0.1838, -0.1825, -0.6453,  ...,  0.1047, -0.2391,  0.2737],\n",
      "         [-0.0782, -0.0182, -0.5398,  ...,  0.1688, -0.0379,  0.1957],\n",
      "         [-0.0287, -0.1989, -0.5932,  ..., -0.0846,  0.0249,  0.1973]],\n",
      "\n",
      "        [[-0.4394,  0.1112, -0.0132,  ...,  0.7057,  0.2560, -0.2822],\n",
      "         [-0.6279,  0.0111, -0.1138,  ...,  0.5685,  0.1231,  0.0280],\n",
      "         [-0.1779, -0.2752, -0.1264,  ...,  0.5473,  0.3652,  0.0602],\n",
      "         ...,\n",
      "         [-0.4023, -0.4451,  0.0590,  ...,  0.7547,  0.1740, -0.0484],\n",
      "         [-0.3742,  0.2566, -0.1256,  ...,  0.7587,  0.3768, -0.1702],\n",
      "         [-0.3603, -0.0465, -0.0843,  ...,  0.5669,  0.4837, -0.0364]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2338675260543823\n",
      "Model outputs:  tensor([[[ 0.0268, -0.3552, -0.4947,  ..., -0.1191, -0.0716,  0.2819],\n",
      "         [ 0.0021, -0.2266, -0.6223,  ...,  0.1347,  0.1792,  0.2720],\n",
      "         [-0.0644, -0.2439, -0.3520,  ..., -0.1195,  0.0278, -0.0421],\n",
      "         ...,\n",
      "         [-0.1842, -0.0372, -0.4712,  ...,  0.0154, -0.4809,  0.1039],\n",
      "         [-0.1263, -0.3172, -0.5197,  ...,  0.0156, -0.2910,  0.0935],\n",
      "         [-0.2514, -0.2443, -0.1645,  ..., -0.2285,  0.1241,  0.0262]],\n",
      "\n",
      "        [[ 0.0606, -0.4713, -0.1009,  ..., -0.0073,  0.1665,  0.2733],\n",
      "         [ 0.0687, -0.3397, -0.6370,  ...,  0.2633, -0.1061,  0.5103],\n",
      "         [-0.1199, -0.0207, -0.5593,  ..., -0.3121, -0.0724,  0.0867],\n",
      "         ...,\n",
      "         [-0.2691, -0.2764, -0.5733,  ..., -0.0987, -0.4432,  0.2741],\n",
      "         [-0.2164, -0.4324, -0.3476,  ...,  0.1090, -0.1436,  0.0850],\n",
      "         [-0.0853, -0.0276, -0.5116,  ..., -0.2856, -0.0419,  0.3291]],\n",
      "\n",
      "        [[ 0.1186, -0.5107, -0.3299,  ..., -0.4261,  0.3994, -0.0038],\n",
      "         [ 0.0854, -0.3381, -0.3083,  ..., -0.1231,  0.2242,  0.0087],\n",
      "         [ 0.0907, -0.1448, -0.2222,  ..., -0.6027,  0.4531, -0.0934],\n",
      "         ...,\n",
      "         [-0.3452,  0.0216, -0.7919,  ...,  0.9826,  0.0388, -0.3199],\n",
      "         [-0.4273, -0.4063, -0.4909,  ...,  0.7508, -0.1991, -0.3522],\n",
      "         [-0.5550,  0.2607, -0.5777,  ...,  0.6157,  0.0269, -0.1269]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2012, -0.3669, -0.3117,  ..., -0.0239, -0.1940, -0.2701],\n",
      "         [-0.1842, -0.3128, -0.5039,  ...,  0.1132, -0.1559, -0.1958],\n",
      "         [-0.1773, -0.1401, -0.2970,  ..., -0.3258, -0.0452, -0.3420],\n",
      "         ...,\n",
      "         [-0.2698, -0.1071, -0.3806,  ..., -0.1995, -0.2425, -0.3479],\n",
      "         [-0.1592, -0.4411, -0.2110,  ...,  0.4309,  0.0229, -0.4021],\n",
      "         [ 0.0539, -0.0269, -0.4307,  ..., -0.1957, -0.0485, -0.1306]],\n",
      "\n",
      "        [[-0.1618, -0.0479, -0.4960,  ...,  0.3534, -0.1512,  0.0741],\n",
      "         [-0.2299,  0.1099, -0.7664,  ...,  0.2735,  0.0092,  0.0816],\n",
      "         [-0.1082,  0.1029, -0.6678,  ...,  0.3008,  0.2186, -0.2795],\n",
      "         ...,\n",
      "         [-0.0568, -0.0351, -0.5531,  ...,  0.3834, -0.2986,  0.1050],\n",
      "         [-0.1638, -0.1113, -0.4918,  ...,  0.4790, -0.1154,  0.1123],\n",
      "         [-0.3926, -0.2290, -0.3813,  ...,  0.1781, -0.4024, -0.1111]],\n",
      "\n",
      "        [[-0.0360, -0.3900, -0.4051,  ..., -0.2623,  0.4865,  0.2504],\n",
      "         [ 0.0770, -0.2166, -0.5052,  ..., -0.4181,  0.6632,  0.6458],\n",
      "         [ 0.0269, -0.2258, -0.4786,  ..., -0.6238,  0.7070,  0.4512],\n",
      "         ...,\n",
      "         [ 0.1811, -0.0767, -0.4382,  ..., -0.1986,  0.4507,  0.3007],\n",
      "         [-0.3351, -0.3055, -0.1894,  ..., -0.2938,  0.2656,  0.3378],\n",
      "         [-0.2702, -0.0673, -0.4588,  ..., -0.4558,  0.5859,  0.5503]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0818878412246704\n",
      "Model outputs:  tensor([[[-2.1773e-01, -1.5875e-01, -5.0578e-01,  ...,  2.6916e-01,\n",
      "           4.5665e-02,  1.2985e-02],\n",
      "         [-1.2264e-01, -1.2774e-01, -5.1971e-01,  ...,  9.4299e-02,\n",
      "          -6.2419e-02,  1.0988e-01],\n",
      "         [-2.6430e-01, -5.4425e-02, -4.0969e-01,  ...,  1.0035e-01,\n",
      "          -1.2738e-02, -1.4740e-02],\n",
      "         ...,\n",
      "         [-6.2098e-01, -1.7175e-01, -4.8873e-01,  ...,  1.2493e-01,\n",
      "          -7.3975e-01, -3.3592e-02],\n",
      "         [-3.2474e-01,  1.8128e-01, -7.2385e-01,  ...,  4.7167e-02,\n",
      "          -1.1114e-01,  3.8649e-02],\n",
      "         [-2.2890e-01, -7.6790e-02, -6.0928e-01,  ...,  3.5944e-02,\n",
      "           1.2584e-01,  1.8321e-01]],\n",
      "\n",
      "        [[ 9.1427e-02, -4.0567e-01, -2.4985e-01,  ..., -2.0097e-01,\n",
      "          -3.3606e-01,  8.1870e-02],\n",
      "         [ 2.0436e-01, -4.8796e-01, -1.3190e-01,  ..., -3.5710e-01,\n",
      "          -3.4810e-01,  4.7829e-01],\n",
      "         [ 1.7435e-01, -9.2694e-02, -1.1518e-01,  ..., -5.1447e-01,\n",
      "          -1.5212e-01,  4.4481e-01],\n",
      "         ...,\n",
      "         [-3.5818e-01, -9.2023e-02, -3.1855e-01,  ..., -4.1317e-01,\n",
      "          -7.8228e-01,  5.5975e-01],\n",
      "         [-1.5495e-01,  4.3759e-02, -1.2465e-01,  ..., -2.8519e-01,\n",
      "          -2.3926e-01,  1.6536e-01],\n",
      "         [-2.1831e-02, -2.7412e-01, -2.9304e-01,  ..., -9.3022e-02,\n",
      "          -2.8381e-01,  3.6787e-01]],\n",
      "\n",
      "        [[-1.4997e-01,  2.1429e-02, -7.2303e-01,  ...,  2.3376e-01,\n",
      "          -3.1267e-02,  7.7946e-03],\n",
      "         [-1.0189e-02, -4.0122e-01, -4.8235e-01,  ...,  1.5532e-01,\n",
      "           6.6016e-02,  1.5991e-01],\n",
      "         [-3.2140e-01, -2.2519e-02, -5.2846e-01,  ...,  3.3765e-01,\n",
      "          -2.5754e-01, -1.8302e-02],\n",
      "         ...,\n",
      "         [-6.8182e-01,  7.7227e-02, -5.7896e-01,  ...,  2.7110e-01,\n",
      "          -5.2382e-01,  1.4544e-02],\n",
      "         [-6.1757e-01,  2.6424e-01, -5.2052e-01,  ...,  2.3013e-01,\n",
      "           1.8203e-01, -3.2710e-01],\n",
      "         [-8.4479e-01,  9.8246e-04, -5.2344e-01,  ...,  2.5021e-01,\n",
      "          -1.3988e-01, -1.8040e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.6732e-01, -4.5788e-02,  5.2719e-02,  ...,  5.2883e-01,\n",
      "           1.2469e-01,  2.0400e-02],\n",
      "         [-4.2172e-01, -3.4825e-01, -5.3838e-02,  ...,  2.1125e-01,\n",
      "           2.1760e-01,  1.6215e-01],\n",
      "         [-4.4904e-01, -2.4135e-01, -3.6126e-02,  ...,  1.4651e-01,\n",
      "           9.6089e-02, -5.9594e-02],\n",
      "         ...,\n",
      "         [-8.9922e-01, -6.6106e-02, -3.2543e-01,  ...,  3.3351e-01,\n",
      "          -4.4752e-01, -2.0717e-01],\n",
      "         [-5.8445e-01,  8.1843e-02, -1.1482e-01,  ...,  2.7643e-01,\n",
      "           1.5093e-01, -3.6016e-01],\n",
      "         [-3.0922e-01, -1.3961e-01, -9.3288e-02,  ...,  1.3752e-01,\n",
      "          -4.4405e-02,  4.4241e-02]],\n",
      "\n",
      "        [[-3.8281e-01,  1.3243e-01, -1.3953e-03,  ...,  6.0522e-01,\n",
      "           1.4132e-01, -2.1803e-01],\n",
      "         [-2.7753e-01, -3.8474e-01,  2.3483e-01,  ...,  6.5591e-01,\n",
      "           1.0021e-01,  7.3806e-02],\n",
      "         [-3.2988e-01,  5.8401e-03,  8.7267e-02,  ...,  6.7707e-01,\n",
      "           3.1860e-01, -1.3092e-01],\n",
      "         ...,\n",
      "         [-5.8115e-01,  2.2936e-01,  2.8873e-02,  ...,  5.3799e-01,\n",
      "          -2.6769e-01,  2.0454e-01],\n",
      "         [-4.6831e-01,  3.3459e-02, -1.9758e-01,  ...,  7.1346e-01,\n",
      "           2.2351e-01, -1.4465e-01],\n",
      "         [-4.4275e-01, -4.7968e-02, -1.3581e-01,  ...,  5.4115e-01,\n",
      "           2.4973e-02, -2.3169e-01]],\n",
      "\n",
      "        [[-4.5231e-01,  2.7280e-01,  1.1733e-01,  ...,  5.0176e-01,\n",
      "           1.2722e-01, -1.9020e-01],\n",
      "         [-2.3968e-01, -1.0269e-01,  8.2438e-02,  ...,  4.2841e-01,\n",
      "           2.2037e-01,  1.1655e-02],\n",
      "         [-1.2662e-01, -1.8766e-01,  5.4984e-02,  ...,  5.4806e-01,\n",
      "           1.4072e-01, -1.7997e-01],\n",
      "         ...,\n",
      "         [-8.5295e-01,  8.8634e-02, -2.1525e-01,  ...,  5.2889e-01,\n",
      "          -4.4667e-01,  1.7306e-04],\n",
      "         [-6.0639e-01, -5.7613e-02, -9.5544e-02,  ...,  3.0821e-01,\n",
      "           3.5085e-02, -3.8181e-01],\n",
      "         [-6.2372e-01, -1.7193e-01, -5.3704e-02,  ...,  4.0927e-01,\n",
      "           1.4343e-01, -1.7087e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0123411417007446\n",
      "Model outputs:  tensor([[[-4.9875e-01,  1.0073e-01, -7.4611e-01,  ...,  2.8255e-01,\n",
      "          -8.3941e-02, -1.1254e-01],\n",
      "         [-5.9989e-01, -2.1192e-02, -6.2177e-01,  ...,  4.6707e-01,\n",
      "          -7.1647e-03, -1.8864e-01],\n",
      "         [-5.8525e-01,  2.4003e-01, -8.0760e-01,  ...,  3.7556e-02,\n",
      "           1.1106e-01, -2.4787e-01],\n",
      "         ...,\n",
      "         [-5.2811e-01, -2.7176e-01, -7.5995e-01,  ...,  4.4605e-01,\n",
      "          -3.1329e-01,  1.7834e-02],\n",
      "         [-6.2616e-01, -4.2773e-02, -8.6117e-01,  ...,  3.7864e-01,\n",
      "          -4.3493e-01,  3.6487e-03],\n",
      "         [-1.5650e-01, -2.3245e-01, -4.3013e-01,  ...,  4.3251e-01,\n",
      "          -5.5682e-02, -1.9049e-01]],\n",
      "\n",
      "        [[-3.3046e-01, -9.8321e-02, -6.6400e-01,  ...,  1.2921e-01,\n",
      "          -3.4680e-01,  1.0646e-01],\n",
      "         [-3.7112e-01,  6.1342e-02, -5.8915e-01,  ...,  2.4440e-01,\n",
      "          -5.6024e-02, -3.8852e-01],\n",
      "         [-2.5062e-01,  1.5524e-01, -3.8584e-01,  ...,  3.1148e-03,\n",
      "          -3.7888e-01, -5.1510e-01],\n",
      "         ...,\n",
      "         [-4.9662e-01, -1.9142e-01, -6.9907e-01,  ...,  3.5958e-01,\n",
      "          -1.6239e-01,  1.1071e-01],\n",
      "         [-9.0865e-02, -3.4278e-02, -7.8050e-01,  ...,  1.7416e-01,\n",
      "          -1.7577e-01,  5.3820e-01],\n",
      "         [-6.7468e-02, -2.1479e-01, -3.6415e-01,  ...,  2.6132e-01,\n",
      "           2.3871e-03, -1.3942e-01]],\n",
      "\n",
      "        [[-3.8372e-01,  1.8842e-01, -5.7987e-01,  ...,  1.8408e-01,\n",
      "          -2.3429e-01, -1.7322e-01],\n",
      "         [-4.2087e-01, -1.4806e-01, -4.5766e-01,  ...,  2.3203e-01,\n",
      "          -2.5766e-01, -2.9465e-01],\n",
      "         [-1.5719e-01,  2.9291e-02, -6.0223e-01,  ..., -1.1714e-01,\n",
      "          -1.8663e-01, -2.2707e-01],\n",
      "         ...,\n",
      "         [-2.5198e-01, -1.7423e-01, -7.0600e-01,  ...,  2.6434e-01,\n",
      "          -2.0669e-01,  6.4247e-02],\n",
      "         [-1.6309e-01, -1.0520e-01, -9.4328e-01,  ...,  2.5483e-01,\n",
      "          -1.5956e-01,  1.7015e-01],\n",
      "         [ 1.1219e-01, -2.9829e-02, -3.5227e-01,  ...,  2.9200e-01,\n",
      "          -4.8190e-01, -1.7219e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.7896e-01,  4.2942e-02, -1.8395e-01,  ...,  1.5242e-01,\n",
      "          -4.3995e-02, -1.9196e-01],\n",
      "         [-7.7568e-01,  3.1404e-01, -2.5934e-01,  ...,  3.1195e-01,\n",
      "           1.4112e-01, -1.8868e-01],\n",
      "         [-4.1884e-01, -5.7465e-02, -2.0420e-01,  ...,  1.6113e-01,\n",
      "           3.6078e-02, -1.1192e-01],\n",
      "         ...,\n",
      "         [-5.2369e-01, -2.7000e-01, -3.3885e-01,  ...,  7.1952e-01,\n",
      "           1.4061e-01, -1.1626e-01],\n",
      "         [-7.6371e-01,  1.1685e-01, -5.7520e-01,  ...,  4.9176e-01,\n",
      "           1.3955e-02, -2.2902e-01],\n",
      "         [ 2.5526e-01, -4.1202e-01,  1.0446e-01,  ...,  2.6940e-01,\n",
      "           2.4855e-01, -5.3662e-02]],\n",
      "\n",
      "        [[-6.3900e-01,  2.6274e-01, -5.5097e-01,  ..., -1.6184e-01,\n",
      "          -2.9790e-01,  9.8455e-02],\n",
      "         [-7.6981e-01,  3.1485e-01, -5.3172e-01,  ...,  2.6947e-01,\n",
      "          -1.2062e-02,  1.6142e-02],\n",
      "         [-4.7084e-01,  3.6213e-01, -4.4907e-01,  ..., -9.4876e-02,\n",
      "           3.9812e-01, -9.6249e-03],\n",
      "         ...,\n",
      "         [-6.9385e-01, -1.6929e-01, -6.8277e-01,  ...,  1.1806e-01,\n",
      "          -6.4652e-03, -1.1565e-02],\n",
      "         [-8.4684e-01,  8.3944e-02, -7.5829e-01,  ...,  4.0689e-01,\n",
      "           1.3545e-01,  4.0906e-01],\n",
      "         [ 5.0997e-02, -5.2971e-01, -1.6044e-01,  ...,  3.3946e-02,\n",
      "           2.7566e-02, -7.2516e-04]],\n",
      "\n",
      "        [[ 8.0195e-03, -7.5873e-02, -5.6159e-01,  ..., -1.2658e-01,\n",
      "          -1.9871e-01, -1.1751e-01],\n",
      "         [-5.5257e-02, -2.5802e-01, -4.4250e-01,  ...,  1.9051e-03,\n",
      "          -7.5510e-02, -4.9074e-01],\n",
      "         [ 6.4820e-02, -1.9715e-01, -3.2991e-01,  ..., -4.1261e-01,\n",
      "          -1.1665e-01, -3.0837e-01],\n",
      "         ...,\n",
      "         [ 1.5416e-02, -1.5029e-01, -4.6452e-01,  ...,  2.8781e-01,\n",
      "           5.3913e-02, -4.9068e-02],\n",
      "         [-1.3002e-01, -2.8224e-01, -6.2918e-01,  ...,  1.7783e-01,\n",
      "          -2.7041e-01, -1.7382e-01],\n",
      "         [-1.1387e-01, -5.4816e-01, -1.6653e-01,  ...,  1.3100e-01,\n",
      "          -1.1301e-01, -2.4334e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1420553922653198\n",
      "Model outputs:  tensor([[[-7.3916e-02, -1.5217e-01, -6.1788e-01,  ..., -2.8479e-02,\n",
      "          -9.9205e-02,  4.0487e-01],\n",
      "         [-4.3092e-02, -2.2224e-01, -3.0564e-01,  ..., -1.9126e-01,\n",
      "          -8.1890e-02,  5.0344e-01],\n",
      "         [-1.3651e-01, -2.0326e-01, -3.4838e-01,  ...,  2.2579e-01,\n",
      "          -1.9304e-02,  3.9191e-01],\n",
      "         ...,\n",
      "         [ 6.6913e-02, -2.7681e-02, -1.7271e-01,  ..., -2.2979e-01,\n",
      "          -1.3312e-01, -5.3835e-02],\n",
      "         [-1.4363e-01, -1.8870e-01, -3.5210e-01,  ..., -4.7194e-01,\n",
      "          -1.5713e-01,  1.2419e-01],\n",
      "         [ 1.5507e-01, -8.4431e-02, -2.2687e-01,  ..., -2.6415e-01,\n",
      "          -3.8417e-01,  2.6632e-01]],\n",
      "\n",
      "        [[ 8.6613e-02, -1.4218e-01, -4.2747e-01,  ..., -5.2690e-02,\n",
      "           7.7581e-01,  5.1517e-01],\n",
      "         [-1.1883e-01, -1.4725e-01, -4.8012e-01,  ..., -4.0362e-01,\n",
      "           4.7179e-01,  1.6979e-01],\n",
      "         [-1.0897e-01, -9.5422e-02, -4.4229e-01,  ..., -1.1892e-01,\n",
      "           3.8900e-01,  1.9386e-01],\n",
      "         ...,\n",
      "         [-2.4277e-01, -9.6620e-02, -2.4542e-01,  ..., -3.4798e-01,\n",
      "           7.8433e-01,  1.8199e-01],\n",
      "         [-1.2863e-01, -7.7370e-02, -4.2749e-01,  ..., -4.9568e-01,\n",
      "           6.7955e-01,  2.4035e-01],\n",
      "         [-1.3148e-01, -3.5085e-02, -5.8414e-01,  ..., -7.7840e-01,\n",
      "           3.3357e-01,  5.3080e-01]],\n",
      "\n",
      "        [[-1.5733e-01, -1.4822e-01, -4.4608e-01,  ...,  4.9243e-02,\n",
      "          -1.4928e-01,  2.9785e-01],\n",
      "         [-1.4595e-01,  2.0544e-02, -3.7736e-01,  ..., -5.3410e-03,\n",
      "          -2.5281e-01,  1.9026e-01],\n",
      "         [-2.4100e-01, -1.2437e-01, -4.5123e-01,  ...,  3.0149e-01,\n",
      "          -3.2522e-01,  2.4967e-01],\n",
      "         ...,\n",
      "         [-1.8663e-01,  2.0845e-02, -2.4663e-01,  ..., -3.6768e-01,\n",
      "           5.0691e-02, -3.0994e-01],\n",
      "         [ 1.4623e-02,  1.4173e-02, -3.1378e-01,  ..., -3.9340e-01,\n",
      "          -2.7761e-01,  1.9363e-01],\n",
      "         [ 6.2720e-02, -2.2652e-01, -2.2822e-01,  ..., -4.3750e-01,\n",
      "          -6.4199e-02,  2.8148e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.3723e-01, -1.1197e-01, -2.5922e-01,  ...,  5.6202e-01,\n",
      "           2.3959e-02, -3.3445e-01],\n",
      "         [ 1.9413e-01, -7.8396e-02,  5.0258e-03,  ...,  3.9144e-01,\n",
      "          -1.7983e-02, -5.8703e-01],\n",
      "         [ 2.2159e-01, -4.3379e-01, -4.1537e-01,  ...,  6.0507e-01,\n",
      "           1.1942e-02, -3.5499e-01],\n",
      "         ...,\n",
      "         [ 2.4092e-02, -3.1457e-02,  7.2713e-02,  ...,  5.3498e-01,\n",
      "           9.7809e-02, -8.2061e-01],\n",
      "         [ 3.2843e-02, -1.5431e-01,  8.7029e-02,  ...,  2.4199e-01,\n",
      "          -2.1360e-01, -8.8741e-01],\n",
      "         [-1.6661e-01,  1.5204e-02,  8.3347e-02,  ...,  5.0752e-01,\n",
      "          -1.4006e-01, -9.4831e-01]],\n",
      "\n",
      "        [[ 1.5610e-04, -3.8213e-01,  1.0120e-03,  ...,  4.0649e-01,\n",
      "          -8.3330e-03, -6.3774e-01],\n",
      "         [-1.2303e-01, -1.4640e-01,  8.2203e-02,  ...,  6.1787e-01,\n",
      "          -5.2556e-02, -5.5004e-01],\n",
      "         [-5.1347e-02, -2.6633e-01, -1.7900e-01,  ...,  6.7103e-01,\n",
      "           7.6163e-02, -3.6343e-01],\n",
      "         ...,\n",
      "         [-4.1022e-02, -1.6530e-01,  1.0330e-02,  ...,  5.0622e-01,\n",
      "           5.9054e-02, -8.2675e-01],\n",
      "         [ 1.1794e-01, -6.4369e-02, -5.8308e-02,  ...,  3.1816e-01,\n",
      "           6.5196e-02, -7.6980e-01],\n",
      "         [ 7.4869e-03, -2.1732e-01, -4.4410e-03,  ...,  4.7760e-01,\n",
      "          -1.7854e-01, -7.8356e-01]],\n",
      "\n",
      "        [[ 1.3009e-01, -3.7018e-01, -1.0999e-01,  ...,  3.5928e-01,\n",
      "           1.4168e-01, -3.0076e-01],\n",
      "         [ 5.3645e-03, -2.6341e-01,  5.7985e-02,  ...,  4.7070e-01,\n",
      "          -1.5754e-01, -5.3606e-01],\n",
      "         [-1.0442e-01, -3.5212e-01, -1.7906e-01,  ...,  5.7980e-01,\n",
      "           4.0313e-02, -3.2485e-01],\n",
      "         ...,\n",
      "         [-3.7871e-02, -1.4256e-01, -5.2157e-02,  ...,  5.5317e-01,\n",
      "           2.2101e-02, -8.4505e-01],\n",
      "         [ 1.5846e-01, -2.0756e-01, -1.8229e-03,  ...,  2.0971e-01,\n",
      "           1.4724e-01, -1.0241e+00],\n",
      "         [-1.0274e-02, -1.0889e-01,  2.1291e-01,  ...,  2.3231e-01,\n",
      "          -2.1726e-02, -6.2371e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2354679107666016\n",
      "Model outputs:  tensor([[[-0.2216,  0.1228, -1.1155,  ...,  0.3750, -0.1197, -0.0120],\n",
      "         [-0.1673, -0.0847, -0.5390,  ...,  0.6489,  0.0814, -0.1073],\n",
      "         [-0.3785,  0.3161, -0.8093,  ...,  0.3527,  0.0920, -0.1177],\n",
      "         ...,\n",
      "         [-0.0816,  0.2460, -0.7092,  ...,  0.4990,  0.1141, -0.1911],\n",
      "         [-0.0934, -0.0474, -0.5561,  ...,  0.3659,  0.1580,  0.0974],\n",
      "         [-0.2653,  0.2318, -0.6189,  ...,  0.2009,  0.1836, -0.2802]],\n",
      "\n",
      "        [[-0.4963,  0.2747, -0.4228,  ...,  0.8635,  0.0906, -0.0640],\n",
      "         [-0.4074,  0.2782, -0.3277,  ...,  0.6798,  0.2990,  0.0447],\n",
      "         [-0.4852,  0.6879, -0.0350,  ...,  0.7287,  0.4692, -0.4341],\n",
      "         ...,\n",
      "         [-0.3519,  0.2555, -0.6456,  ...,  0.4717,  0.3197, -0.1650],\n",
      "         [-0.2500, -0.0584, -0.4468,  ...,  0.8304,  0.3615, -0.0132],\n",
      "         [-0.3753,  0.1563, -0.0698,  ...,  0.4576,  0.2230, -0.3280]],\n",
      "\n",
      "        [[-0.3820,  0.1538, -0.7409,  ...,  0.3382, -0.3000,  0.4662],\n",
      "         [-0.1472, -0.0841, -0.1200,  ...,  0.0341, -0.2031,  0.2415],\n",
      "         [-0.1715,  0.1740, -0.3576,  ..., -0.0397, -0.2343,  0.1522],\n",
      "         ...,\n",
      "         [-0.0565, -0.1097, -0.2482,  ..., -0.1056, -0.2168,  0.1541],\n",
      "         [-0.0741, -0.1338, -0.4689,  ..., -0.1039, -0.1157,  0.5159],\n",
      "         [-0.1321, -0.1370, -0.7161,  ..., -0.3315, -0.4093,  0.1940]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7923,  0.2048, -0.2502,  ...,  0.6793,  0.1605, -0.3432],\n",
      "         [-0.4316,  0.2039, -0.1149,  ...,  0.8532,  0.3255, -0.2697],\n",
      "         [-0.5507,  0.6179, -0.2114,  ...,  0.9062,  0.5638, -0.3270],\n",
      "         ...,\n",
      "         [-0.2165,  0.1727, -0.1162,  ...,  0.6770,  0.2629, -0.2063],\n",
      "         [-0.2441, -0.0177,  0.0784,  ...,  0.6761,  0.4735, -0.1605],\n",
      "         [-0.2526,  0.0911,  0.0567,  ...,  0.6291,  0.2443, -0.3978]],\n",
      "\n",
      "        [[-0.5283,  0.0278, -0.7900,  ...,  0.0590, -0.0752,  0.2524],\n",
      "         [-0.3175,  0.0539, -0.3732,  ...,  0.2866,  0.1065,  0.1191],\n",
      "         [-0.1533,  0.0539, -0.3479,  ..., -0.0456,  0.0475,  0.1506],\n",
      "         ...,\n",
      "         [-0.1634, -0.0495, -0.3751,  ..., -0.0548, -0.0573,  0.0692],\n",
      "         [-0.1594, -0.1718, -0.4746,  ...,  0.1701, -0.0291,  0.3587],\n",
      "         [-0.2550, -0.0226, -0.4282,  ..., -0.3720, -0.0218,  0.0587]],\n",
      "\n",
      "        [[-0.0837, -0.0356, -0.4368,  ..., -0.1126,  0.1457, -0.5545],\n",
      "         [-0.3090, -0.2052,  0.0468,  ...,  0.3431, -0.0019, -0.5043],\n",
      "         [-0.0566,  0.0603, -0.1927,  ...,  0.1292,  0.2209, -0.7153],\n",
      "         ...,\n",
      "         [-0.0231, -0.0191, -0.2871,  ..., -0.4920,  0.2799,  0.2286],\n",
      "         [ 0.1046, -0.5339, -0.0815,  ..., -0.4041,  0.4536, -0.0098],\n",
      "         [ 0.1157, -0.2259, -0.1774,  ..., -0.3707,  0.3225, -0.4009]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.090872049331665\n",
      "Model outputs:  tensor([[[ 2.1072e-01, -3.5456e-01, -3.7479e-01,  ...,  4.0082e-02,\n",
      "          -4.6492e-01,  7.2765e-01],\n",
      "         [ 1.1999e-01, -4.9231e-01, -3.4987e-01,  ..., -2.2026e-01,\n",
      "          -2.0529e-01,  2.6952e-01],\n",
      "         [ 1.1885e-01, -8.9827e-02, -3.7354e-01,  ..., -2.6345e-01,\n",
      "          -2.0870e-01,  1.3966e-01],\n",
      "         ...,\n",
      "         [-4.2169e-02, -2.0517e-01, -1.6023e-01,  ..., -3.2761e-01,\n",
      "          -2.7132e-01,  1.3206e-01],\n",
      "         [-1.0905e-01, -3.0662e-01, -2.0948e-01,  ..., -5.2071e-02,\n",
      "          -1.3654e-01,  3.7089e-01],\n",
      "         [ 8.9198e-02, -5.3553e-01, -4.3053e-01,  ..., -1.6606e-01,\n",
      "          -2.4540e-01,  3.5606e-01]],\n",
      "\n",
      "        [[-4.4818e-01, -1.4775e-01, -1.9914e-01,  ...,  6.7281e-01,\n",
      "           3.4580e-01, -2.6711e-01],\n",
      "         [-3.7576e-01,  1.9863e-02,  4.6975e-02,  ...,  5.9605e-01,\n",
      "           5.1506e-01, -3.4454e-01],\n",
      "         [-5.0260e-01, -8.0273e-02, -3.3326e-02,  ...,  6.0329e-01,\n",
      "           7.1463e-02, -1.8993e-01],\n",
      "         ...,\n",
      "         [-5.2403e-01,  1.3604e-01, -3.0677e-01,  ...,  3.9544e-01,\n",
      "           2.2775e-01, -2.2842e-01],\n",
      "         [-3.5505e-01, -1.1684e-01, -2.5551e-01,  ...,  5.3499e-01,\n",
      "           1.2547e-01, -7.8477e-02],\n",
      "         [-3.7407e-01,  2.3703e-01, -3.8902e-01,  ...,  5.7254e-01,\n",
      "           3.3393e-01, -5.3053e-02]],\n",
      "\n",
      "        [[-8.2338e-03, -2.8974e-01, -4.6785e-01,  ...,  3.5199e-01,\n",
      "          -3.3673e-02,  3.4922e-01],\n",
      "         [-1.5984e-01, -2.3123e-01, -3.4161e-01,  ..., -1.6724e-01,\n",
      "           6.5866e-02,  2.4679e-02],\n",
      "         [-2.7004e-01, -3.0765e-01, -5.5079e-01,  ...,  2.1966e-02,\n",
      "           1.0502e-01,  3.0206e-03],\n",
      "         ...,\n",
      "         [-8.7289e-02,  7.3842e-02, -2.6656e-01,  ..., -1.6731e-01,\n",
      "           4.5143e-03, -1.8401e-01],\n",
      "         [ 1.0777e-01, -1.6955e-01, -2.8463e-01,  ...,  1.0960e-01,\n",
      "          -2.0846e-02,  5.0098e-02],\n",
      "         [-4.6265e-02, -1.8462e-01, -2.8996e-01,  ...,  1.1526e-01,\n",
      "           9.7544e-02,  1.6124e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.4151e-03, -4.0577e-01, -7.5258e-01,  ...,  4.9123e-01,\n",
      "          -5.3216e-02,  5.6339e-02],\n",
      "         [-3.7921e-01,  1.9869e-01, -6.7556e-01,  ...,  3.3302e-01,\n",
      "           7.2519e-03, -3.6083e-02],\n",
      "         [-1.5854e-01,  7.2457e-02, -5.9440e-01,  ...,  2.5119e-01,\n",
      "          -1.1254e-01,  9.8868e-03],\n",
      "         ...,\n",
      "         [-3.0191e-01,  9.3157e-02, -6.6636e-01,  ...,  7.4408e-02,\n",
      "          -4.4614e-02,  5.2572e-02],\n",
      "         [-1.5031e-01, -2.7517e-01, -6.5105e-01,  ...,  5.2309e-01,\n",
      "          -6.3785e-02,  1.2403e-01],\n",
      "         [-1.7085e-01, -2.8013e-01, -6.2176e-01,  ...,  5.0641e-01,\n",
      "           1.7150e-01,  1.1731e-01]],\n",
      "\n",
      "        [[-5.0937e-01, -2.9073e-02, -3.9578e-01,  ...,  8.3699e-01,\n",
      "           2.9048e-01,  1.2971e-01],\n",
      "         [-6.1823e-01,  1.1513e-01, -4.0476e-01,  ...,  3.5365e-01,\n",
      "           1.6996e-01,  6.0163e-02],\n",
      "         [-2.5951e-01, -7.0625e-02, -6.4764e-01,  ...,  6.6905e-01,\n",
      "           1.0994e-01,  1.1827e-01],\n",
      "         ...,\n",
      "         [-6.4537e-01,  1.7382e-01, -6.2302e-01,  ...,  5.4251e-01,\n",
      "          -4.4010e-02, -1.1262e-01],\n",
      "         [-5.5932e-01, -9.1172e-02, -5.6470e-01,  ...,  6.3933e-01,\n",
      "           2.1125e-01,  1.5098e-02],\n",
      "         [-3.2508e-01,  9.3256e-02, -4.3381e-01,  ...,  8.2634e-01,\n",
      "           4.3668e-01, -1.3894e-01]],\n",
      "\n",
      "        [[-1.9332e-01, -2.6646e-01, -4.3265e-01,  ..., -2.1353e-01,\n",
      "          -3.8276e-01,  2.2469e-01],\n",
      "         [ 5.2923e-04, -1.9361e-01, -1.3011e-02,  ..., -1.5166e-01,\n",
      "          -3.6561e-01,  1.4460e-01],\n",
      "         [ 1.3272e-02, -1.6396e-01, -2.8309e-01,  ..., -1.9561e-01,\n",
      "          -3.1237e-01,  2.3723e-01],\n",
      "         ...,\n",
      "         [ 8.9502e-02, -5.1295e-02, -2.1672e-01,  ..., -2.2493e-01,\n",
      "          -3.8062e-01,  2.4101e-01],\n",
      "         [ 3.8613e-03, -3.2575e-01, -2.9613e-01,  ...,  1.1369e-01,\n",
      "          -2.1497e-01,  2.8647e-01],\n",
      "         [-2.9634e-02, -2.7308e-01, -3.4164e-01,  ...,  4.6850e-02,\n",
      "          -3.7407e-01,  2.3190e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8652873635292053\n",
      "Model outputs:  tensor([[[-0.3052,  0.0401, -0.5928,  ..., -0.1811,  0.2741, -0.0637],\n",
      "         [-0.3071, -0.1466, -0.4545,  ...,  0.2808,  0.2592, -0.0305],\n",
      "         [-0.5880,  0.1995, -0.6767,  ..., -0.0039,  0.1660,  0.0913],\n",
      "         ...,\n",
      "         [-0.3947,  0.0843, -0.5788,  ...,  0.4381,  0.2168,  0.1734],\n",
      "         [-0.4253, -0.0647, -0.4838,  ...,  0.0660,  0.0807,  0.0791],\n",
      "         [-0.4627, -0.0487, -0.3824,  ...,  0.0745,  0.1625,  0.0300]],\n",
      "\n",
      "        [[-0.6516,  0.2662, -0.5537,  ...,  0.0464,  0.0630, -0.2230],\n",
      "         [-0.5972, -0.1415, -0.5142,  ...,  0.0106,  0.2697, -0.0875],\n",
      "         [-0.5784,  0.0712, -0.5769,  ...,  0.1215,  0.1784, -0.1890],\n",
      "         ...,\n",
      "         [-0.5772,  0.0308, -0.4882,  ...,  0.5831,  0.3690, -0.0827],\n",
      "         [-0.6032, -0.0646, -0.4276,  ...,  0.2167,  0.0954, -0.2461],\n",
      "         [-0.4342,  0.2690, -0.4812,  ...,  0.0869,  0.2685, -0.2258]],\n",
      "\n",
      "        [[-0.3288, -0.3550, -0.2187,  ...,  0.0977,  0.1867,  0.0258],\n",
      "         [-0.4408, -0.2080, -0.2021,  ..., -0.0427,  0.2723, -0.3492],\n",
      "         [-0.1997,  0.0351, -0.1363,  ...,  0.2941,  0.2521, -0.3963],\n",
      "         ...,\n",
      "         [-0.2394, -0.3236, -0.2838,  ...,  0.5457,  0.2625, -0.1743],\n",
      "         [-0.2809, -0.1929, -0.3278,  ...,  0.3984,  0.1019, -0.4548],\n",
      "         [-0.2468, -0.0775, -0.1192,  ...,  0.2225,  0.1585, -0.4739]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0407, -0.2167, -0.0987,  ..., -0.5556, -0.2812,  0.1639],\n",
      "         [ 0.0026, -0.2429, -0.2875,  ..., -0.5131, -0.0327, -0.0172],\n",
      "         [ 0.2170, -0.2662, -0.3669,  ..., -0.4363, -0.1596,  0.1987],\n",
      "         ...,\n",
      "         [ 0.0443, -0.3191, -0.2726,  ..., -0.2018, -0.1756,  0.4201],\n",
      "         [ 0.0340, -0.3550, -0.3269,  ..., -0.3951, -0.3034, -0.0388],\n",
      "         [ 0.0520, -0.2512, -0.3141,  ..., -0.3360, -0.0708, -0.0203]],\n",
      "\n",
      "        [[-0.3748, -0.0071, -0.3886,  ...,  0.3174, -0.0415, -0.2686],\n",
      "         [-0.1491, -0.0585, -0.8562,  ...,  0.0018,  0.0797, -0.1435],\n",
      "         [ 0.0727, -0.1393, -0.5871,  ...,  0.0307, -0.0602, -0.3315],\n",
      "         ...,\n",
      "         [-0.3354, -0.0443, -0.6384,  ...,  0.3868, -0.0140, -0.0218],\n",
      "         [-0.5084,  0.1004, -0.8282,  ...,  0.3718, -0.0772, -0.1042],\n",
      "         [-0.4435, -0.0676, -0.7700,  ...,  0.1405,  0.0915, -0.1479]],\n",
      "\n",
      "        [[-0.5051,  0.0758, -0.2711,  ..., -0.0298,  0.0439, -0.2585],\n",
      "         [-0.2428, -0.3654, -0.3597,  ..., -0.0592,  0.2806, -0.2723],\n",
      "         [-0.2259,  0.1554, -0.4690,  ...,  0.5179,  0.3178, -0.3892],\n",
      "         ...,\n",
      "         [-0.5197, -0.0582, -0.3736,  ...,  0.6612,  0.4676, -0.0200],\n",
      "         [-0.5017, -0.0839, -0.2220,  ...,  0.1714, -0.0579, -0.1914],\n",
      "         [-0.2364, -0.1267, -0.2559,  ...,  0.4074,  0.1510, -0.2629]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.481024146080017\n",
      "Model outputs:  tensor([[[-0.0301, -0.2298, -0.2775,  ..., -0.0543,  0.0094,  0.1235],\n",
      "         [-0.2121, -0.1969, -0.6098,  ..., -0.1465, -0.2617,  0.0987],\n",
      "         [ 0.0239, -0.4813, -0.4334,  ..., -0.1106, -0.1600,  0.3254],\n",
      "         ...,\n",
      "         [ 0.1319, -0.2609, -0.6496,  ..., -0.0176, -0.0225,  0.2097],\n",
      "         [ 0.1725, -0.4970, -0.4292,  ...,  0.0035, -0.0336,  0.4194],\n",
      "         [-0.0948, -0.1520, -0.3875,  ..., -0.1406, -0.1711,  0.1758]],\n",
      "\n",
      "        [[-0.2278, -0.0196, -0.4597,  ...,  0.2163,  0.1201,  0.0327],\n",
      "         [-0.1703,  0.0538, -0.5738,  ...,  0.5013, -0.0191, -0.1778],\n",
      "         [-0.0570, -0.2726, -0.4971,  ...,  0.6060, -0.0482,  0.1921],\n",
      "         ...,\n",
      "         [-0.1955, -0.3457, -0.5296,  ...,  0.4727, -0.0047, -0.2252],\n",
      "         [-0.0433, -0.2866, -0.7314,  ...,  0.5104,  0.0553, -0.0985],\n",
      "         [-0.4262,  0.0236, -0.7019,  ...,  0.3233, -0.1419, -0.2074]],\n",
      "\n",
      "        [[-0.3515, -0.0052, -0.6519,  ...,  0.3055,  0.2734, -0.0423],\n",
      "         [ 0.0334,  0.0103, -0.9781,  ...,  0.4605, -0.0907, -0.1144],\n",
      "         [-0.4328, -0.2538, -0.5581,  ...,  0.7453,  0.0334, -0.0298],\n",
      "         ...,\n",
      "         [-0.0669, -0.0352, -0.6461,  ...,  0.5337,  0.2031, -0.0239],\n",
      "         [ 0.0723, -0.2981, -0.7148,  ...,  0.6018,  0.3986,  0.0351],\n",
      "         [-0.3624, -0.0912, -0.6785,  ...,  0.3600,  0.2075, -0.3382]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3129,  0.0813, -0.1470,  ...,  0.4529,  0.3037, -0.0197],\n",
      "         [-0.3254,  0.1846, -0.4736,  ...,  0.8383,  0.3375, -0.3921],\n",
      "         [-0.2781, -0.4101, -0.0063,  ...,  0.7205,  0.1276, -0.0067],\n",
      "         ...,\n",
      "         [-0.3259, -0.1724, -0.2489,  ...,  0.8246,  0.3597,  0.0213],\n",
      "         [-0.1938, -0.2894,  0.0911,  ...,  0.7029,  0.4601, -0.4139],\n",
      "         [-0.5712,  0.1569, -0.0753,  ...,  0.7396,  0.4241, -0.3271]],\n",
      "\n",
      "        [[-0.2434, -0.0584, -0.4436,  ..., -0.1883,  0.0824,  0.0894],\n",
      "         [-0.0590, -0.1531, -0.3902,  ..., -0.0589, -0.0613,  0.0340],\n",
      "         [-0.5070, -0.5222, -0.2269,  ...,  0.0233, -0.5502,  0.2566],\n",
      "         ...,\n",
      "         [-0.1196, -0.0264, -0.5124,  ...,  0.0251, -0.1845,  0.3500],\n",
      "         [-0.0790, -0.0618, -0.7120,  ..., -0.0199,  0.2506,  0.2406],\n",
      "         [ 0.0210, -0.2067, -0.6391,  ..., -0.2945, -0.0362,  0.2957]],\n",
      "\n",
      "        [[-0.1979, -0.2261, -0.4281,  ..., -0.0192,  0.0262, -0.1121],\n",
      "         [-0.0995, -0.2524, -0.4566,  ..., -0.1630, -0.3984, -0.2724],\n",
      "         [-0.1578, -0.5387, -0.1023,  ..., -0.1048,  0.0018,  0.0490],\n",
      "         ...,\n",
      "         [-0.0386, -0.3650, -0.3342,  ...,  0.1263, -0.1707, -0.1863],\n",
      "         [ 0.2736, -0.6797, -0.2712,  ...,  0.2124,  0.2781, -0.2124],\n",
      "         [-0.2561, -0.2352, -0.3316,  ...,  0.0333, -0.1461, -0.3339]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1518489122390747\n",
      "Model outputs:  tensor([[[ 0.0359, -0.1377, -0.3827,  ..., -0.0935, -0.0569,  0.3225],\n",
      "         [ 0.3139, -0.3758, -0.5798,  ..., -0.1786, -0.5316,  0.2191],\n",
      "         [ 0.0235, -0.4526, -0.2919,  ..., -0.3256, -0.0212,  0.4433],\n",
      "         ...,\n",
      "         [ 0.0067, -0.3191, -0.6748,  ..., -0.0784, -0.0646,  0.2102],\n",
      "         [-0.1553, -0.2734, -0.3954,  ...,  0.1050, -0.1072,  0.3417],\n",
      "         [-0.1191, -0.3189, -0.5173,  ..., -0.2387, -0.2818,  0.4099]],\n",
      "\n",
      "        [[ 0.1306, -0.3540,  0.0354,  ...,  0.5352, -0.2219, -0.6225],\n",
      "         [ 0.1410, -0.2774, -0.0918,  ...,  0.3496, -0.1665, -0.4805],\n",
      "         [ 0.1020, -0.5513, -0.0852,  ...,  0.3275, -0.2673, -0.4253],\n",
      "         ...,\n",
      "         [ 0.0333, -0.0394,  0.1126,  ...,  0.3873, -0.1288, -0.3611],\n",
      "         [ 0.1595, -0.5858, -0.1016,  ...,  0.4315,  0.0313, -0.2439],\n",
      "         [ 0.5462, -0.4718, -0.2701,  ...,  0.5853, -0.1233, -0.2304]],\n",
      "\n",
      "        [[ 0.1848, -0.4483,  0.2679,  ...,  0.5072,  0.0478, -0.6654],\n",
      "         [ 0.1371, -0.4054,  0.0080,  ...,  0.3014, -0.2543, -0.5531],\n",
      "         [ 0.1786, -0.4447,  0.2602,  ...,  0.3689, -0.1278, -0.4925],\n",
      "         ...,\n",
      "         [ 0.0503, -0.1859, -0.1088,  ...,  0.5158, -0.0419, -0.3131],\n",
      "         [ 0.0934, -0.2943,  0.0442,  ...,  0.4257, -0.0090, -0.1627],\n",
      "         [ 0.1450, -0.5002, -0.0258,  ...,  0.4206, -0.2277, -0.4503]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4687, -0.1290, -0.2652,  ...,  0.5602, -0.0136,  0.0522],\n",
      "         [-0.2717, -0.0209, -0.1574,  ...,  0.6396,  0.0154, -0.2027],\n",
      "         [-0.3827, -0.0869, -0.4629,  ...,  0.4984,  0.1883,  0.0023],\n",
      "         ...,\n",
      "         [-0.3038,  0.0491, -0.0251,  ...,  0.8601,  0.0692, -0.3391],\n",
      "         [-0.4005, -0.0030, -0.2193,  ...,  0.5214,  0.1621, -0.2581],\n",
      "         [-0.4454, -0.2676, -0.2241,  ...,  0.4676,  0.1658, -0.2431]],\n",
      "\n",
      "        [[ 0.1215, -0.3554, -0.2014,  ..., -0.1929, -0.2436,  0.3657],\n",
      "         [ 0.0011, -0.1872, -0.2252,  ..., -0.3153, -0.2760,  0.3038],\n",
      "         [-0.1407, -0.0721, -0.2064,  ..., -0.0870, -0.2739,  0.1916],\n",
      "         ...,\n",
      "         [-0.2705, -0.4926, -0.2478,  ..., -0.0458, -0.2784,  0.3391],\n",
      "         [-0.3010, -0.5098, -0.3976,  ...,  0.0412, -0.4204,  0.5155],\n",
      "         [ 0.0123, -0.5047, -0.3784,  ..., -0.1188, -0.3306,  0.2889]],\n",
      "\n",
      "        [[-0.0962, -0.2791, -0.4763,  ..., -0.3964, -0.2680,  0.1367],\n",
      "         [-0.0177, -0.0234, -0.4402,  ..., -0.1853,  0.1021,  0.1815],\n",
      "         [-0.0371, -0.2932, -0.3998,  ...,  0.0165, -0.1594, -0.1962],\n",
      "         ...,\n",
      "         [-0.0212, -0.3190, -0.3629,  ...,  0.0587,  0.0483,  0.0832],\n",
      "         [-0.0967, -0.4038, -0.2388,  ...,  0.3208, -0.2030,  0.1764],\n",
      "         [-0.1648, -0.2516, -0.2943,  ...,  0.1812, -0.0365,  0.0508]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2452880144119263\n",
      "Model outputs:  tensor([[[-5.4625e-01,  2.1121e-01, -6.1273e-02,  ...,  3.4348e-01,\n",
      "           2.7004e-01, -1.8111e-01],\n",
      "         [-3.9013e-01,  1.2491e-01,  4.6066e-02,  ...,  6.5569e-01,\n",
      "           2.8057e-01, -3.9446e-01],\n",
      "         [-4.4169e-01,  8.9471e-02, -3.7115e-02,  ...,  6.5652e-01,\n",
      "           2.0568e-01,  3.8615e-02],\n",
      "         ...,\n",
      "         [-1.8842e-01,  2.4934e-01, -2.7209e-02,  ...,  4.2880e-01,\n",
      "           3.3547e-01, -4.2544e-01],\n",
      "         [-3.0447e-01,  1.5098e-01, -1.4801e-01,  ...,  3.9512e-01,\n",
      "          -1.1924e-01, -3.9576e-01],\n",
      "         [-4.6517e-01,  3.3629e-01,  1.2324e-01,  ...,  5.7364e-01,\n",
      "           2.4593e-01, -2.8554e-01]],\n",
      "\n",
      "        [[-6.7134e-01,  2.8454e-01,  6.0317e-02,  ...,  5.3837e-01,\n",
      "           2.3129e-01, -1.6968e-01],\n",
      "         [-5.8241e-01,  3.7166e-03, -7.4177e-02,  ...,  5.2291e-01,\n",
      "           3.1469e-01,  7.9088e-02],\n",
      "         [-5.2453e-01, -4.4635e-02, -1.9301e-01,  ...,  3.7321e-01,\n",
      "           2.4200e-01, -9.4817e-02],\n",
      "         ...,\n",
      "         [-4.0030e-01,  1.7766e-01, -1.2171e-03,  ...,  3.8215e-01,\n",
      "           3.3480e-01, -3.7453e-01],\n",
      "         [-4.1561e-01,  2.4562e-01,  1.4129e-02,  ...,  7.9865e-02,\n",
      "          -3.9560e-02, -4.9115e-01],\n",
      "         [-4.7398e-01,  8.8294e-02, -3.1692e-01,  ...,  5.0097e-01,\n",
      "           3.0900e-01, -7.5969e-02]],\n",
      "\n",
      "        [[-3.8334e-01,  1.1291e-02, -7.8224e-02,  ...,  3.5768e-01,\n",
      "           1.4811e-01, -8.2738e-02],\n",
      "         [-4.0470e-01, -9.3153e-02,  4.9047e-02,  ...,  4.2520e-01,\n",
      "           2.5072e-01, -1.3167e-01],\n",
      "         [-6.3038e-01,  3.8651e-01, -6.0461e-02,  ...,  4.0483e-01,\n",
      "           3.4790e-01, -1.5366e-02],\n",
      "         ...,\n",
      "         [-5.2885e-01,  2.9102e-01,  3.6222e-02,  ...,  1.6000e-01,\n",
      "           1.2257e-01, -3.7497e-01],\n",
      "         [-3.8996e-01,  2.1264e-01, -3.5666e-03,  ...,  3.9225e-01,\n",
      "           3.4370e-02, -2.9092e-01],\n",
      "         [-2.0170e-01,  3.7833e-01,  5.9680e-02,  ...,  5.2855e-01,\n",
      "           3.7061e-01, -2.8940e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.5404e-02, -3.3213e-01, -3.4272e-01,  ..., -4.4072e-01,\n",
      "           4.0442e-01,  5.2455e-01],\n",
      "         [-1.2361e-01, -1.6926e-01, -1.9424e-02,  ..., -2.8429e-01,\n",
      "           5.7748e-01,  3.5561e-01],\n",
      "         [-2.0273e-01, -3.2529e-01, -2.3856e-01,  ..., -2.3677e-01,\n",
      "           5.7927e-01,  5.7801e-01],\n",
      "         ...,\n",
      "         [ 1.0711e-01, -1.7946e-01, -3.8784e-01,  ..., -6.4198e-01,\n",
      "           3.0036e-01,  3.3463e-01],\n",
      "         [-4.4340e-02, -2.4718e-01, -1.7894e-01,  ..., -6.4651e-01,\n",
      "           5.1236e-01,  3.3401e-01],\n",
      "         [ 3.3020e-03, -2.0345e-01, -4.4366e-01,  ..., -4.0604e-01,\n",
      "           5.5220e-01,  4.5981e-01]],\n",
      "\n",
      "        [[-2.3348e-01,  5.8215e-02, -1.7256e-01,  ...,  2.7931e-01,\n",
      "           1.6249e-01, -7.7192e-02],\n",
      "         [-3.3664e-01,  2.1708e-02, -2.6133e-01,  ...,  3.9318e-01,\n",
      "           2.6953e-01,  3.9034e-02],\n",
      "         [-6.2298e-01,  3.7015e-03,  1.5733e-03,  ...,  4.8449e-01,\n",
      "           4.0569e-02, -1.3524e-02],\n",
      "         ...,\n",
      "         [-2.8491e-01,  1.6598e-02, -1.6182e-01,  ..., -3.7442e-02,\n",
      "           1.7999e-01, -5.0884e-01],\n",
      "         [-2.4450e-01,  1.1498e-01,  6.4649e-02,  ...,  1.7669e-01,\n",
      "           3.4787e-01, -2.6691e-01],\n",
      "         [-4.4337e-01,  2.1624e-01, -1.0513e-01,  ...,  4.2393e-01,\n",
      "           2.7024e-01, -6.2811e-04]],\n",
      "\n",
      "        [[-4.9476e-01,  3.4868e-01, -9.1236e-02,  ...,  4.7185e-01,\n",
      "           3.6558e-01,  3.8291e-02],\n",
      "         [-3.2172e-01,  9.6695e-02,  2.2490e-02,  ...,  5.6864e-01,\n",
      "           6.7258e-02,  1.0385e-01],\n",
      "         [-5.3375e-01,  2.2158e-01, -1.9339e-01,  ...,  7.1296e-01,\n",
      "           3.0500e-01,  2.0068e-02],\n",
      "         ...,\n",
      "         [-2.6529e-01,  2.7541e-01,  4.6247e-02,  ...,  1.7504e-01,\n",
      "           1.0222e-01, -1.2718e-01],\n",
      "         [-5.6544e-01,  4.4670e-01, -1.7164e-01,  ...,  3.8889e-01,\n",
      "           2.0432e-01, -4.3551e-01],\n",
      "         [-3.1850e-01,  3.3736e-01, -3.9432e-02,  ...,  4.5737e-01,\n",
      "           2.3924e-01, -9.6319e-02]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.834175705909729\n",
      "Model outputs:  tensor([[[ 0.3681, -0.6453,  0.0914,  ..., -0.4722,  0.0735, -0.2479],\n",
      "         [ 0.5696, -0.4323,  0.1387,  ..., -0.5655,  0.0641, -0.2693],\n",
      "         [ 0.1023, -0.3451, -0.0498,  ..., -0.7456,  0.2254, -0.0107],\n",
      "         ...,\n",
      "         [-0.2777, -0.1820, -0.3600,  ..., -0.0483,  0.0683, -0.3776],\n",
      "         [ 0.0374, -0.2913, -0.4520,  ...,  0.0362,  0.0863, -0.2810],\n",
      "         [ 0.0040, -0.3829, -0.3164,  ..., -0.3306, -0.1449, -0.2791]],\n",
      "\n",
      "        [[-0.1706, -0.3337, -0.0899,  ..., -0.1992, -0.0522,  0.0417],\n",
      "         [-0.2219,  0.0994, -0.2569,  ..., -0.2160,  0.1301, -0.2359],\n",
      "         [-0.3451, -0.0882, -0.1250,  ...,  0.2768, -0.1485,  0.0801],\n",
      "         ...,\n",
      "         [-0.4076, -0.0206, -0.7214,  ...,  0.1631, -0.0387,  0.0983],\n",
      "         [-0.6044,  0.1178, -0.6416,  ...,  0.3728,  0.0589,  0.0039],\n",
      "         [-0.6131,  0.0677, -0.5924,  ...,  0.1206,  0.2495,  0.0182]],\n",
      "\n",
      "        [[ 0.0422, -0.3305,  0.1239,  ...,  0.1408, -0.1211, -0.4623],\n",
      "         [ 0.3709, -0.2264,  0.1217,  ...,  0.2604, -0.2525, -0.8005],\n",
      "         [ 0.1751, -0.0698,  0.4097,  ...,  0.3736, -0.2798, -0.6126],\n",
      "         ...,\n",
      "         [ 0.3640, -0.6980,  0.0611,  ...,  0.4307,  0.2481, -0.3072],\n",
      "         [ 0.0370, -0.4106,  0.0054,  ...,  0.3902, -0.2054, -0.4742],\n",
      "         [ 0.3047, -0.3173,  0.1592,  ...,  0.3918, -0.1136, -0.5877]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1411, -0.2395,  0.0564,  ...,  0.1510,  0.0572, -0.1941],\n",
      "         [ 0.1983, -0.2373,  0.2091,  ...,  0.3746,  0.0107, -0.1565],\n",
      "         [-0.1358, -0.2678,  0.3400,  ...,  0.3487, -0.0815, -0.0952],\n",
      "         ...,\n",
      "         [ 0.0145,  0.0572, -0.2717,  ...,  0.7447,  0.3002,  0.1138],\n",
      "         [-0.2584, -0.0323, -0.0324,  ...,  0.9383,  0.4194, -0.2815],\n",
      "         [-0.3230,  0.0896, -0.0391,  ...,  0.5047,  0.2682, -0.3150]],\n",
      "\n",
      "        [[ 0.2038, -0.4550,  0.0246,  ..., -1.1118, -0.0551,  0.1905],\n",
      "         [ 0.4826, -0.2298, -0.0660,  ..., -0.8266, -0.1448, -0.1096],\n",
      "         [ 0.1628, -0.2927,  0.2045,  ..., -0.5405,  0.0702,  0.0529],\n",
      "         ...,\n",
      "         [-0.1120, -0.3575, -0.3413,  ..., -0.2650, -0.0822,  0.0369],\n",
      "         [-0.2326, -0.1061, -0.3201,  ..., -0.4566,  0.1120,  0.0542],\n",
      "         [ 0.0223, -0.5760, -0.2969,  ..., -0.6397,  0.3056,  0.1944]],\n",
      "\n",
      "        [[ 0.0196, -0.3385, -0.0794,  ..., -0.5804, -0.2883,  0.1656],\n",
      "         [ 0.2096, -0.2965, -0.0200,  ..., -0.4959, -0.2913,  0.1395],\n",
      "         [-0.0998, -0.3288,  0.1170,  ..., -0.1027, -0.3294,  0.3349],\n",
      "         ...,\n",
      "         [-0.3447, -0.3310, -0.5728,  ..., -0.1516, -0.1567,  0.2157],\n",
      "         [-0.0995, -0.2140, -0.5382,  ..., -0.1528, -0.4426,  0.1904],\n",
      "         [-0.3085, -0.5119, -0.4015,  ..., -0.1946, -0.2070, -0.0825]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.516335368156433\n",
      "Model outputs:  tensor([[[-0.4941, -0.0194, -0.6406,  ...,  0.4878, -0.3401, -0.0246],\n",
      "         [-0.3445,  0.2844, -0.6714,  ...,  0.0760,  0.0710, -0.1523],\n",
      "         [-0.3587,  0.0655, -0.3084,  ...,  0.1093, -0.4108, -0.1165],\n",
      "         ...,\n",
      "         [-0.1989, -0.0399, -0.6441,  ..., -0.1895, -0.3561, -0.0129],\n",
      "         [-0.3607,  0.0911, -0.5286,  ...,  0.0692, -0.0282, -0.0245],\n",
      "         [-0.3469, -0.0937, -0.7610,  ...,  0.1020, -0.3683, -0.1275]],\n",
      "\n",
      "        [[-0.5429,  0.2892, -0.5462,  ...,  0.1024, -0.3895, -0.0035],\n",
      "         [-0.6972,  0.3255, -0.5343,  ...,  0.2380, -0.0916, -0.1903],\n",
      "         [-0.3832,  0.2548, -0.4589,  ...,  0.1178, -0.2600, -0.1732],\n",
      "         ...,\n",
      "         [-0.3089,  0.0973, -0.5767,  ...,  0.1618, -0.1412, -0.2587],\n",
      "         [-0.2515,  0.0860, -0.5251,  ...,  0.1449, -0.1946,  0.0129],\n",
      "         [-0.6049,  0.1627, -0.5606,  ...,  0.0059, -0.2821,  0.0050]],\n",
      "\n",
      "        [[-0.5296,  0.3006, -0.2946,  ...,  0.7682,  0.2564, -0.1569],\n",
      "         [-0.7383,  0.3330, -0.3972,  ...,  0.1828,  0.3634, -0.4668],\n",
      "         [-0.5871,  0.3755, -0.2369,  ...,  0.3681, -0.0975, -0.1725],\n",
      "         ...,\n",
      "         [-0.1928, -0.0584,  0.1196,  ...,  0.3521,  0.4233, -0.4022],\n",
      "         [-0.3162,  0.0734, -0.2527,  ...,  0.4633,  0.1416, -0.1341],\n",
      "         [-0.7054,  0.4291, -0.4736,  ...,  0.4273,  0.0749,  0.0279]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5835,  0.1375, -0.6691,  ...,  0.6201,  0.1784, -0.2625],\n",
      "         [-0.5910,  0.1010, -0.6257,  ...,  0.4871,  0.1718, -0.3680],\n",
      "         [-0.5979,  0.3698, -0.5670,  ...,  0.3584,  0.1649, -0.4249],\n",
      "         ...,\n",
      "         [-0.3811,  0.0069, -0.5950,  ...,  0.4710,  0.0772, -0.2175],\n",
      "         [-0.3901, -0.1149, -0.5522,  ...,  0.6852, -0.0592, -0.1838],\n",
      "         [-0.5342,  0.4157, -0.6343,  ...,  0.3937, -0.0792, -0.0973]],\n",
      "\n",
      "        [[-0.0335,  0.0730, -0.2056,  ...,  0.2278, -0.0021, -0.3191],\n",
      "         [-0.1847, -0.0753, -0.3097,  ...,  0.2095, -0.1561, -0.3661],\n",
      "         [-0.1515, -0.0611, -0.4477,  ..., -0.3035, -0.0206, -0.3082],\n",
      "         ...,\n",
      "         [-0.4305, -0.1810, -0.0358,  ..., -0.1439, -0.1524, -0.7133],\n",
      "         [-0.1261, -0.2311, -0.2180,  ...,  0.0820, -0.0716, -0.2322],\n",
      "         [-0.1580, -0.0625, -0.4133,  ..., -0.0267, -0.0340, -0.1689]],\n",
      "\n",
      "        [[-0.4936,  0.1404, -0.6394,  ...,  0.0575,  0.1844, -0.1026],\n",
      "         [-0.6939,  0.1691, -0.3897,  ...,  0.1673,  0.1367,  0.1490],\n",
      "         [-0.6153,  0.0767, -0.1766,  ..., -0.0025,  0.2045,  0.1639],\n",
      "         ...,\n",
      "         [-0.4794, -0.3227, -0.0392,  ...,  0.1346,  0.2132,  0.0174],\n",
      "         [-0.7976,  0.1061, -0.4304,  ...,  0.1312,  0.3329,  0.0279],\n",
      "         [-0.7147,  0.0681, -0.5678,  ..., -0.1325,  0.1026,  0.4088]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0340133905410767\n",
      "Model outputs:  tensor([[[-0.1493, -0.2581, -0.3485,  ..., -0.0328, -0.0412, -0.1282],\n",
      "         [-0.4214, -0.1927, -0.3775,  ..., -0.0519, -0.3670, -0.1026],\n",
      "         [-0.3436, -0.0185, -0.3998,  ...,  0.1298, -0.3592,  0.0938],\n",
      "         ...,\n",
      "         [-0.1243, -0.0105, -0.8407,  ...,  0.0836, -0.1789, -0.0362],\n",
      "         [-0.3705, -0.0376, -0.3287,  ...,  0.4079, -0.1371,  0.0751],\n",
      "         [-0.0101, -0.0181, -0.5108,  ...,  0.4160, -0.0232,  0.0880]],\n",
      "\n",
      "        [[-0.1518, -0.0126, -0.7234,  ..., -0.2213, -0.1287, -0.1379],\n",
      "         [-0.3156, -0.2560, -0.6298,  ..., -0.1672, -0.0301,  0.1838],\n",
      "         [-0.3412, -0.0607, -0.4380,  ..., -0.0241, -0.2857,  0.1299],\n",
      "         ...,\n",
      "         [-0.2483, -0.0121, -0.6361,  ...,  0.0340, -0.0614,  0.0198],\n",
      "         [-0.0546, -0.0918, -0.7309,  ...,  0.1633,  0.1233,  0.2152],\n",
      "         [-0.0882, -0.1121, -0.9924,  ...,  0.0729, -0.3224,  0.1817]],\n",
      "\n",
      "        [[-0.3276, -0.0265, -0.6508,  ...,  0.3076, -0.0733,  0.1723],\n",
      "         [-0.2946,  0.0117, -0.6784,  ...,  0.1836, -0.1107, -0.2486],\n",
      "         [-0.1620, -0.1796, -0.5189,  ..., -0.0875, -0.2977, -0.0252],\n",
      "         ...,\n",
      "         [-0.1424, -0.2394, -0.7645,  ..., -0.1487, -0.2371, -0.0075],\n",
      "         [-0.1446, -0.4295, -0.4951,  ...,  0.2570, -0.0874,  0.1082],\n",
      "         [-0.0753, -0.0775, -0.7579,  ...,  0.2250, -0.0194,  0.2825]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4974,  0.0553, -0.4885,  ...,  0.1412,  0.2662, -0.0178],\n",
      "         [-0.4112, -0.3076, -0.3002,  ...,  0.2788,  0.3159, -0.2551],\n",
      "         [-0.4940,  0.2375, -0.2512,  ...,  0.1459, -0.0210, -0.3218],\n",
      "         ...,\n",
      "         [-0.6119, -0.1688, -0.2167,  ...,  0.4677,  0.1833,  0.0428],\n",
      "         [-0.4309, -0.1196, -0.2868,  ...,  0.3770,  0.1579,  0.0340],\n",
      "         [-0.1981, -0.0269, -0.2180,  ...,  0.3174,  0.3536, -0.1978]],\n",
      "\n",
      "        [[-0.5316, -0.0618, -0.0924,  ...,  0.3539,  0.0051, -0.1851],\n",
      "         [-0.5850,  0.0636, -0.1413,  ...,  0.6502,  0.0388, -0.2107],\n",
      "         [-0.4149,  0.1153, -0.5017,  ...,  0.6447,  0.1655, -0.2336],\n",
      "         ...,\n",
      "         [-0.4514, -0.0955, -0.1886,  ...,  0.4820, -0.0453, -0.0644],\n",
      "         [-0.2030, -0.1017, -0.3805,  ...,  0.8202,  0.3299, -0.3013],\n",
      "         [-0.2637, -0.0658, -0.2453,  ...,  0.4874,  0.0906, -0.0327]],\n",
      "\n",
      "        [[-0.2795,  0.1371, -0.2330,  ...,  0.4440,  0.3110, -0.1011],\n",
      "         [-0.7033,  0.0570, -0.2271,  ...,  0.0170, -0.0326, -0.3064],\n",
      "         [-0.3319, -0.0157, -0.2351,  ...,  0.4350,  0.0169, -0.1915],\n",
      "         ...,\n",
      "         [-0.5016, -0.1611, -0.3023,  ...,  0.3549,  0.0894, -0.0067],\n",
      "         [-0.5604, -0.1790, -0.2145,  ...,  0.4794,  0.3967,  0.1297],\n",
      "         [-0.3169, -0.0948, -0.0960,  ...,  0.2529,  0.2222,  0.0193]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1855993270874023\n",
      "Model outputs:  tensor([[[-8.6573e-02,  8.4249e-02, -4.2949e-01,  ...,  1.0794e-02,\n",
      "           1.8955e-01,  1.8001e-01],\n",
      "         [-1.4642e-01, -3.0518e-01, -5.7332e-01,  ...,  1.4403e-01,\n",
      "          -4.9636e-02,  6.9481e-04],\n",
      "         [ 2.3064e-01, -3.4181e-01, -5.2539e-01,  ...,  1.1450e-01,\n",
      "           2.1027e-01,  3.7898e-01],\n",
      "         ...,\n",
      "         [-1.7000e-01, -2.2184e-01, -6.5724e-01,  ..., -1.2175e-01,\n",
      "          -3.1954e-01,  3.6140e-01],\n",
      "         [-1.6261e-01, -4.4375e-01, -5.7752e-01,  ..., -1.4632e-01,\n",
      "          -1.0811e-01,  4.7037e-01],\n",
      "         [-4.4935e-01, -4.0898e-01, -5.2291e-01,  ..., -5.9494e-02,\n",
      "           6.2911e-02,  6.1457e-02]],\n",
      "\n",
      "        [[-2.6761e-01, -2.9697e-02, -4.2361e-01,  ..., -4.3584e-02,\n",
      "           5.9820e-02, -1.1581e-02],\n",
      "         [-5.3918e-02,  1.3237e-01, -5.7573e-01,  ..., -1.3390e-01,\n",
      "           6.6014e-02,  7.1302e-02],\n",
      "         [ 1.0925e-02, -1.3495e-01, -6.2538e-01,  ...,  1.0663e-01,\n",
      "           1.0902e-01,  2.4366e-01],\n",
      "         ...,\n",
      "         [-6.5807e-02, -3.7319e-01, -5.1217e-01,  ..., -1.2324e-01,\n",
      "          -2.1727e-01,  2.3703e-01],\n",
      "         [-2.2163e-01, -1.1765e-01, -5.4248e-01,  ...,  1.4006e-01,\n",
      "           1.1771e-01, -1.0502e-01],\n",
      "         [-4.9163e-01, -1.1820e-01, -6.1551e-01,  ...,  1.0446e-01,\n",
      "          -1.6122e-01,  1.7114e-01]],\n",
      "\n",
      "        [[-5.3367e-01, -6.0998e-02, -3.3656e-01,  ...,  1.3929e-01,\n",
      "           3.3764e-01, -1.1193e-01],\n",
      "         [-7.6562e-01,  1.9565e-01, -4.9914e-01,  ...,  2.4410e-01,\n",
      "           2.7064e-01,  2.2394e-01],\n",
      "         [-2.4256e-01, -4.1823e-01, -4.6507e-01,  ...,  3.1157e-01,\n",
      "           3.9036e-01,  4.5260e-02],\n",
      "         ...,\n",
      "         [-5.4018e-01,  9.6764e-02, -7.0780e-01,  ...,  2.2982e-01,\n",
      "           1.2318e-01,  5.7897e-02],\n",
      "         [-6.5094e-01,  1.3098e-01, -3.4940e-01,  ...,  2.4829e-01,\n",
      "           1.9832e-01,  3.1822e-02],\n",
      "         [-7.2897e-01, -1.8426e-01, -5.0519e-01,  ...,  1.4382e-01,\n",
      "          -1.3356e-01,  2.4475e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.1290e-01,  2.4434e-01, -1.9971e-01,  ...,  6.4610e-01,\n",
      "           1.6589e-01, -3.6156e-01],\n",
      "         [-4.9345e-01,  1.0845e-01, -2.0516e-01,  ...,  5.6050e-01,\n",
      "           2.7721e-01, -2.0752e-01],\n",
      "         [-3.1160e-01, -2.2872e-01, -3.5435e-01,  ...,  6.9290e-01,\n",
      "           3.5965e-01, -2.4408e-01],\n",
      "         ...,\n",
      "         [-2.9985e-01, -1.8135e-01, -1.7608e-01,  ...,  5.2262e-01,\n",
      "           1.4450e-01, -2.6673e-01],\n",
      "         [-3.0209e-01, -9.9124e-02, -3.5796e-01,  ...,  7.0673e-01,\n",
      "           3.0452e-01, -8.2020e-02],\n",
      "         [-5.3810e-01,  4.2071e-02,  1.2585e-01,  ...,  4.6829e-01,\n",
      "           1.8111e-01, -1.9759e-01]],\n",
      "\n",
      "        [[-4.8621e-01,  7.0814e-02, -5.1622e-01,  ...,  4.2321e-01,\n",
      "           2.4642e-01, -9.8571e-02],\n",
      "         [-6.6438e-01, -1.7791e-01, -3.2252e-01,  ...,  1.8270e-01,\n",
      "           1.3667e-01,  2.5327e-01],\n",
      "         [-2.3415e-01, -3.5082e-01, -4.8118e-01,  ...,  2.8409e-01,\n",
      "           4.5614e-01, -2.5230e-01],\n",
      "         ...,\n",
      "         [-8.0520e-01, -5.4684e-02, -4.6967e-01,  ...,  8.1758e-02,\n",
      "           2.9384e-01,  2.8369e-01],\n",
      "         [-4.5608e-01,  5.1286e-02, -4.0159e-01,  ...,  2.1118e-02,\n",
      "           2.3361e-01, -1.1364e-01],\n",
      "         [-5.5544e-01, -3.4934e-01, -6.1247e-01,  ...,  2.2224e-01,\n",
      "           3.1564e-02,  1.8019e-01]],\n",
      "\n",
      "        [[-1.9679e-01, -9.4864e-02, -4.8934e-01,  ..., -1.8939e-01,\n",
      "          -3.3198e-02, -1.6781e-01],\n",
      "         [ 1.0450e-01,  1.3931e-02, -3.6480e-01,  ..., -3.0254e-01,\n",
      "          -2.1760e-01,  2.4056e-01],\n",
      "         [-8.0785e-02, -3.8773e-01, -6.0762e-01,  ...,  1.4806e-01,\n",
      "          -1.8521e-01,  3.5883e-01],\n",
      "         ...,\n",
      "         [-4.0313e-01, -3.1158e-01, -7.2517e-01,  ...,  8.8872e-02,\n",
      "          -3.1043e-01,  2.6995e-01],\n",
      "         [-1.7286e-01, -3.2946e-01, -2.5983e-01,  ..., -3.4306e-01,\n",
      "          -1.2631e-01,  1.7061e-01],\n",
      "         [-2.9827e-01, -4.7110e-01, -3.8071e-01,  ..., -8.9972e-02,\n",
      "          -2.8743e-01,  1.3732e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1493431329727173\n",
      "Model outputs:  tensor([[[-5.5973e-02, -5.0324e-01, -3.0022e-01,  ..., -1.7843e-01,\n",
      "          -1.8731e-01, -1.4695e-01],\n",
      "         [ 6.8889e-02, -4.5171e-01, -3.5513e-01,  ..., -1.8092e-01,\n",
      "          -4.3758e-01, -4.7036e-01],\n",
      "         [-1.9101e-01, -4.0299e-01, -4.5630e-01,  ..., -1.2937e-01,\n",
      "           2.8857e-02, -1.9468e-01],\n",
      "         ...,\n",
      "         [-1.4321e-01, -3.6007e-01, -2.7434e-01,  ..., -1.3307e-01,\n",
      "          -1.7077e-01, -2.2688e-01],\n",
      "         [ 7.8438e-02, -1.7813e-01,  3.7867e-02,  ..., -2.1092e-01,\n",
      "          -1.1630e-01, -3.5519e-01],\n",
      "         [ 2.1896e-01, -5.0347e-01, -5.8802e-01,  ...,  9.5868e-02,\n",
      "          -5.3533e-03, -3.6919e-01]],\n",
      "\n",
      "        [[-4.2143e-01, -1.0497e-01, -6.4411e-01,  ...,  4.0577e-01,\n",
      "          -3.7030e-01,  2.8368e-01],\n",
      "         [-2.8489e-01, -1.5104e-01, -5.1497e-01,  ...,  5.0259e-01,\n",
      "           1.5536e-01, -7.6010e-03],\n",
      "         [-2.6931e-01, -2.8673e-01, -4.3590e-01,  ...,  6.8253e-01,\n",
      "           2.4807e-01, -2.1190e-01],\n",
      "         ...,\n",
      "         [-2.6333e-01,  7.1275e-02, -7.6935e-01,  ...,  5.3804e-01,\n",
      "           3.7563e-01, -1.6617e-01],\n",
      "         [-3.9546e-01,  6.4507e-02, -2.7646e-01,  ...,  2.1082e-01,\n",
      "           1.2506e-01, -1.8415e-01],\n",
      "         [-2.7371e-01, -2.0638e-01, -5.3865e-01,  ...,  5.5302e-01,\n",
      "           2.9071e-01, -1.3426e-01]],\n",
      "\n",
      "        [[-3.0970e-01, -2.5241e-01, -1.8815e-01,  ..., -3.4928e-01,\n",
      "          -3.8835e-01,  3.2831e-01],\n",
      "         [ 5.4831e-02, -5.5158e-01, -9.5348e-02,  ..., -1.8093e-01,\n",
      "          -2.2148e-01,  2.1190e-01],\n",
      "         [ 1.7964e-02, -3.6872e-01, -3.5720e-01,  ..., -1.9162e-01,\n",
      "          -1.9720e-01,  4.2664e-01],\n",
      "         ...,\n",
      "         [-7.0672e-02, -3.3306e-01, -3.7921e-01,  ..., -3.2167e-01,\n",
      "          -3.9741e-01,  2.5435e-01],\n",
      "         [-6.5613e-02, -2.4616e-01, -3.6730e-01,  ..., -3.2541e-01,\n",
      "          -1.4202e-01,  1.7126e-01],\n",
      "         [-2.5018e-01, -3.8697e-01, -4.9976e-01,  ..., -3.2713e-01,\n",
      "          -2.6456e-01,  4.1251e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.7920e-01, -1.7904e-01, -5.1471e-01,  ..., -1.7419e-01,\n",
      "           3.1826e-01,  3.6006e-01],\n",
      "         [ 4.9419e-01, -1.8270e-01, -2.1842e-01,  ..., -2.3420e-01,\n",
      "           2.4908e-01,  1.1244e-01],\n",
      "         [ 7.4812e-02, -3.6120e-01, -3.3763e-01,  ..., -3.6571e-01,\n",
      "           3.1750e-01,  2.2044e-01],\n",
      "         ...,\n",
      "         [ 2.8889e-01, -3.5450e-01, -4.6955e-01,  ..., -5.2381e-01,\n",
      "           3.6364e-01,  8.6074e-02],\n",
      "         [ 4.6780e-01, -1.9595e-01, -2.0349e-01,  ..., -5.7282e-01,\n",
      "           2.9888e-01,  1.5884e-01],\n",
      "         [ 2.1645e-01, -2.6845e-01, -4.2841e-01,  ..., -3.5298e-02,\n",
      "           2.9987e-01,  2.3816e-01]],\n",
      "\n",
      "        [[-5.3124e-01, -1.6376e-01,  1.2710e-01,  ...,  4.1196e-01,\n",
      "           6.9530e-02, -1.9301e-01],\n",
      "         [-5.1094e-01, -2.7202e-01, -3.3897e-02,  ...,  5.5221e-01,\n",
      "           2.3813e-01, -2.9857e-01],\n",
      "         [-3.8407e-01, -2.3313e-01, -2.6413e-01,  ...,  6.2367e-01,\n",
      "           3.3941e-01, -1.8318e-01],\n",
      "         ...,\n",
      "         [-5.5669e-01,  4.3023e-03, -2.5961e-01,  ...,  4.6100e-01,\n",
      "           3.3994e-01, -2.7258e-01],\n",
      "         [-1.2375e-01, -1.1910e-01, -1.8552e-01,  ...,  1.6995e-01,\n",
      "           2.2812e-01, -2.7368e-01],\n",
      "         [-3.5030e-01,  5.5769e-02, -3.0096e-01,  ...,  5.5646e-01,\n",
      "           3.7171e-01, -2.7558e-01]],\n",
      "\n",
      "        [[-1.7848e-01, -2.1667e-01, -5.2464e-01,  ..., -2.7016e-01,\n",
      "          -1.1539e-01,  8.9085e-02],\n",
      "         [ 3.5518e-04, -2.3953e-01, -6.6903e-01,  ..., -5.9636e-02,\n",
      "          -1.7111e-01,  1.7309e-02],\n",
      "         [-1.8752e-01, -3.3073e-01, -3.3970e-01,  ..., -1.6357e-01,\n",
      "          -9.8730e-02,  1.5593e-01],\n",
      "         ...,\n",
      "         [-9.3066e-02, -1.0511e-01, -4.6205e-01,  ...,  2.3686e-01,\n",
      "           2.9498e-02, -9.9244e-02],\n",
      "         [ 3.6046e-02, -1.5965e-02, -3.3736e-01,  ..., -3.4921e-01,\n",
      "           4.2845e-02,  1.7656e-01],\n",
      "         [-3.6729e-02, -2.4990e-01, -3.5972e-01,  ..., -8.9014e-02,\n",
      "          -1.8111e-01,  1.6821e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9839277267456055\n",
      "Model outputs:  tensor([[[-0.3921, -0.0963, -0.8646,  ...,  0.0922,  0.0715, -0.0235],\n",
      "         [-0.3454,  0.1251, -0.8586,  ...,  0.1220, -0.0071,  0.4190],\n",
      "         [-0.2356, -0.2532, -0.6714,  ...,  0.1120,  0.0677, -0.0399],\n",
      "         ...,\n",
      "         [-0.5567, -0.1130, -0.6675,  ..., -0.0077,  0.0174,  0.2631],\n",
      "         [-0.4871, -0.1704, -0.5381,  ..., -0.0468,  0.0655,  0.1126],\n",
      "         [-0.7298, -0.1097, -0.8920,  ...,  0.2206, -0.5414,  0.3646]],\n",
      "\n",
      "        [[-0.4090, -0.1423, -0.5360,  ...,  0.3537,  0.0636,  0.0450],\n",
      "         [-0.5324,  0.1804, -0.5822,  ...,  0.5216,  0.1023,  0.1711],\n",
      "         [-0.4563, -0.2171, -0.5276,  ...,  0.4943,  0.2861,  0.1184],\n",
      "         ...,\n",
      "         [-0.6042, -0.1183, -0.4877,  ...,  0.7576,  0.1627, -0.0959],\n",
      "         [-0.8586, -0.0716, -0.5667,  ...,  0.2253,  0.0607, -0.0502],\n",
      "         [-0.7701, -0.0231, -0.7103,  ...,  0.4072,  0.2586,  0.3081]],\n",
      "\n",
      "        [[-0.6820, -0.1699, -0.4867,  ..., -0.0597,  0.2966,  0.0754],\n",
      "         [-0.3757, -0.0789, -0.1472,  ...,  0.4014,  0.1301, -0.2287],\n",
      "         [-0.4118,  0.0250, -0.6146,  ...,  0.6522,  0.3745, -0.0072],\n",
      "         ...,\n",
      "         [-0.6522, -0.0536, -0.4928,  ...,  0.8510,  0.1327, -0.1373],\n",
      "         [-0.9478,  0.0844, -0.0170,  ...,  0.5391,  0.3266, -0.2386],\n",
      "         [-0.6628, -0.0552, -0.4179,  ...,  0.6401,  0.2543,  0.1105]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1742, -0.5107, -0.4744,  ..., -0.0866, -0.2586,  0.1579],\n",
      "         [-0.2576, -0.2622, -0.6303,  ..., -0.0838, -0.3182,  0.5121],\n",
      "         [-0.2241, -0.3375, -0.5700,  ..., -0.1644, -0.3237,  0.4938],\n",
      "         ...,\n",
      "         [-0.1693, -0.4091, -0.4273,  ..., -0.0503, -0.5153,  0.4140],\n",
      "         [-0.3734, -0.4207, -0.3982,  ..., -0.2523, -0.2827,  0.3603],\n",
      "         [-0.3463, -0.3731, -0.5254,  ..., -0.1170, -0.4162,  0.5483]],\n",
      "\n",
      "        [[ 0.0920, -0.2300, -0.2116,  ...,  0.4643, -0.2649, -0.6791],\n",
      "         [ 0.0175,  0.0403, -0.1608,  ...,  0.5661, -0.1234, -0.4071],\n",
      "         [-0.0165, -0.0190, -0.1596,  ...,  0.6004, -0.0035, -0.3345],\n",
      "         ...,\n",
      "         [-0.1192, -0.1463,  0.1442,  ...,  0.7362, -0.1823, -0.5518],\n",
      "         [-0.0078, -0.1947,  0.1484,  ...,  0.2805, -0.2483, -0.5015],\n",
      "         [ 0.1496, -0.2771, -0.1863,  ...,  0.8359, -0.3265, -0.4890]],\n",
      "\n",
      "        [[-0.5264, -0.0528, -0.4907,  ...,  0.5521,  0.5820, -0.3136],\n",
      "         [-0.5558,  0.3163, -0.4909,  ...,  0.6847,  0.3025, -0.1909],\n",
      "         [-0.5587, -0.0173, -0.3999,  ...,  0.4973,  0.4112, -0.1231],\n",
      "         ...,\n",
      "         [-0.9219, -0.0557, -0.2899,  ...,  0.7067,  0.3511, -0.3305],\n",
      "         [-0.3092, -0.1545, -0.3607,  ...,  0.6926,  0.1312,  0.0994],\n",
      "         [-0.7484, -0.0651, -0.4558,  ...,  0.9354,  0.0475, -0.0544]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1359171867370605\n",
      "Model outputs:  tensor([[[-0.2209, -0.1086, -0.2850,  ...,  0.1441, -0.1472, -0.3068],\n",
      "         [-0.3414,  0.0363, -0.6638,  ...,  0.2131, -0.0130, -0.0080],\n",
      "         [-0.3117, -0.0660, -0.5607,  ...,  0.4770, -0.1943, -0.0452],\n",
      "         ...,\n",
      "         [ 0.2330, -0.2216, -0.7269,  ...,  0.2927, -0.1905,  0.0312],\n",
      "         [ 0.1180, -0.1953, -0.7493,  ...,  0.4509, -0.1851,  0.0506],\n",
      "         [ 0.0737, -0.0651, -0.5582,  ...,  0.3669, -0.3804, -0.0845]],\n",
      "\n",
      "        [[-0.3332,  0.0514, -0.2146,  ...,  0.0697,  0.2649, -0.3770],\n",
      "         [-0.5838,  0.2537, -0.6040,  ...,  0.2478,  0.2708,  0.1478],\n",
      "         [-0.8536,  0.1197, -0.5646,  ...,  0.1398, -0.0993, -0.0720],\n",
      "         ...,\n",
      "         [-0.2451, -0.1344, -0.6848,  ...,  0.1516,  0.0790,  0.0752],\n",
      "         [-0.5155,  0.2844, -0.5400,  ...,  0.4514,  0.2281,  0.0196],\n",
      "         [-0.4212, -0.2715, -0.3011,  ...,  0.0447,  0.0905,  0.2018]],\n",
      "\n",
      "        [[-0.2366, -0.0789, -0.0780,  ..., -0.1491, -0.1785, -0.1531],\n",
      "         [-0.1759, -0.1500, -0.6626,  ..., -0.2235, -0.2125,  0.1444],\n",
      "         [-0.2234, -0.3326, -0.6263,  ..., -0.0120, -0.3431,  0.2184],\n",
      "         ...,\n",
      "         [ 0.1289, -0.3081, -0.3100,  ...,  0.0393, -0.2678,  0.1434],\n",
      "         [-0.0853, -0.1501, -0.3115,  ...,  0.0933, -0.2942,  0.4598],\n",
      "         [-0.2113, -0.4078, -0.1178,  ..., -0.3140, -0.7140,  0.1545]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0414, -0.2671,  0.0965,  ..., -0.2198,  0.0115, -0.7795],\n",
      "         [ 0.1767, -0.0886, -0.2486,  ...,  0.1441,  0.1781, -0.6554],\n",
      "         [-0.1181, -0.2324, -0.1424,  ...,  0.4633,  0.0334, -0.6630],\n",
      "         ...,\n",
      "         [ 0.1372, -0.4683, -0.3538,  ..., -0.1683,  0.0159, -0.1557],\n",
      "         [ 0.2101, -0.4277,  0.0110,  ..., -0.3251,  0.3393,  0.2030],\n",
      "         [ 0.2644, -0.7380, -0.0100,  ..., -0.5669, -0.0082, -0.6890]],\n",
      "\n",
      "        [[-0.0236,  0.1583,  0.1107,  ...,  0.4386,  0.1520, -0.4355],\n",
      "         [-0.3961,  0.2164, -0.2531,  ...,  0.4723,  0.3463, -0.2565],\n",
      "         [-0.4937, -0.2235, -0.1638,  ...,  0.5570,  0.2775, -0.1567],\n",
      "         ...,\n",
      "         [-0.2421, -0.0677, -0.3320,  ...,  0.7332,  0.2685, -0.3982],\n",
      "         [-0.5030, -0.0248, -0.3954,  ...,  0.6200,  0.0175, -0.0613],\n",
      "         [-0.0536, -0.0633, -0.1128,  ...,  0.4864,  0.2849,  0.0297]],\n",
      "\n",
      "        [[-0.2963, -0.1866, -0.0488,  ...,  0.3842,  0.1437, -0.2814],\n",
      "         [-0.4306, -0.0499, -0.2332,  ...,  0.3554,  0.2125, -0.2361],\n",
      "         [-0.5332,  0.0102, -0.2677,  ...,  0.2616,  0.0586, -0.2341],\n",
      "         ...,\n",
      "         [-0.1584, -0.2893, -0.1811,  ...,  0.3405,  0.0858, -0.3727],\n",
      "         [-0.3547, -0.1108, -0.1954,  ...,  0.8382,  0.0611, -0.0909],\n",
      "         [-0.2227, -0.3700, -0.0555,  ...,  0.6634, -0.0010, -0.1752]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0770981311798096\n",
      "Model outputs:  tensor([[[-0.5651,  0.0257, -0.2123,  ...,  0.5108,  0.2851, -0.6327],\n",
      "         [-0.2599, -0.0246, -0.2872,  ...,  0.2293,  0.3675, -0.1563],\n",
      "         [-0.4026,  0.0466, -0.2361,  ...,  0.5311,  0.2837, -0.3657],\n",
      "         ...,\n",
      "         [-0.4810, -0.3128, -0.1997,  ...,  0.4516,  0.2854, -0.2421],\n",
      "         [-0.4573,  0.0798, -0.4700,  ...,  0.5156,  0.4465, -0.1019],\n",
      "         [-0.6742,  0.2932, -0.5632,  ...,  0.5224,  0.2781, -0.2310]],\n",
      "\n",
      "        [[-0.3455, -0.0253, -0.3138,  ..., -0.1141,  0.0993, -0.3908],\n",
      "         [-0.6639,  0.0222, -0.7304,  ..., -0.2825,  0.2275, -0.3225],\n",
      "         [-0.5023, -0.0320, -0.6762,  ...,  0.0195,  0.0429, -0.1379],\n",
      "         ...,\n",
      "         [-0.4141, -0.1894, -0.7889,  ...,  0.0490,  0.0419, -0.0883],\n",
      "         [-0.7609, -0.1996, -0.6391,  ..., -0.3676, -0.0320,  0.3136],\n",
      "         [-0.5779,  0.1582, -0.5978,  ..., -0.1204, -0.1085,  0.0907]],\n",
      "\n",
      "        [[-0.1408,  0.0217, -0.4217,  ...,  0.4149,  0.2083, -0.4186],\n",
      "         [-0.4222,  0.0143, -0.2090,  ...,  0.1803,  0.1839, -0.2472],\n",
      "         [-0.6779,  0.1105,  0.0567,  ...,  0.5970,  0.0114, -0.6039],\n",
      "         ...,\n",
      "         [-0.0919,  0.1501, -0.2479,  ...,  0.6794,  0.3203, -0.1716],\n",
      "         [-0.4376,  0.0629, -0.5719,  ...,  0.3877,  0.3003,  0.1138],\n",
      "         [-0.2927,  0.1486, -0.6206,  ...,  0.3066,  0.1946, -0.0938]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1583, -0.2818, -0.0182,  ..., -0.2061, -0.2805, -0.1600],\n",
      "         [-0.2592, -0.3882, -0.1150,  ..., -0.1680, -0.1489,  0.0988],\n",
      "         [-0.2152, -0.0638, -0.1254,  ..., -0.2814, -0.0311, -0.0107],\n",
      "         ...,\n",
      "         [-0.1737, -0.2618, -0.5120,  ..., -0.0923, -0.0956,  0.1098],\n",
      "         [-0.3867, -0.2057, -0.6104,  ..., -0.2400, -0.3640,  0.3155],\n",
      "         [-0.0977, -0.2881, -0.6433,  ..., -0.2556, -0.4341,  0.2664]],\n",
      "\n",
      "        [[ 0.0485, -0.1026, -0.6066,  ..., -0.2244,  0.0790, -0.0895],\n",
      "         [-0.1702,  0.0875, -0.4443,  ..., -0.2650, -0.3612, -0.1203],\n",
      "         [-0.2917,  0.1134, -0.3397,  ..., -0.2966,  0.2317, -0.1940],\n",
      "         ...,\n",
      "         [-0.0165, -0.4425, -0.4608,  ..., -0.0054, -0.1787,  0.1325],\n",
      "         [-0.2438, -0.3297, -0.6091,  ..., -0.3625, -0.3172,  0.2344],\n",
      "         [-0.1950, -0.0095, -0.4157,  ..., -0.2369,  0.0843,  0.0016]],\n",
      "\n",
      "        [[ 0.2076, -0.3250, -0.1136,  ..., -0.0351, -0.2964,  0.0988],\n",
      "         [-0.2785, -0.2103, -0.1600,  ..., -0.3751, -0.2677,  0.0801],\n",
      "         [-0.2294, -0.0604, -0.4649,  ..., -0.4687, -0.1965,  0.0154],\n",
      "         ...,\n",
      "         [-0.0567, -0.2858, -0.4151,  ...,  0.0846, -0.2033,  0.4252],\n",
      "         [-0.1853, -0.4993, -0.2428,  ..., -0.4048, -0.3191,  0.6451],\n",
      "         [-0.2705, -0.4603, -0.2572,  ..., -0.5736, -0.4050,  0.3117]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1468193531036377\n",
      "Model outputs:  tensor([[[-1.7698e-01, -1.7836e-01, -1.8042e-01,  ..., -2.8587e-03,\n",
      "           1.9852e-01,  6.6284e-02],\n",
      "         [-6.1557e-01, -7.0773e-03, -4.6903e-01,  ...,  1.7295e-01,\n",
      "           1.2946e-01,  3.7158e-01],\n",
      "         [-3.0302e-01, -1.4874e-01, -3.1511e-01,  ...,  1.8421e-01,\n",
      "           1.9989e-01,  1.9291e-01],\n",
      "         ...,\n",
      "         [-8.6614e-01,  1.4607e-01, -2.4221e-01,  ...,  3.1732e-01,\n",
      "           1.1002e-01,  3.4644e-01],\n",
      "         [-8.5582e-01,  2.7218e-01, -4.8867e-01,  ...,  5.0968e-01,\n",
      "           1.0791e-01,  1.2236e-01],\n",
      "         [-5.6294e-01,  2.4426e-01, -3.2116e-01,  ...,  2.6845e-01,\n",
      "           3.2232e-02,  2.7115e-01]],\n",
      "\n",
      "        [[ 9.7251e-02, -4.8207e-01, -1.1544e-01,  ...,  8.3064e-02,\n",
      "          -1.9193e-01,  1.6994e-01],\n",
      "         [-8.6494e-04, -4.5882e-01, -2.9147e-01,  ...,  6.2140e-02,\n",
      "          -1.0327e-01,  3.7865e-01],\n",
      "         [ 9.2914e-03, -4.1247e-01, -3.9873e-01,  ..., -7.5214e-02,\n",
      "          -8.6726e-02,  2.3522e-01],\n",
      "         ...,\n",
      "         [-4.8007e-01, -1.0780e-01, -5.4362e-01,  ..., -2.5442e-03,\n",
      "          -6.3344e-01,  3.0483e-01],\n",
      "         [-6.2350e-01, -1.5173e-01, -3.1369e-01,  ...,  1.1934e-01,\n",
      "          -2.7034e-01,  2.3327e-01],\n",
      "         [-2.4534e-01, -2.7456e-01, -3.2038e-01,  ..., -1.7976e-01,\n",
      "          -4.2347e-01,  3.3428e-01]],\n",
      "\n",
      "        [[-2.7011e-01, -2.8158e-01, -2.3318e-03,  ...,  1.7090e-01,\n",
      "           3.3592e-01, -3.3843e-03],\n",
      "         [-3.0955e-01, -4.0348e-02, -1.4394e-01,  ...,  7.0151e-01,\n",
      "           2.5312e-01, -5.5804e-02],\n",
      "         [-4.6131e-01,  9.1937e-02,  1.4373e-01,  ...,  6.4272e-01,\n",
      "           2.0206e-01,  1.4916e-05],\n",
      "         ...,\n",
      "         [-6.5053e-01,  1.0293e-01, -4.8414e-01,  ...,  7.6797e-01,\n",
      "          -2.0025e-01,  6.8903e-02],\n",
      "         [-5.0921e-01,  5.2130e-02, -1.3640e-01,  ...,  5.3340e-01,\n",
      "           2.6387e-01,  2.6499e-01],\n",
      "         [-5.2297e-01,  2.6780e-01, -3.0033e-01,  ...,  5.0887e-01,\n",
      "          -9.8344e-02,  1.0184e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.7539e-02, -3.4805e-01, -2.5107e-01,  ...,  2.7692e-01,\n",
      "           4.0675e-01, -1.0256e-01],\n",
      "         [-3.6762e-01,  5.1293e-02, -1.5961e-01,  ...,  5.2731e-01,\n",
      "           1.7176e-01,  3.0006e-03],\n",
      "         [-2.0981e-01, -1.3177e-01,  3.6748e-02,  ...,  4.9208e-01,\n",
      "           4.4882e-01, -3.2012e-01],\n",
      "         ...,\n",
      "         [-7.7061e-01, -2.0112e-01, -4.5572e-01,  ...,  5.8473e-01,\n",
      "          -1.1774e-01,  3.7593e-01],\n",
      "         [-7.7721e-01, -5.7480e-02, -1.4520e-01,  ...,  6.0656e-01,\n",
      "           2.5231e-01, -3.6296e-02],\n",
      "         [-5.5438e-01, -7.4705e-02,  1.8283e-02,  ...,  4.3650e-01,\n",
      "           1.1243e-01,  9.6491e-02]],\n",
      "\n",
      "        [[ 2.8836e-01, -6.4857e-01, -3.2125e-02,  ..., -1.6134e-01,\n",
      "          -1.5988e-01,  3.2947e-01],\n",
      "         [-1.7707e-01, -2.5329e-01, -2.8798e-01,  ..., -9.3568e-02,\n",
      "          -1.9723e-01,  5.3040e-01],\n",
      "         [-4.0566e-02, -3.5324e-01, -2.8181e-01,  ..., -4.5753e-01,\n",
      "          -2.0587e-01,  4.9599e-01],\n",
      "         ...,\n",
      "         [-4.5741e-01, -3.3858e-01, -4.0709e-01,  ..., -2.3090e-01,\n",
      "          -4.6754e-01,  6.2086e-01],\n",
      "         [-1.0095e-01, -5.6211e-01,  8.5713e-02,  ..., -1.5706e-01,\n",
      "          -2.2675e-01,  4.5875e-01],\n",
      "         [-2.1757e-01, -4.2895e-01, -3.6554e-01,  ...,  5.0424e-02,\n",
      "          -3.1547e-01,  5.6409e-01]],\n",
      "\n",
      "        [[-2.7160e-01, -8.7836e-02, -6.9040e-02,  ...,  2.1771e-01,\n",
      "           2.4888e-01, -1.9106e-02],\n",
      "         [-5.2975e-01, -2.3790e-01, -2.0082e-01,  ...,  5.9104e-01,\n",
      "           2.5864e-01,  6.5106e-02],\n",
      "         [-3.3873e-01,  3.4756e-01, -1.8310e-01,  ...,  6.0412e-01,\n",
      "           3.6645e-01, -1.2539e-01],\n",
      "         ...,\n",
      "         [-5.8889e-01, -3.5813e-02, -4.6928e-01,  ...,  7.2355e-01,\n",
      "          -1.2600e-01,  3.0461e-01],\n",
      "         [-6.3416e-01,  2.8817e-01, -1.3619e-01,  ...,  7.5804e-01,\n",
      "           3.6792e-01,  9.7482e-02],\n",
      "         [-8.6333e-01,  3.2312e-01, -2.7293e-01,  ...,  7.0638e-01,\n",
      "          -2.3337e-01,  2.4372e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.6154539585113525\n",
      "Model outputs:  tensor([[[ 2.1026e-01, -2.9419e-01, -4.4284e-01,  ..., -3.4747e-01,\n",
      "           5.7055e-01,  5.5351e-01],\n",
      "         [ 1.0766e-01, -1.2983e-01, -3.5680e-01,  ..., -2.4926e-01,\n",
      "           9.0345e-02,  3.6930e-01],\n",
      "         [ 1.5461e-01, -1.3067e-01, -4.1067e-01,  ...,  3.9744e-02,\n",
      "          -3.7917e-02,  1.3338e-01],\n",
      "         ...,\n",
      "         [-1.2107e-01, -3.2201e-01, -5.5409e-01,  ..., -5.7128e-02,\n",
      "           1.7190e-01,  3.2919e-01],\n",
      "         [ 9.5475e-02, -1.3712e-01, -3.4566e-01,  ..., -4.2544e-01,\n",
      "           2.2537e-01,  5.6844e-01],\n",
      "         [ 2.3701e-01, -1.7027e-02, -3.2153e-01,  ..., -1.8566e-01,\n",
      "           2.3555e-01,  1.9691e-01]],\n",
      "\n",
      "        [[-4.2348e-01,  7.6000e-02, -2.1147e-01,  ...,  7.7790e-01,\n",
      "           3.1598e-01,  5.6542e-03],\n",
      "         [-7.0576e-01, -4.6840e-02, -9.2186e-02,  ...,  3.9227e-01,\n",
      "           2.0890e-01, -2.6154e-01],\n",
      "         [-6.7120e-01, -2.6671e-01, -2.2165e-01,  ...,  7.7389e-01,\n",
      "           7.5763e-02, -2.8637e-02],\n",
      "         ...,\n",
      "         [-5.4941e-01, -2.1185e-01, -8.9674e-02,  ...,  6.6934e-01,\n",
      "           6.5759e-02, -1.0405e-01],\n",
      "         [-3.4272e-01,  1.3659e-02, -1.0577e-01,  ...,  5.1938e-01,\n",
      "           1.1417e-01, -1.8942e-01],\n",
      "         [-6.7182e-01,  2.6082e-01, -9.8174e-03,  ...,  4.5013e-01,\n",
      "           4.6146e-01, -3.1600e-01]],\n",
      "\n",
      "        [[-5.6511e-01,  1.1085e-01, -3.7871e-01,  ...,  5.9492e-01,\n",
      "           1.9164e-01, -5.0959e-03],\n",
      "         [-4.3098e-01,  6.8735e-02, -3.2688e-01,  ...,  2.0967e-01,\n",
      "           9.8110e-02, -2.2923e-01],\n",
      "         [-3.7380e-01, -2.7932e-01, -4.6447e-01,  ...,  7.7223e-01,\n",
      "          -1.9288e-01, -7.2290e-02],\n",
      "         ...,\n",
      "         [-4.5197e-01, -2.3534e-01, -3.7113e-01,  ...,  6.4434e-01,\n",
      "          -8.8309e-02, -1.5178e-01],\n",
      "         [-6.4747e-01,  1.2996e-01, -1.1126e-01,  ...,  4.8440e-01,\n",
      "           3.3049e-01, -7.7909e-02],\n",
      "         [-7.4189e-01, -2.2174e-02, -2.5477e-01,  ...,  1.9738e-01,\n",
      "           2.7600e-01, -1.9851e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.4938e-02, -2.1958e-01, -3.5714e-01,  ...,  2.4396e-01,\n",
      "          -1.1636e-01,  5.0394e-01],\n",
      "         [-4.0734e-01, -2.6841e-01, -3.5260e-01,  ...,  1.6384e-02,\n",
      "          -2.0564e-01,  9.1278e-02],\n",
      "         [-1.8414e-01, -4.2783e-01, -7.6958e-01,  ...,  8.9379e-02,\n",
      "          -3.4049e-01,  3.8274e-01],\n",
      "         ...,\n",
      "         [-3.8576e-02, -4.5613e-01, -5.5428e-01,  ..., -5.0040e-03,\n",
      "          -5.6907e-01,  2.5404e-01],\n",
      "         [ 6.4522e-02, -1.9007e-01, -1.7905e-01,  ..., -8.7718e-02,\n",
      "          -5.1167e-01,  1.6155e-01],\n",
      "         [-2.5926e-01, -2.5904e-01, -3.4155e-01,  ..., -1.4059e-01,\n",
      "          -2.0164e-01,  1.4898e-01]],\n",
      "\n",
      "        [[-5.2154e-01,  6.3008e-02, -6.6994e-01,  ...,  4.5555e-01,\n",
      "           1.7298e-01,  7.6750e-02],\n",
      "         [-6.8572e-01,  3.5556e-02, -5.6602e-01,  ...,  2.5052e-01,\n",
      "           2.8412e-01, -2.2696e-01],\n",
      "         [-4.6736e-01, -2.7393e-01, -4.9327e-01,  ...,  5.4180e-01,\n",
      "          -2.6417e-01,  4.5886e-02],\n",
      "         ...,\n",
      "         [-4.1440e-01, -4.8645e-01, -5.4337e-01,  ...,  5.9725e-01,\n",
      "          -2.3580e-02,  2.5043e-01],\n",
      "         [-7.0600e-01,  1.2050e-01, -6.1282e-01,  ...,  7.2662e-01,\n",
      "          -9.3001e-04, -2.8992e-02],\n",
      "         [-6.9858e-01,  1.8789e-02, -4.6377e-01,  ...,  3.7241e-01,\n",
      "           2.3099e-02, -3.0661e-01]],\n",
      "\n",
      "        [[-5.6698e-01, -3.1443e-01, -5.6732e-01,  ...,  2.2297e-01,\n",
      "          -2.8567e-01,  1.9783e-01],\n",
      "         [-5.2018e-01,  1.9440e-03, -8.9738e-01,  ...,  1.1410e-01,\n",
      "           5.4676e-02,  1.0878e-01],\n",
      "         [-5.8079e-01, -5.1234e-01, -4.5167e-01,  ...,  2.8060e-01,\n",
      "          -7.6403e-02,  1.2662e-01],\n",
      "         ...,\n",
      "         [-4.7358e-01, -1.9504e-01, -6.5623e-01,  ...,  3.0385e-01,\n",
      "          -1.6696e-01,  2.7602e-04],\n",
      "         [-4.8760e-01, -2.5792e-01, -4.1664e-01,  ...,  1.3627e-01,\n",
      "          -1.9596e-01,  1.8219e-01],\n",
      "         [-5.5539e-01, -3.5572e-02, -4.1863e-01,  ..., -2.5941e-02,\n",
      "          -3.7611e-02, -1.2303e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9391412734985352\n",
      "Model outputs:  tensor([[[-0.2562, -0.1250, -0.8048,  ...,  0.3454, -0.3081,  0.1185],\n",
      "         [-0.7143, -0.2795, -0.8807,  ...,  0.3358, -0.1697,  0.3437],\n",
      "         [-0.2321, -0.2687, -0.8202,  ...,  0.5807,  0.0517,  0.2054],\n",
      "         ...,\n",
      "         [-0.2628, -0.3870, -0.5719,  ...,  0.4109, -0.0712,  0.1246],\n",
      "         [-0.2439, -0.2414, -0.5324,  ...,  0.5672,  0.0352,  0.0588],\n",
      "         [-0.2632, -0.0241, -0.7092,  ...,  0.3920, -0.1053,  0.0166]],\n",
      "\n",
      "        [[-0.1745, -0.4013, -0.2372,  ...,  0.1686, -0.3155,  0.7219],\n",
      "         [ 0.1312, -0.3770, -0.5539,  ..., -0.0747, -0.4114,  0.5212],\n",
      "         [ 0.1112, -0.4605, -0.4718,  ...,  0.2320,  0.0152,  0.4573],\n",
      "         ...,\n",
      "         [ 0.0669, -0.2413, -0.5403,  ...,  0.1902, -0.1507,  0.5610],\n",
      "         [ 0.1759, -0.4057, -0.1472,  ..., -0.0742, -0.2870,  0.3898],\n",
      "         [-0.0566, -0.2692, -0.4231,  ...,  0.0528, -0.2397,  0.1736]],\n",
      "\n",
      "        [[-0.1217, -0.3285, -0.7517,  ...,  0.6127,  0.0859,  0.2067],\n",
      "         [-0.1230, -0.1780, -0.7592,  ...,  0.2914, -0.0648,  0.1150],\n",
      "         [-0.1367, -0.1266, -0.7623,  ...,  0.7259,  0.2169,  0.0831],\n",
      "         ...,\n",
      "         [-0.2208, -0.2312, -0.6776,  ...,  0.6149,  0.0401, -0.0214],\n",
      "         [-0.1368, -0.3173, -0.5284,  ...,  0.3092, -0.1437,  0.2098],\n",
      "         [ 0.0653,  0.1510, -0.6840,  ...,  0.4621, -0.0855,  0.0318]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3597, -0.2007, -0.5725,  ...,  0.0650, -0.3400,  0.5958],\n",
      "         [-0.0059, -0.3394, -0.4617,  ...,  0.1380, -0.2846,  0.4171],\n",
      "         [ 0.0301, -0.3072, -0.0899,  ...,  0.2880, -0.1333,  0.3277],\n",
      "         ...,\n",
      "         [ 0.0214, -0.1184, -0.2627,  ...,  0.0155,  0.0248,  0.4145],\n",
      "         [ 0.1407, -0.4399, -0.3365,  ..., -0.0823, -0.2206,  0.2702],\n",
      "         [ 0.2416, -0.4116, -0.3760,  ..., -0.1438, -0.2887,  0.2376]],\n",
      "\n",
      "        [[-0.4901, -0.2706,  0.0665,  ...,  0.8211,  0.5390, -0.2341],\n",
      "         [-0.3743, -0.1402, -0.3223,  ...,  0.8734,  0.1074, -0.0513],\n",
      "         [-0.1490, -0.3943, -0.3612,  ...,  0.7504,  0.3096,  0.0137],\n",
      "         ...,\n",
      "         [ 0.1104,  0.0179,  0.0185,  ...,  1.0018,  0.2780,  0.1889],\n",
      "         [-0.3965, -0.0943,  0.0703,  ...,  0.7257,  0.2363, -0.0019],\n",
      "         [-0.0816, -0.0683, -0.0091,  ...,  0.8599,  0.0390, -0.2144]],\n",
      "\n",
      "        [[-0.0985, -0.2913, -0.2369,  ..., -0.0683, -0.2076,  0.3512],\n",
      "         [-0.1247, -0.0533, -0.5079,  ...,  0.3072, -0.3141,  0.6862],\n",
      "         [ 0.0961, -0.5124, -0.3863,  ...,  0.0863,  0.1008,  0.5934],\n",
      "         ...,\n",
      "         [ 0.0559, -0.2090, -0.3938,  ..., -0.0178,  0.0082,  0.4220],\n",
      "         [ 0.0700, -0.2559, -0.3976,  ..., -0.1514, -0.5807,  0.3460],\n",
      "         [-0.0076, -0.2568, -0.3694,  ...,  0.0324, -0.2389,  0.4050]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2509311437606812\n",
      "Model outputs:  tensor([[[-0.5424,  0.0104, -0.1495,  ...,  0.2861,  0.0282, -0.2869],\n",
      "         [-0.2962, -0.1605, -0.5370,  ...,  0.5836,  0.1662,  0.0247],\n",
      "         [-0.5069, -0.1187, -0.2780,  ...,  0.3163, -0.0042, -0.0182],\n",
      "         ...,\n",
      "         [-0.4230,  0.0886, -0.2265,  ...,  0.2386, -0.0171, -0.0588],\n",
      "         [-0.6229, -0.1989, -0.1926,  ...,  0.2069,  0.1470, -0.0814],\n",
      "         [-0.3460, -0.2638, -0.2626,  ...,  0.5284,  0.3039,  0.0531]],\n",
      "\n",
      "        [[-0.0887, -0.3264,  0.1399,  ...,  0.3011, -0.1457, -0.6945],\n",
      "         [ 0.0038, -0.3644, -0.0767,  ...,  0.3651, -0.1230, -0.3407],\n",
      "         [ 0.0267, -0.2448,  0.0591,  ..., -0.0205, -0.1803, -0.7947],\n",
      "         ...,\n",
      "         [ 0.3127, -0.4089, -0.1140,  ..., -0.2745,  0.1627, -0.6443],\n",
      "         [ 0.1832, -0.5213, -0.0430,  ..., -0.1999, -0.0388, -0.5602],\n",
      "         [ 0.0701, -0.4826, -0.0958,  ...,  0.2460, -0.2302, -0.6526]],\n",
      "\n",
      "        [[-0.5181, -0.1697, -0.5025,  ...,  0.2301, -0.0278, -0.2182],\n",
      "         [-0.3213,  0.0299, -0.7426,  ...,  0.3264, -0.3304, -0.0896],\n",
      "         [-0.3168,  0.0573, -0.6131,  ...,  0.1545, -0.0705, -0.0786],\n",
      "         ...,\n",
      "         [-0.3739, -0.1765, -0.4941,  ..., -0.0134, -0.1444, -0.3037],\n",
      "         [-0.1437, -0.0939, -0.5962,  ..., -0.0191, -0.1254, -0.1477],\n",
      "         [-0.3028, -0.1885, -0.6515,  ...,  0.4000, -0.0450, -0.0206]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6867, -0.0390, -0.1343,  ...,  0.1332,  0.0137, -0.0029],\n",
      "         [-0.7388, -0.0939, -0.3720,  ...,  0.2955,  0.0712,  0.1700],\n",
      "         [-0.4570, -0.0650, -0.1899,  ...,  0.2742,  0.0338, -0.0111],\n",
      "         ...,\n",
      "         [-0.4201,  0.1683, -0.2232,  ...,  0.3104, -0.0073, -0.1377],\n",
      "         [-0.5141, -0.1778, -0.1298,  ...,  0.0374, -0.0279, -0.1955],\n",
      "         [-0.1670, -0.1437, -0.3328,  ...,  0.5445,  0.0054, -0.1982]],\n",
      "\n",
      "        [[-0.3228, -0.2757,  0.0017,  ...,  0.4523,  0.1640, -0.2519],\n",
      "         [-0.5922, -0.0523, -0.1834,  ...,  0.6815,  0.2567, -0.0440],\n",
      "         [-0.4895, -0.1158, -0.2662,  ...,  0.3189,  0.2244, -0.3145],\n",
      "         ...,\n",
      "         [-0.6979, -0.0019, -0.0169,  ...,  0.6874,  0.2461, -0.1902],\n",
      "         [-0.5308,  0.0057, -0.1334,  ...,  0.3489,  0.0929, -0.2279],\n",
      "         [-0.4668, -0.3687, -0.3407,  ...,  0.7347,  0.3429, -0.2111]],\n",
      "\n",
      "        [[ 0.1136, -0.4013, -0.2969,  ..., -0.2228,  0.2688, -0.0398],\n",
      "         [-0.0379, -0.0733, -0.2323,  ..., -0.0266,  0.2433, -0.5755],\n",
      "         [ 0.1309, -0.3827, -0.4073,  ..., -0.1662, -0.1251, -0.4481],\n",
      "         ...,\n",
      "         [-0.1958, -0.2162, -0.3674,  ..., -0.4019,  0.3839, -0.1313],\n",
      "         [ 0.3555, -0.2008, -0.1942,  ..., -0.3162,  0.4252, -0.1163],\n",
      "         [-0.0051, -0.0951, -0.3260,  ..., -0.1727,  0.3192, -0.2764]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8288236856460571\n",
      "Model outputs:  tensor([[[-0.4525,  0.2711, -0.5718,  ...,  0.7201,  0.1945, -0.1475],\n",
      "         [-0.6053, -0.0322, -0.2146,  ...,  0.8146,  0.0075, -0.2365],\n",
      "         [-0.4448,  0.2249, -0.3584,  ...,  0.6096, -0.0487,  0.1511],\n",
      "         ...,\n",
      "         [-0.4045,  0.0308, -0.0183,  ...,  0.2503,  0.1203, -0.1486],\n",
      "         [-0.5098,  0.1158, -0.1608,  ...,  0.6564,  0.0087, -0.0315],\n",
      "         [-0.4621,  0.1148, -0.1628,  ...,  0.4506,  0.0893, -0.1543]],\n",
      "\n",
      "        [[-0.5332,  0.3944, -0.3038,  ...,  0.5556,  0.1361, -0.2768],\n",
      "         [-0.3429, -0.1399, -0.0520,  ...,  0.6939,  0.1730,  0.1707],\n",
      "         [-0.3479,  0.2575, -0.4003,  ...,  0.7920,  0.1406,  0.0627],\n",
      "         ...,\n",
      "         [-0.3772,  0.0026, -0.0352,  ...,  0.4055, -0.2569, -0.2198],\n",
      "         [-0.7805,  0.2912,  0.1501,  ...,  0.4816,  0.0460, -0.4294],\n",
      "         [-0.5184,  0.0505, -0.2994,  ...,  0.3713,  0.1689,  0.0036]],\n",
      "\n",
      "        [[-0.5603,  0.2848, -0.3447,  ...,  0.7215,  0.0205,  0.0412],\n",
      "         [-0.7274,  0.2680, -0.4027,  ...,  0.6993,  0.3989,  0.1127],\n",
      "         [-0.5852,  0.1897, -0.5561,  ...,  0.5727,  0.1978,  0.0825],\n",
      "         ...,\n",
      "         [-0.4198, -0.0241,  0.2971,  ...,  0.3852,  0.2054, -0.2075],\n",
      "         [-0.7629,  0.0906, -0.1886,  ...,  0.3934,  0.0528,  0.1561],\n",
      "         [-0.5217,  0.4273, -0.2795,  ...,  0.5412,  0.0519, -0.0309]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4063, -0.0222, -0.2929,  ...,  0.5162,  0.0402, -0.2816],\n",
      "         [-0.5559,  0.0201, -0.3446,  ...,  0.5896,  0.1630, -0.0422],\n",
      "         [-0.7596,  0.1569, -0.2747,  ...,  0.6385,  0.1221, -0.1340],\n",
      "         ...,\n",
      "         [-0.3235, -0.0061,  0.2970,  ...,  0.3360,  0.0249, -0.0948],\n",
      "         [-0.6877,  0.1498, -0.0898,  ...,  0.4680, -0.0374, -0.1060],\n",
      "         [-0.5947,  0.0842, -0.1969,  ...,  0.8019,  0.1869, -0.2174]],\n",
      "\n",
      "        [[-0.5960,  0.3134, -0.4746,  ...,  0.2731, -0.0314,  0.1647],\n",
      "         [-0.6535,  0.2448, -0.4556,  ...,  0.3121,  0.4469,  0.2477],\n",
      "         [-0.6530,  0.2311, -0.4829,  ...,  0.1372,  0.2330,  0.2928],\n",
      "         ...,\n",
      "         [-0.5138,  0.2587, -0.0337,  ...,  0.2439,  0.2171, -0.0258],\n",
      "         [-0.8548,  0.2394, -0.4416,  ..., -0.0594,  0.1540,  0.3991],\n",
      "         [-0.6435,  0.3555, -0.6569,  ...,  0.4091,  0.0612,  0.4719]],\n",
      "\n",
      "        [[-0.4956,  0.3815, -0.8636,  ...,  0.6672, -0.0765, -0.0151],\n",
      "         [-0.3630,  0.4681, -0.7003,  ...,  0.8311, -0.0460, -0.0017],\n",
      "         [-0.3358,  0.1785, -0.5895,  ...,  0.7868,  0.0410,  0.0388],\n",
      "         ...,\n",
      "         [-0.3514,  0.0210, -0.4033,  ...,  0.7161,  0.0374, -0.0940],\n",
      "         [-0.8562,  0.4516, -0.4880,  ...,  0.6089, -0.1197, -0.1605],\n",
      "         [-0.5165,  0.0918, -0.4678,  ...,  0.6450, -0.1264,  0.1183]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9522730112075806\n",
      "Model outputs:  tensor([[[ 0.3165, -0.1582, -0.5165,  ..., -0.0751,  0.3647,  0.2607],\n",
      "         [ 0.4215, -0.1707, -0.2451,  ..., -0.1485,  0.1497,  0.1517],\n",
      "         [ 0.0493, -0.3333, -0.4739,  ..., -0.0968,  0.2637,  0.3895],\n",
      "         ...,\n",
      "         [-0.0459,  0.0827, -0.4908,  ..., -0.1500,  0.1221,  0.1740],\n",
      "         [ 0.3174, -0.2180, -0.4689,  ..., -0.1765,  0.3284, -0.0951],\n",
      "         [ 0.3187, -0.3290, -0.6475,  ..., -0.1180,  0.3922,  0.5595]],\n",
      "\n",
      "        [[-0.1854, -0.1975, -0.2611,  ..., -0.1220, -0.1143,  0.5903],\n",
      "         [-0.0676, -0.4453, -0.2684,  ..., -0.1182, -0.6632,  0.0994],\n",
      "         [-0.0394, -0.4074, -0.2179,  ..., -0.1101, -0.0452,  0.4253],\n",
      "         ...,\n",
      "         [-0.1261, -0.2315, -0.3213,  ...,  0.0827, -0.4391,  0.3708],\n",
      "         [ 0.0170, -0.4030, -0.3250,  ...,  0.0377, -0.1274,  0.3784],\n",
      "         [ 0.1482, -0.2452, -0.2367,  ...,  0.0562, -0.1506,  0.0020]],\n",
      "\n",
      "        [[-0.4015, -0.1577, -0.5885,  ...,  0.5593,  0.2341,  0.2976],\n",
      "         [-0.1821, -0.3960, -0.3233,  ...,  0.4806, -0.2264, -0.0239],\n",
      "         [-0.6070, -0.0210, -0.4111,  ...,  0.4960, -0.0652,  0.0592],\n",
      "         ...,\n",
      "         [-0.6172, -0.0178, -0.2509,  ...,  0.6909,  0.0021,  0.1805],\n",
      "         [-0.0349, -0.1513, -0.3598,  ...,  0.5712,  0.1647,  0.0867],\n",
      "         [-0.4465, -0.0461, -0.7361,  ...,  0.4007,  0.2972,  0.2889]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0861, -0.4655,  0.0862,  ...,  0.4653, -0.2661, -0.3301],\n",
      "         [ 0.2881, -0.3989, -0.1653,  ...,  0.3380, -0.2206, -0.5767],\n",
      "         [-0.0300, -0.3586,  0.1814,  ...,  0.4676, -0.0970, -0.3503],\n",
      "         ...,\n",
      "         [ 0.1583, -0.2577, -0.0475,  ...,  0.6985, -0.2573, -0.2372],\n",
      "         [ 0.0133, -0.6262,  0.0455,  ...,  0.6097, -0.2241, -0.4924],\n",
      "         [ 0.2277, -0.4166, -0.1011,  ...,  0.6541,  0.0047, -0.2036]],\n",
      "\n",
      "        [[-0.4119, -0.0284, -0.5676,  ...,  0.5773,  0.1493,  0.0299],\n",
      "         [-0.4025, -0.2616, -0.1237,  ...,  0.4731,  0.1177,  0.0608],\n",
      "         [-0.5830, -0.0645, -0.5058,  ...,  0.4504,  0.2368,  0.0834],\n",
      "         ...,\n",
      "         [-0.5479, -0.0736, -0.6122,  ...,  0.3416,  0.2240,  0.2788],\n",
      "         [-0.2236, -0.4232, -0.5247,  ...,  0.2093,  0.2368, -0.0780],\n",
      "         [-0.3898, -0.3819, -0.4599,  ...,  0.3527,  0.3280,  0.1361]],\n",
      "\n",
      "        [[-0.1248, -0.1065, -0.6211,  ...,  0.3841, -0.1586,  0.1485],\n",
      "         [ 0.1875, -0.2550, -0.4218,  ...,  0.3548, -0.4590, -0.1785],\n",
      "         [-0.1312, -0.0967, -0.6916,  ...,  0.3631, -0.1616,  0.2691],\n",
      "         ...,\n",
      "         [-0.1072, -0.0351, -0.5842,  ...,  0.6329, -0.1286, -0.0170],\n",
      "         [ 0.0180, -0.0275, -0.7075,  ...,  0.1302,  0.0382,  0.0603],\n",
      "         [ 0.0057, -0.2573, -0.5932,  ...,  0.4924,  0.0133,  0.0171]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8533398509025574\n",
      "Model outputs:  tensor([[[ 0.0466, -0.7457, -0.4353,  ...,  0.5543,  0.2178, -0.1440],\n",
      "         [-0.3627,  0.0942, -0.2355,  ...,  0.4981,  0.0461, -0.2565],\n",
      "         [-0.4761, -0.0818, -0.2595,  ...,  0.5306, -0.0868, -0.3203],\n",
      "         ...,\n",
      "         [-0.1915, -0.2905, -0.2358,  ...,  0.8653,  0.3694, -0.0040],\n",
      "         [-0.4374, -0.4097, -0.3136,  ...,  0.2496,  0.1689, -0.1606],\n",
      "         [-0.0571, -0.1486, -0.3371,  ...,  0.4212,  0.0270, -0.2138]],\n",
      "\n",
      "        [[-0.4799, -0.3698, -0.5395,  ...,  0.6718,  0.2128,  0.2073],\n",
      "         [-0.6100, -0.0114, -0.3763,  ...,  0.4363,  0.2172,  0.2572],\n",
      "         [-0.6739, -0.0914, -0.3155,  ...,  0.3612, -0.0735,  0.0640],\n",
      "         ...,\n",
      "         [-0.1446, -0.3651, -0.6139,  ...,  0.4191,  0.2234,  0.1831],\n",
      "         [-0.8931,  0.0205, -0.3356,  ...,  0.1806,  0.2621,  0.0433],\n",
      "         [-0.8340, -0.2312, -0.3632,  ...,  0.0971,  0.1154, -0.1811]],\n",
      "\n",
      "        [[ 0.0828, -0.4194, -0.6358,  ...,  0.3088, -0.0143,  0.4408],\n",
      "         [-0.4515, -0.2388, -0.3172,  ..., -0.2416, -0.1997,  0.1462],\n",
      "         [-0.2599, -0.1619, -0.5725,  ..., -0.1679, -0.1533,  0.0994],\n",
      "         ...,\n",
      "         [ 0.1723, -0.4135, -0.3852,  ...,  0.3059, -0.0608,  0.1909],\n",
      "         [ 0.0194, -0.3065, -0.4027,  ..., -0.0969,  0.0028,  0.1352],\n",
      "         [-0.0051, -0.2202, -0.5096,  ..., -0.0506, -0.1382,  0.2743]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3008, -0.5604, -0.5810,  ...,  0.2185, -0.4095,  0.4987],\n",
      "         [-0.2858, -0.4030, -0.6776,  ..., -0.4900, -0.4597,  0.4298],\n",
      "         [-0.1862, -0.3739, -0.5843,  ..., -0.0880, -0.3437,  0.4907],\n",
      "         ...,\n",
      "         [-0.1858, -0.3061, -0.1674,  ..., -0.1009, -0.1445,  0.3723],\n",
      "         [ 0.1747, -0.5163, -0.6144,  ..., -0.2403, -0.1059,  0.3353],\n",
      "         [-0.1989, -0.2369, -0.7742,  ..., -0.2127,  0.0240,  0.3172]],\n",
      "\n",
      "        [[-0.2344, -0.6205, -0.2736,  ..., -0.1301, -0.4478,  0.2575],\n",
      "         [ 0.0681, -0.3450, -0.5905,  ..., -0.1832, -0.2084,  0.1432],\n",
      "         [-0.1126, -0.4066, -0.7458,  ...,  0.1816, -0.2164,  0.1792],\n",
      "         ...,\n",
      "         [ 0.0383, -0.1525, -0.5390,  ...,  0.3865, -0.3375,  0.3216],\n",
      "         [-0.0051, -0.3183, -0.4722,  ...,  0.0282, -0.2705,  0.0472],\n",
      "         [ 0.0461, -0.4106, -0.4236,  ...,  0.1905, -0.3115,  0.2503]],\n",
      "\n",
      "        [[-0.3277, -0.5111, -0.6733,  ...,  0.3540, -0.0050,  0.1840],\n",
      "         [-0.4375, -0.1127, -0.7569,  ...,  0.2155,  0.0711,  0.0738],\n",
      "         [-0.4297, -0.2794, -0.7788,  ...,  0.0875, -0.1316, -0.1260],\n",
      "         ...,\n",
      "         [-0.3941, -0.3187, -0.3985,  ...,  0.2621, -0.0558,  0.0827],\n",
      "         [-0.0982, -0.5910, -0.5739,  ..., -0.2395,  0.1700,  0.1267],\n",
      "         [-0.4093, -0.1766, -0.5484,  ..., -0.0291, -0.1901,  0.2486]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.280540108680725\n",
      "Model outputs:  tensor([[[-0.1929, -0.1395, -0.2958,  ..., -0.4125, -0.4119,  0.1626],\n",
      "         [-0.3110, -0.4383,  0.1034,  ...,  0.0859,  0.0884, -0.0637],\n",
      "         [-0.0764, -0.2892, -0.4113,  ..., -0.4117, -0.3840,  0.1890],\n",
      "         ...,\n",
      "         [-0.4519, -0.1834, -0.7408,  ..., -0.2486, -0.3598,  0.1470],\n",
      "         [-0.0284, -0.1038, -0.5991,  ...,  0.0327, -0.0606,  0.2932],\n",
      "         [-0.5670, -0.0371, -0.5839,  ...,  0.0511, -0.1776,  0.2385]],\n",
      "\n",
      "        [[ 0.0734, -0.3894, -0.0654,  ..., -0.2002, -0.3710,  0.1519],\n",
      "         [-0.2737, -0.2601, -0.3589,  ...,  0.0655,  0.0977,  0.1109],\n",
      "         [-0.0826, -0.2254, -0.2392,  ..., -0.5125,  0.0465,  0.2930],\n",
      "         ...,\n",
      "         [-0.0579, -0.3717, -0.5846,  ...,  0.1654, -0.1761,  0.3861],\n",
      "         [-0.3557,  0.0467, -0.4295,  ..., -0.1653, -0.2295,  0.3337],\n",
      "         [-0.3623, -0.2781, -0.4508,  ...,  0.4695, -0.2182,  0.3447]],\n",
      "\n",
      "        [[-0.5368,  0.0228, -0.5628,  ...,  0.2442, -0.1877, -0.3531],\n",
      "         [-0.4363, -0.2536, -0.3876,  ...,  0.7131,  0.0909, -0.2395],\n",
      "         [-0.1101, -0.0743, -0.3561,  ...,  0.2248, -0.0225, -0.2807],\n",
      "         ...,\n",
      "         [-0.4629, -0.1870, -0.7279,  ...,  0.8212,  0.0175, -0.1119],\n",
      "         [-0.3739, -0.0575, -0.5434,  ...,  0.4942,  0.1445,  0.0293],\n",
      "         [-0.5802, -0.1860, -0.7097,  ...,  0.7845, -0.3298,  0.0120]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0841, -0.1562, -0.3389,  ..., -0.2980,  0.0364, -0.1736],\n",
      "         [-0.1462, -0.3679,  0.1250,  ..., -0.0078,  0.0329, -0.1161],\n",
      "         [-0.0411, -0.3116,  0.0157,  ..., -0.2855, -0.3399,  0.2351],\n",
      "         ...,\n",
      "         [-0.4834, -0.2141, -0.3221,  ..., -0.1238, -0.3461,  0.6047],\n",
      "         [-0.1584, -0.0824, -0.2493,  ..., -0.0296, -0.2226,  0.0896],\n",
      "         [-0.2510, -0.2852, -0.5874,  ...,  0.0875, -0.5409,  0.5207]],\n",
      "\n",
      "        [[-0.0979, -0.2324,  0.1597,  ..., -0.3953, -0.3031,  0.1006],\n",
      "         [-0.1552, -0.2826, -0.2271,  ...,  0.1398, -0.2370,  0.3150],\n",
      "         [ 0.0938, -0.4882, -0.1740,  ..., -0.4866, -0.2991,  0.3127],\n",
      "         ...,\n",
      "         [-0.3840, -0.3490, -0.4506,  ..., -0.0836, -0.1678,  0.6547],\n",
      "         [-0.3251, -0.0012, -0.4714,  ..., -0.2753, -0.4448,  0.4030],\n",
      "         [-0.3140, -0.2779, -0.4467,  ...,  0.1189, -0.5918,  0.2498]],\n",
      "\n",
      "        [[-0.6547,  0.3139, -0.4622,  ..., -0.0716,  0.0380, -0.3080],\n",
      "         [-0.4467, -0.1628, -0.3449,  ...,  0.2590,  0.4499, -0.1553],\n",
      "         [-0.2224, -0.1618, -0.1049,  ..., -0.0724,  0.2318, -0.1290],\n",
      "         ...,\n",
      "         [-0.8568,  0.1269, -0.5397,  ...,  0.8536,  0.2417,  0.4384],\n",
      "         [-0.9119,  0.0502, -0.3358,  ...,  0.0389,  0.0974,  0.0323],\n",
      "         [-0.6954, -0.0109, -0.5865,  ...,  0.1542, -0.1011,  0.2343]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.1100969314575195\n",
      "Model outputs:  tensor([[[-0.3751, -0.2479, -0.1953,  ...,  0.5552,  0.0241, -0.1305],\n",
      "         [ 0.0985, -0.4157, -0.1567,  ...,  0.4732, -0.0648,  0.0524],\n",
      "         [-0.2518, -0.2758,  0.1161,  ...,  0.5572,  0.0598, -0.2041],\n",
      "         ...,\n",
      "         [-0.5460, -0.0481, -0.1622,  ...,  0.3950, -0.2700, -0.1873],\n",
      "         [-0.3257, -0.0439, -0.6109,  ...,  0.5705, -0.0172,  0.1396],\n",
      "         [-0.3169, -0.1445, -0.1987,  ...,  0.6007,  0.1840, -0.0564]],\n",
      "\n",
      "        [[-0.1819, -0.2963, -0.5437,  ...,  0.1299, -0.0256,  0.2583],\n",
      "         [-0.1045, -0.4381, -0.5135,  ...,  0.2645, -0.2778,  0.2705],\n",
      "         [-0.2227, -0.5281, -0.4025,  ...,  0.1963, -0.1120, -0.0922],\n",
      "         ...,\n",
      "         [-0.3779, -0.3265, -0.4844,  ...,  0.1074, -0.3327,  0.2735],\n",
      "         [-0.3911, -0.0868, -0.5281,  ...,  0.0274, -0.2520,  0.1743],\n",
      "         [-0.3978, -0.4660, -0.6672,  ...,  0.2811, -0.2595,  0.4810]],\n",
      "\n",
      "        [[ 0.0418, -0.1924, -0.3909,  ...,  0.4904, -0.0541, -0.2659],\n",
      "         [ 0.0830, -0.1221, -0.7993,  ...,  0.2748, -0.2864, -0.0062],\n",
      "         [ 0.0274, -0.3613, -0.4454,  ...,  0.2597, -0.1527,  0.0612],\n",
      "         ...,\n",
      "         [-0.0783, -0.0852, -0.7676,  ...,  0.4407, -0.1751,  0.0930],\n",
      "         [-0.1780, -0.2062, -0.6910,  ...,  0.5626, -0.2228,  0.2616],\n",
      "         [-0.2781, -0.1554, -0.7737,  ...,  0.3338, -0.2620,  0.2576]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1214, -0.2534, -0.3471,  ...,  0.2120, -0.1769,  0.0212],\n",
      "         [ 0.2698, -0.2795, -0.4884,  ...,  0.0676, -0.3934,  0.1811],\n",
      "         [ 0.0527, -0.1817, -0.3741,  ...,  0.2204, -0.3023,  0.3338],\n",
      "         ...,\n",
      "         [ 0.0502, -0.3135, -0.5639,  ...,  0.0830,  0.1161, -0.0099],\n",
      "         [-0.1875, -0.3211, -0.6877,  ...,  0.1441, -0.2486,  0.4060],\n",
      "         [-0.1828, -0.3436, -0.4583,  ...,  0.0537, -0.1005,  0.2964]],\n",
      "\n",
      "        [[-0.1960, -0.1892, -0.4833,  ...,  0.8013,  0.0683,  0.0427],\n",
      "         [ 0.0495, -0.3839, -0.6817,  ...,  0.5217, -0.2423,  0.0865],\n",
      "         [ 0.0244, -0.1774, -0.5830,  ...,  0.7328, -0.0619, -0.0796],\n",
      "         ...,\n",
      "         [-0.2168,  0.0857, -0.9267,  ...,  0.7846, -0.0387, -0.0333],\n",
      "         [-0.2404, -0.0727, -0.8187,  ...,  0.5026,  0.2789,  0.1238],\n",
      "         [-0.3060, -0.3137, -0.6759,  ...,  0.7244,  0.0629, -0.1393]],\n",
      "\n",
      "        [[-0.1567, -0.3312, -0.6841,  ...,  0.1662, -0.3725,  0.0770],\n",
      "         [-0.0754, -0.3158, -0.9229,  ...,  0.1447, -0.1603,  0.0516],\n",
      "         [-0.0840, -0.3249, -0.6653,  ...,  0.4457, -0.1598,  0.0752],\n",
      "         ...,\n",
      "         [-0.3937, -0.1500, -0.6212,  ...,  0.1557,  0.0782,  0.0651],\n",
      "         [-0.2500, -0.3279, -0.7626,  ...,  0.2070,  0.0808,  0.0726],\n",
      "         [-0.2774, -0.1900, -0.6606,  ...,  0.2885, -0.0023,  0.0527]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9849135279655457\n",
      "Model outputs:  tensor([[[ 0.1530, -0.2431, -0.3126,  ..., -0.2950,  0.0692,  0.0811],\n",
      "         [-0.4440, -0.2905, -0.4374,  ..., -0.0699, -0.0752,  0.2309],\n",
      "         [-0.2439, -0.4543, -0.3905,  ..., -0.0398, -0.3858,  0.1217],\n",
      "         ...,\n",
      "         [ 0.0134, -0.3226, -0.1682,  ...,  0.0446, -0.1771,  0.1401],\n",
      "         [ 0.0705, -0.3172, -0.2279,  ..., -0.1597, -0.2483,  0.4311],\n",
      "         [-0.1740, -0.0174, -0.3218,  ..., -0.3932, -0.1251,  0.2085]],\n",
      "\n",
      "        [[-0.2249,  0.2543, -0.3909,  ..., -0.0815, -0.1913,  0.2655],\n",
      "         [-0.5608,  0.1400, -0.6782,  ...,  0.2470, -0.0472,  0.0475],\n",
      "         [-0.6723, -0.1334, -0.4129,  ...,  0.1679,  0.0694,  0.1785],\n",
      "         ...,\n",
      "         [-0.4529,  0.1544, -0.6696,  ...,  0.0813, -0.0265, -0.0410],\n",
      "         [-0.2235, -0.2937, -0.5021,  ...,  0.3157, -0.2787,  0.1426],\n",
      "         [-0.1913,  0.1749, -0.6218,  ..., -0.3090,  0.1373,  0.0703]],\n",
      "\n",
      "        [[ 0.2442, -0.1478,  0.0828,  ...,  0.0167,  0.0296, -0.9563],\n",
      "         [-0.0740, -0.3653, -0.0042,  ...,  0.4629, -0.1537, -0.6819],\n",
      "         [ 0.0486, -0.4448,  0.1561,  ...,  0.5437, -0.0383, -0.8377],\n",
      "         ...,\n",
      "         [-0.0495, -0.5163,  0.1469,  ...,  0.5580, -0.0877, -0.5560],\n",
      "         [ 0.1091, -0.4204,  0.2006,  ...,  0.4845, -0.1201, -0.6131],\n",
      "         [ 0.0358, -0.2114,  0.0328,  ...,  0.2509,  0.0175, -0.6352]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0615,  0.2679, -0.7098,  ...,  0.2736,  0.4909, -0.0653],\n",
      "         [-0.8243,  0.1200, -0.4886,  ...,  0.5362,  0.2717, -0.0232],\n",
      "         [-0.5745,  0.0373, -0.3728,  ...,  0.2640,  0.2953,  0.0406],\n",
      "         ...,\n",
      "         [-0.4003,  0.0286, -0.5299,  ...,  0.4049,  0.2183, -0.0886],\n",
      "         [-0.5289, -0.0745, -0.3242,  ...,  0.4251,  0.1127,  0.3175],\n",
      "         [-0.6399, -0.0561, -0.3230,  ...,  0.2888,  0.3047, -0.2743]],\n",
      "\n",
      "        [[-0.3169,  0.3539, -0.5463,  ...,  0.0871,  0.0935, -0.2723],\n",
      "         [-0.1986,  0.0740, -0.5494,  ...,  0.7867,  0.0654, -0.3098],\n",
      "         [-0.6580,  0.0694, -0.6352,  ...,  0.7557,  0.1609, -0.0649],\n",
      "         ...,\n",
      "         [-0.3236,  0.0835, -0.5794,  ...,  0.7470,  0.0704, -0.1717],\n",
      "         [-0.2322,  0.0060, -0.3515,  ...,  0.4803,  0.0079, -0.1506],\n",
      "         [-0.1650,  0.0466, -0.3022,  ...,  0.5200,  0.1139, -0.0631]],\n",
      "\n",
      "        [[ 0.2542, -0.3086,  0.0247,  ..., -0.4754,  0.3577, -0.1192],\n",
      "         [ 0.2855, -0.1225, -0.2919,  ..., -0.3184,  0.4409, -0.1070],\n",
      "         [ 0.0171, -0.2927, -0.1833,  ..., -0.4092,  0.3950, -0.2781],\n",
      "         ...,\n",
      "         [-0.5158,  0.3898, -0.6078,  ...,  0.8235,  0.2957, -0.1539],\n",
      "         [-0.1391,  0.0465, -0.4839,  ...,  0.6828,  0.1206,  0.1576],\n",
      "         [-0.3727,  0.3456, -0.4502,  ...,  0.8352,  0.3311, -0.2852]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9373989105224609\n",
      "Model outputs:  tensor([[[-2.8078e-01,  4.6360e-02, -8.2798e-01,  ..., -1.2338e-01,\n",
      "          -1.8915e-02,  1.6776e-01],\n",
      "         [-3.1878e-01, -7.1350e-02, -6.6539e-01,  ...,  9.2468e-02,\n",
      "          -3.7671e-01,  1.0630e-02],\n",
      "         [-3.4434e-01,  8.9605e-02, -8.7362e-01,  ...,  2.4695e-01,\n",
      "          -9.5423e-02, -1.2562e-01],\n",
      "         ...,\n",
      "         [-1.4319e-01,  6.9231e-02, -6.5768e-01,  ..., -3.1927e-01,\n",
      "          -1.3412e-01, -1.2035e-01],\n",
      "         [-4.1542e-01, -5.7498e-02, -5.3561e-01,  ..., -2.4176e-01,\n",
      "          -1.7230e-01, -1.9209e-01],\n",
      "         [-3.4110e-01, -1.9887e-01, -4.2681e-01,  ..., -1.3550e-01,\n",
      "          -2.7134e-01, -2.0216e-01]],\n",
      "\n",
      "        [[-6.8131e-01, -3.3301e-01, -4.4181e-01,  ...,  1.8007e-01,\n",
      "           1.6500e-02, -3.3764e-01],\n",
      "         [-5.1963e-01,  7.7565e-02, -2.4679e-01,  ...,  4.1643e-01,\n",
      "           8.2794e-03,  1.1531e-02],\n",
      "         [-5.9361e-01, -5.5773e-02, -5.3980e-01,  ...,  3.7090e-01,\n",
      "           2.3234e-01, -8.8569e-02],\n",
      "         ...,\n",
      "         [-3.9885e-01,  4.2588e-01, -2.7379e-01,  ...,  1.1817e-01,\n",
      "           1.2854e-01, -2.7875e-01],\n",
      "         [-1.4307e-01, -3.2725e-02, -2.9618e-01,  ...,  3.2301e-01,\n",
      "           5.4522e-02, -2.3789e-01],\n",
      "         [-3.1350e-01, -2.4303e-01, -2.7148e-01,  ...,  2.2168e-01,\n",
      "           2.1735e-01, -2.6336e-02]],\n",
      "\n",
      "        [[-4.0885e-01,  1.6357e-01, -6.1192e-01,  ...,  9.8962e-03,\n",
      "          -2.0238e-01,  1.7656e-01],\n",
      "         [-4.9466e-01,  1.2339e-02, -3.7871e-01,  ...,  2.1122e-01,\n",
      "           1.5131e-01,  1.3259e-01],\n",
      "         [-5.2285e-01, -1.7052e-01, -3.2176e-01,  ...,  2.4464e-01,\n",
      "           2.8100e-01, -6.5710e-02],\n",
      "         ...,\n",
      "         [-6.8932e-01,  2.5888e-01, -1.9284e-01,  ...,  3.0607e-02,\n",
      "           1.3244e-01, -9.8577e-02],\n",
      "         [-2.6221e-01,  2.1222e-01, -5.3137e-01,  ..., -8.1188e-02,\n",
      "           8.7497e-02, -1.5247e-01],\n",
      "         [-5.0809e-01,  3.7365e-03, -4.8612e-01,  ..., -1.1934e-02,\n",
      "           1.6321e-01,  3.9872e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6755e-01,  2.0746e-02, -6.7051e-01,  ...,  4.6510e-02,\n",
      "          -1.4336e-01, -1.0242e-02],\n",
      "         [-3.3613e-01, -7.3333e-02, -5.3332e-01,  ...,  9.3406e-03,\n",
      "          -1.3509e-01, -1.4665e-01],\n",
      "         [-4.0766e-01, -1.7191e-01, -6.4277e-01,  ..., -6.1916e-02,\n",
      "          -3.2818e-02, -2.0800e-01],\n",
      "         ...,\n",
      "         [-2.8233e-01,  5.0484e-02, -4.5854e-01,  ..., -1.0411e-01,\n",
      "          -2.2763e-01, -4.3510e-01],\n",
      "         [-2.9844e-01, -6.7996e-02, -3.8689e-01,  ...,  1.1102e-01,\n",
      "          -2.1690e-01, -3.8263e-01],\n",
      "         [-2.5964e-01,  1.0526e-01, -5.2786e-01,  ...,  2.3497e-01,\n",
      "          -2.4885e-01, -4.2973e-02]],\n",
      "\n",
      "        [[-4.4835e-01, -1.5938e-01, -6.8637e-01,  ...,  4.6647e-02,\n",
      "           1.8545e-01, -1.2960e-01],\n",
      "         [-4.9334e-01, -9.1908e-03, -3.8163e-01,  ...,  4.2831e-01,\n",
      "           8.5756e-02, -2.5386e-01],\n",
      "         [-6.2496e-01, -2.1981e-01, -1.3701e-01,  ...,  3.4314e-01,\n",
      "           4.8736e-01, -2.3384e-01],\n",
      "         ...,\n",
      "         [-1.9262e-01,  6.9778e-02, -1.3665e-01,  ...,  2.9394e-01,\n",
      "           4.9248e-02, -2.2704e-01],\n",
      "         [-4.7287e-01,  4.2675e-02, -2.2313e-01,  ...,  3.5364e-01,\n",
      "          -6.0013e-04, -4.1878e-01],\n",
      "         [-3.9493e-01,  1.0009e-02, -5.9103e-02,  ...,  2.7353e-01,\n",
      "           2.1463e-01, -2.2064e-01]],\n",
      "\n",
      "        [[ 1.0218e-01, -2.8700e-01,  1.4071e-01,  ...,  1.6705e-01,\n",
      "          -2.1457e-01, -5.2097e-01],\n",
      "         [ 2.2483e-01, -2.9571e-01,  1.1846e-01,  ...,  3.9429e-01,\n",
      "          -4.1324e-01, -5.6113e-01],\n",
      "         [ 5.2420e-02, -3.5525e-01, -2.3524e-01,  ...,  4.0110e-01,\n",
      "          -6.4596e-02, -3.9451e-01],\n",
      "         ...,\n",
      "         [ 3.3542e-01, -7.3728e-02,  1.5785e-02,  ..., -4.3805e-01,\n",
      "          -5.1407e-02, -4.7611e-01],\n",
      "         [ 2.1348e-01, -3.1229e-01,  2.3700e-02,  ..., -2.2535e-01,\n",
      "          -1.4193e-01, -6.0424e-01],\n",
      "         [ 1.6557e-01, -2.1526e-01, -3.3081e-02,  ..., -1.4323e-01,\n",
      "          -4.9420e-02, -4.0938e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.220467209815979\n",
      "Model outputs:  tensor([[[-0.5917, -0.0445, -0.2187,  ...,  0.5875,  0.1830,  0.0020],\n",
      "         [-0.6221, -0.2711, -0.1172,  ...,  0.7334,  0.1807,  0.0363],\n",
      "         [-0.5251,  0.0878, -0.1525,  ...,  0.3165,  0.3499, -0.2648],\n",
      "         ...,\n",
      "         [-0.3276, -0.0868, -0.1478,  ...,  0.3956,  0.2995, -0.4225],\n",
      "         [-0.5000,  0.0428, -0.2479,  ...,  0.2795,  0.3784, -0.3063],\n",
      "         [-0.5287,  0.1106, -0.1888,  ...,  0.4220,  0.1163, -0.1789]],\n",
      "\n",
      "        [[-0.3526, -0.0961, -0.6830,  ...,  0.8729,  0.6165, -0.3769],\n",
      "         [-0.2190, -0.2023, -0.4668,  ...,  1.0190,  0.3186,  0.0604],\n",
      "         [-0.5154, -0.0196, -0.6040,  ...,  0.6136,  0.0286, -0.1228],\n",
      "         ...,\n",
      "         [-0.2716,  0.0748, -0.5544,  ...,  0.6304,  0.0997, -0.4675],\n",
      "         [-0.5441,  0.1163, -0.4029,  ...,  0.5888,  0.3013, -0.4879],\n",
      "         [-0.0987,  0.2384, -0.6436,  ...,  0.5046,  0.1271, -0.0869]],\n",
      "\n",
      "        [[-0.3404, -0.3453, -0.1601,  ..., -0.0069,  0.0471,  0.2694],\n",
      "         [-0.0461, -0.4085, -0.2512,  ...,  0.0805,  0.0384,  0.3287],\n",
      "         [-0.0275,  0.1217, -0.3986,  ...,  0.0326, -0.0334,  0.3233],\n",
      "         ...,\n",
      "         [-0.0885,  0.2438, -0.2192,  ..., -0.2782, -0.0428,  0.0783],\n",
      "         [-0.2591, -0.0513, -0.0654,  ..., -0.1947, -0.1685, -0.1937],\n",
      "         [-0.2681,  0.2730, -0.5559,  ..., -0.0954, -0.0674,  0.0794]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3628, -0.2031, -0.3633,  ...,  0.3469,  0.1005, -0.0440],\n",
      "         [-0.3692, -0.3358, -0.4026,  ...,  0.4926,  0.0265,  0.1481],\n",
      "         [-0.2147,  0.1098, -0.7039,  ...,  0.2825,  0.0054, -0.1475],\n",
      "         ...,\n",
      "         [-0.3157,  0.0615, -0.5022,  ...,  0.0418,  0.2554, -0.1602],\n",
      "         [-0.1921,  0.1916, -0.3315,  ...,  0.0778, -0.1579, -0.1359],\n",
      "         [-0.4098,  0.3160, -0.6382,  ...,  0.1463,  0.2228, -0.2948]],\n",
      "\n",
      "        [[-0.3354, -0.3294, -0.2986,  ...,  0.1298, -0.1761,  0.2240],\n",
      "         [-0.2524, -0.5931, -0.2354,  ..., -0.1839, -0.0102,  0.4846],\n",
      "         [-0.0737, -0.1653, -0.3604,  ..., -0.1442, -0.0982, -0.1738],\n",
      "         ...,\n",
      "         [-0.1484,  0.1575, -0.2709,  ..., -0.0933, -0.1587, -0.0426],\n",
      "         [-0.3730, -0.1195, -0.1101,  ..., -0.1830, -0.0361,  0.1632],\n",
      "         [-0.3103, -0.0780, -0.5540,  ..., -0.1626, -0.3874,  0.1501]],\n",
      "\n",
      "        [[-0.6575, -0.3710, -0.5049,  ...,  0.4152,  0.0637, -0.0993],\n",
      "         [-0.5432, -0.1728, -0.5630,  ...,  0.6241,  0.1727,  0.2278],\n",
      "         [-0.2973, -0.1373, -0.5474,  ...,  0.2063,  0.1126, -0.1131],\n",
      "         ...,\n",
      "         [-0.2738,  0.2158, -0.2948,  ..., -0.0558,  0.2369, -0.2024],\n",
      "         [-0.5429,  0.3532, -0.5454,  ...,  0.2191,  0.0248, -0.2018],\n",
      "         [-0.6744,  0.2842, -0.6992,  ...,  0.4031,  0.1048, -0.2102]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.8798214197158813\n",
      "Model outputs:  tensor([[[-0.5518, -0.0674, -0.4173,  ...,  0.1166,  0.0441, -0.1294],\n",
      "         [-0.4462, -0.2740, -0.2284,  ...,  0.4835, -0.0125,  0.0692],\n",
      "         [-0.3449, -0.4962, -0.1951,  ...,  0.5124,  0.4647,  0.0449],\n",
      "         ...,\n",
      "         [-0.2936, -0.0089,  0.0267,  ...,  0.0984,  0.0032, -0.0214],\n",
      "         [-0.1947, -0.5923,  0.1793,  ...,  0.0217, -0.1657, -0.1421],\n",
      "         [-0.6724, -0.3524,  0.0254,  ...,  0.2573, -0.0244, -0.2666]],\n",
      "\n",
      "        [[-0.2659, -0.0283, -0.0119,  ...,  0.4329, -0.0920, -0.2654],\n",
      "         [-0.4189, -0.2979, -0.1383,  ...,  0.5353,  0.3350, -0.2129],\n",
      "         [-0.3290, -0.1772, -0.2256,  ...,  0.5156,  0.2807, -0.0495],\n",
      "         ...,\n",
      "         [-0.2413,  0.1886, -0.2245,  ...,  0.6403,  0.2629, -0.1789],\n",
      "         [-0.0638, -0.4681, -0.1104,  ...,  0.1648,  0.2341, -0.3272],\n",
      "         [-0.2892, -0.1079, -0.0655,  ...,  0.5074,  0.3094,  0.0223]],\n",
      "\n",
      "        [[ 0.0765, -0.4630, -0.2604,  ..., -0.1345, -0.3869,  0.0574],\n",
      "         [ 0.0224, -0.5540, -0.3839,  ..., -0.0942, -0.2756,  0.6736],\n",
      "         [ 0.3541, -0.4042, -0.3713,  ..., -0.2075,  0.0264,  0.4225],\n",
      "         ...,\n",
      "         [ 0.0674, -0.3698, -0.2121,  ..., -0.2072, -0.2070,  0.2629],\n",
      "         [ 0.1671, -0.5369, -0.0583,  ..., -0.2100, -0.1956,  0.1107],\n",
      "         [-0.2571, -0.4926, -0.3465,  ..., -0.3206, -0.3860,  0.3354]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2315, -0.1417,  0.0434,  ...,  0.4763,  0.2056, -0.0965],\n",
      "         [-0.3651, -0.1665, -0.1390,  ...,  0.4187,  0.2789, -0.1452],\n",
      "         [-0.4654,  0.0607,  0.0460,  ...,  0.6780,  0.1612, -0.3454],\n",
      "         ...,\n",
      "         [-0.2918, -0.0419, -0.1306,  ...,  0.3448,  0.1379, -0.0769],\n",
      "         [-0.1202, -0.3750,  0.1192,  ...,  0.3640,  0.3536, -0.2258],\n",
      "         [-0.1944, -0.3690,  0.0899,  ...,  0.6519,  0.1834,  0.0592]],\n",
      "\n",
      "        [[ 0.0136, -0.3408, -0.3289,  ...,  0.0530, -0.1092,  0.2175],\n",
      "         [-0.2609, -0.5604, -0.3550,  ..., -0.1433, -0.1616,  0.4394],\n",
      "         [ 0.0283, -0.4848, -0.2846,  ..., -0.0558, -0.1201,  0.4419],\n",
      "         ...,\n",
      "         [-0.2531, -0.5257, -0.2623,  ..., -0.0871, -0.3351,  0.0739],\n",
      "         [-0.1259, -0.4660, -0.3404,  ...,  0.0502, -0.1541, -0.0275],\n",
      "         [-0.0946, -0.3811, -0.1481,  ..., -0.0840, -0.1509,  0.5054]],\n",
      "\n",
      "        [[-0.3063, -0.2676, -0.4576,  ...,  0.1968, -0.1314,  0.1199],\n",
      "         [-0.3150, -0.1703, -0.5422,  ...,  0.3957, -0.4593,  0.0252],\n",
      "         [-0.0782, -0.2781, -0.7505,  ...,  0.4775,  0.2246, -0.1241],\n",
      "         ...,\n",
      "         [-0.4955, -0.1307, -0.4260,  ...,  0.1345, -0.3166, -0.1328],\n",
      "         [ 0.1375, -0.2568, -0.6123,  ...,  0.2079, -0.2217, -0.3587],\n",
      "         [-0.0617, -0.5536, -0.3999,  ...,  0.3214, -0.0787, -0.0061]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.9932273030281067\n",
      "Model outputs:  tensor([[[-0.2828, -0.2078, -0.5544,  ...,  0.6647,  0.1711,  0.2586],\n",
      "         [-0.0932,  0.0859, -0.5501,  ...,  0.6285, -0.2772, -0.0189],\n",
      "         [ 0.0824, -0.0275, -0.7005,  ...,  0.6624, -0.1595, -0.4153],\n",
      "         ...,\n",
      "         [-0.1606, -0.0463, -0.4970,  ...,  0.5322, -0.0451, -0.1433],\n",
      "         [-0.2468,  0.1761, -0.5690,  ...,  0.3905, -0.1196, -0.1663],\n",
      "         [-0.2210,  0.0017, -0.6153,  ...,  0.2082, -0.0744,  0.0065]],\n",
      "\n",
      "        [[-0.3741, -0.1482, -0.7015,  ...,  0.6920,  0.3212,  0.3253],\n",
      "         [-0.1173,  0.2598, -0.4760,  ...,  0.7853,  0.2485, -0.0422],\n",
      "         [-0.4469, -0.1472, -0.6623,  ...,  0.6005,  0.0978,  0.1342],\n",
      "         ...,\n",
      "         [-0.5138,  0.2808, -0.5488,  ...,  0.7392,  0.1313, -0.2004],\n",
      "         [-0.4010,  0.2800, -0.4221,  ...,  0.6719,  0.2498, -0.1273],\n",
      "         [-0.4238,  0.1484, -0.4491,  ...,  0.5587,  0.2311, -0.1659]],\n",
      "\n",
      "        [[-0.1199, -0.0902, -0.3625,  ..., -0.0119, -0.2972,  0.2826],\n",
      "         [ 0.1388, -0.2024, -0.2195,  ...,  0.2501, -0.0898,  0.1102],\n",
      "         [ 0.1067, -0.3444, -0.2832,  ...,  0.2244,  0.0285,  0.2551],\n",
      "         ...,\n",
      "         [-0.0538, -0.3024, -0.5652,  ...,  0.0213, -0.2691,  0.1903],\n",
      "         [-0.1874, -0.1506, -0.3429,  ...,  0.1877, -0.2028, -0.2702],\n",
      "         [-0.0129, -0.2237, -0.3088,  ..., -0.0362,  0.0263,  0.1299]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2007, -0.3766, -0.1803,  ..., -0.0236,  0.1636, -0.2613],\n",
      "         [ 0.1535, -0.3028, -0.2765,  ...,  0.2417, -0.1066, -0.4090],\n",
      "         [-0.1861, -0.1540, -0.2426,  ...,  0.3034,  0.1500, -0.4899],\n",
      "         ...,\n",
      "         [-0.1378,  0.0040, -0.4399,  ..., -0.1126,  0.1147,  0.3621],\n",
      "         [ 0.1442, -0.2514, -0.1036,  ..., -0.1204,  0.0383,  0.0457],\n",
      "         [ 0.1798, -0.1427, -0.2854,  ..., -0.3431,  0.1225, -0.1904]],\n",
      "\n",
      "        [[-0.2282, -0.4580, -0.5175,  ...,  0.2852,  0.0701,  0.2749],\n",
      "         [-0.2679, -0.2778, -0.4038,  ...,  0.3120, -0.1061,  0.0646],\n",
      "         [-0.0716, -0.1671, -0.5001,  ...,  0.2668, -0.1685,  0.2409],\n",
      "         ...,\n",
      "         [-0.2828,  0.0720, -0.3549,  ...,  0.2952, -0.0636, -0.1538],\n",
      "         [-0.1946,  0.1128, -0.3613,  ...,  0.1836, -0.0173,  0.0615],\n",
      "         [-0.0526,  0.0689, -0.5246,  ...,  0.2084,  0.1495, -0.0462]],\n",
      "\n",
      "        [[-0.3440, -0.3088, -0.5966,  ...,  0.3155, -0.1536,  0.1082],\n",
      "         [-0.0357, -0.2349, -0.4847,  ...,  0.6693, -0.1173, -0.1673],\n",
      "         [-0.1128, -0.0535, -0.6169,  ...,  0.5656, -0.0811,  0.1635],\n",
      "         ...,\n",
      "         [-0.3647,  0.0689, -0.4828,  ...,  0.3527, -0.0601,  0.0863],\n",
      "         [-0.2993,  0.0359, -0.6840,  ...,  0.2422, -0.1849, -0.1200],\n",
      "         [-0.1394,  0.2689, -0.6035,  ...,  0.1568, -0.0344, -0.2261]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  0.7658873796463013\n",
      "Model outputs:  tensor([[[-1.6666e-02, -1.5974e-01, -4.3581e-01,  ..., -3.3416e-01,\n",
      "          -2.0991e-01,  1.3913e-01],\n",
      "         [-1.7813e-01, -3.4448e-01, -3.0776e-01,  ..., -3.4147e-01,\n",
      "          -2.7504e-01, -6.3309e-02],\n",
      "         [ 8.3463e-02,  9.4272e-02, -4.0963e-01,  ..., -2.5164e-01,\n",
      "           1.1143e-01,  1.1367e-01],\n",
      "         ...,\n",
      "         [-2.0944e-01, -2.6781e-01, -1.6240e-01,  ..., -1.8868e-01,\n",
      "          -1.5068e-01,  1.2565e-01],\n",
      "         [-5.3573e-02, -1.5648e-01, -8.9723e-02,  ..., -1.7052e-01,\n",
      "           5.6269e-02, -2.2308e-03],\n",
      "         [-2.9835e-01, -2.7773e-01, -1.3503e-01,  ..., -2.5778e-01,\n",
      "          -1.8454e-01,  4.4020e-02]],\n",
      "\n",
      "        [[-5.0758e-01, -1.9076e-01, -4.6739e-01,  ...,  7.7774e-02,\n",
      "           1.7631e-01,  1.5192e-01],\n",
      "         [-1.9684e-01, -1.3498e-01, -4.6626e-01,  ...,  1.4565e-01,\n",
      "           1.4404e-01, -7.1344e-02],\n",
      "         [-4.0561e-01,  2.0477e-01, -3.3033e-01,  ..., -1.7325e-02,\n",
      "           2.3541e-01, -1.0634e-01],\n",
      "         ...,\n",
      "         [-6.4685e-01,  3.0912e-01, -4.2735e-01,  ..., -1.9108e-01,\n",
      "           2.0962e-01,  2.4929e-01],\n",
      "         [-3.7171e-01, -7.4184e-02, -3.9077e-01,  ...,  2.4192e-01,\n",
      "           4.6704e-01, -1.2125e-01],\n",
      "         [-4.6494e-01,  1.7415e-01, -5.6180e-01,  ..., -3.7473e-02,\n",
      "           2.7659e-01, -1.2215e-01]],\n",
      "\n",
      "        [[-6.3622e-01, -9.3696e-02, -1.4340e-01,  ...,  9.1358e-04,\n",
      "           1.7367e-01, -2.8943e-01],\n",
      "         [-4.5126e-01, -1.5626e-01, -2.8377e-01,  ...,  6.3240e-02,\n",
      "           3.0285e-01, -3.5033e-01],\n",
      "         [-2.3314e-01, -3.4304e-02, -6.6173e-02,  ..., -1.5471e-02,\n",
      "           4.6122e-01, -2.6308e-01],\n",
      "         ...,\n",
      "         [-3.8077e-01,  6.1265e-02, -2.4514e-01,  ...,  4.4677e-01,\n",
      "          -2.4166e-01, -1.9496e-01],\n",
      "         [-2.7980e-01, -3.4667e-01, -2.2598e-01,  ...,  1.3455e-01,\n",
      "           2.6366e-01, -3.3208e-01],\n",
      "         [-2.2131e-01, -4.2431e-04,  1.3490e-01,  ...,  2.7615e-01,\n",
      "           1.8643e-01, -2.6725e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.5706e-02, -3.4683e-01, -4.1827e-01,  ..., -5.6450e-01,\n",
      "           3.5991e-01, -1.1385e-01],\n",
      "         [ 7.7395e-02, -3.1565e-01, -2.3617e-01,  ..., -5.0921e-01,\n",
      "           1.1028e-02, -1.0084e-01],\n",
      "         [ 7.6684e-03, -2.6458e-01, -1.5679e-01,  ..., -5.1967e-01,\n",
      "           9.2031e-02, -2.3654e-01],\n",
      "         ...,\n",
      "         [-6.3111e-02, -2.9664e-01, -1.3333e-01,  ..., -4.8140e-01,\n",
      "           1.6509e-01, -3.3605e-01],\n",
      "         [ 5.0515e-02, -3.6261e-01, -9.2063e-02,  ..., -3.1434e-01,\n",
      "           2.9157e-02, -3.8961e-01],\n",
      "         [-7.6956e-03, -2.6999e-01,  1.5646e-01,  ..., -5.4082e-01,\n",
      "           2.4282e-02, -5.1064e-01]],\n",
      "\n",
      "        [[-3.5625e-01, -9.7636e-02, -7.8188e-01,  ...,  5.5894e-01,\n",
      "           3.0912e-01, -2.4005e-01],\n",
      "         [-1.3609e-01,  6.8362e-02, -6.3013e-01,  ...,  3.1698e-01,\n",
      "          -5.9009e-02, -4.5364e-01],\n",
      "         [-4.1052e-01,  2.6312e-04, -6.0489e-01,  ...,  1.9249e-01,\n",
      "           2.3235e-01, -3.4192e-01],\n",
      "         ...,\n",
      "         [-3.1087e-01, -2.5067e-02, -5.5649e-01,  ...,  1.7012e-01,\n",
      "           2.8643e-02, -1.5596e-01],\n",
      "         [-2.0093e-01, -1.2082e-01, -4.2274e-01,  ...,  4.6852e-01,\n",
      "           2.7112e-03, -2.5295e-01],\n",
      "         [-1.9889e-01,  1.2824e-01, -4.5802e-01,  ...,  4.0100e-01,\n",
      "          -6.0627e-02, -1.9594e-01]],\n",
      "\n",
      "        [[-4.8496e-01, -7.2244e-02, -5.3981e-01,  ...,  1.2723e-01,\n",
      "           2.1883e-01,  2.9840e-02],\n",
      "         [-6.1386e-01, -8.7450e-02, -2.6178e-01,  ...,  1.5422e-01,\n",
      "          -3.7972e-02, -1.6306e-01],\n",
      "         [-2.5612e-01,  1.6062e-01, -3.6421e-01,  ..., -1.2201e-01,\n",
      "           4.0764e-01, -3.4553e-02],\n",
      "         ...,\n",
      "         [-7.8782e-01,  1.1614e-01, -4.8705e-01,  ...,  2.3627e-01,\n",
      "           1.3935e-01,  1.3127e-01],\n",
      "         [-1.7396e-01,  2.9718e-02, -3.9035e-01,  ...,  3.0422e-01,\n",
      "           3.3922e-01, -6.1362e-02],\n",
      "         [-3.9333e-01,  2.1138e-01, -4.8342e-01,  ...,  1.6765e-02,\n",
      "           4.0809e-01, -2.1673e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.143110752105713\n",
      "Model outputs:  tensor([[[ 0.4051, -0.4210, -0.2880,  ..., -0.4275,  0.4623,  0.3033],\n",
      "         [ 0.3738, -0.1099, -0.5758,  ..., -0.2753,  0.1079,  0.3566],\n",
      "         [ 0.1059, -0.3685, -0.4343,  ..., -0.3978,  0.3810,  0.0264],\n",
      "         ...,\n",
      "         [ 0.2177, -0.5008, -0.2462,  ..., -0.4624,  0.0472, -0.1895],\n",
      "         [ 0.2458, -0.6478, -0.2827,  ..., -0.4411,  0.2338, -0.1578],\n",
      "         [ 0.2375, -0.3461, -0.2992,  ..., -0.4579,  0.3159,  0.1121]],\n",
      "\n",
      "        [[-0.0935, -0.4974, -0.2047,  ..., -0.0325, -0.2656,  0.1846],\n",
      "         [-0.2650, -0.2983, -0.4565,  ..., -0.1139, -0.0417,  0.1727],\n",
      "         [ 0.0198, -0.3655, -0.3668,  ..., -0.2087,  0.2092,  0.2733],\n",
      "         ...,\n",
      "         [-0.2103, -0.4764, -0.3100,  ..., -0.2418, -0.5188,  0.2918],\n",
      "         [ 0.0614, -0.7509, -0.4531,  ..., -0.1461, -0.0751,  0.1236],\n",
      "         [-0.0842, -0.6137, -0.3893,  ..., -0.0466, -0.1021,  0.2477]],\n",
      "\n",
      "        [[-0.1049, -0.3034, -0.4466,  ...,  0.2852, -0.0328,  0.0205],\n",
      "         [-0.3039, -0.1472, -0.6037,  ...,  0.1926,  0.1220,  0.1005],\n",
      "         [-0.2511, -0.0717, -0.8006,  ...,  0.0447,  0.0987, -0.0708],\n",
      "         ...,\n",
      "         [-0.2631, -0.0295, -0.6538,  ...,  0.1477, -0.0646, -0.1202],\n",
      "         [-0.1803, -0.2161, -0.6410,  ...,  0.1225, -0.2122, -0.2619],\n",
      "         [-0.0195, -0.3888, -0.7159,  ...,  0.3361,  0.0116, -0.0423]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5457,  0.0042, -0.6075,  ...,  0.4357,  0.2488,  0.0286],\n",
      "         [-0.4976,  0.1895, -0.7600,  ...,  0.5272,  0.3330,  0.0961],\n",
      "         [-0.6832, -0.0024, -0.4827,  ...,  0.3661,  0.5964, -0.0141],\n",
      "         ...,\n",
      "         [-0.5101, -0.2649, -0.4671,  ...,  0.4128,  0.6080, -0.0995],\n",
      "         [-0.1292, -0.5201, -0.3875,  ...,  0.1738,  0.3337, -0.1313],\n",
      "         [-0.7710,  0.0025, -0.5004,  ...,  0.7109,  0.2433, -0.3314]],\n",
      "\n",
      "        [[-0.1130, -0.3076, -0.7336,  ...,  0.3335,  0.0989,  0.0417],\n",
      "         [ 0.0075, -0.0092, -0.7993,  ...,  0.2855,  0.0245,  0.1011],\n",
      "         [-0.0743, -0.0609, -0.5740,  ...,  0.2070,  0.1163,  0.0141],\n",
      "         ...,\n",
      "         [-0.3525, -0.1558, -0.7024,  ...,  0.1418,  0.0250, -0.1938],\n",
      "         [-0.1000, -0.2559, -0.5409,  ...,  0.1753,  0.0043, -0.1711],\n",
      "         [-0.4613, -0.1606, -0.4659,  ...,  0.4318, -0.1001, -0.1762]],\n",
      "\n",
      "        [[-0.4730,  0.1175, -0.1928,  ...,  0.7613,  0.4308, -0.2061],\n",
      "         [-0.5222, -0.1700, -0.3219,  ...,  0.3879,  0.2130, -0.1890],\n",
      "         [-0.4145, -0.0804, -0.2136,  ...,  0.4181,  0.4502, -0.3966],\n",
      "         ...,\n",
      "         [-0.3216,  0.1529, -0.3954,  ...,  0.4968,  0.1398, -0.2166],\n",
      "         [-0.3396, -0.3728, -0.2830,  ...,  0.2531,  0.0904, -0.5654],\n",
      "         [-0.5210,  0.0224, -0.2967,  ...,  0.6150,  0.1179, -0.2594]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0411502122879028\n",
      "Model outputs:  tensor([[[ 1.7596e-01, -1.9580e-01, -2.1428e-01,  ..., -3.1984e-01,\n",
      "          -4.7016e-01,  1.3280e-01],\n",
      "         [-1.2392e-01, -2.1660e-01, -4.2213e-01,  ..., -4.6025e-01,\n",
      "          -1.6809e-01,  1.4273e-01],\n",
      "         [-1.3018e-01, -7.3644e-02, -3.4545e-01,  ..., -1.1926e-01,\n",
      "          -3.4378e-02, -1.0269e-02],\n",
      "         ...,\n",
      "         [-9.0332e-02, -2.3254e-01, -4.8093e-01,  ...,  1.7424e-02,\n",
      "          -4.7619e-02,  1.2556e-01],\n",
      "         [-2.6309e-01, -2.7771e-01, -1.5463e-01,  ..., -1.4067e-01,\n",
      "          -2.2692e-01,  1.5039e-01],\n",
      "         [ 7.8169e-02, -1.2505e-01, -3.4075e-01,  ..., -1.2689e-01,\n",
      "           9.5915e-02,  6.1824e-02]],\n",
      "\n",
      "        [[-5.8880e-01,  3.1601e-03, -2.3022e-01,  ..., -1.7665e-01,\n",
      "          -4.9319e-02, -3.3938e-01],\n",
      "         [-2.4077e-01,  6.9803e-02, -5.5734e-01,  ...,  1.3978e-02,\n",
      "          -4.5777e-03, -2.7178e-01],\n",
      "         [-3.7158e-01,  3.6078e-02, -5.8667e-01,  ..., -2.0975e-01,\n",
      "           1.2339e-01,  2.9941e-03],\n",
      "         ...,\n",
      "         [-2.5332e-01, -4.0669e-02, -4.4815e-01,  ...,  1.6798e-01,\n",
      "           2.4309e-01, -3.6058e-02],\n",
      "         [-3.7819e-01, -3.1398e-01, -6.5646e-01,  ..., -1.0656e-01,\n",
      "          -2.9268e-01,  1.7433e-01],\n",
      "         [-5.2509e-01,  1.7945e-01, -6.0376e-01,  ..., -1.8583e-01,\n",
      "           1.6113e-01,  1.7418e-01]],\n",
      "\n",
      "        [[ 6.1663e-02, -3.6941e-01, -2.0828e-01,  ..., -3.9743e-01,\n",
      "          -3.1838e-01, -2.7405e-02],\n",
      "         [-1.7118e-01, -1.6011e-01, -3.4249e-01,  ..., -3.2179e-01,\n",
      "          -3.0135e-01, -3.6581e-02],\n",
      "         [-2.5552e-02, -3.1633e-01, -4.1694e-01,  ..., -7.2158e-02,\n",
      "           1.0382e-02, -1.1758e-01],\n",
      "         ...,\n",
      "         [ 2.0060e-01, -3.0028e-01, -3.6301e-01,  ..., -1.7620e-01,\n",
      "          -1.7621e-02,  1.3583e-01],\n",
      "         [-2.6449e-01, -4.5579e-01, -4.2507e-01,  ..., -4.9430e-02,\n",
      "          -4.5507e-01,  5.8664e-02],\n",
      "         [ 1.1841e-01, -2.6382e-01, -1.8521e-01,  ..., -1.7640e-01,\n",
      "          -2.2299e-01,  1.0956e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.7757e-01, -1.2361e-01, -6.2942e-02,  ..., -3.7428e-01,\n",
      "          -2.7398e-01,  2.8265e-03],\n",
      "         [-1.9630e-01, -2.7511e-01,  3.2799e-02,  ..., -2.5625e-01,\n",
      "          -2.7530e-01, -4.0048e-02],\n",
      "         [-3.3148e-01, -2.5064e-01, -3.8465e-01,  ..., -3.5621e-01,\n",
      "          -2.8699e-01,  3.2673e-01],\n",
      "         ...,\n",
      "         [-9.3294e-02, -2.1241e-01, -3.2213e-01,  ...,  1.1510e-01,\n",
      "          -1.7702e-01,  8.9750e-02],\n",
      "         [-6.6387e-02, -4.8253e-01, -3.3093e-01,  ..., -1.7211e-01,\n",
      "           5.0904e-02,  7.8280e-02],\n",
      "         [ 6.1181e-04, -2.8418e-01, -2.1020e-01,  ..., -1.5397e-01,\n",
      "          -2.0556e-01,  2.3864e-01]],\n",
      "\n",
      "        [[-3.0904e-01, -8.3365e-02, -5.4350e-01,  ..., -1.9258e-01,\n",
      "           1.8807e-01,  2.2557e-01],\n",
      "         [-6.8610e-01,  1.9966e-01, -5.5266e-01,  ...,  9.4523e-02,\n",
      "           1.7681e-01,  1.9393e-01],\n",
      "         [-4.7155e-01, -3.9225e-02, -3.3775e-01,  ...,  7.8042e-02,\n",
      "           2.1947e-01, -4.1159e-02],\n",
      "         ...,\n",
      "         [-6.7409e-01,  1.4993e-01, -3.8536e-01,  ...,  1.8479e-01,\n",
      "           2.3611e-01,  7.3872e-02],\n",
      "         [-5.0688e-01, -8.3738e-02, -4.6816e-01,  ...,  3.2536e-01,\n",
      "           3.0881e-01, -1.0365e-02],\n",
      "         [-6.3732e-01,  5.3024e-02, -4.2401e-01,  ..., -5.6663e-02,\n",
      "           2.6942e-01, -7.6205e-02]],\n",
      "\n",
      "        [[ 1.6226e-02, -3.9730e-01, -1.9948e-02,  ...,  2.2498e-01,\n",
      "           6.5420e-02, -8.1541e-01],\n",
      "         [-1.4720e-01, -3.0149e-01,  1.8253e-01,  ...,  1.9629e-01,\n",
      "          -2.2898e-01, -6.7037e-01],\n",
      "         [ 2.6145e-02, -2.5203e-01, -1.9647e-01,  ...,  1.6816e-01,\n",
      "           1.3710e-01, -7.3221e-01],\n",
      "         ...,\n",
      "         [ 2.4769e-01, -2.5562e-01, -5.8712e-02,  ...,  1.7465e-01,\n",
      "           4.5238e-02, -5.6801e-01],\n",
      "         [-9.6812e-02, -2.6541e-01,  7.8837e-03,  ...,  1.9475e-01,\n",
      "          -1.5470e-01, -6.3485e-01],\n",
      "         [-1.2196e-01, -3.6478e-01, -1.2382e-01,  ...,  5.0924e-01,\n",
      "          -1.5790e-02, -6.0619e-01]]], grad_fn=<ViewBackward0>)\n",
      "Loss:  1.0544053316116333\n",
      "Model outputs:  tensor([[[ 0.0221, -0.1955, -0.6702,  ..., -0.0298, -0.0783,  0.3156],\n",
      "         [-0.3147,  0.0653, -0.3834,  ...,  0.1680, -0.1585,  0.2630],\n",
      "         [-0.1193, -0.4185, -0.2216,  ...,  0.1230, -0.1382,  0.1145],\n",
      "         ...,\n",
      "         [ 0.1002, -0.4104, -0.3265,  ..., -0.0219,  0.0503,  0.3271],\n",
      "         [-0.2416, -0.2224, -0.3891,  ...,  0.1421, -0.1028,  0.4280],\n",
      "         [ 0.1184, -0.5853, -0.2823,  ..., -0.1095,  0.1053,  0.3247]],\n",
      "\n",
      "        [[-0.4992,  0.0439, -0.5724,  ...,  0.4312,  0.2508, -0.1116],\n",
      "         [-0.4955,  0.2357, -0.3756,  ...,  0.6529,  0.2902,  0.1316],\n",
      "         [-0.4713,  0.1211, -0.4043,  ...,  0.5375,  0.2418,  0.2214],\n",
      "         ...,\n",
      "         [-0.4653,  0.0438, -0.4621,  ...,  0.7221,  0.4493,  0.1033],\n",
      "         [-0.7597,  0.2032, -0.4700,  ...,  0.4547,  0.4862,  0.2523],\n",
      "         [-0.4828, -0.0457, -0.2323,  ...,  0.3876,  0.4130,  0.2368]],\n",
      "\n",
      "        [[-0.0505, -0.1835, -0.4705,  ...,  0.0862, -0.2463,  0.1491],\n",
      "         [-0.1538, -0.2871, -0.6426,  ...,  0.0348, -0.2306,  0.3086],\n",
      "         [-0.0168, -0.2528, -0.3963,  ...,  0.1176, -0.0354,  0.4729],\n",
      "         ...,\n",
      "         [ 0.0398, -0.5522, -0.2746,  ...,  0.1434,  0.0541,  0.3604],\n",
      "         [-0.1182, -0.2700, -0.3953,  ...,  0.1165,  0.0522,  0.1120],\n",
      "         [-0.0037, -0.3943, -0.0456,  ..., -0.1114, -0.1519,  0.4573]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7923,  0.0429, -0.5024,  ...,  0.4841,  0.2259,  0.0158],\n",
      "         [-0.7139,  0.0120, -0.6265,  ...,  0.5548,  0.3069,  0.0985],\n",
      "         [-0.3846, -0.0417, -0.3180,  ...,  0.6641,  0.3596,  0.2175],\n",
      "         ...,\n",
      "         [-0.4716,  0.1646, -0.4627,  ...,  0.5264,  0.5281,  0.0692],\n",
      "         [-0.5316, -0.1645, -0.7092,  ...,  0.4885,  0.2167,  0.2746],\n",
      "         [-0.2571, -0.1745, -0.0101,  ...,  0.3048,  0.6673,  0.1540]],\n",
      "\n",
      "        [[-0.3860,  0.0842, -0.2542,  ...,  0.8298,  0.3393, -0.0919],\n",
      "         [-0.2415,  0.0670, -0.4207,  ...,  0.8431,  0.5153, -0.1739],\n",
      "         [-0.2389,  0.1186, -0.5658,  ...,  0.7513,  0.3806,  0.0681],\n",
      "         ...,\n",
      "         [-0.0497, -0.1402, -0.2909,  ...,  0.6971,  0.3540,  0.0605],\n",
      "         [-0.1880, -0.1889, -0.2059,  ...,  0.6665,  0.2996, -0.0949],\n",
      "         [-0.3571, -0.2255, -0.1170,  ...,  0.4597,  0.2787,  0.2773]],\n",
      "\n",
      "        [[-0.1495, -0.2609, -0.6767,  ...,  0.5421,  0.1305,  0.1503],\n",
      "         [-0.4299, -0.0411, -0.7337,  ...,  0.6184,  0.0404, -0.2226],\n",
      "         [-0.1621, -0.0940, -0.8227,  ...,  0.5272, -0.2091,  0.1542],\n",
      "         ...,\n",
      "         [-0.2521, -0.3003, -0.7358,  ...,  0.4186, -0.0400,  0.0758],\n",
      "         [-0.1322,  0.0356, -0.7692,  ...,  0.4094, -0.0138,  0.0345],\n",
      "         [-0.1138, -0.1007, -0.6592,  ...,  0.4790,  0.2002, -0.0140]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.3315131664276123\n",
      "Model outputs:  tensor([[[-0.3522, -0.3946, -0.3804,  ...,  0.0232, -0.5033, -0.1577],\n",
      "         [-0.1753, -0.1346, -0.5177,  ...,  0.0906, -0.2235, -0.0653],\n",
      "         [-0.2745, -0.3468, -0.3108,  ..., -0.0786, -0.1523, -0.1271],\n",
      "         ...,\n",
      "         [-0.2954, -0.5033, -0.3480,  ...,  0.1257, -0.2389,  0.1750],\n",
      "         [-0.4211, -1.0600, -0.0329,  ...,  0.1829, -0.2937,  0.0577],\n",
      "         [-0.3057, -0.2715, -0.5511,  ..., -0.0043, -0.3234, -0.0221]],\n",
      "\n",
      "        [[ 0.1184, -0.2318, -0.1346,  ..., -0.3261,  0.2393,  0.1416],\n",
      "         [ 0.1541,  0.0294, -0.1864,  ..., -0.3319,  0.3780,  0.0033],\n",
      "         [ 0.4760, -0.1935, -0.3324,  ..., -0.4483,  0.2953,  0.0030],\n",
      "         ...,\n",
      "         [ 0.1606, -0.3212, -0.6020,  ..., -0.5276,  0.0383,  0.4662],\n",
      "         [ 0.1073, -0.9000, -0.0239,  ..., -0.1927, -0.0870,  0.4113],\n",
      "         [ 0.4093, -0.2066, -0.4177,  ..., -0.6254,  0.1636,  0.1706]],\n",
      "\n",
      "        [[-0.0266, -0.1422, -0.2130,  ..., -0.3394, -0.3146,  0.1494],\n",
      "         [ 0.2029, -0.1614, -0.3419,  ..., -0.4757, -0.1986,  0.1191],\n",
      "         [-0.0381, -0.3541, -0.3250,  ..., -0.1717, -0.2104,  0.1232],\n",
      "         ...,\n",
      "         [-0.0344, -0.6524, -0.2027,  ..., -0.4026, -0.4107,  0.5619],\n",
      "         [-0.1811, -0.9098, -0.1184,  ...,  0.1280, -0.3841,  0.5687],\n",
      "         [-0.0300, -0.4819, -0.2428,  ..., -0.5683, -0.4575,  0.2802]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1651, -0.3368, -0.3049,  ...,  0.2230,  0.1083, -0.2195],\n",
      "         [-0.5021,  0.1880, -0.3572,  ...,  0.0040,  0.1749, -0.2152],\n",
      "         [-0.3109, -0.0243, -0.4353,  ..., -0.1127,  0.4006, -0.4351],\n",
      "         ...,\n",
      "         [-0.4803, -0.3719, -0.3147,  ...,  0.4767, -0.0225, -0.1641],\n",
      "         [-0.1382, -0.9540, -0.2844,  ...,  0.5403,  0.1110,  0.1965],\n",
      "         [-0.4974, -0.2066, -0.0743,  ...,  0.2748, -0.0243, -0.1131]],\n",
      "\n",
      "        [[-0.0280, -0.2891, -0.4418,  ..., -0.7071,  0.4620,  0.4724],\n",
      "         [ 0.3273, -0.4167, -0.2987,  ..., -0.5876,  0.5673,  0.2596],\n",
      "         [ 0.0508, -0.0444, -0.3233,  ..., -0.6046,  0.4086,  0.1922],\n",
      "         ...,\n",
      "         [-0.1166, -0.6489, -0.4520,  ..., -0.6463,  0.1051,  0.5531],\n",
      "         [-0.2868, -0.8014, -0.1463,  ..., -0.4033,  0.3253,  0.5518],\n",
      "         [ 0.0191, -0.1489, -0.2772,  ..., -0.5864,  0.4475,  0.6446]],\n",
      "\n",
      "        [[-0.0417, -0.2522, -0.2764,  ..., -0.2860, -0.1770,  0.0475],\n",
      "         [-0.0086, -0.0793, -0.3145,  ..., -0.1553, -0.2753, -0.1065],\n",
      "         [ 0.2070, -0.3701, -0.3537,  ..., -0.0461, -0.0877,  0.1878],\n",
      "         ...,\n",
      "         [-0.0447, -0.3461, -0.4231,  ..., -0.0189, -0.4393,  0.1505],\n",
      "         [-0.0543, -0.8832, -0.1697,  ..., -0.0031, -0.3254,  0.2756],\n",
      "         [ 0.0637, -0.2702, -0.1953,  ..., -0.3263, -0.2107,  0.0873]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Loss:  1.2413735389709473\n",
      "Model outputs:  tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.0956, -0.1385, -0.3351,  ...,  0.4038, -0.3588, -0.2479],\n",
      "         [-0.4128, -0.1132, -0.8046,  ...,  0.6448, -0.1284,  0.0063],\n",
      "         ...,\n",
      "         [-0.3435, -0.2076, -0.7283,  ...,  0.5570, -0.3084,  0.0670],\n",
      "         [-0.1259, -0.1265, -0.6241,  ...,  0.5988, -0.3510, -0.2600],\n",
      "         [-0.1796, -0.0142, -0.5799,  ...,  0.4775, -0.2888, -0.0953]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.0914,  0.0872,  0.1236,  ...,  0.1084, -0.3066,  0.1427],\n",
      "         [-0.1931, -0.4087, -0.2750,  ...,  0.3060,  0.0271,  0.3211],\n",
      "         ...,\n",
      "         [-0.1557, -0.0491, -0.3823,  ...,  0.0658, -0.4282,  0.5775],\n",
      "         [-0.2971, -0.0273, -0.3674,  ...,  0.0657, -0.4321,  0.3560],\n",
      "         [-0.0019, -0.1415, -0.3936,  ...,  0.0861, -0.3849,  0.3718]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.1205, -0.1255,  0.1574,  ...,  0.3170, -0.2914, -0.1131],\n",
      "         [-0.2550, -0.2198, -0.5262,  ...,  0.2019, -0.3973,  0.3142],\n",
      "         ...,\n",
      "         [-0.2580, -0.4052, -0.4268,  ..., -0.0274, -0.3200,  0.3135],\n",
      "         [-0.0197, -0.1809, -0.4210,  ..., -0.1488, -0.3146,  0.1570],\n",
      "         [-0.4086, -0.2305, -0.5416,  ...,  0.0910, -0.2144,  0.1405]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.1266,  0.0177, -0.4743,  ...,  0.4859, -0.3613,  0.0429],\n",
      "         [-0.1973, -0.2991, -0.4567,  ...,  0.4540, -0.1071, -0.0505],\n",
      "         ...,\n",
      "         [-0.2636, -0.0662, -0.7167,  ...,  0.3616, -0.1604, -0.1538],\n",
      "         [-0.3098, -0.0736, -0.7819,  ...,  0.4144, -0.1964, -0.1931],\n",
      "         [-0.3859,  0.2966, -0.6717,  ...,  0.4241, -0.0697, -0.2298]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.1200,  0.0500, -0.1077,  ...,  0.2213, -0.2401,  0.3482],\n",
      "         [-0.2183, -0.3356, -0.5249,  ...,  0.1116, -0.3766,  0.1772],\n",
      "         ...,\n",
      "         [-0.2832, -0.4101, -0.3837,  ...,  0.0939, -0.1496,  0.4351],\n",
      "         [-0.1920, -0.0242, -0.4441,  ..., -0.1020, -0.3625,  0.4952],\n",
      "         [-0.0913, -0.4254, -0.3941,  ..., -0.0815, -0.2452,  0.3902]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.0015,  0.0768, -0.3919,  ...,  0.2827, -0.2199,  0.1570],\n",
      "         [-0.0798, -0.1908, -0.6619,  ...,  0.5987,  0.0550,  0.0459],\n",
      "         ...,\n",
      "         [-0.3640, -0.3494, -0.5229,  ...,  0.3571, -0.2846, -0.0533],\n",
      "         [-0.0910, -0.0227, -0.5142,  ...,  0.4107, -0.2117, -0.2472],\n",
      "         [-0.2117,  0.0520, -0.4437,  ...,  0.2637, -0.1165,  0.0224]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "NaNs in model outputs!\n",
      "Epoch [10/10], Loss: 1.2414\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "d_model = 256\n",
    "nhead = 4\n",
    "num_layers = 3\n",
    "dim_feedforward = 512\n",
    "lr = 1e-6\n",
    "batch_size = 64\n",
    "seq_len = 50\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cpu')\n",
    "model = EEGSeq2SeqPredictor(d_model, nhead, num_layers, dim_feedforward).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for (input_batch_X,), (input_batch_Y,) in zip(train_loader_X, train_loader_Y):\n",
    "        # Move data to device\n",
    "        input_batch_X, input_batch_Y = input_batch_X.to(device), input_batch_Y.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Check for NaNs or Infs in the input\n",
    "        if torch.isnan(input_batch_X).any() or torch.isinf(input_batch_X).any():\n",
    "            print(\"NaN or Inf found in input_batch_X\")\n",
    "            break\n",
    "\n",
    "        if torch.isnan(input_batch_Y).any() or torch.isinf(input_batch_Y).any():\n",
    "            print(\"NaN or Inf found in input_batch_Y\")\n",
    "            break\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_batch_X, input_batch_Y[:-1])\n",
    "\n",
    "        # DEBUG: Print model outputs and check for NaNs\n",
    "        print(\"Model outputs: \", outputs)\n",
    "        if torch.isnan(outputs).any():\n",
    "            print(\"NaNs in model outputs!\")\n",
    "            break\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, input_batch_Y[1:])\n",
    "        \n",
    "        # DEBUG: Print loss\n",
    "        print(\"Loss: \", loss.item())\n",
    "\n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"NaN loss, stopping training\")\n",
    "            break\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Check for NaN in gradients during training\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:  # Gradients will be None for non-leaf tensors\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"NaN found in gradient of {name}, stopping training\")\n",
    "                    break\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"Stopping training due to NaN loss\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6a63709-695c-421c-b407-c7f65f23e791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenate all the mini-batch outputs (assuming `outputs` is a list of mini-batches)\n",
    "predicted_outputs_concatenated = torch.cat(tuple(outputs), dim=0)\n",
    "\n",
    "# Pad with zeros\n",
    "missing_rows = test_data_Y.shape[0] - predicted_outputs_concatenated.shape[0]\n",
    "padding = torch.zeros(missing_rows, predicted_outputs_concatenated.shape[1])\n",
    "predicted_outputs_padded = torch.cat([predicted_outputs_concatenated, padding], dim=0)\n",
    "\n",
    "# Convert them to numpy arrays if they are tensors\n",
    "predicted_outputs_np = predicted_outputs_padded.cpu().detach().numpy()\n",
    "target_values_np = test_data_Y.cpu().detach().numpy()\n",
    "\n",
    "# Reshape to 2D arrays for inverse transform\n",
    "predicted_outputs_2D = predicted_outputs_np.reshape(-1, predicted_outputs_np.shape[-1])\n",
    "target_values_2D = target_values_np.reshape(-1, target_values_np.shape[-1])\n",
    "\n",
    "# Inverse transform to original scale\n",
    "predicted_outputs_original_scale_2D = scaler.inverse_transform(predicted_outputs_2D)\n",
    "target_values_original_scale_2D = scaler.inverse_transform(target_values_2D)\n",
    "\n",
    "# Reshape back to original shape\n",
    "predicted_outputs_original_scale = predicted_outputs_original_scale_2D.reshape(predicted_outputs_np.shape)\n",
    "target_values_original_scale = target_values_original_scale_2D.reshape(target_values_np.shape)\n",
    "\n",
    "# Now, let's plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(predicted_outputs_original_scale[:60000, 0], label='Predicted')\n",
    "plt.plot(target_values_original_scale[:60000, 0], label='Target')\n",
    "plt.ylabel('EEG Scale')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66261b22-79de-45e1-b10d-6872084a1e2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two rows of original EEG data:\n",
      "tensor([[-0.6333, -1.2585, -1.6254,  0.2342,  0.0960, -0.0076, -0.4069, -0.8812,\n",
      "         -0.0866, -0.6010, -0.7219, -1.6760, -1.2949,  0.0681,  0.2029, -0.4570,\n",
      "         -1.6365, -1.4307,  2.2113, -0.3489,  1.6131,  1.5028, -1.7472, -1.0527,\n",
      "          0.7514, -0.9212,  1.2911, -1.5288,  1.8260,  3.2119, -0.0216, -0.8384],\n",
      "        [-0.6378, -1.2665, -1.6291,  0.2324,  0.0904, -0.0125, -0.4251, -0.8902,\n",
      "         -0.0889, -0.6050, -0.7361, -1.6834, -1.2985,  0.0652,  0.1991, -0.4614,\n",
      "         -1.6399, -1.4380,  2.2024, -0.3504,  1.6098,  1.4929, -1.7507, -1.0554,\n",
      "          0.7428, -0.9440,  1.2826, -1.5356,  1.8222,  3.2103, -0.0275, -0.8457]])\n",
      "\n",
      "First two rows of inverse-transformed predicted data:\n",
      "[[[        nan         nan         nan ...         nan         nan\n",
      "           nan]\n",
      "  [-0.09562747 -0.1384525  -0.33506355 ...  0.40375543 -0.3587941\n",
      "   -0.24791013]\n",
      "  [-0.4127862  -0.11322996 -0.80464566 ...  0.644756   -0.12835798\n",
      "    0.00629196]\n",
      "  ...\n",
      "  [-0.34353977 -0.20764333 -0.7282696  ...  0.557025   -0.30836773\n",
      "    0.06698374]\n",
      "  [-0.12594144 -0.12645718 -0.62413156 ...  0.5987508  -0.3510071\n",
      "   -0.25996566]\n",
      "  [-0.17960078 -0.0142175  -0.5799004  ...  0.47753373 -0.28881544\n",
      "   -0.09527469]]\n",
      "\n",
      " [[        nan         nan         nan ...         nan         nan\n",
      "           nan]\n",
      "  [-0.09139434  0.08723859  0.12364807 ...  0.10836092 -0.30664986\n",
      "    0.14273638]\n",
      "  [-0.1930972  -0.40865642 -0.27500212 ...  0.3059856   0.02709926\n",
      "    0.32110277]\n",
      "  ...\n",
      "  [-0.1556754  -0.04912969 -0.3823468  ...  0.06583875 -0.42819563\n",
      "    0.57747144]\n",
      "  [-0.29711294 -0.02729207 -0.36735037 ...  0.06568946 -0.43212202\n",
      "    0.35602793]\n",
      "  [-0.00192267 -0.1414807  -0.39361307 ...  0.0860796  -0.38494217\n",
      "    0.3717569 ]]]\n",
      "Shape of predicted_outputs_original_scale: (63, 1000, 32)\n",
      "Are there nans in the predicted outputs? tensor(True)\n",
      "Are there nans in the predicted_outputs_np? True\n",
      "Are there nans in predicted_outputs_original_scale? True\n",
      "Scaling test:  [[0.97024208 0.96439999 0.61837691 0.44264838 0.07532659 0.41907924\n",
      "  0.56987246 0.98265226 0.26013541 0.0947447  0.49379462 0.27397551\n",
      "  0.5291451  0.5682029  0.10195496 0.23405208 0.40801076 0.92224703\n",
      "  0.17851707 0.79198811 0.73234521 0.40877303 0.65160073 0.83256258\n",
      "  0.23087517 0.41223267 0.75714361 0.2130174  0.78350551 0.85150943\n",
      "  0.54325394 0.88307417]\n",
      " [0.39106023 0.07748395 0.38723298 0.21844822 0.94397401 0.78766793\n",
      "  0.78348537 0.74965844 0.59420808 0.36641357 0.1535927  0.9386752\n",
      "  0.54219917 0.74207376 0.95293353 0.66777631 0.58624023 0.03568922\n",
      "  0.06988613 0.35781705 0.29243526 0.03768116 0.52303149 0.49006877\n",
      "  0.02478367 0.4799437  0.14594239 0.33877526 0.28245962 0.23888316\n",
      "  0.06475083 0.54303735]]\n",
      "Are there nans in the original data? tensor(0, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# Using some of the original data for testing\n",
    "test_data = eeg_data[:10]\n",
    "test_data_standardized = scaler.transform(test_data)\n",
    "test_data_inverse = scaler.inverse_transform(test_data_standardized)\n",
    "\n",
    "# Convert to NumPy arrays if they are tensors\n",
    "predicted_outputs_np = predicted_outputs.cpu().detach().numpy()\n",
    "\n",
    "# Reshape the arrays to 2D if needed for inverse transform\n",
    "predicted_outputs_2D = predicted_outputs_np.reshape(-1, predicted_outputs_np.shape[-1])\n",
    "\n",
    "# Use the same `scaler` object to inverse transform the predicted output\n",
    "predicted_outputs_original_scale_2D = scaler.inverse_transform(predicted_outputs_2D)\n",
    "\n",
    "# Reshape back to the original shape if needed\n",
    "predicted_outputs_original_scale = predicted_outputs_original_scale_2D.reshape(predicted_outputs_np.shape)\n",
    "# Display the first two rows of the original EEG data\n",
    "print(\"First two rows of original EEG data:\")\n",
    "print(eeg_data[:2, :])\n",
    "\n",
    "# Display the first two rows of the inverse-transformed predicted data\n",
    "print(\"\\nFirst two rows of inverse-transformed predicted data:\")\n",
    "print(predicted_outputs_original_scale[:2, :])\\\n",
    "\n",
    "print(\"Shape of predicted_outputs_original_scale:\", predicted_outputs_original_scale.shape)\n",
    "print(\"Are there nans in the predicted outputs?\", torch.isnan(predicted_outputs).any())\n",
    "print(\"Are there nans in the predicted_outputs_np?\", np.isnan(predicted_outputs_np).any())\n",
    "print(\"Are there nans in predicted_outputs_original_scale?\", np.isnan(predicted_outputs_original_scale).any())\n",
    "# Create a mockup 'eeg_data' subset with 32 features but only 2 samples\n",
    "test_scale = np.random.rand(2, 32)\n",
    "\n",
    "# Test the scaling and inverse transformation\n",
    "print(\"Scaling test: \", scaler.inverse_transform(scaler.transform(test_scale)))\n",
    "\n",
    "print(\"Are there nans in the original data?\", np.isnan(eeg_data).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3bff38ca-ccd3-4149-bb6c-43064e46bb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `eeg_data` and `predicted_outputs_original_scale` are numpy arrays\n",
    "# Also, we'll plot only the first channel (column) for demonstration.\n",
    "\n",
    "# Configure Matplotlib to have a black background\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "plt.rcParams['text.color'] = 'white'\n",
    "plt.rcParams['axes.labelcolor'] = 'white'\n",
    "plt.rcParams['xtick.color'] = 'white'\n",
    "plt.rcParams['ytick.color'] = 'white'\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "# Plot first 10,000 datapoints of original EEG data for the first channel\n",
    "plt.plot(eeg_data[:10000, 0], label='Target (Original EEG Data) - First Channel', color='orange')\n",
    "\n",
    "# Plot first 10,000 datapoints of predicted outputs for the first channel\n",
    "plt.plot(predicted_outputs_original_scale[:10000, 0], label='Predicted Outputs - First Channel', color='red')\n",
    "\n",
    "# Add axis labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('EEG Scale')\n",
    "plt.title('EEG Data and Predicted Outputs')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa22f0b5-7895-4b76-b1f6-c72a8ba01774",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outputs_original_scale_2D = scaler.inverse_transform(predicted_outputs_2D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94b84c82-8567-4c13-8584-4ffd2e1c2f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Outputs Before Inverse Transform: True False\n",
      "Predicted Outputs After Inverse Transform: True False\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Outputs Before Inverse Transform:\", np.isnan(predicted_outputs_2D).any(), np.isinf(predicted_outputs_2D).any())\n",
    "print(\"Predicted Outputs After Inverse Transform:\", np.isnan(predicted_outputs_original_scale_2D).any(), np.isinf(predicted_outputs_original_scale_2D).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706f5f7-3d79-4084-87d3-d19e4741699c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47206f6e-2b6b-4b4d-8982-af47ce434141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b56a01db-50e6-4972-8d49-bd7b76889e65",
   "metadata": {},
   "source": [
    "# test validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df44bfd-90e3-4b95-afc7-8e79f1c5b385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# Define your actual dimensions here \n",
    "seq_len = 1000  # Sequence length\n",
    "batch_size = 64 # Batch size\n",
    "time_aligned_feature_dim = eeg_data.shape[-1]  # Last dimension of your EEG tensor\n",
    "hidden_size = 128  # You can decide the hidden size\n",
    "global_feature_dim = 112  # from all_features\n",
    "transformer_feature_dim = 64  # from transformer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b0d1ce6-3389-44c0-bc8b-073441e102ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EEGPredictor:\n\tsize mismatch for transformer_blocks.0.transformer_layer.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([32, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Load the saved model state dictionary\u001b[39;00m\n\u001b[1;32m     56\u001b[0m saved_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/finalmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model in evaluation mode\u001b[39;00m\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# Set the model in evaluation mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EEGPredictor:\n\tsize mismatch for transformer_blocks.0.transformer_layer.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_layer.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.0.transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_layer.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.1.transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_layer.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([1056, 352]) from checkpoint, the shape in current model is torch.Size([96, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([1056]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([352, 352]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 352]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([352, 512]) from checkpoint, the shape in current model is torch.Size([32, 512]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for transformer_blocks.2.transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([352]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([32, 352]) from checkpoint, the shape in current model is torch.Size([32, 32])."
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, \n",
    "            nhead, \n",
    "            dim_feedforward\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "class EEGPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(EEGPredictor, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, nhead, dim_feedforward) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, 32)  # Output dimension is 32, for 32 EEG channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.transformer_blocks[i](x)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "d_model = 352  # Replace with your actual value\n",
    "nhead = 4  # Replace with your actual value\n",
    "num_layers = 3  # Replace with your actual value\n",
    "dim_feedforward = 512  # Replace with your actual value\n",
    "\n",
    "# Define the model architecture (must be same as the architecture of the saved model)\n",
    "model = EEGPredictor(d_model, nhead, num_layers, dim_feedforward)\n",
    "model.to(device)\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "saved_model_path = \"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/finalmodel.pth\"\n",
    "model.load_state_dict(torch.load(saved_model_path, map_location=device))\n",
    "model.eval()  # Set the model in evaluation mode\n",
    "  # Set the model in evaluation mode\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Validation Loop\n",
    "total_val_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, labels in val_loader:\n",
    "        batch_data, labels = batch_data.to(device), labels.to(device)\n",
    "        outputs = model(batch_data)\n",
    "        loss = nn.MSELoss()(outputs, labels)\n",
    "        \n",
    "        total_val_loss += loss.item() * batch_data.size(0)\n",
    "\n",
    "print(f'Validation loss: {total_val_loss / len(val_loader.dataset):.4f}')\n",
    "\n",
    "# Testing Loop\n",
    "total_test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, labels in test_loader:\n",
    "        batch_data, labels = batch_data.to(device), labels.to(device)\n",
    "        outputs = model(batch_data)\n",
    "        loss = nn.MSELoss()(outputs, labels)\n",
    "        \n",
    "        total_test_loss += loss.item() * batch_data.size(0)\n",
    "\n",
    "print(f'Test loss: {total_test_loss / len(test_loader.dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11c2d73-5e5e-473a-8af7-f2ab810e56fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state_dict:  odict_keys(['transformer_blocks.0.transformer_layer.self_attn.in_proj_weight', 'transformer_blocks.0.transformer_layer.self_attn.in_proj_bias', 'transformer_blocks.0.transformer_layer.self_attn.out_proj.weight', 'transformer_blocks.0.transformer_layer.self_attn.out_proj.bias', 'transformer_blocks.0.transformer_layer.linear1.weight', 'transformer_blocks.0.transformer_layer.linear1.bias', 'transformer_blocks.0.transformer_layer.linear2.weight', 'transformer_blocks.0.transformer_layer.linear2.bias', 'transformer_blocks.0.transformer_layer.norm1.weight', 'transformer_blocks.0.transformer_layer.norm1.bias', 'transformer_blocks.0.transformer_layer.norm2.weight', 'transformer_blocks.0.transformer_layer.norm2.bias', 'transformer_blocks.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_blocks.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_blocks.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_blocks.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_blocks.0.transformer_encoder.layers.0.linear1.weight', 'transformer_blocks.0.transformer_encoder.layers.0.linear1.bias', 'transformer_blocks.0.transformer_encoder.layers.0.linear2.weight', 'transformer_blocks.0.transformer_encoder.layers.0.linear2.bias', 'transformer_blocks.0.transformer_encoder.layers.0.norm1.weight', 'transformer_blocks.0.transformer_encoder.layers.0.norm1.bias', 'transformer_blocks.0.transformer_encoder.layers.0.norm2.weight', 'transformer_blocks.0.transformer_encoder.layers.0.norm2.bias', 'transformer_blocks.1.transformer_layer.self_attn.in_proj_weight', 'transformer_blocks.1.transformer_layer.self_attn.in_proj_bias', 'transformer_blocks.1.transformer_layer.self_attn.out_proj.weight', 'transformer_blocks.1.transformer_layer.self_attn.out_proj.bias', 'transformer_blocks.1.transformer_layer.linear1.weight', 'transformer_blocks.1.transformer_layer.linear1.bias', 'transformer_blocks.1.transformer_layer.linear2.weight', 'transformer_blocks.1.transformer_layer.linear2.bias', 'transformer_blocks.1.transformer_layer.norm1.weight', 'transformer_blocks.1.transformer_layer.norm1.bias', 'transformer_blocks.1.transformer_layer.norm2.weight', 'transformer_blocks.1.transformer_layer.norm2.bias', 'transformer_blocks.1.transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_blocks.1.transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_blocks.1.transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_blocks.1.transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_blocks.1.transformer_encoder.layers.0.linear1.weight', 'transformer_blocks.1.transformer_encoder.layers.0.linear1.bias', 'transformer_blocks.1.transformer_encoder.layers.0.linear2.weight', 'transformer_blocks.1.transformer_encoder.layers.0.linear2.bias', 'transformer_blocks.1.transformer_encoder.layers.0.norm1.weight', 'transformer_blocks.1.transformer_encoder.layers.0.norm1.bias', 'transformer_blocks.1.transformer_encoder.layers.0.norm2.weight', 'transformer_blocks.1.transformer_encoder.layers.0.norm2.bias', 'transformer_blocks.2.transformer_layer.self_attn.in_proj_weight', 'transformer_blocks.2.transformer_layer.self_attn.in_proj_bias', 'transformer_blocks.2.transformer_layer.self_attn.out_proj.weight', 'transformer_blocks.2.transformer_layer.self_attn.out_proj.bias', 'transformer_blocks.2.transformer_layer.linear1.weight', 'transformer_blocks.2.transformer_layer.linear1.bias', 'transformer_blocks.2.transformer_layer.linear2.weight', 'transformer_blocks.2.transformer_layer.linear2.bias', 'transformer_blocks.2.transformer_layer.norm1.weight', 'transformer_blocks.2.transformer_layer.norm1.bias', 'transformer_blocks.2.transformer_layer.norm2.weight', 'transformer_blocks.2.transformer_layer.norm2.bias', 'transformer_blocks.2.transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_blocks.2.transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_blocks.2.transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_blocks.2.transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_blocks.2.transformer_encoder.layers.0.linear1.weight', 'transformer_blocks.2.transformer_encoder.layers.0.linear1.bias', 'transformer_blocks.2.transformer_encoder.layers.0.linear2.weight', 'transformer_blocks.2.transformer_encoder.layers.0.linear2.bias', 'transformer_blocks.2.transformer_encoder.layers.0.norm1.weight', 'transformer_blocks.2.transformer_encoder.layers.0.norm1.bias', 'transformer_blocks.2.transformer_encoder.layers.0.norm2.weight', 'transformer_blocks.2.transformer_encoder.layers.0.norm2.bias', 'fc.weight', 'fc.bias'])\n",
      "Model state_dict:  odict_keys(['transformer_blocks.0.transformer_layer.self_attn.in_proj_weight', 'transformer_blocks.0.transformer_layer.self_attn.in_proj_bias', 'transformer_blocks.0.transformer_layer.self_attn.out_proj.weight', 'transformer_blocks.0.transformer_layer.self_attn.out_proj.bias', 'transformer_blocks.0.transformer_layer.linear1.weight', 'transformer_blocks.0.transformer_layer.linear1.bias', 'transformer_blocks.0.transformer_layer.linear2.weight', 'transformer_blocks.0.transformer_layer.linear2.bias', 'transformer_blocks.0.transformer_layer.norm1.weight', 'transformer_blocks.0.transformer_layer.norm1.bias', 'transformer_blocks.0.transformer_layer.norm2.weight', 'transformer_blocks.0.transformer_layer.norm2.bias', 'transformer_blocks.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_blocks.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_blocks.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_blocks.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_blocks.0.transformer_encoder.layers.0.linear1.weight', 'transformer_blocks.0.transformer_encoder.layers.0.linear1.bias', 'transformer_blocks.0.transformer_encoder.layers.0.linear2.weight', 'transformer_blocks.0.transformer_encoder.layers.0.linear2.bias', 'transformer_blocks.0.transformer_encoder.layers.0.norm1.weight', 'transformer_blocks.0.transformer_encoder.layers.0.norm1.bias', 'transformer_blocks.0.transformer_encoder.layers.0.norm2.weight', 'transformer_blocks.0.transformer_encoder.layers.0.norm2.bias', 'transformer_blocks.1.transformer_layer.self_attn.in_proj_weight', 'transformer_blocks.1.transformer_layer.self_attn.in_proj_bias', 'transformer_blocks.1.transformer_layer.self_attn.out_proj.weight', 'transformer_blocks.1.transformer_layer.self_attn.out_proj.bias', 'transformer_blocks.1.transformer_layer.linear1.weight', 'transformer_blocks.1.transformer_layer.linear1.bias', 'transformer_blocks.1.transformer_layer.linear2.weight', 'transformer_blocks.1.transformer_layer.linear2.bias', 'transformer_blocks.1.transformer_layer.norm1.weight', 'transformer_blocks.1.transformer_layer.norm1.bias', 'transformer_blocks.1.transformer_layer.norm2.weight', 'transformer_blocks.1.transformer_layer.norm2.bias', 'transformer_blocks.1.transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_blocks.1.transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_blocks.1.transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_blocks.1.transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_blocks.1.transformer_encoder.layers.0.linear1.weight', 'transformer_blocks.1.transformer_encoder.layers.0.linear1.bias', 'transformer_blocks.1.transformer_encoder.layers.0.linear2.weight', 'transformer_blocks.1.transformer_encoder.layers.0.linear2.bias', 'transformer_blocks.1.transformer_encoder.layers.0.norm1.weight', 'transformer_blocks.1.transformer_encoder.layers.0.norm1.bias', 'transformer_blocks.1.transformer_encoder.layers.0.norm2.weight', 'transformer_blocks.1.transformer_encoder.layers.0.norm2.bias', 'transformer_blocks.2.transformer_layer.self_attn.in_proj_weight', 'transformer_blocks.2.transformer_layer.self_attn.in_proj_bias', 'transformer_blocks.2.transformer_layer.self_attn.out_proj.weight', 'transformer_blocks.2.transformer_layer.self_attn.out_proj.bias', 'transformer_blocks.2.transformer_layer.linear1.weight', 'transformer_blocks.2.transformer_layer.linear1.bias', 'transformer_blocks.2.transformer_layer.linear2.weight', 'transformer_blocks.2.transformer_layer.linear2.bias', 'transformer_blocks.2.transformer_layer.norm1.weight', 'transformer_blocks.2.transformer_layer.norm1.bias', 'transformer_blocks.2.transformer_layer.norm2.weight', 'transformer_blocks.2.transformer_layer.norm2.bias', 'transformer_blocks.2.transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_blocks.2.transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_blocks.2.transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_blocks.2.transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_blocks.2.transformer_encoder.layers.0.linear1.weight', 'transformer_blocks.2.transformer_encoder.layers.0.linear1.bias', 'transformer_blocks.2.transformer_encoder.layers.0.linear2.weight', 'transformer_blocks.2.transformer_encoder.layers.0.linear2.bias', 'transformer_blocks.2.transformer_encoder.layers.0.norm1.weight', 'transformer_blocks.2.transformer_encoder.layers.0.norm1.bias', 'transformer_blocks.2.transformer_encoder.layers.0.norm2.weight', 'transformer_blocks.2.transformer_encoder.layers.0.norm2.bias', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "saved_state_dict = torch.load(\"/home/vincent/AAA_projects/MVCS/Neuroscience/Models/Final Model/finalmodel.pth\")\n",
    "print(\"Saved state_dict: \", saved_state_dict.keys())\n",
    "model_state_dict = model.state_dict()\n",
    "print(\"Model state_dict: \", model_state_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308a730-b820-4bdf-bada-51be5fadfd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your saved test dataset\n",
    "test_data = torch.load(test_dataset_path)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Final Evaluation on Test Data\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for time_data, global_data, trans_data, labels in test_loader:\n",
    "        outputs = model(time_data, global_data, trans_data)\n",
    "        test_loss += nn.MSELoss()(outputs, labels).item()\n",
    "        \n",
    "print(f'Final Test loss: {test_loss / len(test_loader)}')\n",
    "\n",
    "# If you want to save the model\n",
    "torch.save(model.state_dict(), '/path/to/save/final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808a72e-0026-4437-919d-089d256e655d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
